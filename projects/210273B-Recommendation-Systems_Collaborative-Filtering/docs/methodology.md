# Methodology: Recommendation Systems:Collaborative Filtering

**Student:** 210273B
**Research Area:** Recommendation Systems:Collaborative Filtering
**Date:** 2025-09-01

# Methodology: Recommendation Systems: Collaborative Filtering

## 1. Overview

This methodology outlines the steps to enhance the Neural Collaborative Filtering (NCF) model using Self-Supervised Learning (SSL), specifically through a contrastive learning task. The core idea is to introduce an auxiliary objective that regularizes the learned user and item embeddings, making them more robust and discriminative, especially in sparse data environments. We will adapt the foundational NCF architecture by adding a multi-task learning objective that combines the original recommendation loss with a novel contrastive loss derived from augmented views of embeddings. The primary goal is to demonstrate empirically that this approach leads to improved recommendation performance compared to the baseline NCF model.

## 2. Research Design

The research will follow an empirical comparative design. We will implement both the baseline NCF model and our proposed NCF-SSL model. Both models will be trained and evaluated on publicly available benchmark datasets for implicit feedback recommendation. The performance will be assessed using standard top-K ranking metrics. A key aspect of the research design is to conduct a thorough analysis of the learned embeddings, beyond just performance metrics, to understand the regularization effects of the self-supervised task. Ablation studies will be conducted to analyze the contribution of the contrastive loss component.

## 3. Data Collection

### 3.1 Data Sources
The primary data sources will be the publicly available **MovieLens 1M** and **Pinterest-20** datasets. MovieLens 1M is a standard benchmark in recommendation system research, enabling direct comparison with prior work. Pinterest-20 provides a large-scale, real-world implicit feedback dataset, allowing us to evaluate model scalability and generalization across different domains.

### 3.2 Data Description
The **MovieLens 1M** dataset contains 1,000,209 anonymous ratings from 6,040 users on 3,952 movies. Ratings are on a 5-star scale, with associated timestamps. For this research, we will convert these explicit ratings into implicit feedback: any rating of 1 star or above is considered a positive interaction. The dataset is relatively dense compared to industrial datasets but serves as an excellent benchmark for initial validation.

The **Pinterest-20** dataset consists of user interactions with image pins on the Pinterest platform. It contains approximately 20 million implicit feedback records from over 55,000 users and 1.1 million unique pins. Each interaction represents a user saving a pin, which is treated as a positive implicit feedback event. The dataset is highly sparse and reflects real-world recommendation challenges, such as cold-start users and items. No explicit ratings are provided; only binary interactions (save or no save) are available. This dataset enables evaluation of model scalability and generalization in large, diverse environments.

### 3.3 Data Preprocessing
The following preprocessing steps will be applied:
* **Implicit Feedback Conversion:** All explicit ratings (1-5 stars) will be converted to implicit feedback (1 for interacted, 0 for non-interacted).
* **Negative Sampling:** For each positive user-item interaction, we will sample 49 negative (uninteracted) items to form a balanced training set, as is common practice in NCF research [1].
* **Data Splitting:** A leave-one-out cross-validation strategy will be employed. For each user, their last interacted item will be held out as the test item. The remaining interactions, along with the sampled negatives, will form the training and validation sets. A subset of the training data (e.g., 10\%) will be used for validation to tune hyperparameters.
* **User/Item ID Mapping:** User and item IDs will be mapped to contiguous integer indices for embedding layer lookup.

## 4. Model Architecture

The proposed model, **NCF-SSL**, extends the Neural Collaborative Filtering (NCF) framework. It consists of:
* **Embedding Layer:** User and item IDs are mapped to dense, low-dimensional embedding vectors ($\mathbf{p}_u, \mathbf{q}_i$).
* **Dual NCF Towers:**
    * For each user-item pair, two augmented views of their embeddings ($\mathbf{p}_u^{(1)}, \mathbf{q}_i^{(1)}$ and $\mathbf{p}_u^{(2)}, \mathbf{q}_i^{(2)}$) are generated by applying embedding dropout.
    * These two views are fed into two parallel, identical NCF model structures (each comprising a GMF path and an MLP path).
    * **GMF Path:** Element-wise product of augmented MF user and item embeddings, followed by a linear layer.
    * **MLP Path:** Concatenation of augmented MLP user and item embeddings, fed through a multi-layer perceptron (e.g., [64, 32, 16] dimensions with ReLU activations).
    * **NeuMF Layer:** The outputs of the GMF and MLP paths from each tower are concatenated and fed into a final linear output layer with a sigmoid activation function, yielding predicted interaction scores ($\hat{y}_{ui}^{(1)}, \hat{y}_{ui}^{(2)}$).
* **Multi-Task Loss Function:**
    * **Recommendation Loss ($\mathcal{L}_{NCF}$):** Binary Cross-Entropy (BCE) loss computed on the predicted scores from both NCF towers, averaged.
    * **Self-Supervised Contrastive Loss ($\mathcal{L}_{SSL}$):** InfoNCE loss applied to the augmented user embeddings ($\mathbf{p}_u^{(1)}, \mathbf{p}_u^{(2)}$) and item embeddings ($\mathbf{q}_i^{(1)}, \mathbf{q}_i^{(2)}$). This loss maximizes agreement between positive pairs (views of the same entity) and minimizes agreement with negative pairs (views of different entities within the batch).
    * **Total Loss ($\mathcal{L}$):** A weighted sum of $\mathcal{L}_{NCF}$ and $\mathcal{L}_{SSL}$, where $\lambda$ is a hyperparameter for balancing the two objectives.

## 5. Experimental Setup

### 5.1 Evaluation Metrics
The models will be evaluated using standard top-K ranking metrics, which are suitable for implicit feedback recommendations:
* **Hit Ratio at 10 (HR@10):** The proportion of users for whom the ground-truth item is among the top 10 recommended items.
* **Normalized Discounted Cumulative Gain at 10 (NDCG@10):** A ranking-aware metric that penalizes relevant items appearing lower in the recommendation list.

### 5.2 Baseline Models
The primary baseline for comparison will be the original **Neural Collaborative Filtering (NCF)** model [1], specifically its NeuMF variant, which is the best-performing configuration from the original paper. This ensures a fair comparison as our proposed NCF-SSL is an enhancement to this exact architecture.

### 5.3 Hardware/Software Requirements
* **Hardware:** A consumer-grade GPU with at least 8GB VRAM (e.g., NVIDIA RTX 3060/3070/3080 or similar) is recommended for efficient training, especially when using moderate batch sizes for contrastive learning. For larger experiments, higher-end GPUs (e.g., RTX 4090, V100, A100) may be beneficial but are not strictly required. A multi-core CPU and at least 16GB RAM are suggested for data preprocessing and general operation.
* **Software:** Python 3.8+ with key libraries including:
    * PyTorch / TensorFlow (for deep learning framework)
    * NumPy (for numerical operations)
    * Pandas (for data manipulation)
    * Scikit-learn (for utilities like train-test split, if not manual)
    * Matplotlib / Seaborn (for visualization of results and embeddings)

## 6. Implementation Plan

| Phase | Tasks | Duration | Deliverables |
|-------|-------|----------|--------------|
| Phase 1 | **Data Preprocessing & Baseline Setup** | 2 weeks | Clean, processed dataset; executable NCF baseline model |
|         | - Download and clean MovieLens 1M and Pinterest 20 |          | |
|         | - Implement negative sampling strategy |          | |
|         | - Prepare training, validation, and test splits |          | |
|         | - Implement the standard NCF (NeuMF) baseline model |          | |
| Phase 2 | **NCF-SSL Model Implementation** | 3 weeks | Working NCF-SSL model; initial training runs |
|         | - Integrate embedding dropout for augmentation |          | |
|         | - Implement the InfoNCE contrastive loss |          | |
|         | - Combine NCF and SSL losses into a multi-task objective |          | |
|         | - Conduct initial sanity checks on loss values and embeddings |          | |
| Phase 3 | **Experimentation & Hyperparameter Tuning** | 2 weeks | Performance metrics for NCF and NCF-SSL; tuned hyperparameters |
|         | - Train NCF-SSL on MovieLens 1M |          | |
|         | - Tune $\lambda$, $\tau$, and dropout rate using validation set |          | |
|         | - Run full evaluation on test set for both NCF and NCF-SSL |          | |
|         | - Conduct initial ablation studies (e.g., remove SSL component) |          | |
| Phase 4 | **Analysis & Reporting** | 1 week | Comprehensive analysis report; final paper draft |
|         | - Analyze performance differences (HR@10, NDCG@10) |          | |
|         | - Analyze embedding space characteristics (e.g., norm std dev, t-SNE) |          | |
|         | - Write up results, discussion, and conclusion for the paper |          | |

## 7. Risk Analysis

* **Risk 1: Computational Resource Limitations:** Training deep models with contrastive loss (especially with larger batch sizes) can be memory and time-intensive.
    * **Mitigation:** Start with smaller datasets (MovieLens 1M), optimize code for GPU usage, consider gradient accumulation if batch size is limited, and leverage cloud GPU resources if necessary.
* **Risk 2: Hyperparameter Sensitivity and Tuning Complexity:** NCF-SSL introduces new hyperparameters ($\lambda$, $\tau$, dropout rate) that can be difficult to tune optimally.
    * **Mitigation:** Adopt systematic hyperparameter search strategies (e.g., grid search, random search, or Bayesian optimization for key parameters) on a small validation set. Leverage insights from similar SSL papers.
* **Risk 3: Marginal Performance Improvements:** The improvements over the strong NCF baseline might be modest, potentially not justifying the added complexity.
    * **Mitigation:** Focus on detailed analysis of *why* improvements occur (e.g., embedding quality, robustness to sparsity), even if the metric gains are small. Clearly articulate limitations and identify promising future work directions. Document null results fairly.
* **Risk 4: Implementation Bugs:** Complex deep learning models are prone to subtle implementation errors.
    * **Mitigation:** Implement in a modular fashion, write unit tests for critical components (e.g., loss functions), cross-reference with existing open-source implementations where available, and perform sanity checks on loss values and predictions during training.

## 8. Expected Outcomes

* **Validated NCF-SSL Model:** A working implementation of NCF-SSL that demonstrates its feasibility and effectiveness.
* **Empirical Evidence:** Quantitative results showing the performance of NCF-SSL compared to the NCF baseline on benchmark datasets.
* **Insightful Analysis:** A deeper understanding of how self-supervised contrastive learning impacts the learned user and item embeddings, including observations on embedding space characteristics and robustness.
* **Contributions to Research:** A short research paper detailing the methodology, experimental setup, preliminary results, and analysis, contributing to the growing body of knowledge on deep learning for recommendation and self-supervised learning.
* **Foundation for Future Work:** Identification of promising directions for further research, such as more advanced augmentation techniques, integration with GNNs, and evaluation on larger-scale datasets.