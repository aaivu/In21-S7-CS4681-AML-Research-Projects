## II. RELATED WORK
The challenge of deploying large PLMs has spurred significant research in model compression. We categorize relevant work into knowledge distillation, pruning, quantization, and combined approaches.

### A. Knowledge Distillation (KD)
KD trains a compact student model using supervision from a larger teacher [4]. For transformers, various forms of supervision have been explored. Output-level KD matches the student’s output distribution (logits or softmax probabilities) to the teacher’s [3]. Feature-level KD minimizes the distance between intermediate hidden states or attention maps of the student and teacher [6], [12]. DistilBERT [3] effectively used a combination during pre-training. TinyBERT [6] applied multi-layer supervision during task-specific fine-tuning.

A distinct category is Relation-level KD, pioneered by MiniLM [7]. Instead of matching absolute feature values, it distills the relationships within the self-attention mechanism, specifically the scaled dot-product distributions between query, key, and value vectors. MiniLMv2 [10] extended this to multi-head self-attention relations (e.g., Q-Q, K-K, V-V similarity matrices), offering greater flexibility as it doesn’t require the student and teacher to have the same hidden dimension or head count. We adopt MiniLMv2 in Stage 1 due to this flexibility and its focus on capturing core self-attention behavior. Recent extensions include combining feature and relation KD [13] and distilling reasoning steps via preference matching [14].

### B. Pruning
Pruning removes less important parameters to reduce model size and computation. **Unstructured pruning** eliminates individual weights, leading to sparse models that often require specialized hardware or libraries for efficient inference. **Structured pruning** removes entire groups of parameters, such as attention heads [8], FFN neurons/layers, or embedding dimensions, resulting in smaller, dense models compatible with standard hardware. Common criteria for identifying prunable structures include parameter magnitude, gradient magnitude, or activation analysis. We employ magnitude-based structured pruning of attention heads in Stage 2, a well-established and effective technique.

### C. Quantization
Quantization reduces the numerical precision of model weights and, optionally, activations, typically from FP32 to INT8 [5] or even lower bit-widths (e.g., INT4). This drastically reduces the memory footprint (up to 4× for INT8) and can accelerate computation on hardware with native low-precision support. **Post-Training Quantization (PTQ)** applies quantization after training. **Dynamic PTQ** quantizes only weights offline, while activations are quantized/dequantized on-the-fly. **Static PTQ** uses a calibration dataset to determine activation statistics, allowing both weights and activations to be processed using integer arithmetic, potentially offering greater speedups but requiring calibration [9]. **Quantization-Aware Training (QAT)** simulates the quantization process during fine-tuning, inserting "fake quantization" nodes into the computation graph [15]. QAT typically achieves higher accuracy than PTQ, especially at very low bit-widths, but requires retraining. We use dynamic PTQ in Stage 3 for its implementation simplicity and no requirement for retraining or calibration data. Our specific application proves highly aggressive, also removing parameters implicitly, likely through FFN layer manipulation during the quantization process.

### D. Combined Approaches
Given that each technique targets different aspects of model redundancy, combining them holds promise for maximal compression. CompressBERT explored various combinations of KD, pruning, and quantization for BERT. CoFi jointly prunes layers, heads, and FFN dimensions. Works like [18] explored pruning followed by distillation. However, systematic studies detailing the stage-wise contribution, particularly including actual size/latency measurements on standard hardware like CPUs, are less common. EdgeMIN contributes by providing a clear sequential pipeline (KD → Head Pruning → Aggressive PTQ/FFN Pruning) with a detailed ablation study focused on practical efficiency metrics.