# ğŸ§  ResQEdge â€” Research Project (Project ID: SLM002)

**ResQEdge** is a research project focusing on efficient edge deployment of large language models for disaster response.  
This repository contains experiment scripts, datasets, and source code for model training, benchmarking, and evaluation.

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ data/                          # Datasets
â”‚   â””â”€â”€ disaster_corpus/
â”‚       â””â”€â”€ flood.json
â”‚
â”œâ”€â”€ docs/                          # Project documentation
â”‚   â”œâ”€â”€ literature_review.md
â”‚   â”œâ”€â”€ methodology.md
â”‚   â”œâ”€â”€ progress_reports/
â”‚   â”œâ”€â”€ research_proposal.md
â”‚   â””â”€â”€ usage_instructions.md
â”‚
â”œâ”€â”€ experiments/                   # Experiment scripts and configurations
â”‚   â”œâ”€â”€ baseline_benchmarking/
â”‚   â”‚   â”œâ”€â”€ benchmark.py
â”‚   â”‚   â”œâ”€â”€ benchmark_utils.py
â”‚   â”‚   â”œâ”€â”€ plotting_utils.py
â”‚   â”‚   â””â”€â”€ config.yaml
â”‚   â”œâ”€â”€ disaster_corpus_benchmarking/
â”‚   â”‚   â”œâ”€â”€ ptq_benchmark_disaster.py
â”‚   â”‚   â”œâ”€â”€ qat_train_disaster.py
â”‚   â”‚   â”œâ”€â”€ benchmark_utils_disaster.py
â”‚   â”‚   â””â”€â”€ qat_checkpoints/
â”‚   â”œâ”€â”€ ptq_benchmarking/
â”‚   â”‚   â””â”€â”€ benchmark.py
â”‚   â””â”€â”€ qat_benchmarking/
â”‚       â”œâ”€â”€ qat_train.py
â”‚       â””â”€â”€ qat_benchmark.py
â”‚
â”œâ”€â”€ results/                       # Experiment outputs and plots
â”‚
â”œâ”€â”€ src/                           # Model training and inference scripts
â”‚   â”œâ”€â”€ resqedge_train.py
â”‚   â”œâ”€â”€ resqedge_inference.py
â”‚   â””â”€â”€ model_checkpoints/
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Environment Setup

To ensure reproducibility, itâ€™s recommended to use a **Python virtual environment**.

### 1. Create and activate the virtual environment

#### ğŸªŸ Windows (PowerShell)
```bash
python -m venv .venv
.venv\Scripts\activate
```

#### ğŸ§ Linux / ğŸ macOS
```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

---

## ğŸ§ª Running Experiments

### 1. Baseline Benchmarking
Run:
```bash
python experiments/baseline_benchmarking/benchmark.py
```

- Configuration can be adjusted in `experiments/baseline_benchmarking/config.yaml`.  
- Results (metrics, plots, and CSV files) will be stored in:
  ```
  results/baseline_benchmarking_results/
  ```

---

### 2. PTQ (Post-Training Quantization) Benchmarking
Run:
```bash
python experiments/ptq_benchmarking/benchmark.py
```

Results will be saved in:
```
results/ptq_benchmarking_results/
```

---

### 3. QAT (Quantization-Aware Training) Benchmarking

#### ğŸ§© Step 1 â€” Train (if you donâ€™t have a checkpoint)
Before running the QAT benchmark, make sure you have trained the model.  
If you **donâ€™t** have an existing checkpoint in:
```
experiments/qat_benchmarking/qat_checkpoints/
```
run the following:
```bash
python experiments/qat_benchmarking/qat_train.py
```

This will create a new QAT-trained model checkpoint.

#### ğŸ§ª Step 2 â€” Run the benchmark
Once the training is complete (or if you already have a checkpoint), run:
```bash
python experiments/qat_benchmarking/qat_benchmark.py
```

Results will be saved in:
```
results/qat_benchmarking_results/
```

> âš ï¸ **Note:** Checkpoints are not pushed to Git.  
> You must train the model locally using `qat_train.py` before running `qat_benchmark.py` if no checkpoint exists.

---

### 4. Disaster Corpus Benchmarking

The `flood.json` dataset is located at:
```
data/disaster_corpus/flood.json
```

This dataset is used for both **disaster-specific QAT and PTQ** experiments and also for model training under `src/`.

To reproduce disaster corpus experiments:

#### (a) Run PTQ Benchmark
```bash
python experiments/disaster_corpus_benchmarking/ptq_benchmark_disaster.py
```

#### (b) Train and Run QAT Benchmark
If you donâ€™t have a QAT checkpoint:
```bash
python experiments/disaster_corpus_benchmarking/qat_train_disaster.py
```
Then benchmark:
```bash
python experiments/disaster_corpus_benchmarking/qat_train_disaster.py
```

Results will be stored in:
```
results/disaster_corpus_benchmarking_results/
```

---

## ğŸ§© Training and Inference (Source Code)

The core ResQEdge model is trained and evaluated from the `src/` directory.

### 1. Train the Model
```bash
python src/resqedge_train.py
```

### 2. Run Inference
```bash
python src/resqedge_inference.py
```

> âš ï¸ **Note:** Model checkpoints generated during training (in `src/model_checkpoints/`) are not committed to Git.  
> You must train the model locally before running inference or benchmarking.

---

## ğŸ“¦ Results and Outputs

Each experiment automatically creates its own results directory under `results/`.  
Typical contents include:
- `*.csv` files (quantitative results)
- `plots/` (visualizations)
- `.zip` archives (compressed experiment summaries)

Example:
```
results/baseline_benchmarking_results/
â”œâ”€â”€ baseline_results.csv
â”œâ”€â”€ plots/
â”‚   â”œâ”€â”€ bar_latency.png
â”‚   â”œâ”€â”€ scatter_Perplexity_vs_F1.png
â”‚   â””â”€â”€ ...
â””â”€â”€ results_YYYYMMDD_HHMMSS.zip
```

---

## ğŸ§­ Notes

- All experiment configurations (paths, models, datasets) are defined in their respective `config.yaml` or script headers.  
- Make sure your working directory is the **project root** when running scripts.
- Before running quantized experiments (PTQ/QAT), ensure the base models have been trained or are available locally.

---

## ğŸ Citation
If you use this repository or related findings in your research, please cite:

```
ResQEdge: Efficient Edge Deployment of Quantized Language Models for Disaster Response
Project ID: SLM002
University of Moratuwa
```
