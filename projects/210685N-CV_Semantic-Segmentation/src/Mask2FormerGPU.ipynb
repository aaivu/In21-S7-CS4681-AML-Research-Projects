{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Utils\n","metadata":{"id":"F3Rjk_xXGMUY"}},{"cell_type":"code","source":"import torch\nimport torch.distributed as dist\nfrom typing import List, Optional, Tuple\nfrom functools import partial\n\n\nclass NestedTensor:\n    \"\"\"\n    A nested tensor is a tensor along with a mask indicating which elements are valid.\n    This is useful for batching images of different sizes.\n    \"\"\"\n\n    def __init__(self, tensors: torch.Tensor, mask: Optional[torch.Tensor]):\n        self.tensors = tensors\n        self.mask = mask\n\n    def decompose(self) -> Tuple[torch.Tensor, torch.Tensor]:\n        return self.tensors, self.mask\n\n    @property\n    def device(self):\n        return self.tensors.device\n\n    def to(self, device):\n        return NestedTensor(self.tensors.to(device), self.mask.to(device) if self.mask is not None else None)\n\n\ndef nested_tensor_from_tensor_list(tensor_list: List[torch.Tensor]) -> NestedTensor:\n    \"\"\"\n    Create a nested tensor from a list of tensors.\n\n    Pads the tensors to the same size and creates a mask indicating valid regions.\n\n    Args:\n        tensor_list: List of tensors with shape [C, H_i, W_i] (for images) or [N_i, H_i, W_i] (for masks)\n\n    Returns:\n        NestedTensor containing:\n            - tensors: Padded tensors [B, C, H_max, W_max] or [B, N_max, H_max, W_max]\n            - mask: Binary mask [B, H_max, W_max] (False = valid, True = padding)\n    \"\"\"\n    if len(tensor_list) == 0:\n        # Handle empty list case\n        return NestedTensor(torch.empty(0), torch.empty(0))\n\n    # Determine the number of dimensions based on the first tensor\n    num_dims = tensor_list[0].ndim\n\n    # Calculate maximum dimensions\n    max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n\n    # Create output tensor and mask\n    batch_shape = [len(tensor_list)] + max_size\n\n    device = tensor_list[0].device\n    dtype = tensor_list[0].dtype\n\n    tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n\n    # Mask shape depends on whether it's images or masks\n    if num_dims == 3: # Images [C, H, W] -> mask [B, H, W]\n        mask = torch.ones((len(tensor_list), max_size[1], max_size[2]), dtype=torch.bool, device=device)\n        for i, img in enumerate(tensor_list):\n             tensor[i, :img.shape[0], :img.shape[1], :img.shape[2]].copy_(img)\n             mask[i, :img.shape[1], :img.shape[2]] = False\n    elif num_dims == 3: # Masks [N, H, W] -> mask [B, H, W] (mask should indicate spatial padding only)\n         mask = torch.ones((len(tensor_list), max_size[1], max_size[2]), dtype=torch.bool, device=device)\n         for i, img in enumerate(tensor_list):\n             # Pad masks along N dimension as well before copying\n             padded_img = torch.zeros(max_size, dtype=dtype, device=device)\n             padded_img[:img.shape[0], :img.shape[1], :img.shape[2]].copy_(img)\n             tensor[i].copy_(padded_img)\n             mask[i, :img.shape[1], :img.shape[2]] = False\n    else:\n        # Handle other cases if necessary\n        mask = None # Or raise an error\n        for i, img in enumerate(tensor_list):\n             tensor[i].copy_(img) # Assuming no padding needed for other dims\n\n    return NestedTensor(tensor, mask)\n\n\ndef _max_by_axis(the_list: List[List[int]]) -> List[int]:\n    \"\"\"Find maximum size along each axis.\"\"\"\n    if not the_list:\n        return []\n    maxes = list(the_list[0])\n    for sublist in the_list[1:]:\n        for index, item in enumerate(sublist):\n            maxes[index] = max(maxes[index], item)\n    return maxes\n\n\ndef is_dist_avail_and_initialized() -> bool:\n    \"\"\"Check if distributed training is available and initialized.\"\"\"\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\n\n\ndef get_world_size() -> int:\n    \"\"\"Get world size for distributed training.\"\"\"\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank() -> int:\n    \"\"\"Get rank for distributed training.\"\"\"\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_main_process() -> bool:\n    \"\"\"Check if current process is the main process.\"\"\"\n    return get_rank() == 0\n\n\ndef save_on_master(*args, **kwargs):\n    \"\"\"Save checkpoint only on the main process.\"\"\"\n    if is_main_process():\n        torch.save(*args, **kwargs)\n\n\nclass ImageList:\n    \"\"\"\n    Structure that holds a list of images (tensors) of possibly different sizes.\n\n    Similar to detectron2's ImageList but without the detectron2 dependency.\n    \"\"\"\n\n    def __init__(self, tensor: torch.Tensor, image_sizes: List[Tuple[int, int]]):\n        \"\"\"\n        Args:\n            tensor: Batched images tensor [B, C, H, W]\n            image_sizes: List of (height, width) tuples for each image\n        \"\"\"\n        self.tensor = tensor\n        self.image_sizes = image_sizes\n\n    @property\n    def device(self) -> torch.device:\n        return self.tensor.device\n\n    def __len__(self) -> int:\n        return len(self.image_sizes)\n\n    def __getitem__(self, idx: int) -> torch.Tensor:\n        \"\"\"Get image at index with its original size.\"\"\"\n        h, w = self.image_sizes[idx]\n        return self.tensor[idx, :, :h, :w]\n\n    @staticmethod\n    def from_tensors(\n        tensors: List[torch.Tensor],\n        size_divisibility: int = 0,\n        pad_value: float = 0.0\n    ) -> \"ImageList\":\n        \"\"\"\n        Create ImageList from list of image tensors.\n\n        Args:\n            tensors: List of image tensors\n            size_divisibility: If > 0, pad dimensions to be divisible by this\n            pad_value: Value to use for padding\n\n        Returns:\n            ImageList instance\n        \"\"\"\n        assert len(tensors) > 0\n        assert isinstance(tensors[0], torch.Tensor)\n\n        image_sizes = [(tensor.shape[-2], tensor.shape[-1]) for tensor in tensors]\n\n        # Calculate target size\n        max_size = list(max(zip(*[tensor.shape for tensor in tensors])))\n\n        if size_divisibility > 0:\n            # Pad to be divisible by size_divisibility\n            stride = size_divisibility\n            max_size[-2] = (max_size[-2] + stride - 1) // stride * stride\n            max_size[-1] = (max_size[-1] + stride - 1) // stride * stride\n\n        # Create batched tensor\n        batch_shape = [len(tensors)] + list(tensors[0].shape[:-2]) + list(max_size[-2:])\n        device = tensors[0].device\n        batched_tensor = torch.full(batch_shape, pad_value, dtype=tensors[0].dtype, device=device)\n\n        # Copy images\n        for i, tensor in enumerate(tensors):\n            batched_tensor[i, ..., :tensor.shape[-2], :tensor.shape[-1]].copy_(tensor)\n\n        return ImageList(batched_tensor, image_sizes)\n\n\ndef multi_apply(func, *args, **kwargs):\n    \"\"\"\n    Apply function to multiple inputs and concatenate outputs.\n\n    Useful for applying the same function to multiple levels of features.\n    \"\"\"\n    pfunc = partial(func, **kwargs) if kwargs else func\n    map_results = map(pfunc, *args)\n    return tuple(map(list, zip(*map_results)))\n\n\ndef inverse_sigmoid(x: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:\n    \"\"\"\n    Inverse of sigmoid function.\n\n    Args:\n        x: Input tensor (should be in range [0, 1])\n        eps: Small epsilon to avoid numerical issues\n\n    Returns:\n        Inverse sigmoid of x\n    \"\"\"\n    x = x.clamp(min=0, max=1)\n    x1 = x.clamp(min=eps)\n    x2 = (1 - x).clamp(min=eps)\n    return torch.log(x1 / x2)","metadata":{"id":"mtKJkSHjGJ2l","trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:33:34.121083Z","iopub.execute_input":"2025-09-21T16:33:34.121833Z","iopub.status.idle":"2025-09-21T16:33:34.141749Z","shell.execute_reply.started":"2025-09-21T16:33:34.121806Z","shell.execute_reply":"2025-09-21T16:33:34.141127Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# Losses","metadata":{"id":"kwyKaxYeGVzC"}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nfrom scipy.optimize import linear_sum_assignment\nfrom typing import Dict, List, Tuple\n\n\nclass HungarianMatcher(nn.Module):\n    \"\"\"\n    This class computes an assignment between the targets and the predictions.\n\n    For efficiency reasons, the targets don't include the no_object class. Because of this,\n    in general, there are more predictions than targets. We do a 1-to-1 matching,\n    and the remaining predictions are un-matched (and thus treated as background).\n    \"\"\"\n\n    def __init__(\n        self,\n        cost_class: float = 1.0,\n        cost_mask: float = 1.0,\n        cost_dice: float = 1.0,\n        num_points: int = 0,\n    ):\n        \"\"\"\n        Args:\n            cost_class: Weight of the classification cost\n            cost_mask: Weight of the sigmoid CE cost for masks\n            cost_dice: Weight of the dice cost for masks\n            num_points: Number of points to sample for matching\n        \"\"\"\n        super().__init__()\n        self.cost_class = cost_class\n        self.cost_mask = cost_mask\n        self.cost_dice = cost_dice\n        self.num_points = num_points\n\n    @torch.no_grad()\n    def forward(\n        self,\n        outputs: Dict[str, torch.Tensor],\n        targets: List[Dict[str, torch.Tensor]]\n    ) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n        \"\"\"\n        Performs the matching.\n\n        Args:\n            outputs: Model outputs containing:\n                - \"pred_logits\": [batch_size, num_queries, num_classes]\n                - \"pred_masks\": [batch_size, num_queries, H, W]\n\n            targets: List of target dictionaries (one per batch item) containing:\n                - \"labels\": [num_target_masks] containing class labels\n                - \"masks\": [num_target_masks, H, W] containing target masks\n\n        Returns:\n            List of tuples (index_i, index_j) where:\n                - index_i: indices of selected predictions (in order)\n                - index_j: indices of corresponding selected targets (in order)\n            For each batch element, it returns (index_i, index_j) such that:\n                - len(index_i) = len(index_j) = min(num_queries, num_target_masks)\n                - For all k: prediction index_i[k] is matched to target index_j[k]\n        \"\"\"\n        batch_size, num_queries = outputs[\"pred_logits\"].shape[:2]\n\n        # We flatten to compute the cost matrices in a batch\n        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n        out_mask = outputs[\"pred_masks\"].flatten(0, 1)  # [batch_size * num_queries, H, W]\n\n        # Concatenate target labels\n        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n\n        # Manually pad and concatenate target masks\n        target_masks_list = [v[\"masks\"] for v in targets]\n\n        # Find max dimensions across all masks in the batch\n        max_h = 0\n        max_w = 0\n        total_num_targets_in_batch = 0\n        for masks_per_image in target_masks_list:\n            total_num_targets_in_batch += masks_per_image.shape[0]\n            if masks_per_image.numel() > 0:\n                max_h = max(max_h, masks_per_image.shape[1])\n                max_w = max(max_w, masks_per_image.shape[2])\n\n        device = out_mask.device\n        if total_num_targets_in_batch > 0:\n            padded_masks = []\n            for masks_per_image in target_masks_list:\n                num_masks, h, w = masks_per_image.shape\n                # Pad each mask tensor to max_h, max_w\n                padding = (0, max_w - w, 0, max_h - h) # (pad_w_left, pad_w_right, pad_h_top, pad_h_bottom)\n                padded_mask = F.pad(masks_per_image, padding, \"constant\", 0)\n                padded_masks.append(padded_mask)\n\n            # Concatenate all padded masks from the batch\n            tgt_mask = torch.cat(padded_masks, dim=0) # [total_num_targets_in_batch, H_max, W_max]\n        else:\n             # Handle case where there are no masks in the entire batch\n             tgt_mask = torch.empty(0, max_h, max_w, dtype=torch.bool, device=device)\n\n        # Compute classification cost\n        # out_prob: [batch_size * num_queries, num_classes]\n        # tgt_ids: [total_num_targets_in_batch]\n        if total_num_targets_in_batch > 0:\n            cost_class = -out_prob[:, tgt_ids]\n        else:\n            # If no target masks, classification cost is zero with correct dimensions\n            cost_class = torch.zeros((batch_size * num_queries, 0), dtype=out_prob.dtype, device=device)\n\n\n        # Compute mask costs\n        if self.num_points > 0:\n            # Sample points for efficiency\n            if tgt_mask.numel() > 0:\n                out_mask_sampled = self._sample_points_for_matching(out_mask, self.num_points)\n                tgt_mask_sampled = self._sample_points_for_matching(tgt_mask, self.num_points)\n            else:\n                 # If no target masks, sampled masks are empty\n                 out_mask_sampled = torch.empty(out_mask.shape[0], self.num_points, dtype=out_mask.dtype, device=device)\n                 tgt_mask_sampled = torch.empty(0, self.num_points, dtype=tgt_mask.dtype, device=device)\n\n        else:\n            out_mask_sampled = out_mask.flatten(1)\n            tgt_mask_sampled = tgt_mask.flatten(1)\n\n        # Compute sigmoid cross-entropy cost\n        with torch.cuda.amp.autocast(enabled=False):\n            out_mask_sampled = out_mask_sampled.float()\n            tgt_mask_sampled = tgt_mask_sampled.float()\n\n            # Compute costs only if there are target masks\n            if tgt_mask_sampled.shape[0] > 0:\n                # Sigmoid CE cost\n                cost_mask = self._batch_sigmoid_ce_cost(out_mask_sampled, tgt_mask_sampled)\n\n                # Dice cost\n                cost_dice = self._batch_dice_cost(out_mask_sampled, tgt_mask_sampled)\n            else:\n                # If no target masks, costs are zero with correct dimensions\n                cost_mask = torch.zeros((out_mask_sampled.shape[0], 0), dtype=torch.float32, device=device)\n                cost_dice = torch.zeros((out_mask_sampled.shape[0], 0), dtype=torch.float32, device=device)\n\n\n        # Final cost matrix\n        # Need to ensure all cost matrices have the same second dimension (number of targets)\n        # cost_class should have shape [batch_size * num_queries, total_num_targets_in_batch]\n        # cost_mask should have shape [batch_size * num_queries, total_num_targets_in_batch]\n        # cost_dice should have shape [batch_size * num_queries, total_num_targets_in_batch]\n        # Based on the logic, this should be the case now.\n\n        C = self.cost_mask * cost_mask + self.cost_class * cost_class + self.cost_dice * cost_dice\n        C = C.reshape(batch_size, num_queries, -1)\n\n        # Perform Hungarian matching for each batch element\n        indices = []\n        sizes = [len(v[\"labels\"]) for v in targets]\n\n        for i, c in enumerate(C.cpu()):\n            if sizes[i] == 0:\n                # No targets in this batch element\n                indices.append((\n                    torch.empty(0, dtype=torch.long),\n                    torch.empty(0, dtype=torch.long)\n                ))\n            else:\n                # Extract cost matrix for this batch element\n                c_i = c[:, sum(sizes[:i]):sum(sizes[:i+1])]\n\n                # Solve assignment problem\n                row_ind, col_ind = linear_sum_assignment(c_i.numpy())\n\n                indices.append((\n                    torch.as_tensor(row_ind, dtype=torch.long),\n                    torch.as_tensor(col_ind, dtype=torch.long)\n                ))\n\n        return [(torch.as_tensor(i, dtype=torch.long), torch.as_tensor(j, dtype=torch.long))\n                for i, j in indices]\n\n    def _sample_points_for_matching(self, masks: torch.Tensor, num_points: int) -> torch.Tensor:\n        \"\"\"\n        Uniformly sample points from masks for matching.\n\n        Args:\n            masks: [N, H, W] tensor of masks\n            num_points: Number of points to sample\n\n        Returns:\n            [N, num_points] tensor of sampled mask values\n        \"\"\"\n        N, H, W = masks.shape\n\n        # Handle empty masks case\n        if N == 0 or H == 0 or W == 0:\n            return torch.empty(N, num_points, dtype=masks.dtype, device=masks.device)\n\n        # Generate random point coordinates\n        # Use same points for all masks to ensure fair comparison\n        with torch.no_grad():\n            points_idx = torch.randperm(H * W, device=masks.device)[:num_points]\n\n        # Sample mask values at these points\n        masks_flat = masks.flatten(1)  # [N, H*W]\n        masks_sampled = masks_flat[:, points_idx]  # [N, num_points]\n\n        return masks_sampled\n\n    def _batch_sigmoid_ce_cost(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute sigmoid cross-entropy cost between all pairs.\n\n        Args:\n            inputs: [N, P] tensor of predicted logits\n            targets: [M, P] tensor of target values (0 or 1)\n\n        Returns:\n            [N, M] cost matrix\n        \"\"\"\n        # Compute pairwise sigmoid CE\n        # Note: This is memory efficient for reasonable batch sizes\n        N, P = inputs.shape\n        M = targets.shape[0]\n\n        # Handle empty targets case\n        if M == 0:\n            return torch.zeros((N, 0), dtype=inputs.dtype, device=inputs.device)\n\n\n        # Expand dimensions for broadcasting\n        inputs_exp = inputs.unsqueeze(1)  # [N, 1, P]\n        targets_exp = targets.unsqueeze(0)  # [1, M, P]\n\n        # Compute sigmoid CE for all pairs\n        ce = F.binary_cross_entropy_with_logits(\n            inputs_exp.expand(N, M, P),\n            targets_exp.expand(N, M, P),\n            reduction='none'\n        )\n\n        return ce.mean(dim=2)  # [N, M]\n\n    def _batch_dice_cost(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute dice cost between all pairs.\n\n        Args:\n            inputs: [N, P] tensor of predicted logits\n            targets: [M, P] tensor of target values (0 or 1)\n\n        Returns:\n            [N, M] cost matrix\n        \"\"\"\n        N, P = inputs.shape\n        M = targets.shape[0]\n\n        # Handle empty targets case\n        if M == 0:\n             return torch.zeros((N, 0), dtype=inputs.dtype, device=inputs.device)\n\n\n        # Apply sigmoid to get probabilities\n        inputs = inputs.sigmoid()\n\n        # Expand dimensions for broadcasting\n        inputs_exp = inputs.unsqueeze(1)  # [N, 1, P]\n        targets_exp = targets.unsqueeze(0)  # [1, M, P]\n\n        # Compute dice coefficient for all pairs\n        numerator = 2 * (inputs_exp * targets_exp).sum(dim=2)  # [N, M]\n        denominator = inputs_exp.sum(dim=2) + targets_exp.sum(dim=2)  # [N, M]\n\n        # Dice cost (1 - dice coefficient)\n        dice_cost = 1 - (numerator + 1) / (denominator + 1)\n\n        return dice_cost  # [N, M]","metadata":{"id":"M0b_n7cYGRjh","trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:31:30.310362Z","iopub.execute_input":"2025-09-21T16:31:30.310604Z","iopub.status.idle":"2025-09-21T16:31:30.331744Z","shell.execute_reply.started":"2025-09-21T16:31:30.310587Z","shell.execute_reply":"2025-09-21T16:31:30.331026Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import numpy as np\n\n\ndef dice_loss(\n    inputs: torch.Tensor,\n    targets: torch.Tensor,\n    num_masks: float,\n) -> torch.Tensor:\n    \"\"\"\n    Compute the DICE loss, similar to generalized IOU for masks.\n\n    Args:\n        inputs: Predicted masks [N, H, W]\n        targets: Target masks [N, H, W]\n        num_masks: Number of masks to normalize the loss\n\n    Returns:\n        Normalized dice loss\n    \"\"\"\n    inputs = inputs.sigmoid()\n    inputs = inputs.flatten(1)\n    targets = targets.flatten(1)\n\n    numerator = 2 * (inputs * targets).sum(-1)\n    denominator = inputs.sum(-1) + targets.sum(-1)\n    loss = 1 - (numerator + 1) / (denominator + 1)\n\n    return loss.sum() / num_masks\n\n\ndef sigmoid_ce_loss(\n    inputs: torch.Tensor,\n    targets: torch.Tensor,\n    num_masks: float,\n) -> torch.Tensor:\n    \"\"\"\n    Sigmoid cross-entropy loss.\n\n    Args:\n        inputs: Predicted logits [N, H, W]\n        targets: Target masks [N, H, W]\n        num_masks: Number of masks to normalize the loss\n\n    Returns:\n        Normalized sigmoid CE loss\n    \"\"\"\n    loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n    return loss.mean(1).sum() / num_masks\n\n\nclass SetCriterion(nn.Module):\n    \"\"\"\n    This class computes the loss for Mask2Former.\n    The process happens in two steps:\n        1) We compute hungarian assignment between ground truth masks and predictions\n        2) We supervise each pair of matched ground-truth / prediction (supervise class and mask)\n    \"\"\"\n\n    def __init__(\n        self,\n        num_classes: int,\n        matcher: HungarianMatcher,\n        weight_dict: Dict[str, float],\n        eos_coef: float,\n        losses: List[str],\n        num_points: int = 12544,\n        oversample_ratio: float = 3.0,\n        importance_sample_ratio: float = 0.75,\n    ):\n        \"\"\"\n        Args:\n            num_classes: Number of object categories\n            matcher: Module to compute assignment between targets and predictions\n            weight_dict: Dict containing weights for different losses\n            eos_coef: Relative classification weight of the no-object category\n            losses: List of losses to be applied\n            num_points: Number of points sampled for point losses\n            oversample_ratio: Oversampling ratio for point sampling\n            importance_sample_ratio: Ratio of points sampled via importance sampling\n        \"\"\"\n        super().__init__()\n        self.num_classes = num_classes\n        self.matcher = matcher\n        self.weight_dict = weight_dict\n        self.eos_coef = eos_coef\n        self.losses = losses\n\n        # Point sampling parameters\n        self.num_points = num_points\n        self.oversample_ratio = oversample_ratio\n        self.importance_sample_ratio = importance_sample_ratio\n\n        # Create no-object class weight\n        empty_weight = torch.ones(self.num_classes + 1)\n        empty_weight[-1] = self.eos_coef\n        self.register_buffer(\"empty_weight\", empty_weight)\n\n    def loss_labels(\n        self,\n        outputs: Dict[str, torch.Tensor],\n        targets: List[Dict[str, torch.Tensor]],\n        indices: List[Tuple[torch.Tensor, torch.Tensor]],\n        num_masks: int,\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"Classification loss.\"\"\"\n        assert \"pred_logits\" in outputs\n        src_logits = outputs[\"pred_logits\"]\n\n        # Prepare target classes\n        idx = self._get_src_permutation_idx(indices)\n        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n        target_classes = torch.full(\n            src_logits.shape[:2],\n            self.num_classes,\n            dtype=torch.int64,\n            device=src_logits.device\n        )\n        target_classes[idx] = target_classes_o\n\n        # Compute cross-entropy loss\n        loss_ce = F.cross_entropy(\n            src_logits.transpose(1, 2),\n            target_classes,\n            self.empty_weight\n        )\n\n        losses = {\"loss_ce\": loss_ce}\n        return losses\n\n    def loss_masks(\n        self,\n        outputs: Dict[str, torch.Tensor],\n        targets: List[Dict[str, torch.Tensor]],\n        indices: List[Tuple[torch.Tensor, torch.Tensor]],\n        num_masks: int,\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"Compute mask losses (dice + sigmoid CE).\"\"\"\n        assert \"pred_masks\" in outputs\n\n        src_idx = self._get_src_permutation_idx(indices)\n        tgt_idx = self._get_tgt_permutation_idx(indices)\n\n        src_masks = outputs[\"pred_masks\"]\n        src_masks = src_masks[src_idx]\n\n        # Get target masks\n        target_masks = [t[\"masks\"] for t in targets]\n        target_masks, valid = nested_tensor_from_tensor_list(target_masks).decompose()\n        target_masks = target_masks.to(src_masks)\n        target_masks = target_masks[tgt_idx]\n\n        # Point sampling for efficient computation\n        with torch.no_grad():\n            point_coords = self._sample_points(\n                src_masks,\n                target_masks,\n                self.oversample_ratio,\n                self.importance_sample_ratio\n            )\n\n        # Sample points from masks\n        src_masks_sampled = self._point_sample(src_masks, point_coords, align_corners=False)\n        target_masks_sampled = self._point_sample(target_masks, point_coords, align_corners=False)\n\n        src_masks_sampled = src_masks_sampled.flatten(1)\n        target_masks_sampled = target_masks_sampled.flatten(1)\n\n        losses = {\n            \"loss_mask\": sigmoid_ce_loss(src_masks_sampled, target_masks_sampled, num_masks),\n            \"loss_dice\": dice_loss(src_masks_sampled, target_masks_sampled, num_masks),\n        }\n\n        return losses\n\n    def _get_src_permutation_idx(self, indices):\n        \"\"\"Get source permutation indices.\"\"\"\n        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n        src_idx = torch.cat([src for (src, _) in indices])\n        return batch_idx, src_idx\n\n    def _get_tgt_permutation_idx(self, indices):\n        \"\"\"Get target permutation indices.\"\"\"\n        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n        return batch_idx, tgt_idx\n\n    def _sample_points(\n        self,\n        src_masks: torch.Tensor,\n        tgt_masks: torch.Tensor,\n        oversample_ratio: float,\n        importance_sample_ratio: float,\n    ) -> torch.Tensor:\n        \"\"\"\n        Sample points for mask loss computation.\n\n        Combines uniform sampling with importance sampling based on uncertainty.\n        \"\"\"\n        N, H, W = src_masks.shape\n\n        num_points = int(self.num_points * oversample_ratio)\n        num_uncertain_points = int(importance_sample_ratio * num_points)\n        num_random_points = num_points - num_uncertain_points\n\n        # Uniform random sampling\n        point_coords = torch.rand(N, num_random_points, 2, device=src_masks.device)\n        point_coords = point_coords * torch.tensor([W, H], device=src_masks.device) - 0.5\n\n        # Importance sampling based on prediction uncertainty\n        if num_uncertain_points > 0:\n            point_logits = src_masks.flatten(1)\n            point_uncertainties = -torch.abs(point_logits)\n\n            # Sample points with high uncertainty\n            _, uncertain_idx = torch.topk(point_uncertainties, num_uncertain_points, dim=1)\n\n            # Convert flat indices to 2D coordinates\n            uncertain_coords = torch.stack([\n                uncertain_idx % W,\n                uncertain_idx // W\n            ], dim=-1).float()\n\n            # Add small random noise\n            uncertain_coords += torch.rand_like(uncertain_coords) - 0.5\n\n            # Combine uniform and importance samples\n            point_coords = torch.cat([point_coords, uncertain_coords], dim=1)\n\n        return point_coords\n\n    def _point_sample(\n        self,\n        input: torch.Tensor,\n        point_coords: torch.Tensor,\n        align_corners: bool = False,\n    ) -> torch.Tensor:\n        \"\"\"\n        Sample features at specified point coordinates.\n\n        Args:\n            input: Input tensor [N, H, W]\n            point_coords: Point coordinates [N, P, 2]\n            align_corners: Whether to align corners in grid_sample\n\n        Returns:\n            Sampled features [N, P, 1]\n        \"\"\"\n        N, H, W = input.shape\n\n        # Normalize coordinates to [-1, 1]\n        point_coords = point_coords.clone()\n        point_coords[..., 0] = 2.0 * point_coords[..., 0] / W - 1.0\n        point_coords[..., 1] = 2.0 * point_coords[..., 1] / H - 1.0\n\n        # Add batch dimension and sample\n        output = F.grid_sample(\n            input.unsqueeze(1),\n            point_coords.unsqueeze(1),\n            mode=\"bilinear\",\n            padding_mode=\"zeros\",\n            align_corners=align_corners,\n        )\n\n        return output.squeeze(1).transpose(1, 2)\n\n    def get_loss(\n        self,\n        loss: str,\n        outputs: Dict[str, torch.Tensor],\n        targets: List[Dict[str, torch.Tensor]],\n        indices: List[Tuple[torch.Tensor, torch.Tensor]],\n        num_masks: int,\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"Dispatch to specific loss function.\"\"\"\n        loss_map = {\n            \"labels\": self.loss_labels,\n            \"masks\": self.loss_masks,\n        }\n\n        assert loss in loss_map, f\"Loss {loss} not supported\"\n        return loss_map[loss](outputs, targets, indices, num_masks)\n\n    def forward(\n        self,\n        outputs: Dict[str, torch.Tensor],\n        targets: List[Dict[str, torch.Tensor]],\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Compute all losses.\n\n        Args:\n            outputs: Model outputs containing:\n                - pred_logits: [B, num_queries, num_classes]\n                - pred_masks: [B, num_queries, H, W]\n                - aux_outputs: List of intermediate outputs\n            targets: List of target dictionaries containing:\n                - labels: [num_objects] tensor of class labels\n                - masks: [num_objects, H, W] tensor of target masks\n\n        Returns:\n            Dict of losses\n        \"\"\"\n        # Retrieve outputs\n        outputs_without_aux = {k: v for k, v in outputs.items() if k != \"aux_outputs\"}\n\n        # Match predictions with targets\n        indices = self.matcher(outputs_without_aux, targets)\n\n        # Count total number of masks across batch\n        num_masks = sum(len(t[\"labels\"]) for t in targets)\n        num_masks = torch.as_tensor([num_masks], dtype=torch.float, device=outputs[\"pred_logits\"].device)\n\n        # Ensure at least 1 to avoid division by zero\n        num_masks = torch.clamp(num_masks, min=1)\n\n        # Compute all requested losses\n        losses = {}\n        for loss in self.losses:\n            losses.update(self.get_loss(loss, outputs, targets, indices, num_masks))\n\n        # Process auxiliary outputs (intermediate layer predictions)\n        if \"aux_outputs\" in outputs:\n            for i, aux_outputs in enumerate(outputs[\"aux_outputs\"]):\n                indices = self.matcher(aux_outputs, targets)\n                for loss in self.losses:\n                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_masks)\n                    l_dict = {k + f\"_{i}\": v for k, v in l_dict.items()}\n                    losses.update(l_dict)\n\n        return losses","metadata":{"id":"Wdh6tLIUGsbA","trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:31:30.332862Z","iopub.execute_input":"2025-09-21T16:31:30.333108Z","iopub.status.idle":"2025-09-21T16:31:30.358861Z","shell.execute_reply.started":"2025-09-21T16:31:30.333090Z","shell.execute_reply":"2025-09-21T16:31:30.358232Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Models\n","metadata":{"id":"fFjK_7oEG3zK"}},{"cell_type":"code","source":"from typing import Dict, List, Tuple, Optional\n\n\n\nclass Mask2Former(nn.Module):\n    \"\"\"\n    PyTorch-native implementation of Mask2Former.\n\n    Main architecture for mask classification, supporting:\n    - Instance segmentation\n    - Semantic segmentation\n    - Panoptic segmentation\n    \"\"\"\n\n    def __init__(\n        self,\n        backbone: nn.Module,\n        pixel_decoder: nn.Module,\n        transformer_decoder: nn.Module,\n        num_queries: int = 100,\n        object_mask_threshold: float = 0.0,\n        overlap_threshold: float = 0.8,\n        metadata: Optional[Dict] = None,\n        pixel_mean: Tuple[float, float, float] = (0.485, 0.456, 0.406),\n        pixel_std: Tuple[float, float, float] = (0.229, 0.224, 0.225),\n        semantic_on: bool = True,\n        instance_on: bool = False,\n        panoptic_on: bool = False,\n        test_topk_per_image: int = 100,\n        num_classes: int = 133,  # COCO panoptic classes\n    ):\n        \"\"\"\n        Args:\n            backbone: Backbone network (e.g., ResNet)\n            pixel_decoder: Pixel decoder module\n            transformer_decoder: Transformer decoder for mask prediction\n            num_queries: Number of query embeddings\n            object_mask_threshold: Threshold for object masks\n            overlap_threshold: Threshold for mask overlap\n            metadata: Dataset metadata\n            pixel_mean: Mean values for input normalization\n            pixel_std: Std values for input normalization\n            semantic_on: Enable semantic segmentation\n            instance_on: Enable instance segmentation\n            panoptic_on: Enable panoptic segmentation\n            test_topk_per_image: Number of top predictions per image\n            num_classes: Number of classes\n        \"\"\"\n        super().__init__()\n\n        self.backbone = backbone\n        self.pixel_decoder = pixel_decoder\n        self.predictor = transformer_decoder\n\n        self.num_queries = num_queries\n        self.object_mask_threshold = object_mask_threshold\n        self.overlap_threshold = overlap_threshold\n        self.metadata = metadata if metadata is not None else {}\n\n        # Normalization parameters\n        self.register_buffer(\"pixel_mean\", torch.tensor(pixel_mean).view(-1, 1, 1))\n        self.register_buffer(\"pixel_std\", torch.tensor(pixel_std).view(-1, 1, 1))\n\n        # Task flags\n        self.semantic_on = semantic_on\n        self.instance_on = instance_on\n        self.panoptic_on = panoptic_on\n\n        self.test_topk_per_image = test_topk_per_image\n        self.num_classes = num_classes\n\n    def forward(self, images: torch.Tensor, targets: Optional[List[Dict]] = None):\n        \"\"\"\n        Args:\n            images: Batched images tensor [B, C, H, W]\n            targets: Ground truth targets (for training)\n\n        Returns:\n            dict: Model outputs containing:\n                - 'pred_logits': Classification logits [B, num_queries, num_classes]\n                - 'pred_masks': Mask predictions [B, num_queries, H, W]\n                - 'aux_outputs': Auxiliary outputs from intermediate layers\n        \"\"\"\n        # Normalize images\n        images = (images - self.pixel_mean) / self.pixel_std\n\n        # Extract features from backbone\n        features = self.backbone(images)\n\n        # Generate mask features and positional embeddings\n        mask_features, transformer_encoder_features, multi_scale_features = self.pixel_decoder(features)\n\n        # Predict masks and classes\n        predictions = self.predictor(\n            multi_scale_features,\n            mask_features,\n            targets=targets\n        )\n\n        # Postprocess for inference\n        if not self.training:\n            # Process predictions for the specific task\n            if self.semantic_on:\n                predictions = self.semantic_inference(predictions, images)\n            elif self.instance_on:\n                predictions = self.instance_inference(predictions, images)\n            elif self.panoptic_on:\n                predictions = self.panoptic_inference(predictions, images)\n\n        return predictions\n\n    def semantic_inference(self, outputs: Dict, images: torch.Tensor) -> Dict:\n        \"\"\"Semantic segmentation inference.\"\"\"\n        mask_cls = outputs[\"pred_logits\"]\n        mask_pred = outputs[\"pred_masks\"]\n\n        # For semantic segmentation, we average over all queries\n        mask_cls = F.softmax(mask_cls, dim=-1)[..., :-1]  # Remove no-object class\n        mask_pred = mask_pred.sigmoid()\n        semseg = torch.einsum(\"bqc,bqhw->bchw\", mask_cls, mask_pred)\n\n        # Resize to original image size\n        B, _, H_img, W_img = images.shape\n        semseg = F.interpolate(\n            semseg,\n            size=(H_img, W_img),\n            mode=\"bilinear\",\n            align_corners=False\n        )\n\n        return {\"sem_seg\": semseg}\n\n    def instance_inference(self, outputs: Dict, images: torch.Tensor) -> List[Dict]:\n        \"\"\"Instance segmentation inference.\"\"\"\n        mask_cls = outputs[\"pred_logits\"]\n        mask_pred = outputs[\"pred_masks\"]\n\n        # Get image size\n        B, _, H_img, W_img = images.shape\n\n        results = []\n\n        # Process each image in the batch\n        for i in range(B):\n            scores, labels = mask_cls[i].max(-1)\n            mask_pred_i = mask_pred[i]\n\n            # Remove background predictions\n            keep = labels != self.num_classes\n            scores = scores[keep]\n            labels = labels[keep]\n            mask_pred_i = mask_pred_i[keep]\n\n            # Apply threshold\n            keep = scores > self.object_mask_threshold\n            scores = scores[keep]\n            labels = labels[keep]\n            mask_pred_i = mask_pred_i[keep]\n\n            # Get top-k predictions\n            if len(scores) > self.test_topk_per_image:\n                indices = torch.argsort(scores, descending=True)[:self.test_topk_per_image]\n                scores = scores[indices]\n                labels = labels[indices]\n                mask_pred_i = mask_pred_i[indices]\n\n            # Resize masks to original size\n            mask_pred_i = F.interpolate(\n                mask_pred_i.unsqueeze(0),\n                size=(H_img, W_img),\n                mode=\"bilinear\",\n                align_corners=False\n            ).squeeze(0)\n\n            # Binarize masks\n            mask_pred_i = mask_pred_i.sigmoid() > 0.5\n\n            results.append({\n                \"instances\": {\n                    \"pred_masks\": mask_pred_i,\n                    \"scores\": scores,\n                    \"pred_classes\": labels\n                }\n            })\n\n        return results\n\n    def panoptic_inference(self, outputs: Dict, images: torch.Tensor) -> List[Dict]:\n        \"\"\"Panoptic segmentation inference.\"\"\"\n        # Panoptic segmentation combines instance and semantic results\n        # This is a simplified version - full implementation would need\n        # more sophisticated merging of instance and stuff predictions\n\n        mask_cls = outputs[\"pred_logits\"]\n        mask_pred = outputs[\"pred_masks\"]\n\n        B, _, H_img, W_img = images.shape\n\n        results = []\n\n        for i in range(B):\n            scores, labels = mask_cls[i].max(-1)\n            mask_pred_i = mask_pred[i].sigmoid()\n\n            # Keep high-scoring predictions\n            keep = scores > self.object_mask_threshold\n            scores = scores[keep]\n            labels = labels[keep]\n            mask_pred_i = mask_pred_i[keep]\n\n            # Resize masks\n            mask_pred_i = F.interpolate(\n                mask_pred_i.unsqueeze(0),\n                size=(H_img, W_img),\n                mode=\"bilinear\",\n                align_corners=False\n            ).squeeze(0)\n\n            # Generate panoptic segmentation\n            panoptic_seg = torch.zeros((H_img, W_img), dtype=torch.int64, device=mask_pred_i.device)\n            segments_info = []\n\n            current_segment_id = 1\n\n            # Process each prediction\n            for j in range(len(scores)):\n                mask = mask_pred_i[j] > 0.5\n\n                # Skip empty masks\n                if not mask.any():\n                    continue\n\n                # Check overlap with existing segments\n                overlap = panoptic_seg > 0\n                if (mask & overlap).sum() / mask.sum() > self.overlap_threshold:\n                    continue\n\n                # Add to panoptic segmentation\n                panoptic_seg[mask] = current_segment_id\n\n                segments_info.append({\n                    \"id\": current_segment_id,\n                    \"category_id\": labels[j].item(),\n                    \"score\": scores[j].item()\n                })\n\n                current_segment_id += 1\n\n            results.append({\n                \"panoptic_seg\": (panoptic_seg, segments_info)\n            })\n\n        return results","metadata":{"id":"S3ro7pjeG0g7","trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:31:30.428478Z","iopub.execute_input":"2025-09-21T16:31:30.428708Z","iopub.status.idle":"2025-09-21T16:31:30.445767Z","shell.execute_reply.started":"2025-09-21T16:31:30.428690Z","shell.execute_reply":"2025-09-21T16:31:30.445140Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# Usage","metadata":{"id":"64BC60q-HLaA"}},{"cell_type":"code","source":"def create_dummy_backbone():\n    \"\"\"Create a simple backbone that returns multi-scale features.\"\"\"\n    class DummyBackbone(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv1 = nn.Conv2d(3, 64, 3, stride=2, padding=1)\n            self.conv2 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n            self.conv3 = nn.Conv2d(128, 256, 3, stride=2, padding=1)\n            self.conv4 = nn.Conv2d(256, 512, 3, stride=2, padding=1)\n\n        def forward(self, x):\n            # Return dict of features at different scales\n            features = {}\n            x = self.conv1(x)\n            features['res2'] = x  # 1/4 scale\n            x = self.conv2(x)\n            features['res3'] = x  # 1/8 scale\n            x = self.conv3(x)\n            features['res4'] = x  # 1/16 scale\n            x = self.conv4(x)\n            features['res5'] = x  # 1/32 scale\n            return features\n\n    return DummyBackbone()\n\n\ndef create_dummy_pixel_decoder():\n    \"\"\"Create a simple pixel decoder.\"\"\"\n    class DummyPixelDecoder(nn.Module):\n        def __init__(self, in_channels=[64, 128, 256, 512], mask_dim=256):\n            super().__init__()\n            self.mask_dim = mask_dim\n            # Simple FPN-like structure\n            self.lateral_convs = nn.ModuleList([\n                nn.Conv2d(in_c, mask_dim, 1) for in_c in in_channels\n            ])\n            self.output_conv = nn.Conv2d(mask_dim, mask_dim, 3, padding=1)\n\n        def forward(self, features):\n            # Simple feature pyramid\n            feature_list = [features['res2'], features['res3'], features['res4'], features['res5']]\n\n            # Upsample and combine features\n            mask_features = None\n            for i in range(len(feature_list)-1, -1, -1):\n                lateral = self.lateral_convs[i](feature_list[i])\n                if mask_features is None:\n                    mask_features = lateral\n                else:\n                    # Upsample and add\n                    mask_features = F.interpolate(\n                        mask_features,\n                        size=lateral.shape[-2:],\n                        mode='bilinear',\n                        align_corners=False\n                    )\n                    mask_features = mask_features + lateral\n\n            mask_features = self.output_conv(mask_features)\n\n            # For transformer: return mask features and multi-scale features\n            multi_scale_features = [lateral_conv(feat)\n                                  for lateral_conv, feat in zip(self.lateral_convs, feature_list)]\n\n            return mask_features, None, multi_scale_features\n\n    return DummyPixelDecoder()\n\n\ndef create_dummy_transformer_decoder():\n    \"\"\"Create a simple transformer decoder.\"\"\"\n    class DummyTransformerDecoder(nn.Module):\n        def __init__(self, hidden_dim=256, num_queries=100, num_classes=80):\n            super().__init__()\n            self.num_queries = num_queries\n            self.hidden_dim = hidden_dim\n\n            # Query embeddings\n            self.query_embed = nn.Embedding(num_queries, hidden_dim)\n\n            # Simple transformer layer\n            self.transformer = nn.TransformerDecoderLayer(\n                d_model=hidden_dim,\n                nhead=8,\n                dim_feedforward=2048,\n                dropout=0.1,\n                batch_first=True\n            )\n\n            # Output projections\n            self.class_embed = nn.Linear(hidden_dim, num_classes + 1)  # +1 for no-object\n            self.mask_embed = nn.Sequential(\n                nn.Linear(hidden_dim, hidden_dim),\n                nn.ReLU(),\n                nn.Linear(hidden_dim, hidden_dim)\n            )\n\n        def forward(self, multi_scale_features, mask_features, targets=None):\n            B = mask_features.shape[0]\n\n            # Get query embeddings\n            query_embeds = self.query_embed.weight.unsqueeze(0).expand(B, -1, -1)\n\n            # Simple attention on mask features (placeholder)\n            # In real implementation, this would involve proper cross-attention\n            mask_h, mask_w = mask_features.shape[-2:]\n            mask_features_flat = mask_features.flatten(2).transpose(1, 2)  # [B, H*W, C]\n\n            # Transformer decoder\n            outputs = self.transformer(query_embeds, mask_features_flat)\n\n            # Predict classes\n            outputs_class = self.class_embed(outputs)\n\n            # Predict masks\n            mask_embeds = self.mask_embed(outputs)\n            # Simple dot product with mask features\n            outputs_mask = torch.einsum(\"bqc,bhwc->bqhw\",\n                                      mask_embeds,\n                                      mask_features.permute(0, 2, 3, 1))\n\n            return {\n                \"pred_logits\": outputs_class,\n                \"pred_masks\": outputs_mask,\n                \"aux_outputs\": []  # No auxiliary outputs in this simple version\n            }\n\n    return DummyTransformerDecoder()\n\n\n\n\"\"\"Example of how to use the PyTorch-native Mask2Former.\"\"\"\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n","metadata":{"id":"yPIOeKBLHC7r","trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:31:30.475765Z","iopub.execute_input":"2025-09-21T16:31:30.476021Z","iopub.status.idle":"2025-09-21T16:31:30.489043Z","shell.execute_reply.started":"2025-09-21T16:31:30.475974Z","shell.execute_reply":"2025-09-21T16:31:30.488332Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# Data prerpration","metadata":{"id":"scRbWhg303ma"}},{"cell_type":"code","source":"# import os\n# import requests\n# import zipfile\n# from tqdm import tqdm\n\n# def download_and_unzip(url, download_path, extract_to):\n#     \"\"\"Downloads a file from a URL and unzips it.\"\"\"\n#     file_name = url.split('/')[-1]\n#     zip_path = os.path.join(download_path, file_name)\n\n#     print(f\"Downloading {file_name}...\")\n#     try:\n#         response = requests.get(url, stream=True)\n#         response.raise_for_status()\n#         total_size = int(response.headers.get('content-length', 0))\n\n#         with open(zip_path, 'wb') as f:\n#             for chunk in tqdm(response.iter_content(chunk_size=8192),\n#                               total=total_size // 8192, unit='KB'):\n#                 f.write(chunk)\n\n#         print(f\"Unzipping {file_name}...\")\n#         with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n#             zip_ref.extractall(extract_to)\n\n#         print(f\"Finished with {file_name}.\")\n#     except requests.exceptions.RequestException as e:\n#         print(f\"Error downloading {file_name}: {e}\")\n#     finally:\n#         if os.path.exists(zip_path):\n#             os.remove(zip_path)\n\n# def download_data():\n#     \"\"\"Main function to download all COCO 2014 files.\"\"\"\n#     base_url = \"http://images.cocodataset.org/\"\n#     files_to_download = {\n#         \"train2014.zip\": f\"{base_url}zips/train2014.zip\",\n#         \"val2014.zip\": f\"{base_url}zips/val2014.zip\",\n#         \"annotations_trainval2014.zip\": f\"{base_url}annotations/annotations_trainval2014.zip\"\n#     }\n\n#     base_dir = \"coco\"\n#     os.makedirs(base_dir, exist_ok=True)\n\n#     for file_name, url in files_to_download.items():\n#         download_and_unzip(url, base_dir, base_dir)\n\n#     print(\"COCO 2014 download and extraction complete.\")\n# download_data()","metadata":{"id":"JCHjGkObXoOE","trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:31:30.590415Z","iopub.execute_input":"2025-09-21T16:31:30.591138Z","iopub.status.idle":"2025-09-21T16:31:30.594856Z","shell.execute_reply.started":"2025-09-21T16:31:30.591110Z","shell.execute_reply":"2025-09-21T16:31:30.594226Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Redefine dataset with correct mask decoding and contiguous class ids\nfrom torch.utils.data import Dataset, DataLoader\nfrom pycocotools.coco import COCO\nfrom PIL import Image\nfrom typing import Dict, Any\nimport numpy as np\nimport torch\nimport os\n\nclass CocoDataset(Dataset):\n    def __init__(self, root: str, annotation: str, transforms=None, max_files: int = None):\n        self.root = root\n        self.annotation = annotation\n        self.transforms = transforms\n        self.coco = COCO(annotation)\n        self.ids = list(sorted(self.coco.imgs.keys()))\n        \n        # --- FIX START ---\n        # Limit the number of files if max_files is specified\n        if max_files is not None:\n            self.ids = self.ids[:max_files]\n        # --- FIX END ---\n        \n        # Map COCO category ids to contiguous [0, num_classes-1]\n        cat_ids = sorted(self.coco.getCatIds())\n        self.catId2contiguous = {cid: i for i, cid in enumerate(cat_ids)}\n        # Also store contiguous id to cat id mapping for potential use\n        self.contiguous2catId = {i: cid for i, cid in enumerate(cat_ids)}\n\n\n    def __getitem__(self, index: int):\n        coco = self.coco\n        img_id = self.ids[index]\n        ann_ids = coco.getAnnIds(imgIds=img_id, iscrowd=None)\n        anns = coco.loadAnns(ann_ids)\n        img_info = coco.loadImgs(img_id)[0]\n        path = img_info[\"file_name\"]\n        img_path = os.path.join(self.root, path)\n        img = Image.open(img_path).convert(\"RGB\")\n        w, h = img.size\n\n        masks = []\n        labels = []\n        for ann in anns:\n            # Only process annotations with segmentation and a valid category ID\n            if \"segmentation\" in ann:\n                cid = ann[\"category_id\"]\n                if cid in self.catId2contiguous:\n                    # Convert to binary mask (H, W), uint8 {0,1}\n                    m = coco.annToMask(ann)\n                    if m is not None:\n                        masks.append(m.astype(np.uint8))\n                        # Map to contiguous id\n                        labels.append(self.catId2contiguous[cid])\n\n        if len(masks) > 0:\n            masks = np.stack(masks, axis=0)  # [N, H, W]\n            masks = torch.from_numpy(masks.astype(np.uint8)).bool()\n            labels = torch.tensor(labels, dtype=torch.int64)\n        else:\n            # Ensure masks and labels are empty tensors with correct dimensions if no valid annotations\n            masks = torch.zeros((0, h, w), dtype=torch.bool)\n            labels = torch.zeros((0,), dtype=torch.int64)\n\n\n        target: Dict[str, Any] = {\n            \"masks\": masks,\n            \"labels\": labels,\n            \"image_id\": torch.tensor([img_id]),\n            \"size\": torch.tensor([h, w]),\n            \"orig_size\": torch.tensor([h, w]),\n        }\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self) -> int:\n        return len(self.ids)\n\n# Simple transform: resize shorter side to 512, to tensor in [0,1]\nimport torchvision.transforms.functional as TF\n\ndef get_transform(train: bool, min_size: int = 512, max_size: int = 864):\n    def _tf(img: Image.Image, target: Dict[str, Any]):\n        w, h = img.size\n        # keep aspect ratio, scale so min side = min_size (cap by max_size on long side)\n        scale = min_size / min(h, w)\n        if max(h, w) * scale > max_size:\n            scale = max_size / max(h, w)\n        new_w, new_h = int(round(w * scale)), int(round(h * scale))\n        img_r = img.resize((new_w, new_h), Image.BILINEAR)\n        # resize masks accordingly\n        if target[\"masks\"].numel() > 0:\n            masks = target[\"masks\"].float().unsqueeze(1)  # [N,1,H,W]\n            masks_r = torch.nn.functional.interpolate(\n                masks, size=(new_h, new_w), mode=\"nearest\"\n            ).squeeze(1).bool()\n        else:\n            masks_r = target[\"masks\"]\n        img_t = TF.to_tensor(img_r)  # [0,1]\n        target = {\n            **target,\n            \"masks\": masks_r,\n            \"size\": torch.tensor([new_h, new_w]),\n        }\n        return img_t, target\n    return _tf\n\n# Build datasets/dataloaders (update paths to where you extracted COCO 2014)\ncoco_root = os.path.join(os.getcwd(), \"/kaggle/input/coco-2017-dataset/coco2017\")  # change if needed\ntrain_img_dir = os.path.join(coco_root, \"train2017\")\ntrain_ann_file = os.path.join(coco_root, \"annotations\", \"instances_train2017.json\")\nval_img_dir = os.path.join(coco_root, \"val2017\")\nval_ann_file = os.path.join(coco_root, \"annotations\", \"instances_val2017.json\")\n\n# Pass max_files=20000 to the training dataset\ntrain_dataset = CocoDataset(root=train_img_dir, annotation=train_ann_file, transforms=get_transform(train=True), max_files=20000)\nval_dataset = CocoDataset(root=val_img_dir, annotation=val_ann_file, transforms=get_transform(train=False))\n\ndef collate_fn(batch):\n    images = [item[0] for item in batch]           # list of [C,H,W]\n    targets = [item[1] for item in batch]\n    # Pad to common size\n    images = nested_tensor_from_tensor_list(images).tensors  # [B,C,H_max,W_max]\n    return images, targets\n\n# NOTE: The nested_tensor_from_tensor_list function is missing from your original script.\n# You need to define it or import it from a library like `torchvision.models.detection.mask_rcnn`.\n# For a simple implementation, you can define it as follows:\nfrom torch.nn.utils.rnn import pad_sequence\ndef nested_tensor_from_tensor_list(tensor_list):\n    # assumes all tensors are C, H, W\n    max_H = max([t.shape[1] for t in tensor_list])\n    max_W = max([t.shape[2] for t in tensor_list])\n    \n    padded_tensors = []\n    for tensor in tensor_list:\n        h, w = tensor.shape[1], tensor.shape[2]\n        padding_h = max_H - h\n        padding_w = max_W - w\n        \n        # padding on the right and bottom\n        padded_tensor = torch.nn.functional.pad(tensor, (0, padding_w, 0, padding_h), mode='constant', value=0)\n        padded_tensors.append(padded_tensor)\n        \n    return type('NestedTensor', (object,), {'tensors': torch.stack(padded_tensors)})\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True, collate_fn=collate_fn)\nval_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=4, pin_memory=True, collate_fn=collate_fn)\n\nprint(\"DataLoaders ready:\", len(train_dataset), \"train,\", len(val_dataset), \"val\")","metadata":{"id":"ba2c8ea7","outputId":"1d93a9d2-927f-4bc2-f788-e1e41477eaf2","trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:31:30.600485Z","iopub.execute_input":"2025-09-21T16:31:30.600921Z","iopub.status.idle":"2025-09-21T16:31:51.507452Z","shell.execute_reply.started":"2025-09-21T16:31:30.600897Z","shell.execute_reply":"2025-09-21T16:31:51.506849Z"}},"outputs":[{"name":"stdout","text":"loading annotations into memory...\nDone (t=16.76s)\ncreating index...\nindex created!\nloading annotations into memory...\nDone (t=0.51s)\ncreating index...\nindex created!\nDataLoaders ready: 20000 train, 5000 val\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.cuda.amp import autocast, GradScaler\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Build model (reuses your factory functions already defined in notebook)\nbackbone = create_dummy_backbone()\npixel_decoder = create_dummy_pixel_decoder()\ntransformer_decoder = create_dummy_transformer_decoder()\n\nmodel = Mask2Former(\n    backbone=backbone,\n    pixel_decoder=pixel_decoder,\n    transformer_decoder=transformer_decoder,\n    num_queries=100,\n    num_classes=80,   # COCO has 80 classes\n    instance_on=True,\n    semantic_on=False,\n    panoptic_on=False,\n).to(device)\n\n# Loss / matcher\nmatcher = HungarianMatcher(cost_class=2.0, cost_mask=5.0, cost_dice=5.0, num_points=12544)\nweight_dict = {\"loss_ce\": 2.0, \"loss_mask\": 5.0, \"loss_dice\": 5.0}\ncriterion = SetCriterion(\n    num_classes=80,\n    matcher=matcher,\n    weight_dict=weight_dict,\n    eos_coef=0.1,\n    losses=[\"labels\", \"masks\"],\n).to(device)\n\n# Optimizer / scheduler\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\nscaler = GradScaler(enabled=torch.cuda.is_available())\n\nnum_epochs = 10\nlog_interval = 50\nmax_norm = 0.1  # grad clip\n\ndef move_targets_to_device(targets, device):\n    out = []\n    for t in targets:\n        out.append({\n            \"masks\": t[\"masks\"].to(device),\n            \"labels\": t[\"labels\"].to(device),\n            \"image_id\": t[\"image_id\"].to(device),\n            \"size\": t[\"size\"].to(device),\n            \"orig_size\": t[\"orig_size\"].to(device),\n        })\n    return out\n\nglobal_step = 0\nmodel.train()\nfor epoch in range(num_epochs):\n    running = 0.0\n    for i, (images, targets) in enumerate(train_dataloader):\n        images = images.to(device, non_blocking=True)\n        # Move targets to device and pad masks\n        targets = move_targets_to_device(targets, device)\n        # Pad target masks using nested_tensor_from_tensor_list\n        padded_targets_masks, _ = nested_tensor_from_tensor_list([t[\"masks\"] for t in targets]).decompose()\n        for j, t in enumerate(targets):\n          # Replace original masks with padded ones\n          t[\"masks\"] = padded_targets_masks[j][:t[\"masks\"].shape[0]] # Keep only the actual masks from padding\n\n        optimizer.zero_grad(set_to_none=True)\n        with autocast(enabled=torch.cuda.is_available()):\n            outputs = model(images, targets)  # training forward returns raw outputs\n            losses = criterion(outputs, targets)\n            total_loss = sum(losses[k] * weight_dict.get(k, 1.0) for k in losses)\n\n        scaler.scale(total_loss).backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n        scaler.step(optimizer)\n        scaler.update()\n\n        running += total_loss.item()\n        global_step += 1\n        if (i + 1) % log_interval == 0:\n            avg = running / log_interval\n            print(f\"Epoch {epoch+1}/{num_epochs} | Iter {i+1}/{len(train_dataloader)} | loss {avg:.4f} | lr {scheduler.get_last_lr()[0]:.6f}\")\n            running = 0.0\n\n    scheduler.step()\n\n    # Save checkpoint each epoch\n    ckpt = {\n        \"model\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n        \"scheduler\": scheduler.state_dict(),\n        \"epoch\": epoch + 1,\n    }\n    save_on_master(ckpt, f\"mask2former_dummy_coco_epoch_{epoch+1}.pth\")\n\nprint(\"Training complete.\")","metadata":{"id":"NF4VLGbR68ma","outputId":"d7ec0423-ba30-424f-99bd-236e3bc44588","trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:33:39.610832Z","iopub.execute_input":"2025-09-21T16:33:39.611419Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/2235904764.py:39: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler(enabled=torch.cuda.is_available())\n/tmp/ipykernel_36/2235904764.py:72: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=torch.cuda.is_available()):\n/tmp/ipykernel_36/2708981260.py:126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=False):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 | Iter 50/10000 | loss 13.0988 | lr 0.000100\nEpoch 1/10 | Iter 100/10000 | loss 10.4412 | lr 0.000100\nEpoch 1/10 | Iter 150/10000 | loss 9.7843 | lr 0.000100\nEpoch 1/10 | Iter 200/10000 | loss 9.4524 | lr 0.000100\nEpoch 1/10 | Iter 250/10000 | loss 9.1921 | lr 0.000100\nEpoch 1/10 | Iter 300/10000 | loss 9.4231 | lr 0.000100\nEpoch 1/10 | Iter 350/10000 | loss 9.6170 | lr 0.000100\nEpoch 1/10 | Iter 400/10000 | loss 9.4926 | lr 0.000100\nEpoch 1/10 | Iter 450/10000 | loss 9.0948 | lr 0.000100\nEpoch 1/10 | Iter 500/10000 | loss 9.1012 | lr 0.000100\nEpoch 1/10 | Iter 550/10000 | loss 9.1449 | lr 0.000100\nEpoch 1/10 | Iter 600/10000 | loss 9.4418 | lr 0.000100\nEpoch 1/10 | Iter 650/10000 | loss 8.7961 | lr 0.000100\nEpoch 1/10 | Iter 700/10000 | loss 9.0018 | lr 0.000100\nEpoch 1/10 | Iter 750/10000 | loss 8.9155 | lr 0.000100\nEpoch 1/10 | Iter 800/10000 | loss 9.1677 | lr 0.000100\nEpoch 1/10 | Iter 850/10000 | loss 9.1148 | lr 0.000100\nEpoch 1/10 | Iter 900/10000 | loss 8.6442 | lr 0.000100\nEpoch 1/10 | Iter 950/10000 | loss 9.0008 | lr 0.000100\nEpoch 1/10 | Iter 1000/10000 | loss 8.8073 | lr 0.000100\nEpoch 1/10 | Iter 1050/10000 | loss 8.8380 | lr 0.000100\nEpoch 1/10 | Iter 1100/10000 | loss 8.7668 | lr 0.000100\nEpoch 1/10 | Iter 1150/10000 | loss 8.8512 | lr 0.000100\nEpoch 1/10 | Iter 1200/10000 | loss 8.7565 | lr 0.000100\nEpoch 1/10 | Iter 1250/10000 | loss 8.8283 | lr 0.000100\nEpoch 1/10 | Iter 1300/10000 | loss 8.8001 | lr 0.000100\nEpoch 1/10 | Iter 1350/10000 | loss 8.8866 | lr 0.000100\nEpoch 1/10 | Iter 1400/10000 | loss 8.7055 | lr 0.000100\nEpoch 1/10 | Iter 1450/10000 | loss 8.6072 | lr 0.000100\nEpoch 1/10 | Iter 1500/10000 | loss 8.5990 | lr 0.000100\nEpoch 1/10 | Iter 1550/10000 | loss 8.5449 | lr 0.000100\nEpoch 1/10 | Iter 1600/10000 | loss 8.6546 | lr 0.000100\nEpoch 1/10 | Iter 1650/10000 | loss 8.4717 | lr 0.000100\nEpoch 1/10 | Iter 1700/10000 | loss 8.4447 | lr 0.000100\nEpoch 1/10 | Iter 1750/10000 | loss 8.7265 | lr 0.000100\nEpoch 1/10 | Iter 1800/10000 | loss 8.4232 | lr 0.000100\nEpoch 1/10 | Iter 1850/10000 | loss 8.0305 | lr 0.000100\nEpoch 1/10 | Iter 1900/10000 | loss 8.5311 | lr 0.000100\nEpoch 1/10 | Iter 1950/10000 | loss 7.8793 | lr 0.000100\nEpoch 1/10 | Iter 2000/10000 | loss 8.2798 | lr 0.000100\nEpoch 1/10 | Iter 2050/10000 | loss 8.0675 | lr 0.000100\nEpoch 1/10 | Iter 2100/10000 | loss 7.9647 | lr 0.000100\nEpoch 1/10 | Iter 2150/10000 | loss 8.3158 | lr 0.000100\nEpoch 1/10 | Iter 2200/10000 | loss 7.8255 | lr 0.000100\nEpoch 1/10 | Iter 2250/10000 | loss 8.0480 | lr 0.000100\nEpoch 1/10 | Iter 2300/10000 | loss 7.9984 | lr 0.000100\nEpoch 1/10 | Iter 2350/10000 | loss 8.8198 | lr 0.000100\nEpoch 1/10 | Iter 2400/10000 | loss 8.3517 | lr 0.000100\nEpoch 1/10 | Iter 2450/10000 | loss 7.9328 | lr 0.000100\nEpoch 1/10 | Iter 2500/10000 | loss 8.2915 | lr 0.000100\nEpoch 1/10 | Iter 2550/10000 | loss 7.8722 | lr 0.000100\nEpoch 1/10 | Iter 2600/10000 | loss 8.1028 | lr 0.000100\nEpoch 1/10 | Iter 2650/10000 | loss 7.4732 | lr 0.000100\nEpoch 1/10 | Iter 2700/10000 | loss 7.5390 | lr 0.000100\nEpoch 1/10 | Iter 2750/10000 | loss 7.6471 | lr 0.000100\nEpoch 1/10 | Iter 2800/10000 | loss 7.9545 | lr 0.000100\nEpoch 1/10 | Iter 2850/10000 | loss 8.1602 | lr 0.000100\nEpoch 1/10 | Iter 2900/10000 | loss 7.8390 | lr 0.000100\nEpoch 1/10 | Iter 2950/10000 | loss 7.5354 | lr 0.000100\nEpoch 1/10 | Iter 3000/10000 | loss 7.6200 | lr 0.000100\nEpoch 1/10 | Iter 3050/10000 | loss 7.5708 | lr 0.000100\nEpoch 1/10 | Iter 3100/10000 | loss 8.1427 | lr 0.000100\nEpoch 1/10 | Iter 3150/10000 | loss 7.8894 | lr 0.000100\nEpoch 1/10 | Iter 3200/10000 | loss 7.9032 | lr 0.000100\nEpoch 1/10 | Iter 3250/10000 | loss 7.8861 | lr 0.000100\nEpoch 1/10 | Iter 3300/10000 | loss 7.4801 | lr 0.000100\nEpoch 1/10 | Iter 3350/10000 | loss 7.6810 | lr 0.000100\nEpoch 1/10 | Iter 3400/10000 | loss 7.8965 | lr 0.000100\nEpoch 1/10 | Iter 3450/10000 | loss 7.9933 | lr 0.000100\nEpoch 1/10 | Iter 3500/10000 | loss 7.7347 | lr 0.000100\nEpoch 1/10 | Iter 3550/10000 | loss 7.3238 | lr 0.000100\nEpoch 1/10 | Iter 3600/10000 | loss 7.7472 | lr 0.000100\nEpoch 1/10 | Iter 3650/10000 | loss 7.7443 | lr 0.000100\nEpoch 1/10 | Iter 3700/10000 | loss 7.6319 | lr 0.000100\nEpoch 1/10 | Iter 3750/10000 | loss 7.9587 | lr 0.000100\nEpoch 1/10 | Iter 3800/10000 | loss 7.5414 | lr 0.000100\nEpoch 1/10 | Iter 3850/10000 | loss 8.1708 | lr 0.000100\nEpoch 1/10 | Iter 3900/10000 | loss 7.2198 | lr 0.000100\nEpoch 1/10 | Iter 3950/10000 | loss 7.7952 | lr 0.000100\nEpoch 1/10 | Iter 4000/10000 | loss 7.6334 | lr 0.000100\nEpoch 1/10 | Iter 4050/10000 | loss 7.7663 | lr 0.000100\nEpoch 1/10 | Iter 4100/10000 | loss 8.0696 | lr 0.000100\nEpoch 1/10 | Iter 4150/10000 | loss 7.9731 | lr 0.000100\nEpoch 1/10 | Iter 4200/10000 | loss 7.7448 | lr 0.000100\nEpoch 1/10 | Iter 4250/10000 | loss 7.3239 | lr 0.000100\nEpoch 1/10 | Iter 4300/10000 | loss 7.2922 | lr 0.000100\nEpoch 1/10 | Iter 4350/10000 | loss 7.6870 | lr 0.000100\nEpoch 1/10 | Iter 4400/10000 | loss 7.9892 | lr 0.000100\nEpoch 1/10 | Iter 4450/10000 | loss 7.2370 | lr 0.000100\nEpoch 1/10 | Iter 4500/10000 | loss 7.5126 | lr 0.000100\nEpoch 1/10 | Iter 4550/10000 | loss 7.3446 | lr 0.000100\nEpoch 1/10 | Iter 4600/10000 | loss 7.9528 | lr 0.000100\nEpoch 1/10 | Iter 4650/10000 | loss 7.2732 | lr 0.000100\nEpoch 1/10 | Iter 4700/10000 | loss 7.3751 | lr 0.000100\nEpoch 1/10 | Iter 4750/10000 | loss 7.1855 | lr 0.000100\nEpoch 1/10 | Iter 4800/10000 | loss 7.4521 | lr 0.000100\nEpoch 1/10 | Iter 4850/10000 | loss 8.0887 | lr 0.000100\nEpoch 1/10 | Iter 4900/10000 | loss 7.1647 | lr 0.000100\nEpoch 1/10 | Iter 4950/10000 | loss 7.3429 | lr 0.000100\nEpoch 1/10 | Iter 5000/10000 | loss 7.3072 | lr 0.000100\nEpoch 1/10 | Iter 5050/10000 | loss 7.5159 | lr 0.000100\nEpoch 1/10 | Iter 5100/10000 | loss 7.1032 | lr 0.000100\nEpoch 1/10 | Iter 5150/10000 | loss 7.6768 | lr 0.000100\nEpoch 1/10 | Iter 5200/10000 | loss 7.7966 | lr 0.000100\nEpoch 1/10 | Iter 5250/10000 | loss 7.5946 | lr 0.000100\nEpoch 1/10 | Iter 5300/10000 | loss 7.7673 | lr 0.000100\nEpoch 1/10 | Iter 5350/10000 | loss 7.4142 | lr 0.000100\nEpoch 1/10 | Iter 5400/10000 | loss 7.1525 | lr 0.000100\nEpoch 1/10 | Iter 5450/10000 | loss 6.8726 | lr 0.000100\nEpoch 1/10 | Iter 5500/10000 | loss 7.1605 | lr 0.000100\nEpoch 1/10 | Iter 5550/10000 | loss 7.2508 | lr 0.000100\nEpoch 1/10 | Iter 5600/10000 | loss 7.9736 | lr 0.000100\nEpoch 1/10 | Iter 5650/10000 | loss 7.2165 | lr 0.000100\nEpoch 1/10 | Iter 5700/10000 | loss 7.5058 | lr 0.000100\nEpoch 1/10 | Iter 5750/10000 | loss 8.1330 | lr 0.000100\nEpoch 1/10 | Iter 5800/10000 | loss 6.8362 | lr 0.000100\nEpoch 1/10 | Iter 5850/10000 | loss 6.8346 | lr 0.000100\nEpoch 1/10 | Iter 5900/10000 | loss 7.7372 | lr 0.000100\nEpoch 1/10 | Iter 5950/10000 | loss 7.4475 | lr 0.000100\nEpoch 1/10 | Iter 6000/10000 | loss 7.0378 | lr 0.000100\nEpoch 1/10 | Iter 6050/10000 | loss 7.4643 | lr 0.000100\nEpoch 1/10 | Iter 6100/10000 | loss 7.4976 | lr 0.000100\nEpoch 1/10 | Iter 6150/10000 | loss 7.5137 | lr 0.000100\nEpoch 1/10 | Iter 6200/10000 | loss 7.5234 | lr 0.000100\nEpoch 1/10 | Iter 6250/10000 | loss 7.0703 | lr 0.000100\nEpoch 1/10 | Iter 6300/10000 | loss 7.2748 | lr 0.000100\nEpoch 1/10 | Iter 6350/10000 | loss 7.8317 | lr 0.000100\nEpoch 1/10 | Iter 6400/10000 | loss 7.8715 | lr 0.000100\nEpoch 1/10 | Iter 6450/10000 | loss 7.9364 | lr 0.000100\nEpoch 1/10 | Iter 6500/10000 | loss 7.7332 | lr 0.000100\nEpoch 1/10 | Iter 6550/10000 | loss 7.5572 | lr 0.000100\nEpoch 1/10 | Iter 6600/10000 | loss 7.7224 | lr 0.000100\nEpoch 1/10 | Iter 6650/10000 | loss 7.7690 | lr 0.000100\nEpoch 1/10 | Iter 6700/10000 | loss 7.5707 | lr 0.000100\nEpoch 1/10 | Iter 6750/10000 | loss 7.7188 | lr 0.000100\nEpoch 1/10 | Iter 6800/10000 | loss 7.7620 | lr 0.000100\nEpoch 1/10 | Iter 6850/10000 | loss 7.8050 | lr 0.000100\nEpoch 1/10 | Iter 6900/10000 | loss 7.5323 | lr 0.000100\nEpoch 1/10 | Iter 6950/10000 | loss 7.3594 | lr 0.000100\nEpoch 1/10 | Iter 7000/10000 | loss 7.2842 | lr 0.000100\nEpoch 1/10 | Iter 7050/10000 | loss 7.5422 | lr 0.000100\nEpoch 1/10 | Iter 7100/10000 | loss 7.5682 | lr 0.000100\nEpoch 1/10 | Iter 7150/10000 | loss 7.9188 | lr 0.000100\nEpoch 1/10 | Iter 7200/10000 | loss 7.8640 | lr 0.000100\nEpoch 1/10 | Iter 7250/10000 | loss 7.6603 | lr 0.000100\nEpoch 1/10 | Iter 7300/10000 | loss 7.6841 | lr 0.000100\nEpoch 1/10 | Iter 7350/10000 | loss 6.9962 | lr 0.000100\nEpoch 1/10 | Iter 7400/10000 | loss 7.4225 | lr 0.000100\nEpoch 1/10 | Iter 7450/10000 | loss 7.6994 | lr 0.000100\nEpoch 1/10 | Iter 7500/10000 | loss 7.3519 | lr 0.000100\nEpoch 1/10 | Iter 7550/10000 | loss 7.5728 | lr 0.000100\nEpoch 1/10 | Iter 7600/10000 | loss 7.4688 | lr 0.000100\nEpoch 1/10 | Iter 7650/10000 | loss 7.0947 | lr 0.000100\nEpoch 1/10 | Iter 7700/10000 | loss 7.5479 | lr 0.000100\nEpoch 1/10 | Iter 7750/10000 | loss 7.3486 | lr 0.000100\nEpoch 1/10 | Iter 7800/10000 | loss 7.2589 | lr 0.000100\nEpoch 1/10 | Iter 7850/10000 | loss 7.4188 | lr 0.000100\nEpoch 1/10 | Iter 7900/10000 | loss 7.5965 | lr 0.000100\nEpoch 1/10 | Iter 7950/10000 | loss 7.9293 | lr 0.000100\nEpoch 1/10 | Iter 8000/10000 | loss 7.6658 | lr 0.000100\nEpoch 1/10 | Iter 8050/10000 | loss 7.5314 | lr 0.000100\nEpoch 1/10 | Iter 8100/10000 | loss 7.6731 | lr 0.000100\nEpoch 1/10 | Iter 8150/10000 | loss 7.5135 | lr 0.000100\nEpoch 1/10 | Iter 8200/10000 | loss 7.0896 | lr 0.000100\nEpoch 1/10 | Iter 8250/10000 | loss 7.4636 | lr 0.000100\nEpoch 1/10 | Iter 8300/10000 | loss 7.2491 | lr 0.000100\nEpoch 1/10 | Iter 8350/10000 | loss 7.4446 | lr 0.000100\nEpoch 1/10 | Iter 8400/10000 | loss 7.2178 | lr 0.000100\nEpoch 1/10 | Iter 8450/10000 | loss 7.1087 | lr 0.000100\nEpoch 1/10 | Iter 8500/10000 | loss 7.7975 | lr 0.000100\nEpoch 1/10 | Iter 8550/10000 | loss 7.3982 | lr 0.000100\nEpoch 1/10 | Iter 8600/10000 | loss 7.5134 | lr 0.000100\nEpoch 1/10 | Iter 8650/10000 | loss 7.4229 | lr 0.000100\nEpoch 1/10 | Iter 8700/10000 | loss 7.1965 | lr 0.000100\nEpoch 1/10 | Iter 8750/10000 | loss 7.6319 | lr 0.000100\nEpoch 1/10 | Iter 8800/10000 | loss 7.5609 | lr 0.000100\nEpoch 1/10 | Iter 8850/10000 | loss 6.6738 | lr 0.000100\nEpoch 1/10 | Iter 8900/10000 | loss 7.3674 | lr 0.000100\nEpoch 1/10 | Iter 8950/10000 | loss 7.1831 | lr 0.000100\nEpoch 1/10 | Iter 9000/10000 | loss 7.1344 | lr 0.000100\nEpoch 1/10 | Iter 9050/10000 | loss 6.9455 | lr 0.000100\nEpoch 1/10 | Iter 9100/10000 | loss 7.5729 | lr 0.000100\nEpoch 1/10 | Iter 9150/10000 | loss 7.2544 | lr 0.000100\nEpoch 1/10 | Iter 9200/10000 | loss 7.3028 | lr 0.000100\nEpoch 1/10 | Iter 9250/10000 | loss 7.5511 | lr 0.000100\nEpoch 1/10 | Iter 9300/10000 | loss 7.6135 | lr 0.000100\nEpoch 1/10 | Iter 9350/10000 | loss 7.3729 | lr 0.000100\nEpoch 1/10 | Iter 9400/10000 | loss 6.9242 | lr 0.000100\nEpoch 1/10 | Iter 9450/10000 | loss 7.5352 | lr 0.000100\nEpoch 1/10 | Iter 9500/10000 | loss 7.9278 | lr 0.000100\nEpoch 1/10 | Iter 9550/10000 | loss 8.0163 | lr 0.000100\nEpoch 1/10 | Iter 9600/10000 | loss 7.5113 | lr 0.000100\nEpoch 1/10 | Iter 9650/10000 | loss 7.6341 | lr 0.000100\nEpoch 1/10 | Iter 9700/10000 | loss 7.3603 | lr 0.000100\nEpoch 1/10 | Iter 9750/10000 | loss 7.4429 | lr 0.000100\nEpoch 1/10 | Iter 9800/10000 | loss 7.7563 | lr 0.000100\nEpoch 1/10 | Iter 9850/10000 | loss 7.6774 | lr 0.000100\nEpoch 1/10 | Iter 9900/10000 | loss 7.5356 | lr 0.000100\nEpoch 1/10 | Iter 9950/10000 | loss 7.3120 | lr 0.000100\nEpoch 1/10 | Iter 10000/10000 | loss 7.5020 | lr 0.000100\nEpoch 2/10 | Iter 50/10000 | loss 7.3427 | lr 0.000100\nEpoch 2/10 | Iter 100/10000 | loss 7.4327 | lr 0.000100\nEpoch 2/10 | Iter 150/10000 | loss 7.7001 | lr 0.000100\nEpoch 2/10 | Iter 200/10000 | loss 7.4957 | lr 0.000100\nEpoch 2/10 | Iter 250/10000 | loss 7.4877 | lr 0.000100\nEpoch 2/10 | Iter 300/10000 | loss 7.9394 | lr 0.000100\nEpoch 2/10 | Iter 350/10000 | loss 7.5028 | lr 0.000100\nEpoch 2/10 | Iter 400/10000 | loss 7.3562 | lr 0.000100\nEpoch 2/10 | Iter 450/10000 | loss 7.6072 | lr 0.000100\nEpoch 2/10 | Iter 500/10000 | loss 6.8412 | lr 0.000100\nEpoch 2/10 | Iter 550/10000 | loss 7.3416 | lr 0.000100\nEpoch 2/10 | Iter 600/10000 | loss 7.0932 | lr 0.000100\nEpoch 2/10 | Iter 650/10000 | loss 6.9466 | lr 0.000100\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"h0OmXW_K7SuH","trusted":true},"outputs":[],"execution_count":null}]}