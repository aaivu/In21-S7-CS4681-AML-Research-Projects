{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Utils\n"
      ],
      "metadata": {
        "id": "F3Rjk_xXGMUY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mtKJkSHjGJ2l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.distributed as dist\n",
        "from typing import List, Optional, Tuple\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "class NestedTensor:\n",
        "    \"\"\"\n",
        "    A nested tensor is a tensor along with a mask indicating which elements are valid.\n",
        "    This is useful for batching images of different sizes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tensors: torch.Tensor, mask: Optional[torch.Tensor]):\n",
        "        self.tensors = tensors\n",
        "        self.mask = mask\n",
        "\n",
        "    def decompose(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        return self.tensors, self.mask\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        return self.tensors.device\n",
        "\n",
        "    def to(self, device):\n",
        "        return NestedTensor(self.tensors.to(device), self.mask.to(device) if self.mask is not None else None)\n",
        "\n",
        "\n",
        "def nested_tensor_from_tensor_list(tensor_list: List[torch.Tensor]) -> NestedTensor:\n",
        "    \"\"\"\n",
        "    Create a nested tensor from a list of tensors.\n",
        "\n",
        "    Pads the tensors to the same size and creates a mask indicating valid regions.\n",
        "\n",
        "    Args:\n",
        "        tensor_list: List of tensors with shape [C, H_i, W_i]\n",
        "\n",
        "    Returns:\n",
        "        NestedTensor containing:\n",
        "            - tensors: Padded tensors [B, C, H_max, W_max]\n",
        "            - mask: Binary mask [B, H_max, W_max] (False = valid, True = padding)\n",
        "    \"\"\"\n",
        "    # Calculate maximum dimensions\n",
        "    max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n",
        "\n",
        "    # Create output tensor and mask\n",
        "    batch_shape = [len(tensor_list)] + max_size\n",
        "    b, c, h, w = batch_shape\n",
        "\n",
        "    device = tensor_list[0].device\n",
        "    dtype = tensor_list[0].dtype\n",
        "\n",
        "    tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n",
        "    mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n",
        "\n",
        "    # Copy tensors and update mask\n",
        "    for i, img in enumerate(tensor_list):\n",
        "        tensor[i, :img.shape[0], :img.shape[1], :img.shape[2]].copy_(img)\n",
        "        mask[i, :img.shape[1], :img.shape[2]] = False\n",
        "\n",
        "    return NestedTensor(tensor, mask)\n",
        "\n",
        "\n",
        "def _max_by_axis(the_list: List[List[int]]) -> List[int]:\n",
        "    \"\"\"Find maximum size along each axis.\"\"\"\n",
        "    maxes = the_list[0]\n",
        "    for sublist in the_list[1:]:\n",
        "        for index, item in enumerate(sublist):\n",
        "            maxes[index] = max(maxes[index], item)\n",
        "    return maxes\n",
        "\n",
        "\n",
        "def is_dist_avail_and_initialized() -> bool:\n",
        "    \"\"\"Check if distributed training is available and initialized.\"\"\"\n",
        "    if not dist.is_available():\n",
        "        return False\n",
        "    if not dist.is_initialized():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_world_size() -> int:\n",
        "    \"\"\"Get world size for distributed training.\"\"\"\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "\n",
        "def get_rank() -> int:\n",
        "    \"\"\"Get rank for distributed training.\"\"\"\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "\n",
        "\n",
        "def is_main_process() -> bool:\n",
        "    \"\"\"Check if current process is the main process.\"\"\"\n",
        "    return get_rank() == 0\n",
        "\n",
        "\n",
        "def save_on_master(*args, **kwargs):\n",
        "    \"\"\"Save checkpoint only on the main process.\"\"\"\n",
        "    if is_main_process():\n",
        "        torch.save(*args, **kwargs)\n",
        "\n",
        "\n",
        "class ImageList:\n",
        "    \"\"\"\n",
        "    Structure that holds a list of images (tensors) of possibly different sizes.\n",
        "\n",
        "    Similar to detectron2's ImageList but without the detectron2 dependency.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tensor: torch.Tensor, image_sizes: List[Tuple[int, int]]):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor: Batched images tensor [B, C, H, W]\n",
        "            image_sizes: List of (height, width) tuples for each image\n",
        "        \"\"\"\n",
        "        self.tensor = tensor\n",
        "        self.image_sizes = image_sizes\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        return self.tensor.device\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_sizes)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        \"\"\"Get image at index with its original size.\"\"\"\n",
        "        h, w = self.image_sizes[idx]\n",
        "        return self.tensor[idx, :, :h, :w]\n",
        "\n",
        "    @staticmethod\n",
        "    def from_tensors(\n",
        "        tensors: List[torch.Tensor],\n",
        "        size_divisibility: int = 0,\n",
        "        pad_value: float = 0.0\n",
        "    ) -> \"ImageList\":\n",
        "        \"\"\"\n",
        "        Create ImageList from list of image tensors.\n",
        "\n",
        "        Args:\n",
        "            tensors: List of image tensors\n",
        "            size_divisibility: If > 0, pad dimensions to be divisible by this\n",
        "            pad_value: Value to use for padding\n",
        "\n",
        "        Returns:\n",
        "            ImageList instance\n",
        "        \"\"\"\n",
        "        assert len(tensors) > 0\n",
        "        assert isinstance(tensors[0], torch.Tensor)\n",
        "\n",
        "        image_sizes = [(tensor.shape[-2], tensor.shape[-1]) for tensor in tensors]\n",
        "\n",
        "        # Calculate target size\n",
        "        max_size = list(max(zip(*[tensor.shape for tensor in tensors])))\n",
        "\n",
        "        if size_divisibility > 0:\n",
        "            # Pad to be divisible by size_divisibility\n",
        "            stride = size_divisibility\n",
        "            max_size[-2] = (max_size[-2] + stride - 1) // stride * stride\n",
        "            max_size[-1] = (max_size[-1] + stride - 1) // stride * stride\n",
        "\n",
        "        # Create batched tensor\n",
        "        batch_shape = [len(tensors)] + list(tensors[0].shape[:-2]) + list(max_size[-2:])\n",
        "        device = tensors[0].device\n",
        "        batched_tensor = torch.full(batch_shape, pad_value, dtype=tensors[0].dtype, device=device)\n",
        "\n",
        "        # Copy images\n",
        "        for i, tensor in enumerate(tensors):\n",
        "            batched_tensor[i, ..., :tensor.shape[-2], :tensor.shape[-1]].copy_(tensor)\n",
        "\n",
        "        return ImageList(batched_tensor, image_sizes)\n",
        "\n",
        "\n",
        "def multi_apply(func, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Apply function to multiple inputs and concatenate outputs.\n",
        "\n",
        "    Useful for applying the same function to multiple levels of features.\n",
        "    \"\"\"\n",
        "    pfunc = partial(func, **kwargs) if kwargs else func\n",
        "    map_results = map(pfunc, *args)\n",
        "    return tuple(map(list, zip(*map_results)))\n",
        "\n",
        "\n",
        "def inverse_sigmoid(x: torch.Tensor, eps: float = 1e-5) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Inverse of sigmoid function.\n",
        "\n",
        "    Args:\n",
        "        x: Input tensor (should be in range [0, 1])\n",
        "        eps: Small epsilon to avoid numerical issues\n",
        "\n",
        "    Returns:\n",
        "        Inverse sigmoid of x\n",
        "    \"\"\"\n",
        "    x = x.clamp(min=0, max=1)\n",
        "    x1 = x.clamp(min=eps)\n",
        "    x2 = (1 - x).clamp(min=eps)\n",
        "    return torch.log(x1 / x2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Losses"
      ],
      "metadata": {
        "id": "kwyKaxYeGVzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "\n",
        "class HungarianMatcher(nn.Module):\n",
        "    \"\"\"\n",
        "    This class computes an assignment between the targets and the predictions.\n",
        "\n",
        "    For efficiency reasons, the targets don't include the no_object class. Because of this,\n",
        "    in general, there are more predictions than targets. We do a 1-to-1 matching,\n",
        "    and the remaining predictions are un-matched (and thus treated as background).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        cost_class: float = 1.0,\n",
        "        cost_mask: float = 1.0,\n",
        "        cost_dice: float = 1.0,\n",
        "        num_points: int = 0,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            cost_class: Weight of the classification cost\n",
        "            cost_mask: Weight of the sigmoid CE cost for masks\n",
        "            cost_dice: Weight of the dice cost for masks\n",
        "            num_points: Number of points to sample for matching\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.cost_class = cost_class\n",
        "        self.cost_mask = cost_mask\n",
        "        self.cost_dice = cost_dice\n",
        "        self.num_points = num_points\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(\n",
        "        self,\n",
        "        outputs: Dict[str, torch.Tensor],\n",
        "        targets: List[Dict[str, torch.Tensor]]\n",
        "    ) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"\n",
        "        Performs the matching.\n",
        "\n",
        "        Args:\n",
        "            outputs: Model outputs containing:\n",
        "                - \"pred_logits\": [batch_size, num_queries, num_classes]\n",
        "                - \"pred_masks\": [batch_size, num_queries, H, W]\n",
        "\n",
        "            targets: List of target dictionaries (one per batch item) containing:\n",
        "                - \"labels\": [num_target_masks] containing class labels\n",
        "                - \"masks\": [num_target_masks, H, W] containing target masks\n",
        "\n",
        "        Returns:\n",
        "            List of tuples (index_i, index_j) where:\n",
        "                - index_i: indices of selected predictions (in order)\n",
        "                - index_j: indices of corresponding selected targets (in order)\n",
        "            For each batch element, it returns (index_i, index_j) such that:\n",
        "                - len(index_i) = len(index_j) = min(num_queries, num_target_masks)\n",
        "                - For all k: prediction index_i[k] is matched to target index_j[k]\n",
        "        \"\"\"\n",
        "        batch_size, num_queries = outputs[\"pred_logits\"].shape[:2]\n",
        "\n",
        "        # We flatten to compute the cost matrices in a batch\n",
        "        out_prob = outputs[\"pred_logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
        "        out_mask = outputs[\"pred_masks\"].flatten(0, 1)  # [batch_size * num_queries, H, W]\n",
        "\n",
        "        # Concatenate target labels and masks\n",
        "        tgt_ids = torch.cat([v[\"labels\"] for v in targets])\n",
        "        tgt_mask = torch.cat([v[\"masks\"] for v in targets])\n",
        "\n",
        "        # Compute classification cost\n",
        "        # out_prob: [batch_size * num_queries, num_classes]\n",
        "        # tgt_ids: [total_num_targets_in_batch]\n",
        "        cost_class = -out_prob[:, tgt_ids]\n",
        "\n",
        "        # Compute mask costs\n",
        "        if self.num_points > 0:\n",
        "            # Sample points for efficiency\n",
        "            out_mask_sampled = self._sample_points_for_matching(out_mask, self.num_points)\n",
        "            tgt_mask_sampled = self._sample_points_for_matching(tgt_mask, self.num_points)\n",
        "        else:\n",
        "            out_mask_sampled = out_mask.flatten(1)\n",
        "            tgt_mask_sampled = tgt_mask.flatten(1)\n",
        "\n",
        "        # Compute sigmoid cross-entropy cost\n",
        "        with torch.cuda.amp.autocast(enabled=False):\n",
        "            out_mask_sampled = out_mask_sampled.float()\n",
        "            tgt_mask_sampled = tgt_mask_sampled.float()\n",
        "\n",
        "            # Sigmoid CE cost\n",
        "            cost_mask = self._batch_sigmoid_ce_cost(out_mask_sampled, tgt_mask_sampled)\n",
        "\n",
        "            # Dice cost\n",
        "            cost_dice = self._batch_dice_cost(out_mask_sampled, tgt_mask_sampled)\n",
        "\n",
        "        # Final cost matrix\n",
        "        C = self.cost_mask * cost_mask + self.cost_class * cost_class + self.cost_dice * cost_dice\n",
        "        C = C.reshape(batch_size, num_queries, -1)\n",
        "\n",
        "        # Perform Hungarian matching for each batch element\n",
        "        indices = []\n",
        "        sizes = [len(v[\"labels\"]) for v in targets]\n",
        "\n",
        "        for i, c in enumerate(C.cpu()):\n",
        "            if sizes[i] == 0:\n",
        "                # No targets in this batch element\n",
        "                indices.append((\n",
        "                    torch.empty(0, dtype=torch.long),\n",
        "                    torch.empty(0, dtype=torch.long)\n",
        "                ))\n",
        "            else:\n",
        "                # Extract cost matrix for this batch element\n",
        "                c_i = c[:, sum(sizes[:i]):sum(sizes[:i+1])]\n",
        "\n",
        "                # Solve assignment problem\n",
        "                row_ind, col_ind = linear_sum_assignment(c_i.numpy())\n",
        "\n",
        "                indices.append((\n",
        "                    torch.as_tensor(row_ind, dtype=torch.long),\n",
        "                    torch.as_tensor(col_ind, dtype=torch.long)\n",
        "                ))\n",
        "\n",
        "        return [(torch.as_tensor(i, dtype=torch.long), torch.as_tensor(j, dtype=torch.long))\n",
        "                for i, j in indices]\n",
        "\n",
        "    def _sample_points_for_matching(self, masks: torch.Tensor, num_points: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Uniformly sample points from masks for matching.\n",
        "\n",
        "        Args:\n",
        "            masks: [N, H, W] tensor of masks\n",
        "            num_points: Number of points to sample\n",
        "\n",
        "        Returns:\n",
        "            [N, num_points] tensor of sampled mask values\n",
        "        \"\"\"\n",
        "        N, H, W = masks.shape\n",
        "\n",
        "        # Generate random point coordinates\n",
        "        # Use same points for all masks to ensure fair comparison\n",
        "        with torch.no_grad():\n",
        "            points_idx = torch.randperm(H * W, device=masks.device)[:num_points]\n",
        "\n",
        "        # Sample mask values at these points\n",
        "        masks_flat = masks.flatten(1)  # [N, H*W]\n",
        "        masks_sampled = masks_flat[:, points_idx]  # [N, num_points]\n",
        "\n",
        "        return masks_sampled\n",
        "\n",
        "    def _batch_sigmoid_ce_cost(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute sigmoid cross-entropy cost between all pairs.\n",
        "\n",
        "        Args:\n",
        "            inputs: [N, P] tensor of predicted logits\n",
        "            targets: [M, P] tensor of target values (0 or 1)\n",
        "\n",
        "        Returns:\n",
        "            [N, M] cost matrix\n",
        "        \"\"\"\n",
        "        # Compute pairwise sigmoid CE\n",
        "        # Note: This is memory efficient for reasonable batch sizes\n",
        "        N, P = inputs.shape\n",
        "        M = targets.shape[0]\n",
        "\n",
        "        # Expand dimensions for broadcasting\n",
        "        inputs_exp = inputs.unsqueeze(1)  # [N, 1, P]\n",
        "        targets_exp = targets.unsqueeze(0)  # [1, M, P]\n",
        "\n",
        "        # Compute sigmoid CE for all pairs\n",
        "        ce = F.binary_cross_entropy_with_logits(\n",
        "            inputs_exp.expand(N, M, P),\n",
        "            targets_exp.expand(N, M, P),\n",
        "            reduction='none'\n",
        "        )\n",
        "\n",
        "        return ce.mean(dim=2)  # [N, M]\n",
        "\n",
        "    def _batch_dice_cost(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute dice cost between all pairs.\n",
        "\n",
        "        Args:\n",
        "            inputs: [N, P] tensor of predicted logits\n",
        "            targets: [M, P] tensor of target values (0 or 1)\n",
        "\n",
        "        Returns:\n",
        "            [N, M] cost matrix\n",
        "        \"\"\"\n",
        "        N, P = inputs.shape\n",
        "        M = targets.shape[0]\n",
        "\n",
        "        # Apply sigmoid to get probabilities\n",
        "        inputs = inputs.sigmoid()\n",
        "\n",
        "        # Expand dimensions for broadcasting\n",
        "        inputs_exp = inputs.unsqueeze(1)  # [N, 1, P]\n",
        "        targets_exp = targets.unsqueeze(0)  # [1, M, P]\n",
        "\n",
        "        # Compute dice coefficient for all pairs\n",
        "        numerator = 2 * (inputs_exp * targets_exp).sum(dim=2)  # [N, M]\n",
        "        denominator = inputs_exp.sum(dim=2) + targets_exp.sum(dim=2)  # [N, M]\n",
        "\n",
        "        # Dice cost (1 - dice coefficient)\n",
        "        dice_cost = 1 - (numerator + 1) / (denominator + 1)\n",
        "\n",
        "        return dice_cost  # [N, M]"
      ],
      "metadata": {
        "id": "M0b_n7cYGRjh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def dice_loss(\n",
        "    inputs: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    num_masks: float,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute the DICE loss, similar to generalized IOU for masks.\n",
        "\n",
        "    Args:\n",
        "        inputs: Predicted masks [N, H, W]\n",
        "        targets: Target masks [N, H, W]\n",
        "        num_masks: Number of masks to normalize the loss\n",
        "\n",
        "    Returns:\n",
        "        Normalized dice loss\n",
        "    \"\"\"\n",
        "    inputs = inputs.sigmoid()\n",
        "    inputs = inputs.flatten(1)\n",
        "    targets = targets.flatten(1)\n",
        "\n",
        "    numerator = 2 * (inputs * targets).sum(-1)\n",
        "    denominator = inputs.sum(-1) + targets.sum(-1)\n",
        "    loss = 1 - (numerator + 1) / (denominator + 1)\n",
        "\n",
        "    return loss.sum() / num_masks\n",
        "\n",
        "\n",
        "def sigmoid_ce_loss(\n",
        "    inputs: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    num_masks: float,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Sigmoid cross-entropy loss.\n",
        "\n",
        "    Args:\n",
        "        inputs: Predicted logits [N, H, W]\n",
        "        targets: Target masks [N, H, W]\n",
        "        num_masks: Number of masks to normalize the loss\n",
        "\n",
        "    Returns:\n",
        "        Normalized sigmoid CE loss\n",
        "    \"\"\"\n",
        "    loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
        "    return loss.mean(1).sum() / num_masks\n",
        "\n",
        "\n",
        "class SetCriterion(nn.Module):\n",
        "    \"\"\"\n",
        "    This class computes the loss for Mask2Former.\n",
        "    The process happens in two steps:\n",
        "        1) We compute hungarian assignment between ground truth masks and predictions\n",
        "        2) We supervise each pair of matched ground-truth / prediction (supervise class and mask)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int,\n",
        "        matcher: HungarianMatcher,\n",
        "        weight_dict: Dict[str, float],\n",
        "        eos_coef: float,\n",
        "        losses: List[str],\n",
        "        num_points: int = 12544,\n",
        "        oversample_ratio: float = 3.0,\n",
        "        importance_sample_ratio: float = 0.75,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_classes: Number of object categories\n",
        "            matcher: Module to compute assignment between targets and predictions\n",
        "            weight_dict: Dict containing weights for different losses\n",
        "            eos_coef: Relative classification weight of the no-object category\n",
        "            losses: List of losses to be applied\n",
        "            num_points: Number of points sampled for point losses\n",
        "            oversample_ratio: Oversampling ratio for point sampling\n",
        "            importance_sample_ratio: Ratio of points sampled via importance sampling\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.matcher = matcher\n",
        "        self.weight_dict = weight_dict\n",
        "        self.eos_coef = eos_coef\n",
        "        self.losses = losses\n",
        "\n",
        "        # Point sampling parameters\n",
        "        self.num_points = num_points\n",
        "        self.oversample_ratio = oversample_ratio\n",
        "        self.importance_sample_ratio = importance_sample_ratio\n",
        "\n",
        "        # Create no-object class weight\n",
        "        empty_weight = torch.ones(self.num_classes + 1)\n",
        "        empty_weight[-1] = self.eos_coef\n",
        "        self.register_buffer(\"empty_weight\", empty_weight)\n",
        "\n",
        "    def loss_labels(\n",
        "        self,\n",
        "        outputs: Dict[str, torch.Tensor],\n",
        "        targets: List[Dict[str, torch.Tensor]],\n",
        "        indices: List[Tuple[torch.Tensor, torch.Tensor]],\n",
        "        num_masks: int,\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Classification loss.\"\"\"\n",
        "        assert \"pred_logits\" in outputs\n",
        "        src_logits = outputs[\"pred_logits\"]\n",
        "\n",
        "        # Prepare target classes\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(targets, indices)])\n",
        "        target_classes = torch.full(\n",
        "            src_logits.shape[:2],\n",
        "            self.num_classes,\n",
        "            dtype=torch.int64,\n",
        "            device=src_logits.device\n",
        "        )\n",
        "        target_classes[idx] = target_classes_o\n",
        "\n",
        "        # Compute cross-entropy loss\n",
        "        loss_ce = F.cross_entropy(\n",
        "            src_logits.transpose(1, 2),\n",
        "            target_classes,\n",
        "            self.empty_weight\n",
        "        )\n",
        "\n",
        "        losses = {\"loss_ce\": loss_ce}\n",
        "        return losses\n",
        "\n",
        "    def loss_masks(\n",
        "        self,\n",
        "        outputs: Dict[str, torch.Tensor],\n",
        "        targets: List[Dict[str, torch.Tensor]],\n",
        "        indices: List[Tuple[torch.Tensor, torch.Tensor]],\n",
        "        num_masks: int,\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Compute mask losses (dice + sigmoid CE).\"\"\"\n",
        "        assert \"pred_masks\" in outputs\n",
        "\n",
        "        src_idx = self._get_src_permutation_idx(indices)\n",
        "        tgt_idx = self._get_tgt_permutation_idx(indices)\n",
        "\n",
        "        src_masks = outputs[\"pred_masks\"]\n",
        "        src_masks = src_masks[src_idx]\n",
        "\n",
        "        # Get target masks\n",
        "        target_masks = [t[\"masks\"] for t in targets]\n",
        "        target_masks, valid = nested_tensor_from_tensor_list(target_masks).decompose()\n",
        "        target_masks = target_masks.to(src_masks)\n",
        "        target_masks = target_masks[tgt_idx]\n",
        "\n",
        "        # Point sampling for efficient computation\n",
        "        with torch.no_grad():\n",
        "            point_coords = self._sample_points(\n",
        "                src_masks,\n",
        "                target_masks,\n",
        "                self.oversample_ratio,\n",
        "                self.importance_sample_ratio\n",
        "            )\n",
        "\n",
        "        # Sample points from masks\n",
        "        src_masks_sampled = self._point_sample(src_masks, point_coords, align_corners=False)\n",
        "        target_masks_sampled = self._point_sample(target_masks, point_coords, align_corners=False)\n",
        "\n",
        "        src_masks_sampled = src_masks_sampled.flatten(1)\n",
        "        target_masks_sampled = target_masks_sampled.flatten(1)\n",
        "\n",
        "        losses = {\n",
        "            \"loss_mask\": sigmoid_ce_loss(src_masks_sampled, target_masks_sampled, num_masks),\n",
        "            \"loss_dice\": dice_loss(src_masks_sampled, target_masks_sampled, num_masks),\n",
        "        }\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        \"\"\"Get source permutation indices.\"\"\"\n",
        "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def _get_tgt_permutation_idx(self, indices):\n",
        "        \"\"\"Get target permutation indices.\"\"\"\n",
        "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
        "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
        "        return batch_idx, tgt_idx\n",
        "\n",
        "    def _sample_points(\n",
        "        self,\n",
        "        src_masks: torch.Tensor,\n",
        "        tgt_masks: torch.Tensor,\n",
        "        oversample_ratio: float,\n",
        "        importance_sample_ratio: float,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Sample points for mask loss computation.\n",
        "\n",
        "        Combines uniform sampling with importance sampling based on uncertainty.\n",
        "        \"\"\"\n",
        "        N, H, W = src_masks.shape\n",
        "\n",
        "        num_points = int(self.num_points * oversample_ratio)\n",
        "        num_uncertain_points = int(importance_sample_ratio * num_points)\n",
        "        num_random_points = num_points - num_uncertain_points\n",
        "\n",
        "        # Uniform random sampling\n",
        "        point_coords = torch.rand(N, num_random_points, 2, device=src_masks.device)\n",
        "        point_coords = point_coords * torch.tensor([W, H], device=src_masks.device) - 0.5\n",
        "\n",
        "        # Importance sampling based on prediction uncertainty\n",
        "        if num_uncertain_points > 0:\n",
        "            point_logits = src_masks.flatten(1)\n",
        "            point_uncertainties = -torch.abs(point_logits)\n",
        "\n",
        "            # Sample points with high uncertainty\n",
        "            _, uncertain_idx = torch.topk(point_uncertainties, num_uncertain_points, dim=1)\n",
        "\n",
        "            # Convert flat indices to 2D coordinates\n",
        "            uncertain_coords = torch.stack([\n",
        "                uncertain_idx % W,\n",
        "                uncertain_idx // W\n",
        "            ], dim=-1).float()\n",
        "\n",
        "            # Add small random noise\n",
        "            uncertain_coords += torch.rand_like(uncertain_coords) - 0.5\n",
        "\n",
        "            # Combine uniform and importance samples\n",
        "            point_coords = torch.cat([point_coords, uncertain_coords], dim=1)\n",
        "\n",
        "        return point_coords\n",
        "\n",
        "    def _point_sample(\n",
        "        self,\n",
        "        input: torch.Tensor,\n",
        "        point_coords: torch.Tensor,\n",
        "        align_corners: bool = False,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Sample features at specified point coordinates.\n",
        "\n",
        "        Args:\n",
        "            input: Input tensor [N, H, W]\n",
        "            point_coords: Point coordinates [N, P, 2]\n",
        "            align_corners: Whether to align corners in grid_sample\n",
        "\n",
        "        Returns:\n",
        "            Sampled features [N, P, 1]\n",
        "        \"\"\"\n",
        "        N, H, W = input.shape\n",
        "\n",
        "        # Normalize coordinates to [-1, 1]\n",
        "        point_coords = point_coords.clone()\n",
        "        point_coords[..., 0] = 2.0 * point_coords[..., 0] / W - 1.0\n",
        "        point_coords[..., 1] = 2.0 * point_coords[..., 1] / H - 1.0\n",
        "\n",
        "        # Add batch dimension and sample\n",
        "        output = F.grid_sample(\n",
        "            input.unsqueeze(1),\n",
        "            point_coords.unsqueeze(1),\n",
        "            mode=\"bilinear\",\n",
        "            padding_mode=\"zeros\",\n",
        "            align_corners=align_corners,\n",
        "        )\n",
        "\n",
        "        return output.squeeze(1).transpose(1, 2)\n",
        "\n",
        "    def get_loss(\n",
        "        self,\n",
        "        loss: str,\n",
        "        outputs: Dict[str, torch.Tensor],\n",
        "        targets: List[Dict[str, torch.Tensor]],\n",
        "        indices: List[Tuple[torch.Tensor, torch.Tensor]],\n",
        "        num_masks: int,\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Dispatch to specific loss function.\"\"\"\n",
        "        loss_map = {\n",
        "            \"labels\": self.loss_labels,\n",
        "            \"masks\": self.loss_masks,\n",
        "        }\n",
        "\n",
        "        assert loss in loss_map, f\"Loss {loss} not supported\"\n",
        "        return loss_map[loss](outputs, targets, indices, num_masks)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        outputs: Dict[str, torch.Tensor],\n",
        "        targets: List[Dict[str, torch.Tensor]],\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Compute all losses.\n",
        "\n",
        "        Args:\n",
        "            outputs: Model outputs containing:\n",
        "                - pred_logits: [B, num_queries, num_classes]\n",
        "                - pred_masks: [B, num_queries, H, W]\n",
        "                - aux_outputs: List of intermediate outputs\n",
        "            targets: List of target dictionaries containing:\n",
        "                - labels: [num_objects] tensor of class labels\n",
        "                - masks: [num_objects, H, W] tensor of target masks\n",
        "\n",
        "        Returns:\n",
        "            Dict of losses\n",
        "        \"\"\"\n",
        "        # Retrieve outputs\n",
        "        outputs_without_aux = {k: v for k, v in outputs.items() if k != \"aux_outputs\"}\n",
        "\n",
        "        # Match predictions with targets\n",
        "        indices = self.matcher(outputs_without_aux, targets)\n",
        "\n",
        "        # Count total number of masks across batch\n",
        "        num_masks = sum(len(t[\"labels\"]) for t in targets)\n",
        "        num_masks = torch.as_tensor([num_masks], dtype=torch.float, device=outputs[\"pred_logits\"].device)\n",
        "\n",
        "        # Ensure at least 1 to avoid division by zero\n",
        "        num_masks = torch.clamp(num_masks, min=1)\n",
        "\n",
        "        # Compute all requested losses\n",
        "        losses = {}\n",
        "        for loss in self.losses:\n",
        "            losses.update(self.get_loss(loss, outputs, targets, indices, num_masks))\n",
        "\n",
        "        # Process auxiliary outputs (intermediate layer predictions)\n",
        "        if \"aux_outputs\" in outputs:\n",
        "            for i, aux_outputs in enumerate(outputs[\"aux_outputs\"]):\n",
        "                indices = self.matcher(aux_outputs, targets)\n",
        "                for loss in self.losses:\n",
        "                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_masks)\n",
        "                    l_dict = {k + f\"_{i}\": v for k, v in l_dict.items()}\n",
        "                    losses.update(l_dict)\n",
        "\n",
        "        return losses"
      ],
      "metadata": {
        "id": "Wdh6tLIUGsbA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models\n"
      ],
      "metadata": {
        "id": "fFjK_7oEG3zK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "\n",
        "\n",
        "class Mask2Former(nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch-native implementation of Mask2Former.\n",
        "\n",
        "    Main architecture for mask classification, supporting:\n",
        "    - Instance segmentation\n",
        "    - Semantic segmentation\n",
        "    - Panoptic segmentation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone: nn.Module,\n",
        "        pixel_decoder: nn.Module,\n",
        "        transformer_decoder: nn.Module,\n",
        "        num_queries: int = 100,\n",
        "        object_mask_threshold: float = 0.0,\n",
        "        overlap_threshold: float = 0.8,\n",
        "        metadata: Optional[Dict] = None,\n",
        "        pixel_mean: Tuple[float, float, float] = (0.485, 0.456, 0.406),\n",
        "        pixel_std: Tuple[float, float, float] = (0.229, 0.224, 0.225),\n",
        "        semantic_on: bool = True,\n",
        "        instance_on: bool = False,\n",
        "        panoptic_on: bool = False,\n",
        "        test_topk_per_image: int = 100,\n",
        "        num_classes: int = 133,  # COCO panoptic classes\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            backbone: Backbone network (e.g., ResNet)\n",
        "            pixel_decoder: Pixel decoder module\n",
        "            transformer_decoder: Transformer decoder for mask prediction\n",
        "            num_queries: Number of query embeddings\n",
        "            object_mask_threshold: Threshold for object masks\n",
        "            overlap_threshold: Threshold for mask overlap\n",
        "            metadata: Dataset metadata\n",
        "            pixel_mean: Mean values for input normalization\n",
        "            pixel_std: Std values for input normalization\n",
        "            semantic_on: Enable semantic segmentation\n",
        "            instance_on: Enable instance segmentation\n",
        "            panoptic_on: Enable panoptic segmentation\n",
        "            test_topk_per_image: Number of top predictions per image\n",
        "            num_classes: Number of classes\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.backbone = backbone\n",
        "        self.pixel_decoder = pixel_decoder\n",
        "        self.predictor = transformer_decoder\n",
        "\n",
        "        self.num_queries = num_queries\n",
        "        self.object_mask_threshold = object_mask_threshold\n",
        "        self.overlap_threshold = overlap_threshold\n",
        "        self.metadata = metadata if metadata is not None else {}\n",
        "\n",
        "        # Normalization parameters\n",
        "        self.register_buffer(\"pixel_mean\", torch.tensor(pixel_mean).view(-1, 1, 1))\n",
        "        self.register_buffer(\"pixel_std\", torch.tensor(pixel_std).view(-1, 1, 1))\n",
        "\n",
        "        # Task flags\n",
        "        self.semantic_on = semantic_on\n",
        "        self.instance_on = instance_on\n",
        "        self.panoptic_on = panoptic_on\n",
        "\n",
        "        self.test_topk_per_image = test_topk_per_image\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, images: torch.Tensor, targets: Optional[List[Dict]] = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images: Batched images tensor [B, C, H, W]\n",
        "            targets: Ground truth targets (for training)\n",
        "\n",
        "        Returns:\n",
        "            dict: Model outputs containing:\n",
        "                - 'pred_logits': Classification logits [B, num_queries, num_classes]\n",
        "                - 'pred_masks': Mask predictions [B, num_queries, H, W]\n",
        "                - 'aux_outputs': Auxiliary outputs from intermediate layers\n",
        "        \"\"\"\n",
        "        # Normalize images\n",
        "        images = (images - self.pixel_mean) / self.pixel_std\n",
        "\n",
        "        # Extract features from backbone\n",
        "        features = self.backbone(images)\n",
        "\n",
        "        # Generate mask features and positional embeddings\n",
        "        mask_features, transformer_encoder_features, multi_scale_features = self.pixel_decoder(features)\n",
        "\n",
        "        # Predict masks and classes\n",
        "        predictions = self.predictor(\n",
        "            multi_scale_features,\n",
        "            mask_features,\n",
        "            targets=targets\n",
        "        )\n",
        "\n",
        "        # Postprocess for inference\n",
        "        if not self.training:\n",
        "            # Process predictions for the specific task\n",
        "            if self.semantic_on:\n",
        "                predictions = self.semantic_inference(predictions, images)\n",
        "            elif self.instance_on:\n",
        "                predictions = self.instance_inference(predictions, images)\n",
        "            elif self.panoptic_on:\n",
        "                predictions = self.panoptic_inference(predictions, images)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def semantic_inference(self, outputs: Dict, images: torch.Tensor) -> Dict:\n",
        "        \"\"\"Semantic segmentation inference.\"\"\"\n",
        "        mask_cls = outputs[\"pred_logits\"]\n",
        "        mask_pred = outputs[\"pred_masks\"]\n",
        "\n",
        "        # For semantic segmentation, we average over all queries\n",
        "        mask_cls = F.softmax(mask_cls, dim=-1)[..., :-1]  # Remove no-object class\n",
        "        mask_pred = mask_pred.sigmoid()\n",
        "        semseg = torch.einsum(\"bqc,bqhw->bchw\", mask_cls, mask_pred)\n",
        "\n",
        "        # Resize to original image size\n",
        "        B, _, H_img, W_img = images.shape\n",
        "        semseg = F.interpolate(\n",
        "            semseg,\n",
        "            size=(H_img, W_img),\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False\n",
        "        )\n",
        "\n",
        "        return {\"sem_seg\": semseg}\n",
        "\n",
        "    def instance_inference(self, outputs: Dict, images: torch.Tensor) -> List[Dict]:\n",
        "        \"\"\"Instance segmentation inference.\"\"\"\n",
        "        mask_cls = outputs[\"pred_logits\"]\n",
        "        mask_pred = outputs[\"pred_masks\"]\n",
        "\n",
        "        # Get image size\n",
        "        B, _, H_img, W_img = images.shape\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # Process each image in the batch\n",
        "        for i in range(B):\n",
        "            scores, labels = mask_cls[i].max(-1)\n",
        "            mask_pred_i = mask_pred[i]\n",
        "\n",
        "            # Remove background predictions\n",
        "            keep = labels != self.num_classes\n",
        "            scores = scores[keep]\n",
        "            labels = labels[keep]\n",
        "            mask_pred_i = mask_pred_i[keep]\n",
        "\n",
        "            # Apply threshold\n",
        "            keep = scores > self.object_mask_threshold\n",
        "            scores = scores[keep]\n",
        "            labels = labels[keep]\n",
        "            mask_pred_i = mask_pred_i[keep]\n",
        "\n",
        "            # Get top-k predictions\n",
        "            if len(scores) > self.test_topk_per_image:\n",
        "                indices = torch.argsort(scores, descending=True)[:self.test_topk_per_image]\n",
        "                scores = scores[indices]\n",
        "                labels = labels[indices]\n",
        "                mask_pred_i = mask_pred_i[indices]\n",
        "\n",
        "            # Resize masks to original size\n",
        "            mask_pred_i = F.interpolate(\n",
        "                mask_pred_i.unsqueeze(0),\n",
        "                size=(H_img, W_img),\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False\n",
        "            ).squeeze(0)\n",
        "\n",
        "            # Binarize masks\n",
        "            mask_pred_i = mask_pred_i.sigmoid() > 0.5\n",
        "\n",
        "            results.append({\n",
        "                \"instances\": {\n",
        "                    \"pred_masks\": mask_pred_i,\n",
        "                    \"scores\": scores,\n",
        "                    \"pred_classes\": labels\n",
        "                }\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def panoptic_inference(self, outputs: Dict, images: torch.Tensor) -> List[Dict]:\n",
        "        \"\"\"Panoptic segmentation inference.\"\"\"\n",
        "        # Panoptic segmentation combines instance and semantic results\n",
        "        # This is a simplified version - full implementation would need\n",
        "        # more sophisticated merging of instance and stuff predictions\n",
        "\n",
        "        mask_cls = outputs[\"pred_logits\"]\n",
        "        mask_pred = outputs[\"pred_masks\"]\n",
        "\n",
        "        B, _, H_img, W_img = images.shape\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for i in range(B):\n",
        "            scores, labels = mask_cls[i].max(-1)\n",
        "            mask_pred_i = mask_pred[i].sigmoid()\n",
        "\n",
        "            # Keep high-scoring predictions\n",
        "            keep = scores > self.object_mask_threshold\n",
        "            scores = scores[keep]\n",
        "            labels = labels[keep]\n",
        "            mask_pred_i = mask_pred_i[keep]\n",
        "\n",
        "            # Resize masks\n",
        "            mask_pred_i = F.interpolate(\n",
        "                mask_pred_i.unsqueeze(0),\n",
        "                size=(H_img, W_img),\n",
        "                mode=\"bilinear\",\n",
        "                align_corners=False\n",
        "            ).squeeze(0)\n",
        "\n",
        "            # Generate panoptic segmentation\n",
        "            panoptic_seg = torch.zeros((H_img, W_img), dtype=torch.int64, device=mask_pred_i.device)\n",
        "            segments_info = []\n",
        "\n",
        "            current_segment_id = 1\n",
        "\n",
        "            # Process each prediction\n",
        "            for j in range(len(scores)):\n",
        "                mask = mask_pred_i[j] > 0.5\n",
        "\n",
        "                # Skip empty masks\n",
        "                if not mask.any():\n",
        "                    continue\n",
        "\n",
        "                # Check overlap with existing segments\n",
        "                overlap = panoptic_seg > 0\n",
        "                if (mask & overlap).sum() / mask.sum() > self.overlap_threshold:\n",
        "                    continue\n",
        "\n",
        "                # Add to panoptic segmentation\n",
        "                panoptic_seg[mask] = current_segment_id\n",
        "\n",
        "                segments_info.append({\n",
        "                    \"id\": current_segment_id,\n",
        "                    \"category_id\": labels[j].item(),\n",
        "                    \"score\": scores[j].item()\n",
        "                })\n",
        "\n",
        "                current_segment_id += 1\n",
        "\n",
        "            results.append({\n",
        "                \"panoptic_seg\": (panoptic_seg, segments_info)\n",
        "            })\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "S3ro7pjeG0g7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usage"
      ],
      "metadata": {
        "id": "64BC60q-HLaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dummy_backbone():\n",
        "    \"\"\"Create a simple backbone that returns multi-scale features.\"\"\"\n",
        "    class DummyBackbone(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.conv1 = nn.Conv2d(3, 64, 3, stride=2, padding=1)\n",
        "            self.conv2 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n",
        "            self.conv3 = nn.Conv2d(128, 256, 3, stride=2, padding=1)\n",
        "            self.conv4 = nn.Conv2d(256, 512, 3, stride=2, padding=1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            # Return dict of features at different scales\n",
        "            features = {}\n",
        "            x = self.conv1(x)\n",
        "            features['res2'] = x  # 1/4 scale\n",
        "            x = self.conv2(x)\n",
        "            features['res3'] = x  # 1/8 scale\n",
        "            x = self.conv3(x)\n",
        "            features['res4'] = x  # 1/16 scale\n",
        "            x = self.conv4(x)\n",
        "            features['res5'] = x  # 1/32 scale\n",
        "            return features\n",
        "\n",
        "    return DummyBackbone()\n",
        "\n",
        "\n",
        "def create_dummy_pixel_decoder():\n",
        "    \"\"\"Create a simple pixel decoder.\"\"\"\n",
        "    class DummyPixelDecoder(nn.Module):\n",
        "        def __init__(self, in_channels=[64, 128, 256, 512], mask_dim=256):\n",
        "            super().__init__()\n",
        "            self.mask_dim = mask_dim\n",
        "            # Simple FPN-like structure\n",
        "            self.lateral_convs = nn.ModuleList([\n",
        "                nn.Conv2d(in_c, mask_dim, 1) for in_c in in_channels\n",
        "            ])\n",
        "            self.output_conv = nn.Conv2d(mask_dim, mask_dim, 3, padding=1)\n",
        "\n",
        "        def forward(self, features):\n",
        "            # Simple feature pyramid\n",
        "            feature_list = [features['res2'], features['res3'], features['res4'], features['res5']]\n",
        "\n",
        "            # Upsample and combine features\n",
        "            mask_features = None\n",
        "            for i in range(len(feature_list)-1, -1, -1):\n",
        "                lateral = self.lateral_convs[i](feature_list[i])\n",
        "                if mask_features is None:\n",
        "                    mask_features = lateral\n",
        "                else:\n",
        "                    # Upsample and add\n",
        "                    mask_features = F.interpolate(\n",
        "                        mask_features,\n",
        "                        size=lateral.shape[-2:],\n",
        "                        mode='bilinear',\n",
        "                        align_corners=False\n",
        "                    )\n",
        "                    mask_features = mask_features + lateral\n",
        "\n",
        "            mask_features = self.output_conv(mask_features)\n",
        "\n",
        "            # For transformer: return mask features and multi-scale features\n",
        "            multi_scale_features = [lateral_conv(feat)\n",
        "                                  for lateral_conv, feat in zip(self.lateral_convs, feature_list)]\n",
        "\n",
        "            return mask_features, None, multi_scale_features\n",
        "\n",
        "    return DummyPixelDecoder()\n",
        "\n",
        "\n",
        "def create_dummy_transformer_decoder():\n",
        "    \"\"\"Create a simple transformer decoder.\"\"\"\n",
        "    class DummyTransformerDecoder(nn.Module):\n",
        "        def __init__(self, hidden_dim=256, num_queries=100, num_classes=80):\n",
        "            super().__init__()\n",
        "            self.num_queries = num_queries\n",
        "            self.hidden_dim = hidden_dim\n",
        "\n",
        "            # Query embeddings\n",
        "            self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
        "\n",
        "            # Simple transformer layer\n",
        "            self.transformer = nn.TransformerDecoderLayer(\n",
        "                d_model=hidden_dim,\n",
        "                nhead=8,\n",
        "                dim_feedforward=2048,\n",
        "                dropout=0.1,\n",
        "                batch_first=True\n",
        "            )\n",
        "\n",
        "            # Output projections\n",
        "            self.class_embed = nn.Linear(hidden_dim, num_classes + 1)  # +1 for no-object\n",
        "            self.mask_embed = nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, hidden_dim)\n",
        "            )\n",
        "\n",
        "        def forward(self, multi_scale_features, mask_features, targets=None):\n",
        "            B = mask_features.shape[0]\n",
        "\n",
        "            # Get query embeddings\n",
        "            query_embeds = self.query_embed.weight.unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "            # Simple attention on mask features (placeholder)\n",
        "            # In real implementation, this would involve proper cross-attention\n",
        "            mask_h, mask_w = mask_features.shape[-2:]\n",
        "            mask_features_flat = mask_features.flatten(2).transpose(1, 2)  # [B, H*W, C]\n",
        "\n",
        "            # Transformer decoder\n",
        "            outputs = self.transformer(query_embeds, mask_features_flat)\n",
        "\n",
        "            # Predict classes\n",
        "            outputs_class = self.class_embed(outputs)\n",
        "\n",
        "            # Predict masks\n",
        "            mask_embeds = self.mask_embed(outputs)\n",
        "            # Simple dot product with mask features\n",
        "            outputs_mask = torch.einsum(\"bqc,bhwc->bqhw\",\n",
        "                                      mask_embeds,\n",
        "                                      mask_features.permute(0, 2, 3, 1))\n",
        "\n",
        "            return {\n",
        "                \"pred_logits\": outputs_class,\n",
        "                \"pred_masks\": outputs_mask,\n",
        "                \"aux_outputs\": []  # No auxiliary outputs in this simple version\n",
        "            }\n",
        "\n",
        "    return DummyTransformerDecoder()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Example of how to use the PyTorch-native Mask2Former.\"\"\"\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Create model components\n",
        "    backbone = create_dummy_backbone()\n",
        "    pixel_decoder = create_dummy_pixel_decoder()\n",
        "    transformer_decoder = create_dummy_transformer_decoder()\n",
        "\n",
        "    # Create Mask2Former model\n",
        "    model = Mask2Former(\n",
        "        backbone=backbone,\n",
        "        pixel_decoder=pixel_decoder,\n",
        "        transformer_decoder=transformer_decoder,\n",
        "        num_queries=100,\n",
        "        num_classes=80,  # COCO classes\n",
        "        instance_on=True,  # Enable instance segmentation\n",
        "        semantic_on=False,\n",
        "        panoptic_on=False,\n",
        "    ).to(device)\n",
        "\n",
        "    # Create criterion\n",
        "    matcher = HungarianMatcher(\n",
        "        cost_class=2.0,\n",
        "        cost_mask=5.0,\n",
        "        cost_dice=5.0,\n",
        "        num_points=12544,  # Sample points for efficiency\n",
        "    )\n",
        "\n",
        "    weight_dict = {\n",
        "        \"loss_ce\": 2.0,\n",
        "        \"loss_mask\": 5.0,\n",
        "        \"loss_dice\": 5.0,\n",
        "    }\n",
        "\n",
        "    criterion = SetCriterion(\n",
        "        num_classes=80,\n",
        "        matcher=matcher,\n",
        "        weight_dict=weight_dict,\n",
        "        eos_coef=0.1,  # Coefficient for no-object class\n",
        "        losses=[\"labels\", \"masks\"],\n",
        "    ).to(device)\n",
        "\n",
        "    # Create dummy input\n",
        "    batch_size = 2\n",
        "    images = torch.randn(batch_size, 3, 640, 640).to(device)\n",
        "\n",
        "    # Create dummy targets for training\n",
        "    targets = []\n",
        "    for i in range(batch_size):\n",
        "        num_objects = 3  # 3 objects per image\n",
        "        target = {\n",
        "            \"labels\": torch.randint(0, 80, (num_objects,)).to(device),\n",
        "            \"masks\": torch.rand(num_objects, 640, 640).to(device) > 0.5,  # Binary masks\n",
        "        }\n",
        "        targets.append(target)\n",
        "\n",
        "    # Training mode\n",
        "    model.train()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images, targets)\n",
        "\n",
        "    # Calculate losses\n",
        "    losses = criterion(outputs, targets)\n",
        "\n",
        "    # Total loss\n",
        "    total_loss = sum(losses[k] * weight_dict.get(k, 1.0)\n",
        "                    for k in losses if k in weight_dict)\n",
        "\n",
        "    print(\"Training losses:\")\n",
        "    for k, v in losses.items():\n",
        "        print(f\"  {k}: {v.item():.4f}\")\n",
        "    print(f\"Total loss: {total_loss.item():.4f}\")\n",
        "\n",
        "    # Inference mode\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(images)\n",
        "\n",
        "    print(f\"\\nInference output:\")\n",
        "    print(f\"  Number of predictions: {len(predictions)}\")\n",
        "    if isinstance(predictions, list) and len(predictions) > 0:\n",
        "        for i, pred in enumerate(predictions):\n",
        "            if \"instances\" in pred:\n",
        "                inst = pred[\"instances\"]\n",
        "                print(f\"  Image {i}: {len(inst['scores'])} instances detected\")\n",
        "                print(f\"    - Classes: {inst['pred_classes'].tolist()}\")\n",
        "                print(f\"    - Scores: {[f'{s:.3f}' for s in inst['scores'].tolist()]}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "yPIOeKBLHC7r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xd0mjXk-HXr5",
        "outputId": "da1e4bc0-4502-4e46-bc21-ba05429e7376"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-986715518.py:87: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=False):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training losses:\n",
            "  loss_ce: 4.3159\n",
            "  loss_mask: 1.0485\n",
            "  loss_dice: 0.4889\n",
            "Total loss: 16.3188\n",
            "\n",
            "Inference output:\n",
            "  Number of predictions: 2\n",
            "  Image 0: 99 instances detected\n",
            "    - Classes: [18, 36, 78, 77, 5, 36, 55, 6, 3, 51, 65, 51, 70, 50, 7, 17, 6, 3, 59, 28, 65, 40, 7, 66, 35, 37, 68, 37, 5, 12, 33, 46, 7, 59, 6, 21, 50, 23, 3, 33, 25, 49, 56, 27, 33, 65, 44, 12, 52, 64, 40, 50, 9, 50, 21, 19, 33, 6, 75, 14, 7, 64, 44, 24, 59, 19, 66, 55, 19, 35, 33, 67, 40, 70, 59, 7, 70, 2, 50, 30, 37, 65, 42, 1, 17, 27, 41, 63, 65, 5, 6, 66, 35, 61, 37, 43, 75, 0, 2]\n",
            "    - Scores: ['1.644', '1.259', '1.809', '1.978', '1.580', '1.480', '1.162', '1.043', '1.728', '1.400', '1.262', '1.053', '2.016', '1.346', '1.654', '1.376', '1.625', '1.000', '1.317', '1.035', '1.166', '1.434', '1.493', '1.471', '1.370', '1.518', '2.222', '1.972', '1.489', '1.460', '1.152', '1.635', '1.340', '1.400', '1.839', '1.634', '1.895', '1.183', '1.484', '1.508', '1.790', '1.260', '1.944', '0.965', '1.171', '1.652', '1.468', '1.362', '1.725', '1.649', '1.284', '1.024', '1.432', '1.340', '1.095', '0.999', '1.257', '1.601', '1.319', '1.060', '1.296', '1.324', '1.443', '0.967', '1.037', '1.458', '1.341', '1.431', '1.339', '1.470', '1.417', '1.541', '1.398', '1.710', '1.315', '1.509', '1.773', '1.496', '2.091', '1.383', '2.036', '1.132', '1.536', '1.760', '1.335', '1.390', '1.586', '1.850', '1.421', '1.212', '1.069', '1.330', '1.225', '1.286', '1.112', '1.367', '1.021', '1.027', '2.276']\n",
            "  Image 1: 99 instances detected\n",
            "    - Classes: [18, 36, 78, 77, 5, 36, 55, 6, 3, 51, 65, 51, 70, 50, 7, 17, 6, 3, 59, 28, 65, 40, 7, 66, 35, 37, 68, 37, 5, 12, 33, 46, 7, 59, 6, 21, 50, 23, 3, 33, 25, 49, 56, 27, 33, 65, 44, 12, 52, 64, 40, 50, 9, 50, 21, 19, 33, 6, 75, 14, 7, 64, 44, 24, 59, 19, 66, 55, 19, 35, 33, 67, 40, 70, 59, 7, 70, 2, 50, 30, 37, 65, 42, 1, 17, 27, 41, 63, 65, 5, 6, 66, 35, 61, 37, 43, 75, 0, 2]\n",
            "    - Scores: ['1.645', '1.258', '1.811', '1.982', '1.581', '1.479', '1.159', '1.043', '1.728', '1.399', '1.261', '1.052', '2.017', '1.346', '1.655', '1.376', '1.624', '0.999', '1.319', '1.034', '1.163', '1.435', '1.493', '1.470', '1.373', '1.517', '2.219', '1.972', '1.489', '1.460', '1.153', '1.635', '1.340', '1.400', '1.840', '1.633', '1.895', '1.188', '1.482', '1.509', '1.792', '1.261', '1.947', '0.966', '1.171', '1.649', '1.467', '1.363', '1.724', '1.650', '1.285', '1.024', '1.429', '1.341', '1.093', '0.997', '1.256', '1.601', '1.319', '1.064', '1.296', '1.326', '1.441', '0.969', '1.037', '1.454', '1.341', '1.428', '1.334', '1.471', '1.417', '1.540', '1.399', '1.710', '1.317', '1.510', '1.773', '1.497', '2.090', '1.381', '2.034', '1.129', '1.537', '1.754', '1.334', '1.391', '1.585', '1.849', '1.420', '1.212', '1.069', '1.328', '1.228', '1.286', '1.110', '1.368', '1.021', '1.028', '2.277']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WA1ca3xxHY3q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}