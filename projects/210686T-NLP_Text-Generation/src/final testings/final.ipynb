{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "397276a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-4.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Using cached pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /home/cse/llm-env/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (0.35.1)\n",
      "Requirement already satisfied: packaging in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in /home/cse/llm-env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/cse/llm-env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/cse/llm-env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/cse/llm-env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/cse/llm-env/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/cse/llm-env/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/cse/llm-env/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.1.10)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/cse/llm-env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cse/llm-env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/cse/llm-env/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/cse/llm-env/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/cse/llm-env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/cse/llm-env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/cse/llm-env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-4.2.0-py3-none-any.whl (506 kB)\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading aiohttp-3.13.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Using cached pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, frozenlist, dill, attrs, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/13\u001b[0m [datasets]/13\u001b[0m [datasets]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.0 aiosignal-1.4.0 attrs-25.4.0 datasets-4.2.0 dill-0.4.0 frozenlist-1.8.0 multidict-6.7.0 multiprocess-0.70.16 propcache-0.4.1 pyarrow-21.0.0 xxhash-3.6.0 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe9e0e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/cse/llm-env/lib/python3.12/site-packages (4.56.2)\n",
      "Requirement already satisfied: torch in /home/cse/llm-env/lib/python3.12/site-packages (2.8.0)\n",
      "Collecting rouge_score\n",
      "  Using cached rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting bert_score\n",
      "  Using cached bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: filelock in /home/cse/llm-env/lib/python3.12/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/cse/llm-env/lib/python3.12/site-packages (from transformers) (0.35.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/cse/llm-env/lib/python3.12/site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/cse/llm-env/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/cse/llm-env/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/cse/llm-env/lib/python3.12/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /home/cse/llm-env/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/cse/llm-env/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/cse/llm-env/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/cse/llm-env/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/cse/llm-env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/cse/llm-env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/cse/llm-env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /home/cse/llm-env/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/cse/llm-env/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/cse/llm-env/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/cse/llm-env/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/cse/llm-env/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/cse/llm-env/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/cse/llm-env/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/cse/llm-env/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/cse/llm-env/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/cse/llm-env/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/cse/llm-env/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/cse/llm-env/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/cse/llm-env/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/cse/llm-env/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/cse/llm-env/lib/python3.12/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/cse/llm-env/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/cse/llm-env/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/cse/llm-env/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/cse/llm-env/lib/python3.12/site-packages (from torch) (3.4.0)\n",
      "Collecting absl-py (from rouge_score)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting nltk (from rouge_score)\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/cse/llm-env/lib/python3.12/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /home/cse/llm-env/lib/python3.12/site-packages (from bert_score) (2.3.2)\n",
      "Collecting matplotlib (from bert_score)\n",
      "  Downloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/cse/llm-env/lib/python3.12/site-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/cse/llm-env/lib/python3.12/site-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/cse/llm-env/lib/python3.12/site-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/cse/llm-env/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/cse/llm-env/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->bert_score)\n",
      "  Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->bert_score)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->bert_score)\n",
      "  Using cached fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->bert_score)\n",
      "  Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pillow>=8 in /home/cse/llm-env/lib/python3.12/site-packages (from matplotlib->bert_score) (11.3.0)\n",
      "Collecting pyparsing>=3 (from matplotlib->bert_score)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting click (from nltk->rouge_score)\n",
      "  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib (from nltk->rouge_score)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/cse/llm-env/lib/python3.12/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cse/llm-env/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cse/llm-env/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cse/llm-env/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Using cached bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Using cached click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24987 sha256=d72bcf02caacd425fdeb586bdbb564ba5f4263e5ea56773b10b902be6c37b073\n",
      "  Stored in directory: /home/cse/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: pyparsing, kiwisolver, joblib, fonttools, cycler, contourpy, click, absl-py, nltk, matplotlib, rouge_score, bert_score\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/12\u001b[0m [bert_score]2\u001b[0m [matplotlib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 bert_score-0.3.13 click-8.3.0 contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.10.7 nltk-3.9.2 pyparsing-3.2.5 rouge_score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch rouge_score bert_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99df5a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting summac\n",
      "  Using cached summac-0.0.4-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: torch in /home/cse/llm-env/lib/python3.12/site-packages (from summac) (2.8.0)\n",
      "Requirement already satisfied: transformers>=4.24.0 in /home/cse/llm-env/lib/python3.12/site-packages (from summac) (4.56.2)\n",
      "Requirement already satisfied: nltk>=3.6.6 in /home/cse/llm-env/lib/python3.12/site-packages (from summac) (3.9.2)\n",
      "Collecting huggingface-hub<=0.17.0 (from summac)\n",
      "  Using cached huggingface_hub-0.17.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting sentencepiece (from summac)\n",
      "  Using cached sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting protobuf (from summac)\n",
      "  Using cached protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: filelock in /home/cse/llm-env/lib/python3.12/site-packages (from huggingface-hub<=0.17.0->summac) (3.19.1)\n",
      "Requirement already satisfied: fsspec in /home/cse/llm-env/lib/python3.12/site-packages (from huggingface-hub<=0.17.0->summac) (2025.9.0)\n",
      "Requirement already satisfied: requests in /home/cse/llm-env/lib/python3.12/site-packages (from huggingface-hub<=0.17.0->summac) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/cse/llm-env/lib/python3.12/site-packages (from huggingface-hub<=0.17.0->summac) (4.67.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/cse/llm-env/lib/python3.12/site-packages (from huggingface-hub<=0.17.0->summac) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/cse/llm-env/lib/python3.12/site-packages (from huggingface-hub<=0.17.0->summac) (4.15.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/cse/llm-env/lib/python3.12/site-packages (from huggingface-hub<=0.17.0->summac) (25.0)\n",
      "Requirement already satisfied: click in /home/cse/llm-env/lib/python3.12/site-packages (from nltk>=3.6.6->summac) (8.3.0)\n",
      "Requirement already satisfied: joblib in /home/cse/llm-env/lib/python3.12/site-packages (from nltk>=3.6.6->summac) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/cse/llm-env/lib/python3.12/site-packages (from nltk>=3.6.6->summac) (2025.9.18)\n",
      "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting transformers>=4.24.0 (from summac)\n",
      "  Using cached transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n",
      "  Using cached transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "  Using cached transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\n",
      "  Using cached transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
      "  Using cached transformers-4.55.3-py3-none-any.whl.metadata (41 kB)\n",
      "  Using cached transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
      "  Using cached transformers-4.55.1-py3-none-any.whl.metadata (41 kB)\n",
      "INFO: pip is still looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
      "  Using cached transformers-4.54.1-py3-none-any.whl.metadata (41 kB)\n",
      "  Using cached transformers-4.54.0-py3-none-any.whl.metadata (41 kB)\n",
      "  Using cached transformers-4.53.3-py3-none-any.whl.metadata (40 kB)\n",
      "  Using cached transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n",
      "  Using cached transformers-4.53.0-py3-none-any.whl.metadata (39 kB)\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "  Using cached transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
      "  Using cached transformers-4.52.2-py3-none-any.whl.metadata (40 kB)\n",
      "  Using cached transformers-4.52.1-py3-none-any.whl.metadata (38 kB)\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "  Using cached transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
      "  Using cached transformers-4.51.1-py3-none-any.whl.metadata (38 kB)\n",
      "  Using cached transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n",
      "  Using cached transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
      "  Using cached transformers-4.50.2-py3-none-any.whl.metadata (39 kB)\n",
      "  Using cached transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n",
      "  Using cached transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.45.0-py3-none-any.whl.metadata (44 kB)\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.44.1-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.43.2-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.43.1-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.43.0-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.42.2-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.42.1-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.42.0-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
      "  Using cached transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
      "  Using cached transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "  Using cached transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
      "  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
      "  Using cached transformers-4.39.2-py3-none-any.whl.metadata (134 kB)\n",
      "  Using cached transformers-4.39.1-py3-none-any.whl.metadata (134 kB)\n",
      "  Using cached transformers-4.39.0-py3-none-any.whl.metadata (134 kB)\n",
      "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "  Using cached transformers-4.38.1-py3-none-any.whl.metadata (131 kB)\n",
      "  Using cached transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n",
      "  Using cached transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "  Using cached transformers-4.37.1-py3-none-any.whl.metadata (129 kB)\n",
      "  Using cached transformers-4.37.0-py3-none-any.whl.metadata (129 kB)\n",
      "  Using cached transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "  Using cached transformers-4.36.1-py3-none-any.whl.metadata (126 kB)\n",
      "  Using cached transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
      "  Using cached transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/cse/llm-env/lib/python3.12/site-packages (from transformers>=4.24.0->summac) (2.3.3)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers>=4.24.0->summac)\n",
      "  Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/cse/llm-env/lib/python3.12/site-packages (from transformers>=4.24.0->summac) (0.6.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/cse/llm-env/lib/python3.12/site-packages (from requests->huggingface-hub<=0.17.0->summac) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cse/llm-env/lib/python3.12/site-packages (from requests->huggingface-hub<=0.17.0->summac) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cse/llm-env/lib/python3.12/site-packages (from requests->huggingface-hub<=0.17.0->summac) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cse/llm-env/lib/python3.12/site-packages (from requests->huggingface-hub<=0.17.0->summac) (2025.8.3)\n",
      "Requirement already satisfied: setuptools in /home/cse/llm-env/lib/python3.12/site-packages (from torch->summac) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/cse/llm-env/lib/python3.12/site-packages (from torch->summac) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/cse/llm-env/lib/python3.12/site-packages (from torch->summac) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/cse/llm-env/lib/python3.12/site-packages (from torch->summac) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/cse/llm-env/lib/python3.12/site-packages (from torch->summac) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/cse/llm-env/lib/python3.12/site-packages (from torch->summac) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/cse/llm-env/lib/python3.12/site-packages (from torch->summac) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/cse/llm-env/lib/python3.12/site-packages (from torch->summac) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/cse/llm-env/lib/python3.12/site-packages (from torch->summac) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/cse/llm-env/lib/python3.12/site-packages (from torch->summac) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/cse/llm-env/lib/python3.12/site-packages (from torch->summac) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/cse/llm-env/lib/python3.12/site-packages (from torch->summac) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/cse/llm-env/lib/python3.12/site-packages (from torch->summac) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/cse/llm-env/lib/python3.12/site-packages (from torch->summac) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/cse/llm-env/lib/python3.12/site-packages (from torch->summac) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/cse/llm-env/lib/python3.12/site-packages (from torch->summac) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/cse/llm-env/lib/python3.12/site-packages (from torch->summac) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/cse/llm-env/lib/python3.12/site-packages (from torch->summac) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/cse/llm-env/lib/python3.12/site-packages (from torch->summac) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/cse/llm-env/lib/python3.12/site-packages (from sympy>=1.13.3->torch->summac) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/cse/llm-env/lib/python3.12/site-packages (from jinja2->torch->summac) (3.0.3)\n",
      "Using cached summac-0.0.4-py3-none-any.whl (30 kB)\n",
      "Using cached huggingface_hub-0.17.0-py3-none-any.whl (294 kB)\n",
      "Using cached transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "Using cached sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "Installing collected packages: sentencepiece, protobuf, huggingface-hub, tokenizers, transformers, summac\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.35.1\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.35.1:\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.35.1\n",
      "\u001b[2K  Attempting uninstall: tokenizersm\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Found existing installation: tokenizers 0.22.1━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Uninstalling tokenizers-0.22.1:m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [huggingface-hub]\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.22.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [huggingface-hub]\n",
      "\u001b[2K  Attempting uninstall: transformers━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Found existing installation: transformers 4.56.2━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Uninstalling transformers-4.56.2:━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [huggingface-hub]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.56.2━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [huggingface-hub]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [summac]2m4/6\u001b[0m [transformers]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "accelerate 1.10.1 requires huggingface_hub>=0.21.0, but you have huggingface-hub 0.17.0 which is incompatible.\n",
      "diffusers 0.35.1 requires huggingface-hub>=0.34.0, but you have huggingface-hub 0.17.0 which is incompatible.\n",
      "datasets 4.2.0 requires huggingface-hub<2.0,>=0.25.0, but you have huggingface-hub 0.17.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.17.0 protobuf-6.32.1 sentencepiece-0.2.1 summac-0.0.4 tokenizers-0.15.2 transformers-4.35.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install summac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cfb982d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/cse/llm-env/lib/python3.12/site-packages (4.2.0)\n",
      "Requirement already satisfied: huggingface_hub in /home/cse/llm-env/lib/python3.12/site-packages (0.17.0)\n",
      "Collecting huggingface_hub\n",
      "  Using cached huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /home/cse/llm-env/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: packaging in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/cse/llm-env/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/cse/llm-env/lib/python3.12/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/cse/llm-env/lib/python3.12/site-packages (from huggingface_hub) (1.1.10)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/cse/llm-env/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.0)\n",
      "Requirement already satisfied: anyio in /home/cse/llm-env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/cse/llm-env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/cse/llm-env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/cse/llm-env/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /home/cse/llm-env/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/cse/llm-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/cse/llm-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/cse/llm-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/cse/llm-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/cse/llm-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/cse/llm-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/cse/llm-env/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/cse/llm-env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cse/llm-env/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/cse/llm-env/lib/python3.12/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/cse/llm-env/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/cse/llm-env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/cse/llm-env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/cse/llm-env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "Installing collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.17.0\n",
      "    Uninstalling huggingface-hub-0.17.0:\n",
      "      Successfully uninstalled huggingface-hub-0.17.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "summac 0.0.4 requires huggingface-hub<=0.17.0, but you have huggingface-hub 0.35.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface_hub-0.35.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00fa2e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==2.19.1\n",
      "  Using cached datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting huggingface_hub==0.24.6\n",
      "  Using cached huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /home/cse/llm-env/lib/python3.12/site-packages (from datasets==2.19.1) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/cse/llm-env/lib/python3.12/site-packages (from datasets==2.19.1) (2.3.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/cse/llm-env/lib/python3.12/site-packages (from datasets==2.19.1) (21.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets==2.19.1)\n",
      "  Using cached pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.19.1)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/cse/llm-env/lib/python3.12/site-packages (from datasets==2.19.1) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/cse/llm-env/lib/python3.12/site-packages (from datasets==2.19.1) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/cse/llm-env/lib/python3.12/site-packages (from datasets==2.19.1) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/cse/llm-env/lib/python3.12/site-packages (from datasets==2.19.1) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /home/cse/llm-env/lib/python3.12/site-packages (from datasets==2.19.1) (0.70.16)\n",
      "Collecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.1)\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in /home/cse/llm-env/lib/python3.12/site-packages (from datasets==2.19.1) (3.13.0)\n",
      "Requirement already satisfied: packaging in /home/cse/llm-env/lib/python3.12/site-packages (from datasets==2.19.1) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/cse/llm-env/lib/python3.12/site-packages (from datasets==2.19.1) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/cse/llm-env/lib/python3.12/site-packages (from huggingface_hub==0.24.6) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/cse/llm-env/lib/python3.12/site-packages (from aiohttp->datasets==2.19.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/cse/llm-env/lib/python3.12/site-packages (from aiohttp->datasets==2.19.1) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/cse/llm-env/lib/python3.12/site-packages (from aiohttp->datasets==2.19.1) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/cse/llm-env/lib/python3.12/site-packages (from aiohttp->datasets==2.19.1) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/cse/llm-env/lib/python3.12/site-packages (from aiohttp->datasets==2.19.1) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/cse/llm-env/lib/python3.12/site-packages (from aiohttp->datasets==2.19.1) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/cse/llm-env/lib/python3.12/site-packages (from aiohttp->datasets==2.19.1) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/cse/llm-env/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.19.1) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/cse/llm-env/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.19.1) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cse/llm-env/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.19.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cse/llm-env/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.19.1) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/cse/llm-env/lib/python3.12/site-packages (from pandas->datasets==2.19.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/cse/llm-env/lib/python3.12/site-packages (from pandas->datasets==2.19.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/cse/llm-env/lib/python3.12/site-packages (from pandas->datasets==2.19.1) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/cse/llm-env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.1) (1.17.0)\n",
      "Using cached datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "Using cached huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Using cached pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
      "Installing collected packages: pyarrow-hotfix, fsspec, dill, huggingface_hub, datasets\n",
      "\u001b[2K  Attempting uninstall: fsspec\n",
      "\u001b[2K    Found existing installation: fsspec 2025.9.0\n",
      "\u001b[2K    Uninstalling fsspec-2025.9.0:\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.9.0\n",
      "\u001b[2K  Attempting uninstall: dill\n",
      "\u001b[2K    Found existing installation: dill 0.4.0\n",
      "\u001b[2K    Uninstalling dill-0.4.0:\n",
      "\u001b[2K      Successfully uninstalled dill-0.4.0\n",
      "\u001b[2K  Attempting uninstall: huggingface_hub\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.35.3\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.35.3:\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.35.3\n",
      "\u001b[2K  Attempting uninstall: datasets\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [huggingface_hub]\n",
      "\u001b[2K    Found existing installation: datasets 4.2.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [huggingface_hub]\n",
      "\u001b[2K    Uninstalling datasets-4.2.0:\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [huggingface_hub]\n",
      "\u001b[2K      Successfully uninstalled datasets-4.2.00m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/5\u001b[0m [huggingface_hub]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [datasets]4/5\u001b[0m [datasets]ub]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "summac 0.0.4 requires huggingface-hub<=0.17.0, but you have huggingface-hub 0.24.6 which is incompatible.\n",
      "diffusers 0.35.1 requires huggingface-hub>=0.34.0, but you have huggingface-hub 0.24.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.19.1 dill-0.3.8 fsspec-2024.3.1 huggingface_hub-0.24.6 pyarrow-hotfix-0.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets==2.19.1 huggingface_hub==0.24.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0570e909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse/llm-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/cse/llm-env/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Transformers, datasets, evaluation, and utilities\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Evaluation libraries\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "from summac.model_summac import SummaCConv\n",
    "from summac.model_summac import SummaCZS\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7689bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 2.21kB [00:00, 4.40MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the GovReport dataset\n",
    "ds = load_dataset(\"ccdv/govreport-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "818e2284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse/llm-env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/cse/llm-env/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/cse/llm-env/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/cse/llm-env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load LED model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/led-base-16384\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec3aa117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge(preds, refs):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    for pred, ref in zip(preds, refs):\n",
    "        score = scorer.score(ref, pred)\n",
    "        for k in scores:\n",
    "            scores[k].append(score[k].fmeasure)\n",
    "    avg_scores = {k: float(np.mean(scores[k])) for k in scores}\n",
    "    print(\"ROUGE scores:\")\n",
    "    for k, v in avg_scores.items():\n",
    "        print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ae7e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bert_score(preds, refs):\n",
    "    P, R, F1 = bert_score(preds, refs, lang=\"en\")\n",
    "    scores = {\n",
    "        \"precision\": float(P.mean()),\n",
    "        \"recall\": float(R.mean()),\n",
    "        \"f1\": float(F1.mean())\n",
    "    }\n",
    "    print(\"BERTScore:\")\n",
    "    for k, v in scores.items():\n",
    "        print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fd70e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summac.model_summac import SummaCConv\n",
    "from summac.model_summac import SummaCZS\n",
    "\n",
    "# Load SummaC Zero-shot model (recommended for factuality)\n",
    "summac_model = SummaCZS(granularity=\"sentence\", model_name=\"vitc\", device=device)\n",
    "# summac_model.load_from_pretrained()\n",
    "\n",
    "def compute_factuality(preds, refs):\n",
    "    results = summac_model.score(\n",
    "        sources=refs,\n",
    "        summaries=preds,\n",
    "        batch_size=4,\n",
    "        nli_batch_size=32,\n",
    "        return_prob=True,\n",
    "        return_sentence_level=False\n",
    "    )\n",
    "    # Return average probability as factuality score\n",
    "    return float(np.mean([r[\"prob\"] for r in results]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b42f9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def led_summarize(document, max_input_length=16384, max_output_length=1024):\n",
    "    inputs = tokenizer(\n",
    "        document,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length\n",
    "    )\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    global_attention_mask = torch.zeros_like(attention_mask)\n",
    "    global_attention_mask[:, 0] = 1  # global attention on first token\n",
    "\n",
    "    summary_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        global_attention_mask=global_attention_mask,\n",
    "        max_length=max_output_length,\n",
    "        num_beams=4\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73279fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: summarize and evaluate on first 100 test samples\n",
    "N = 100\n",
    "documents = [ds['validation'][i]['report'] for i in range(N)]\n",
    "references = [ds['validation'][i]['summary'] for i in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fabe728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Baseline Summarization: 100%|██████████| 100/100 [07:47<00:00,  4.68s/it]\n"
     ]
    }
   ],
   "source": [
    "baseline_summaries = []\n",
    "for doc in tqdm(documents, desc=\"Baseline Summarization\"):\n",
    "    baseline_summaries.append(led_summarize(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b00b1172",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"baseline_summaries_val.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(baseline_summaries, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e566d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"baseline_summaries_val.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    baseline_summaries = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16f233e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_token_counts(texts, tokenizer, label=\"Input\"):\n",
    "    \"\"\"\n",
    "    Prints the token count for each text in the list.\n",
    "    \"\"\"\n",
    "    for i, text in enumerate(texts):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        print(f\"{label} {i+1}: {len(tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4920f393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_token_count(texts, tokenizer):\n",
    "    \"\"\"\n",
    "    Returns the average token count for a list of texts.\n",
    "    \"\"\"\n",
    "    total_tokens = sum(len(tokenizer.tokenize(text)) for text in texts)\n",
    "    return total_tokens / len(texts) if texts else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c22ff60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19300 > 16384). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9145.07\n"
     ]
    }
   ],
   "source": [
    "print(average_token_count(documents, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a60f4a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19300 > 16384). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report 1: 2204 tokens\n",
      "Report 2: 4639 tokens\n",
      "Report 3: 4287 tokens\n",
      "Report 4: 6353 tokens\n",
      "Report 5: 8092 tokens\n",
      "Report 6: 6941 tokens\n",
      "Report 7: 10364 tokens\n",
      "Report 8: 8434 tokens\n",
      "Report 9: 7722 tokens\n",
      "Report 10: 6630 tokens\n",
      "Report 11: 3038 tokens\n",
      "Report 12: 6997 tokens\n",
      "Report 13: 13846 tokens\n",
      "Report 14: 5558 tokens\n",
      "Report 15: 7736 tokens\n",
      "Report 16: 19300 tokens\n",
      "Report 17: 6576 tokens\n",
      "Report 18: 25889 tokens\n",
      "Report 19: 10745 tokens\n",
      "Report 20: 8499 tokens\n",
      "Report 21: 3869 tokens\n",
      "Report 22: 8527 tokens\n",
      "Report 23: 2448 tokens\n",
      "Report 24: 7085 tokens\n",
      "Report 25: 19090 tokens\n",
      "Report 26: 14331 tokens\n",
      "Report 27: 8483 tokens\n",
      "Report 28: 7889 tokens\n",
      "Report 29: 3624 tokens\n",
      "Report 30: 9715 tokens\n",
      "Report 31: 2471 tokens\n",
      "Report 32: 12382 tokens\n",
      "Report 33: 9859 tokens\n",
      "Report 34: 6193 tokens\n",
      "Report 35: 4682 tokens\n",
      "Report 36: 10321 tokens\n",
      "Report 37: 12944 tokens\n",
      "Report 38: 7725 tokens\n",
      "Report 39: 13224 tokens\n",
      "Report 40: 13767 tokens\n",
      "Report 41: 10273 tokens\n",
      "Report 42: 9044 tokens\n",
      "Report 43: 14587 tokens\n",
      "Report 44: 3178 tokens\n",
      "Report 45: 10667 tokens\n",
      "Report 46: 2848 tokens\n",
      "Report 47: 27256 tokens\n",
      "Report 48: 8912 tokens\n",
      "Report 49: 5056 tokens\n",
      "Report 50: 8624 tokens\n",
      "Report 51: 20496 tokens\n",
      "Report 52: 18037 tokens\n",
      "Report 53: 12216 tokens\n",
      "Report 54: 9043 tokens\n",
      "Report 55: 5654 tokens\n",
      "Report 56: 13102 tokens\n",
      "Report 57: 8072 tokens\n",
      "Report 58: 7504 tokens\n",
      "Report 59: 7889 tokens\n",
      "Report 60: 16680 tokens\n",
      "Report 61: 5521 tokens\n",
      "Report 62: 15759 tokens\n",
      "Report 63: 7051 tokens\n",
      "Report 64: 7141 tokens\n",
      "Report 65: 6135 tokens\n",
      "Report 66: 13945 tokens\n",
      "Report 67: 6037 tokens\n",
      "Report 68: 11759 tokens\n",
      "Report 69: 10448 tokens\n",
      "Report 70: 3217 tokens\n",
      "Report 71: 7320 tokens\n",
      "Report 72: 13439 tokens\n",
      "Report 73: 2608 tokens\n",
      "Report 74: 2703 tokens\n",
      "Report 75: 2204 tokens\n",
      "Report 76: 21451 tokens\n",
      "Report 77: 2275 tokens\n",
      "Report 78: 13710 tokens\n",
      "Report 79: 8078 tokens\n",
      "Report 80: 9496 tokens\n",
      "Report 81: 8491 tokens\n",
      "Report 82: 9938 tokens\n",
      "Report 83: 5775 tokens\n",
      "Report 84: 5487 tokens\n",
      "Report 85: 7559 tokens\n",
      "Report 86: 7563 tokens\n",
      "Report 87: 16074 tokens\n",
      "Report 88: 8675 tokens\n",
      "Report 89: 5605 tokens\n",
      "Report 90: 5822 tokens\n",
      "Report 91: 4945 tokens\n",
      "Report 92: 6481 tokens\n",
      "Report 93: 4659 tokens\n",
      "Report 94: 14833 tokens\n",
      "Report 95: 3118 tokens\n",
      "Report 96: 5140 tokens\n",
      "Report 97: 20559 tokens\n",
      "Report 98: 11038 tokens\n",
      "Report 99: 3944 tokens\n",
      "Report 100: 16887 tokens\n",
      "Summary 1: 632 tokens\n",
      "Summary 2: 697 tokens\n",
      "Summary 3: 737 tokens\n",
      "Summary 4: 734 tokens\n",
      "Summary 5: 868 tokens\n",
      "Summary 6: 604 tokens\n",
      "Summary 7: 545 tokens\n",
      "Summary 8: 970 tokens\n",
      "Summary 9: 672 tokens\n",
      "Summary 10: 744 tokens\n",
      "Summary 11: 660 tokens\n",
      "Summary 12: 682 tokens\n",
      "Summary 13: 873 tokens\n",
      "Summary 14: 829 tokens\n",
      "Summary 15: 851 tokens\n",
      "Summary 16: 976 tokens\n",
      "Summary 17: 810 tokens\n",
      "Summary 18: 784 tokens\n",
      "Summary 19: 723 tokens\n",
      "Summary 20: 561 tokens\n",
      "Summary 21: 891 tokens\n",
      "Summary 22: 867 tokens\n",
      "Summary 23: 875 tokens\n",
      "Summary 24: 653 tokens\n",
      "Summary 25: 915 tokens\n",
      "Summary 26: 864 tokens\n",
      "Summary 27: 653 tokens\n",
      "Summary 28: 882 tokens\n",
      "Summary 29: 444 tokens\n",
      "Summary 30: 729 tokens\n",
      "Summary 31: 690 tokens\n",
      "Summary 32: 568 tokens\n",
      "Summary 33: 883 tokens\n",
      "Summary 34: 715 tokens\n",
      "Summary 35: 843 tokens\n",
      "Summary 36: 843 tokens\n",
      "Summary 37: 819 tokens\n",
      "Summary 38: 665 tokens\n",
      "Summary 39: 727 tokens\n",
      "Summary 40: 643 tokens\n",
      "Summary 41: 870 tokens\n",
      "Summary 42: 641 tokens\n",
      "Summary 43: 785 tokens\n",
      "Summary 44: 703 tokens\n",
      "Summary 45: 639 tokens\n",
      "Summary 46: 553 tokens\n",
      "Summary 47: 863 tokens\n",
      "Summary 48: 629 tokens\n",
      "Summary 49: 608 tokens\n",
      "Summary 50: 578 tokens\n",
      "Summary 51: 1354 tokens\n",
      "Summary 52: 656 tokens\n",
      "Summary 53: 706 tokens\n",
      "Summary 54: 676 tokens\n",
      "Summary 55: 625 tokens\n",
      "Summary 56: 892 tokens\n",
      "Summary 57: 695 tokens\n",
      "Summary 58: 603 tokens\n",
      "Summary 59: 897 tokens\n",
      "Summary 60: 779 tokens\n",
      "Summary 61: 842 tokens\n",
      "Summary 62: 788 tokens\n",
      "Summary 63: 602 tokens\n",
      "Summary 64: 788 tokens\n",
      "Summary 65: 641 tokens\n",
      "Summary 66: 650 tokens\n",
      "Summary 67: 931 tokens\n",
      "Summary 68: 720 tokens\n",
      "Summary 69: 633 tokens\n",
      "Summary 70: 912 tokens\n",
      "Summary 71: 723 tokens\n",
      "Summary 72: 647 tokens\n",
      "Summary 73: 800 tokens\n",
      "Summary 74: 448 tokens\n",
      "Summary 75: 747 tokens\n",
      "Summary 76: 708 tokens\n",
      "Summary 77: 796 tokens\n",
      "Summary 78: 662 tokens\n",
      "Summary 79: 622 tokens\n",
      "Summary 80: 773 tokens\n",
      "Summary 81: 728 tokens\n",
      "Summary 82: 696 tokens\n",
      "Summary 83: 763 tokens\n",
      "Summary 84: 633 tokens\n",
      "Summary 85: 690 tokens\n",
      "Summary 86: 893 tokens\n",
      "Summary 87: 615 tokens\n",
      "Summary 88: 805 tokens\n",
      "Summary 89: 737 tokens\n",
      "Summary 90: 881 tokens\n",
      "Summary 91: 594 tokens\n",
      "Summary 92: 680 tokens\n",
      "Summary 93: 564 tokens\n",
      "Summary 94: 686 tokens\n",
      "Summary 95: 497 tokens\n",
      "Summary 96: 648 tokens\n",
      "Summary 97: 596 tokens\n",
      "Summary 98: 678 tokens\n",
      "Summary 99: 797 tokens\n",
      "Summary 100: 632 tokens\n"
     ]
    }
   ],
   "source": [
    "print_token_counts(documents, tokenizer, label=\"Report\")\n",
    "print_token_counts(references, tokenizer, label=\"Summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a02247",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_token_counts(baseline_summaries, tokenizer, label=\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "982331bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_document(document, max_tokens=1500):\n",
    "    # Simple chunking by paragraphs, keeping each chunk under max_tokens\n",
    "    paragraphs = document.split('\\n')\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "    for para in paragraphs:\n",
    "        para_tokens = len(tokenizer.tokenize(para))\n",
    "        if current_tokens + para_tokens > max_tokens and current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = para\n",
    "            current_tokens = para_tokens\n",
    "        else:\n",
    "            current_chunk += \"\\n\" + para if current_chunk else para\n",
    "            current_tokens += para_tokens\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65fab588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/cse/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac164683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def robust_sentence_chunk(document, tokenizer, max_tokens=2000, min_chunk_ratio=0.5):\n",
    "    sentences = sent_tokenize(document)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "    min_chunk_tokens = int(max_tokens * min_chunk_ratio)\n",
    "    for sent in sentences:\n",
    "        sent_tokens = len(tokenizer.tokenize(sent))\n",
    "        if current_tokens + sent_tokens > max_tokens and current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sent\n",
    "            current_tokens = sent_tokens\n",
    "        else:\n",
    "            current_chunk += \" \" + sent if current_chunk else sent\n",
    "            current_tokens += sent_tokens\n",
    "    if current_chunk:\n",
    "        if len(tokenizer.tokenize(current_chunk)) < min_chunk_tokens and len(chunks) > 0:\n",
    "            chunks[-1] += \" \" + current_chunk\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65b9e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_summarize(document, summary_length=1024):\n",
    "    chunks = robust_sentence_chunk(document, tokenizer)\n",
    "    chunk_summaries = [led_summarize(chunk, max_output_length=summary_length//2)\n",
    "                       for chunk in chunks]\n",
    "    combined_summary = \" \".join(chunk_summaries)\n",
    "    final_summary = led_summarize(combined_summary, max_output_length=summary_length)\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f90fed8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hierarchical Summarization: 100%|██████████| 100/100 [14:05<00:00,  8.45s/it]\n"
     ]
    }
   ],
   "source": [
    "# Hierarchical summarization and evaluation on first 10 test samples\n",
    "hierarchical_summaries = []\n",
    "for doc in tqdm(documents, desc=\"Hierarchical Summarization\"):\n",
    "    hierarchical_summaries.append(hierarchical_summarize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa13111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hierarchical_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(hierarchical_summaries, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e5a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"hierarchical_summaries.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hierarchical_summaries = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e550e830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hirchial outputs 1: 625 tokens\n",
      "hirchial outputs 2: 1022 tokens\n",
      "hirchial outputs 3: 1022 tokens\n",
      "hirchial outputs 4: 858 tokens\n",
      "hirchial outputs 5: 1022 tokens\n",
      "hirchial outputs 6: 927 tokens\n",
      "hirchial outputs 7: 1022 tokens\n",
      "hirchial outputs 8: 1022 tokens\n",
      "hirchial outputs 9: 1022 tokens\n",
      "hirchial outputs 10: 1022 tokens\n",
      "hirchial outputs 11: 1004 tokens\n",
      "hirchial outputs 12: 1022 tokens\n",
      "hirchial outputs 13: 1022 tokens\n",
      "hirchial outputs 14: 857 tokens\n",
      "hirchial outputs 15: 1022 tokens\n",
      "hirchial outputs 16: 1022 tokens\n",
      "hirchial outputs 17: 870 tokens\n",
      "hirchial outputs 18: 1022 tokens\n",
      "hirchial outputs 19: 1022 tokens\n",
      "hirchial outputs 20: 1022 tokens\n",
      "hirchial outputs 21: 1022 tokens\n",
      "hirchial outputs 22: 1022 tokens\n",
      "hirchial outputs 23: 616 tokens\n",
      "hirchial outputs 24: 1022 tokens\n",
      "hirchial outputs 25: 1005 tokens\n",
      "hirchial outputs 26: 1022 tokens\n",
      "hirchial outputs 27: 1022 tokens\n",
      "hirchial outputs 28: 1022 tokens\n",
      "hirchial outputs 29: 1022 tokens\n",
      "hirchial outputs 30: 1022 tokens\n",
      "hirchial outputs 31: 668 tokens\n",
      "hirchial outputs 32: 1022 tokens\n",
      "hirchial outputs 33: 1022 tokens\n",
      "hirchial outputs 34: 1022 tokens\n",
      "hirchial outputs 35: 1022 tokens\n",
      "hirchial outputs 36: 1022 tokens\n",
      "hirchial outputs 37: 1022 tokens\n",
      "hirchial outputs 38: 1022 tokens\n",
      "hirchial outputs 39: 1022 tokens\n",
      "hirchial outputs 40: 1022 tokens\n",
      "hirchial outputs 41: 1022 tokens\n",
      "hirchial outputs 42: 1022 tokens\n",
      "hirchial outputs 43: 1022 tokens\n",
      "hirchial outputs 44: 1022 tokens\n",
      "hirchial outputs 45: 1022 tokens\n",
      "hirchial outputs 46: 607 tokens\n",
      "hirchial outputs 47: 1022 tokens\n",
      "hirchial outputs 48: 1022 tokens\n",
      "hirchial outputs 49: 1022 tokens\n",
      "hirchial outputs 50: 1022 tokens\n",
      "hirchial outputs 51: 1022 tokens\n",
      "hirchial outputs 52: 1022 tokens\n",
      "hirchial outputs 53: 1022 tokens\n",
      "hirchial outputs 54: 59 tokens\n",
      "hirchial outputs 55: 1022 tokens\n",
      "hirchial outputs 56: 1022 tokens\n",
      "hirchial outputs 57: 987 tokens\n",
      "hirchial outputs 58: 1022 tokens\n",
      "hirchial outputs 59: 1022 tokens\n",
      "hirchial outputs 60: 1022 tokens\n",
      "hirchial outputs 61: 793 tokens\n",
      "hirchial outputs 62: 1022 tokens\n",
      "hirchial outputs 63: 1014 tokens\n",
      "hirchial outputs 64: 1022 tokens\n",
      "hirchial outputs 65: 908 tokens\n",
      "hirchial outputs 66: 1022 tokens\n",
      "hirchial outputs 67: 1022 tokens\n",
      "hirchial outputs 68: 1022 tokens\n",
      "hirchial outputs 69: 1022 tokens\n",
      "hirchial outputs 70: 1022 tokens\n",
      "hirchial outputs 71: 1022 tokens\n",
      "hirchial outputs 72: 1022 tokens\n",
      "hirchial outputs 73: 612 tokens\n",
      "hirchial outputs 74: 600 tokens\n",
      "hirchial outputs 75: 624 tokens\n",
      "hirchial outputs 76: 1022 tokens\n",
      "hirchial outputs 77: 533 tokens\n",
      "hirchial outputs 78: 1021 tokens\n",
      "hirchial outputs 79: 1022 tokens\n",
      "hirchial outputs 80: 1022 tokens\n",
      "hirchial outputs 81: 1021 tokens\n",
      "hirchial outputs 82: 890 tokens\n",
      "hirchial outputs 83: 854 tokens\n",
      "hirchial outputs 84: 1022 tokens\n",
      "hirchial outputs 85: 1022 tokens\n",
      "hirchial outputs 86: 1022 tokens\n",
      "hirchial outputs 87: 1022 tokens\n",
      "hirchial outputs 88: 1022 tokens\n",
      "hirchial outputs 89: 1022 tokens\n",
      "hirchial outputs 90: 1022 tokens\n",
      "hirchial outputs 91: 1022 tokens\n",
      "hirchial outputs 92: 1022 tokens\n",
      "hirchial outputs 93: 1022 tokens\n",
      "hirchial outputs 94: 1022 tokens\n",
      "hirchial outputs 95: 1022 tokens\n",
      "hirchial outputs 96: 1022 tokens\n",
      "hirchial outputs 97: 1022 tokens\n",
      "hirchial outputs 98: 1022 tokens\n",
      "hirchial outputs 99: 1022 tokens\n",
      "hirchial outputs 100: 1022 tokens\n"
     ]
    }
   ],
   "source": [
    "print_token_counts(hierarchical_summaries, tokenizer, label=\"hirchial outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "202cb108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hierarchical Summarization: 100%|██████████| 100/100 [06:36<00:00,  3.97s/it]\n"
     ]
    }
   ],
   "source": [
    "hierarchical_summaries_512 = []\n",
    "for doc in tqdm(documents, desc=\"Hierarchical Summarization\"):\n",
    "    hierarchical_summaries_512.append(hierarchical_summarize(doc, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c26d4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hirchial outputs 1: 294 tokens\n",
      "hirchial outputs 2: 510 tokens\n",
      "hirchial outputs 3: 510 tokens\n",
      "hirchial outputs 4: 510 tokens\n",
      "hirchial outputs 5: 510 tokens\n",
      "hirchial outputs 6: 510 tokens\n",
      "hirchial outputs 7: 510 tokens\n",
      "hirchial outputs 8: 509 tokens\n",
      "hirchial outputs 9: 510 tokens\n",
      "hirchial outputs 10: 510 tokens\n",
      "hirchial outputs 11: 510 tokens\n",
      "hirchial outputs 12: 510 tokens\n",
      "hirchial outputs 13: 510 tokens\n",
      "hirchial outputs 14: 510 tokens\n",
      "hirchial outputs 15: 510 tokens\n",
      "hirchial outputs 16: 510 tokens\n",
      "hirchial outputs 17: 510 tokens\n",
      "hirchial outputs 18: 510 tokens\n",
      "hirchial outputs 19: 253 tokens\n",
      "hirchial outputs 20: 510 tokens\n",
      "hirchial outputs 21: 510 tokens\n",
      "hirchial outputs 22: 510 tokens\n",
      "hirchial outputs 23: 258 tokens\n",
      "hirchial outputs 24: 510 tokens\n",
      "hirchial outputs 25: 510 tokens\n",
      "hirchial outputs 26: 510 tokens\n",
      "hirchial outputs 27: 510 tokens\n",
      "hirchial outputs 28: 510 tokens\n",
      "hirchial outputs 29: 510 tokens\n",
      "hirchial outputs 30: 255 tokens\n",
      "hirchial outputs 31: 254 tokens\n",
      "hirchial outputs 32: 510 tokens\n",
      "hirchial outputs 33: 298 tokens\n",
      "hirchial outputs 34: 510 tokens\n",
      "hirchial outputs 35: 510 tokens\n",
      "hirchial outputs 36: 510 tokens\n",
      "hirchial outputs 37: 510 tokens\n",
      "hirchial outputs 38: 510 tokens\n",
      "hirchial outputs 39: 510 tokens\n",
      "hirchial outputs 40: 509 tokens\n",
      "hirchial outputs 41: 338 tokens\n",
      "hirchial outputs 42: 305 tokens\n",
      "hirchial outputs 43: 510 tokens\n",
      "hirchial outputs 44: 510 tokens\n",
      "hirchial outputs 45: 510 tokens\n",
      "hirchial outputs 46: 254 tokens\n",
      "hirchial outputs 47: 510 tokens\n",
      "hirchial outputs 48: 510 tokens\n",
      "hirchial outputs 49: 510 tokens\n",
      "hirchial outputs 50: 510 tokens\n",
      "hirchial outputs 51: 510 tokens\n",
      "hirchial outputs 52: 510 tokens\n",
      "hirchial outputs 53: 510 tokens\n",
      "hirchial outputs 54: 59 tokens\n",
      "hirchial outputs 55: 510 tokens\n",
      "hirchial outputs 56: 510 tokens\n",
      "hirchial outputs 57: 510 tokens\n",
      "hirchial outputs 58: 510 tokens\n",
      "hirchial outputs 59: 510 tokens\n",
      "hirchial outputs 60: 510 tokens\n",
      "hirchial outputs 61: 510 tokens\n",
      "hirchial outputs 62: 510 tokens\n",
      "hirchial outputs 63: 510 tokens\n",
      "hirchial outputs 64: 510 tokens\n",
      "hirchial outputs 65: 510 tokens\n",
      "hirchial outputs 66: 510 tokens\n",
      "hirchial outputs 67: 510 tokens\n",
      "hirchial outputs 68: 510 tokens\n",
      "hirchial outputs 69: 510 tokens\n",
      "hirchial outputs 70: 510 tokens\n",
      "hirchial outputs 71: 510 tokens\n",
      "hirchial outputs 72: 510 tokens\n",
      "hirchial outputs 73: 269 tokens\n",
      "hirchial outputs 74: 254 tokens\n",
      "hirchial outputs 75: 254 tokens\n",
      "hirchial outputs 76: 510 tokens\n",
      "hirchial outputs 77: 332 tokens\n",
      "hirchial outputs 78: 510 tokens\n",
      "hirchial outputs 79: 510 tokens\n",
      "hirchial outputs 80: 510 tokens\n",
      "hirchial outputs 81: 510 tokens\n",
      "hirchial outputs 82: 510 tokens\n",
      "hirchial outputs 83: 510 tokens\n",
      "hirchial outputs 84: 510 tokens\n",
      "hirchial outputs 85: 510 tokens\n",
      "hirchial outputs 86: 510 tokens\n",
      "hirchial outputs 87: 510 tokens\n",
      "hirchial outputs 88: 510 tokens\n",
      "hirchial outputs 89: 510 tokens\n",
      "hirchial outputs 90: 510 tokens\n",
      "hirchial outputs 91: 510 tokens\n",
      "hirchial outputs 92: 510 tokens\n",
      "hirchial outputs 93: 510 tokens\n",
      "hirchial outputs 94: 510 tokens\n",
      "hirchial outputs 95: 510 tokens\n",
      "hirchial outputs 96: 510 tokens\n",
      "hirchial outputs 97: 510 tokens\n",
      "hirchial outputs 98: 510 tokens\n",
      "hirchial outputs 99: 510 tokens\n",
      "hirchial outputs 100: 510 tokens\n"
     ]
    }
   ],
   "source": [
    "print_token_counts(hierarchical_summaries_512, tokenizer, label=\"hirchial outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6a04570",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hierarchical_summaries_512.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(hierarchical_summaries, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05189d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"hierarchical_summaries_512.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hierarchical_summaries_512 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ed73041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline summaries : \n",
      "ROUGE scores:\n",
      "  rouge1: 0.3869\n",
      "  rouge2: 0.1316\n",
      "  rougeL: 0.1705\n",
      "------------------------------------------\n",
      "1024 summaries : \n",
      "ROUGE scores:\n",
      "  rouge1: 0.4996\n",
      "  rouge2: 0.1575\n",
      "  rougeL: 0.1850\n",
      "------------------------------------------\n",
      "512 summaries : \n",
      "ROUGE scores:\n",
      "  rouge1: 0.4361\n",
      "  rouge2: 0.1278\n",
      "  rougeL: 0.1853\n"
     ]
    }
   ],
   "source": [
    "print('Baseline summaries : ')\n",
    "compute_rouge(baseline_summaries, references)\n",
    "print('------------------------------------------')\n",
    "print('1024 summaries : ')\n",
    "compute_rouge(hierarchical_summaries, references)\n",
    "print('------------------------------------------')\n",
    "print('512 summaries : ')\n",
    "compute_rouge(hierarchical_summaries_512, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c24e3df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline summaries : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse/llm-env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore:\n",
      "  precision: 0.7955\n",
      "  recall: 0.8086\n",
      "  f1: 0.8018\n",
      "------------------------------------------\n",
      "1024 summaries : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse/llm-env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore:\n",
      "  precision: 0.8281\n",
      "  recall: 0.8340\n",
      "  f1: 0.8310\n",
      "------------------------------------------\n",
      "512 summaries : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore:\n",
      "  precision: 0.8340\n",
      "  recall: 0.8372\n",
      "  f1: 0.8355\n"
     ]
    }
   ],
   "source": [
    "print('Baseline summaries : ')\n",
    "compute_bert_score(baseline_summaries, references)\n",
    "print('------------------------------------------')\n",
    "print('1024 summaries : ')\n",
    "compute_bert_score(hierarchical_summaries, references)\n",
    "print('------------------------------------------')\n",
    "print('512 summaries : ')\n",
    "compute_bert_score(hierarchical_summaries_512, references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b87f85e",
   "metadata": {},
   "source": [
    "Long Document \n",
    "Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffac0d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_long_entries_from_dataset(ds_split, tokenizer, min_tokens):\n",
    "    \"\"\"\n",
    "    Extracts 'report' and 'summary' from a HuggingFace dataset split,\n",
    "    and returns two lists:\n",
    "      - reports_long: reports with more than min_tokens tokens\n",
    "      - summaries_long: corresponding summaries\n",
    "    \"\"\"\n",
    "    reports_long = []\n",
    "    summaries_long = []\n",
    "    for entry in ds_split:\n",
    "        report = entry[\"report\"]\n",
    "        summary = entry[\"summary\"]\n",
    "        num_tokens = len(tokenizer.tokenize(report))\n",
    "        if num_tokens > min_tokens:\n",
    "            reports_long.append(report)\n",
    "            summaries_long.append(summary)\n",
    "    return reports_long, summaries_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a679d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_reports_10, long_summaries_10 = get_long_entries_from_dataset(ds[\"validation\"], tokenizer, min_tokens=16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9375567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long outputs 1: 19300 tokens\n",
      "long outputs 2: 25889 tokens\n",
      "long outputs 3: 19090 tokens\n",
      "long outputs 4: 27256 tokens\n",
      "long outputs 5: 20496 tokens\n",
      "long outputs 6: 18037 tokens\n",
      "long outputs 7: 16680 tokens\n",
      "long outputs 8: 21451 tokens\n",
      "long outputs 9: 20559 tokens\n",
      "long outputs 10: 16887 tokens\n",
      "long outputs 11: 17445 tokens\n",
      "long outputs 12: 23155 tokens\n",
      "long outputs 13: 21447 tokens\n",
      "long outputs 14: 25455 tokens\n",
      "long outputs 15: 20315 tokens\n",
      "long outputs 16: 22912 tokens\n",
      "long outputs 17: 25303 tokens\n",
      "long outputs 18: 21922 tokens\n",
      "long outputs 19: 16820 tokens\n",
      "long outputs 20: 18988 tokens\n",
      "long outputs 21: 17660 tokens\n",
      "long outputs 22: 18481 tokens\n",
      "long outputs 23: 21441 tokens\n",
      "long outputs 24: 16980 tokens\n",
      "long outputs 25: 20651 tokens\n",
      "long outputs 26: 21861 tokens\n",
      "long outputs 27: 28010 tokens\n",
      "long outputs 28: 24337 tokens\n",
      "long outputs 29: 26082 tokens\n",
      "long outputs 30: 35176 tokens\n",
      "long outputs 31: 17359 tokens\n",
      "long outputs 32: 19091 tokens\n",
      "long outputs 33: 17707 tokens\n",
      "long outputs 34: 24566 tokens\n",
      "long outputs 35: 19745 tokens\n",
      "long outputs 36: 22506 tokens\n",
      "long outputs 37: 25491 tokens\n",
      "long outputs 38: 19641 tokens\n",
      "long outputs 39: 17557 tokens\n",
      "long outputs 40: 39909 tokens\n",
      "long outputs 41: 27611 tokens\n",
      "long outputs 42: 21895 tokens\n",
      "long outputs 43: 18541 tokens\n",
      "long outputs 44: 22470 tokens\n",
      "long outputs 45: 18430 tokens\n",
      "long outputs 46: 18151 tokens\n",
      "long outputs 47: 17819 tokens\n",
      "long outputs 48: 22647 tokens\n",
      "long outputs 49: 18435 tokens\n",
      "long outputs 50: 21638 tokens\n",
      "long outputs 51: 27168 tokens\n",
      "long outputs 52: 17227 tokens\n",
      "long outputs 53: 20767 tokens\n",
      "long outputs 54: 18501 tokens\n",
      "long outputs 55: 22007 tokens\n",
      "long outputs 56: 16720 tokens\n",
      "long outputs 57: 69298 tokens\n",
      "long outputs 58: 49764 tokens\n",
      "long outputs 59: 25335 tokens\n",
      "long outputs 60: 24485 tokens\n",
      "long outputs 61: 17309 tokens\n",
      "long outputs 62: 24528 tokens\n",
      "long outputs 63: 19157 tokens\n",
      "long outputs 64: 29978 tokens\n",
      "long outputs 65: 21657 tokens\n",
      "long outputs 66: 20724 tokens\n",
      "long outputs 67: 17200 tokens\n",
      "long outputs 68: 22202 tokens\n",
      "long outputs 69: 30325 tokens\n",
      "long outputs 70: 17164 tokens\n",
      "long outputs 71: 23035 tokens\n",
      "long outputs 72: 17231 tokens\n",
      "long outputs 73: 17040 tokens\n",
      "long outputs 74: 20680 tokens\n",
      "long outputs 75: 20467 tokens\n",
      "long outputs 76: 18449 tokens\n",
      "long outputs 77: 19622 tokens\n",
      "long outputs 78: 16591 tokens\n",
      "long outputs 79: 18283 tokens\n",
      "long outputs 80: 28225 tokens\n",
      "long outputs 81: 23801 tokens\n",
      "long outputs 82: 42563 tokens\n",
      "long outputs 83: 17154 tokens\n",
      "long outputs 84: 22105 tokens\n",
      "long outputs 85: 41257 tokens\n",
      "long outputs 86: 61167 tokens\n",
      "long outputs 87: 26874 tokens\n",
      "long outputs 88: 22327 tokens\n",
      "long outputs 89: 52492 tokens\n",
      "long outputs 90: 19816 tokens\n",
      "long outputs 91: 35806 tokens\n",
      "long outputs 92: 27563 tokens\n",
      "long outputs 93: 17391 tokens\n",
      "long outputs 94: 17487 tokens\n",
      "long outputs 95: 18326 tokens\n",
      "long outputs 96: 26946 tokens\n",
      "long outputs 97: 26674 tokens\n",
      "long outputs 98: 21841 tokens\n",
      "long outputs 99: 68083 tokens\n",
      "long outputs 100: 16510 tokens\n",
      "long outputs 101: 18764 tokens\n",
      "long outputs 102: 19473 tokens\n",
      "long outputs 103: 20382 tokens\n",
      "long outputs 104: 17283 tokens\n",
      "long outputs 105: 19774 tokens\n",
      "long outputs 106: 18089 tokens\n",
      "long outputs 107: 24615 tokens\n",
      "long outputs 108: 22254 tokens\n",
      "long outputs 109: 20322 tokens\n",
      "long outputs 110: 26210 tokens\n",
      "long outputs 111: 17835 tokens\n",
      "long outputs 112: 18985 tokens\n",
      "long outputs 113: 27627 tokens\n",
      "long outputs 114: 16488 tokens\n",
      "long outputs 115: 51256 tokens\n",
      "long outputs 116: 18139 tokens\n",
      "long outputs 117: 40641 tokens\n",
      "long outputs 118: 33274 tokens\n"
     ]
    }
   ],
   "source": [
    "print_token_counts(long_reports_10, tokenizer, label=\"long outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57feb5d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'long_reports_10' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlong_reports_16k.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     json.dump(\u001b[43mlong_reports_10\u001b[49m, f, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m, indent=\u001b[32m2\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'long_reports_10' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"long_reports_16k.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(long_reports_10, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7e2e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"long_reports_summary_16k.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(long_summaries_10, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e46ba9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_long = long_reports_10[:100]\n",
    "references_long = long_summaries_10 [:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13049ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23986.17\n"
     ]
    }
   ],
   "source": [
    "print(average_token_count(documents_long, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d805fe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "long Summarization 1024: 100%|██████████| 100/100 [31:36<00:00, 18.97s/it]\n"
     ]
    }
   ],
   "source": [
    "long_summary_output_10_1024 = []\n",
    "for doc in tqdm(documents_long, desc=\"long Summarization 1024\"):\n",
    "    long_summary_output_10_1024.append(hierarchical_summarize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9447d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"long_summary_16k_1024.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(long_summary_output_10_1024, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89b42704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "long Summarization baseline: 100%|██████████| 100/100 [11:26<00:00,  6.87s/it]\n"
     ]
    }
   ],
   "source": [
    "long_summary_output_10_baseline = []\n",
    "for doc in tqdm(documents_long, desc=\"long Summarization baseline\"):\n",
    "    long_summary_output_10_baseline.append(led_summarize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8f53cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"long_summary_16k_baseline.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(long_summary_output_10_baseline, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52627dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "long Summarization 512: 100%|██████████| 100/100 [14:51<00:00,  8.91s/it]\n"
     ]
    }
   ],
   "source": [
    "long_summary_output_10_512 = []\n",
    "for doc in tqdm(documents_long, desc=\"long Summarization 512\"):\n",
    "    long_summary_output_10_512.append(hierarchical_summarize(doc,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b0b0ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"long_summary_16k_512.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(long_summary_output_10_512, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c82fa54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline summaries : \n",
      "ROUGE scores:\n",
      "  rouge1: 0.2068\n",
      "  rouge2: 0.0488\n",
      "  rougeL: 0.1433\n",
      "------------------------------------------\n",
      "1024 summaries : \n",
      "ROUGE scores:\n",
      "  rouge1: 0.4336\n",
      "  rouge2: 0.1202\n",
      "  rougeL: 0.1668\n",
      "------------------------------------------\n",
      "512 summaries : \n",
      "ROUGE scores:\n",
      "  rouge1: 0.4041\n",
      "  rouge2: 0.1030\n",
      "  rougeL: 0.1709\n"
     ]
    }
   ],
   "source": [
    "print('Baseline summaries : ')\n",
    "compute_rouge(long_summary_output_10_baseline, references_long)\n",
    "print('------------------------------------------')\n",
    "print('1024 summaries : ')\n",
    "compute_rouge(long_summary_output_10_1024, references_long)\n",
    "print('------------------------------------------')\n",
    "print('512 summaries : ')\n",
    "compute_rouge(long_summary_output_10_512, references_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3afcee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline summaries : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore:\n",
      "  precision: 0.7632\n",
      "  recall: 0.7795\n",
      "  f1: 0.7710\n",
      "------------------------------------------\n",
      "1024 summaries : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore:\n",
      "  precision: 0.8074\n",
      "  recall: 0.8196\n",
      "  f1: 0.8133\n",
      "------------------------------------------\n",
      "512 summaries : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore:\n",
      "  precision: 0.8213\n",
      "  recall: 0.8289\n",
      "  f1: 0.8250\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print('Baseline summaries : ')\n",
    "compute_bert_score(long_summary_output_10_baseline, references_long)\n",
    "print('------------------------------------------')\n",
    "print('1024 summaries : ')\n",
    "compute_bert_score(long_summary_output_10_1024, references_long)\n",
    "print('------------------------------------------')\n",
    "print('512 summaries : ')\n",
    "compute_bert_score(long_summary_output_10_512, references_long)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
