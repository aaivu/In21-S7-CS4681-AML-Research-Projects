{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13178212,"sourceType":"datasetVersion","datasetId":8350933},{"sourceId":13178272,"sourceType":"datasetVersion","datasetId":8350977},{"sourceId":13178446,"sourceType":"datasetVersion","datasetId":8351098},{"sourceId":13178652,"sourceType":"datasetVersion","datasetId":8351253},{"sourceId":13178721,"sourceType":"datasetVersion","datasetId":8351296},{"sourceId":13179398,"sourceType":"datasetVersion","datasetId":8351781},{"sourceId":13179501,"sourceType":"datasetVersion","datasetId":8351861},{"sourceId":13179510,"sourceType":"datasetVersion","datasetId":8351869},{"sourceId":13189597,"sourceType":"datasetVersion","datasetId":8358483},{"sourceId":13214413,"sourceType":"datasetVersion","datasetId":8375566},{"sourceId":13216217,"sourceType":"datasetVersion","datasetId":8376874},{"sourceId":13258196,"sourceType":"datasetVersion","datasetId":8401392},{"sourceId":13258515,"sourceType":"datasetVersion","datasetId":8401624},{"sourceId":13261013,"sourceType":"datasetVersion","datasetId":8403301},{"sourceId":13261045,"sourceType":"datasetVersion","datasetId":8403320},{"sourceId":13262563,"sourceType":"datasetVersion","datasetId":8404386},{"sourceId":13262636,"sourceType":"datasetVersion","datasetId":8368234}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"1acab503","cell_type":"code","source":"# python imports\nimport argparse\nimport os\nimport time\nimport datetime\nfrom pprint import pprint\nimport numpy as np\nimport random\nimport json\nimport math\nfrom copy import deepcopy\n\n# torch imports\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.nn.functional as F\n# for visualization\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import Dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:19:53.875183Z","iopub.execute_input":"2025-10-04T16:19:53.875459Z","iopub.status.idle":"2025-10-04T16:20:10.496623Z","shell.execute_reply.started":"2025-10-04T16:19:53.875440Z","shell.execute_reply":"2025-10-04T16:20:10.495856Z"}},"outputs":[{"name":"stderr","text":"2025-10-04 16:19:59.374726: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759594799.539038      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759594799.583912      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"id":"c2e74b30","cell_type":"code","source":"# Setup parameters","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:10.497836Z","iopub.execute_input":"2025-10-04T16:20:10.498398Z","iopub.status.idle":"2025-10-04T16:20:10.502293Z","shell.execute_reply.started":"2025-10-04T16:20:10.498373Z","shell.execute_reply":"2025-10-04T16:20:10.501584Z"}},"outputs":[],"execution_count":2},{"id":"0f011ed9","cell_type":"code","source":"# Training parameters - directly set values\ninit_rand_seed = 1234567891\ndataset_name = \"thumos\"\ndevices = ['cuda:0']  # or [0] depending on your setup\ntrain_split = ['validation']\nval_split = ['test']\nmodel_name = \"LocPointTransformer\"\noutput_folder = \"./ckpt/\"\n\n# Dataset parameters\njson_file = \"/kaggle/input/thumos/thumos/annotations/thumos14.json\"\nfeat_folder = \"/kaggle/input/thumos/thumos/i3d_features\"\nfile_prefix = None\nfile_ext = \".npy\"\nfeat_stride = 4\nnum_frames = 16\ndefault_fps = None\ninput_dim = 2048\nnum_classes = 20\ndownsample_rate = 1\nmax_seq_len = 2304\ntrunc_thresh = 0.5\ncrop_ratio = [0.9, 1.0]\nforce_upsampling = False\n\n# Loader parameters\nbatch_size = 2\nnum_workers = 4\n\n# Model architecture parameters\n#backbone_type = 'convTransformer'\nbackbone_type = 'SGP'\nfpn_type = \"identity\"\nbackbone_arch = (2, 2, 5)\nscale_factor = 2\nregression_range = [(0, 4), (4, 8), (8, 16), (16, 32), (32, 64), (64, 10000)]\nn_head = 4\nn_mha_win_size = 19\nembd_kernel_size = 3\nembd_dim = 512\nembd_with_ln = True\nfpn_dim = 512\nfpn_with_ln = True\nfpn_start_level = 0\nhead_dim = 512\nhead_kernel_size = 3\nhead_num_layers = 3\nhead_with_ln = True\nmax_buffer_len_factor = 6.0\nuse_abs_pe = False\nuse_rel_pe = False\n\nn_sgp_win_size = 1       #new -  # window size w for sgp\ndownsample_type = \"max\"  #new -  # how to downsample feature in FPN\nsgp_mlp_dim = 768       #new -  # the numnber of dim in SGP\ninit_conv_vars = 0        #new -  # initialization of gaussian variance for the weight in SGP\nk = 5                    #new -  # the K in SGP\n\n\n# Training configuration\ncenter_sample = \"radius\"\ncenter_sample_radius = 1.5\nloss_weight = 1.0\ncls_prior_prob = 0.01\ninit_loss_norm = 100\nclip_grad_l2norm = 1.0\nhead_empty_cls = []\ndropout = 0.0\ndroppath = 0.1\nlabel_smoothing = 0.0\n\n# Test configuration\npre_nms_thresh = 0.001\npre_nms_topk = 2000\niou_threshold = 0.1\nmin_score = 0.001\nmax_seg_num = 200\nnms_method = 'soft'\nnms_sigma = 0.5\nduration_thresh = 0.05\nmulticlass_nms = True\next_score_file = \"/kaggle/input/thumos/thumos/annotations/thumos14_cls_scores.pkl\"\nvoting_thresh = 0.7\n\n# Optimizer parameters\nopt_type = \"AdamW\"\nmomentum = 0.9\nweight_decay = 0.05\nlearning_rate = 0.0001\nepochs = 30\nwarmup = True\nwarmup_epochs = 5\nschedule_type = \"cosine\"\nschedule_steps = []\nschedule_gamma = 0.1\n\n# Other training parameters\nstart_epoch = 0\nprint_freq = 10\nckpt_freq = 5\noutput = \"\"\nresume = \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:10.503298Z","iopub.execute_input":"2025-10-04T16:20:10.503567Z","iopub.status.idle":"2025-10-04T16:20:10.518009Z","shell.execute_reply.started":"2025-10-04T16:20:10.503545Z","shell.execute_reply":"2025-10-04T16:20:10.517268Z"}},"outputs":[],"execution_count":3},{"id":"631ab31f","cell_type":"code","source":"def fix_random_seed(seed, include_cuda=True):\n    rng_generator = torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    if include_cuda:\n        # training: disable cudnn benchmark to ensure the reproducibility\n        cudnn.enabled = True\n        cudnn.benchmark = False\n        cudnn.deterministic = True\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        # this is needed for CUDA >= 10.2\n        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n        torch.use_deterministic_algorithms(True, warn_only=True)\n    else:\n        cudnn.enabled = True\n        cudnn.benchmark = True\n    return rng_generator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:10.519707Z","iopub.execute_input":"2025-10-04T16:20:10.519894Z","iopub.status.idle":"2025-10-04T16:20:10.530701Z","shell.execute_reply.started":"2025-10-04T16:20:10.519874Z","shell.execute_reply":"2025-10-04T16:20:10.529955Z"}},"outputs":[],"execution_count":4},{"id":"65e89a35","cell_type":"code","source":"# prep for output folder (based on time stamp)\nif not os.path.exists(output_folder):\n    os.mkdir(output_folder)\n\nif len(output) == 0:\n    ts = datetime.datetime.fromtimestamp(int(time.time()))\n    ckpt_folder = os.path.join(output_folder, f'thumos_i3d_{str(ts)}')\nelse:\n    ckpt_folder = os.path.join(output_folder, f'thumos_i3d_{str(output)}')\n\nif not os.path.exists(ckpt_folder):\n    os.mkdir(ckpt_folder)\n\n# tensorboard writer\ntb_writer = SummaryWriter(os.path.join(ckpt_folder, 'logs'))\n\n# fix the random seeds (this will fix everything)\nrng_generator = fix_random_seed(init_rand_seed, include_cuda=True)\n\n# re-scale learning rate / # workers based on number of GPUs\nlearning_rate *= len(devices)\nnum_workers *= len(devices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:10.531368Z","iopub.execute_input":"2025-10-04T16:20:10.531587Z","iopub.status.idle":"2025-10-04T16:20:10.548289Z","shell.execute_reply.started":"2025-10-04T16:20:10.531573Z","shell.execute_reply":"2025-10-04T16:20:10.547531Z"}},"outputs":[],"execution_count":5},{"id":"2832ac8d","cell_type":"code","source":"# Dataset - Thumos14","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:10.549007Z","iopub.execute_input":"2025-10-04T16:20:10.549285Z","iopub.status.idle":"2025-10-04T16:20:10.553494Z","shell.execute_reply.started":"2025-10-04T16:20:10.549261Z","shell.execute_reply":"2025-10-04T16:20:10.552710Z"}},"outputs":[],"execution_count":6},{"id":"88eeb20b","cell_type":"code","source":"class THUMOS14Dataset(Dataset):\n    def __init__(\n        self,\n        is_training,     # if in training mode\n        split,           # split, a tuple/list allowing concat of subsets\n        feat_folder,     # folder for features\n        json_file,       # json file for annotations\n        feat_stride,     # temporal stride of the feats\n        num_frames,      # number of frames for each feat\n        default_fps,     # default fps\n        downsample_rate, # downsample rate for feats\n        max_seq_len,     # maximum sequence length during training\n        trunc_thresh,    # threshold for truncate an action segment\n        crop_ratio,      # a tuple (e.g., (0.9, 1.0)) for random cropping\n        input_dim,       # input feat dim\n        num_classes,     # number of action categories\n        file_prefix,     # feature file prefix if any\n        file_ext,        # feature file extension if any\n        force_upsampling # force to upsample to max_seq_len\n    ):\n        # file path\n        assert os.path.exists(feat_folder) and os.path.exists(json_file)\n        assert isinstance(split, tuple) or isinstance(split, list)\n        assert crop_ratio == None or len(crop_ratio) == 2\n        self.feat_folder = feat_folder\n        if file_prefix is not None:\n            self.file_prefix = file_prefix\n        else:\n            self.file_prefix = ''\n        self.file_ext = file_ext\n        self.json_file = json_file\n\n        # split / training mode\n        self.split = split\n        self.is_training = is_training\n\n        # features meta info\n        self.feat_stride = feat_stride\n        self.num_frames = num_frames\n        self.input_dim = input_dim\n        self.default_fps = default_fps\n        self.downsample_rate = downsample_rate\n        self.max_seq_len = max_seq_len\n        self.trunc_thresh = trunc_thresh\n        self.num_classes = num_classes\n        self.label_dict = None\n        self.crop_ratio = crop_ratio\n\n        # load database and select the subset\n        dict_db, label_dict = self._load_json_db(self.json_file)\n        assert len(label_dict) == num_classes\n        self.data_list = dict_db\n        self.label_dict = label_dict\n\n        # dataset specific attributes\n        self.db_attributes = {\n            'dataset_name': 'thumos-14',\n            'tiou_thresholds': np.linspace(0.3, 0.7, 5),\n            # we will mask out cliff diving\n            'empty_label_ids': [],\n        }\n\n    def get_attributes(self):\n        return self.db_attributes\n\n    def _load_json_db(self, json_file):\n        # load database and select the subset\n        with open(json_file, 'r') as fid:\n            json_data = json.load(fid)\n        json_db = json_data['database']\n\n        # if label_dict is not available\n        if self.label_dict is None:\n            label_dict = {}\n            for key, value in json_db.items():\n                for act in value['annotations']:\n                    label_dict[act['label']] = act['label_id']\n\n        # fill in the db (immutable afterwards)\n        dict_db = tuple()\n        for key, value in json_db.items():\n            # skip the video if not in the split\n            if value['subset'].lower() not in self.split:\n                continue\n            # or does not have the feature file\n            feat_file = os.path.join(self.feat_folder,\n                                     self.file_prefix + key + self.file_ext)\n            if not os.path.exists(feat_file):\n                continue\n\n            # get fps if available\n            if self.default_fps is not None:\n                fps = self.default_fps\n            elif 'fps' in value:\n                fps = value['fps']\n            else:\n                assert False, \"Unknown video FPS.\"\n\n            # get video duration if available\n            if 'duration' in value:\n                duration = value['duration']\n            else:\n                duration = 1e8\n\n            # get annotations if available\n            if ('annotations' in value) and (len(value['annotations']) > 0):\n                # a fun fact of THUMOS: cliffdiving (4) is a subset of diving (7)\n                # our code can now handle this corner case\n                segments, labels = [], []\n                for act in value['annotations']:\n                    segments.append(act['segment'])\n                    labels.append([label_dict[act['label']]])\n\n                segments = np.asarray(segments, dtype=np.float32)\n                labels = np.squeeze(np.asarray(labels, dtype=np.int64), axis=1)\n            else:\n                segments = None\n                labels = None\n            dict_db += ({'id': key,\n                         'fps' : fps,\n                         'duration' : duration,\n                         'segments' : segments,\n                         'labels' : labels\n            }, )\n\n        return dict_db, label_dict\n\n    def __len__(self):\n        return len(self.data_list)\n\n    def __getitem__(self, idx):\n        # directly return a (truncated) data point (so it is very fast!)\n        # auto batching will be disabled in the subsequent dataloader\n        # instead the model will need to decide how to batch / preporcess the data\n        video_item = self.data_list[idx]\n\n        # load features\n        filename = os.path.join(self.feat_folder,\n                                self.file_prefix + video_item['id'] + self.file_ext)\n        feats = np.load(filename).astype(np.float32)\n\n        # deal with downsampling (= increased feat stride)\n        feats = feats[::self.downsample_rate, :]\n        feat_stride = self.feat_stride * self.downsample_rate\n        feat_offset = 0.5 * self.num_frames / feat_stride\n        # T x C -> C x T\n        feats = torch.from_numpy(np.ascontiguousarray(feats.transpose()))\n\n        # convert time stamp (in second) into temporal feature grids\n        # ok to have small negative values here\n        if video_item['segments'] is not None:\n            segments = torch.from_numpy(\n                video_item['segments'] * video_item['fps'] / feat_stride - feat_offset\n            )\n            labels = torch.from_numpy(video_item['labels'])\n        else:\n            segments, labels = None, None\n\n        # return a data dict\n        data_dict = {'video_id'        : video_item['id'],\n                     'feats'           : feats,      # C x T\n                     'segments'        : segments,   # N x 2\n                     'labels'          : labels,     # N\n                     'fps'             : video_item['fps'],\n                     'duration'        : video_item['duration'],\n                     'feat_stride'     : feat_stride,\n                     'feat_num_frames' : self.num_frames}\n\n        # truncate the features during training\n        if self.is_training and (segments is not None):\n            data_dict = truncate_feats(\n                data_dict, self.max_seq_len, self.trunc_thresh, feat_offset, self.crop_ratio\n            )\n\n        return data_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:10.554390Z","iopub.execute_input":"2025-10-04T16:20:10.554857Z","iopub.status.idle":"2025-10-04T16:20:10.570207Z","shell.execute_reply.started":"2025-10-04T16:20:10.554833Z","shell.execute_reply":"2025-10-04T16:20:10.569535Z"}},"outputs":[],"execution_count":7},{"id":"23faa37c","cell_type":"code","source":"\"\"\"2. create dataset / dataloader directly\"\"\"\ntrain_dataset = THUMOS14Dataset(\n    is_training=True,\n    split=train_split,\n    feat_folder=feat_folder,\n    json_file=json_file,\n    feat_stride=feat_stride,\n    num_frames=num_frames,\n    default_fps=default_fps,\n    downsample_rate=downsample_rate,\n    max_seq_len=max_seq_len,\n    trunc_thresh=trunc_thresh,\n    crop_ratio=crop_ratio,\n    input_dim=input_dim,\n    num_classes=num_classes,\n    file_prefix=file_prefix,\n    file_ext=file_ext,\n    force_upsampling=force_upsampling\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:10.570985Z","iopub.execute_input":"2025-10-04T16:20:10.571212Z","iopub.status.idle":"2025-10-04T16:20:11.180648Z","shell.execute_reply.started":"2025-10-04T16:20:10.571191Z","shell.execute_reply":"2025-10-04T16:20:11.179859Z"}},"outputs":[],"execution_count":8},{"id":"54f5b645","cell_type":"code","source":"# update head_empty_cls based on dataset attributes\ntrain_db_vars = train_dataset.get_attributes()\nhead_empty_cls = train_db_vars['empty_label_ids']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.181747Z","iopub.execute_input":"2025-10-04T16:20:11.181943Z","iopub.status.idle":"2025-10-04T16:20:11.185909Z","shell.execute_reply.started":"2025-10-04T16:20:11.181928Z","shell.execute_reply":"2025-10-04T16:20:11.184881Z"}},"outputs":[],"execution_count":9},{"id":"c63ffcb3","cell_type":"code","source":"def trivial_batch_collator(batch):\n    \"\"\"\n        A batch collator that does nothing\n    \"\"\"\n    return batch\n\ndef worker_init_reset_seed(worker_id):\n    \"\"\"\n        Reset random seed for each worker\n    \"\"\"\n    seed = torch.initial_seed() % 2 ** 31\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n\nis_training = True\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    collate_fn=trivial_batch_collator,\n    worker_init_fn=(worker_init_reset_seed if is_training else None),\n    shuffle=is_training,\n    drop_last=is_training,\n    generator=rng_generator,\n    persistent_workers=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.188356Z","iopub.execute_input":"2025-10-04T16:20:11.188627Z","iopub.status.idle":"2025-10-04T16:20:11.200366Z","shell.execute_reply.started":"2025-10-04T16:20:11.188610Z","shell.execute_reply":"2025-10-04T16:20:11.199657Z"}},"outputs":[],"execution_count":10},{"id":"6de2cad1","cell_type":"code","source":"# Create train_cfg and test_cfg dictionaries\ntrain_cfg = {\n    'center_sample': center_sample,\n    'center_sample_radius': center_sample_radius,\n    'loss_weight': loss_weight,\n    'cls_prior_prob': cls_prior_prob,\n    'init_loss_norm': init_loss_norm,\n    'clip_grad_l2norm': clip_grad_l2norm,\n    'head_empty_cls': head_empty_cls,\n    'dropout': dropout,\n    'droppath': droppath,\n    'label_smoothing': label_smoothing\n}\n\ntest_cfg = {\n    'pre_nms_thresh': pre_nms_thresh,\n    'pre_nms_topk': pre_nms_topk,\n    'iou_threshold': iou_threshold,\n    'min_score': min_score,\n    'max_seg_num': max_seg_num,\n    'nms_method': nms_method,\n    'nms_sigma': nms_sigma,\n    'duration_thresh': duration_thresh,\n    'multiclass_nms': multiclass_nms,\n    'ext_score_file': ext_score_file,\n    'voting_thresh': voting_thresh\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.201027Z","iopub.execute_input":"2025-10-04T16:20:11.201304Z","iopub.status.idle":"2025-10-04T16:20:11.212682Z","shell.execute_reply.started":"2025-10-04T16:20:11.201283Z","shell.execute_reply":"2025-10-04T16:20:11.211876Z"}},"outputs":[],"execution_count":11},{"id":"50f9b572-52bf-49bb-83ab-0595b0768e2d","cell_type":"code","source":"class SGPBlock(nn.Module):\n    \"\"\"\n    A simple conv block similar to the basic block used in ResNet\n    \"\"\"\n\n    def __init__(\n            self,\n            n_embd,  # dimension of the input features\n            kernel_size=3,  # conv kernel size\n            n_ds_stride=1,  # downsampling stride for the current layer\n            k=1.5,  # k\n            group=1,  # group for cnn\n            n_out=None,  # output dimension, if None, set to input dim\n            n_hidden=None,  # hidden dim for mlp\n            path_pdrop=0.0,  # drop path rate\n            act_layer=nn.GELU,  # nonlinear activation used after conv, default ReLU,\n            downsample_type='max',\n            init_conv_vars=1  # init gaussian variance for the weight\n    ):\n        super().__init__()\n        # must use odd sized kernel\n        # assert (kernel_size % 2 == 1) and (kernel_size > 1)\n        # padding = kernel_size // 2\n\n        self.kernel_size = kernel_size\n        self.stride = n_ds_stride\n\n        if n_out is None:\n            n_out = n_embd\n\n        self.ln = LayerNorm(n_embd)\n\n        self.gn = nn.GroupNorm(16, n_embd)\n\n        assert kernel_size % 2 == 1\n        # add 1 to avoid have the same size as the instant-level branch\n        up_size = round((kernel_size + 1) * k)\n        up_size = up_size + 1 if up_size % 2 == 0 else up_size\n\n        self.psi = nn.Conv1d(n_embd, n_embd, kernel_size, stride=1, padding=kernel_size // 2, groups=n_embd)\n        self.fc = nn.Conv1d(n_embd, n_embd, 1, stride=1, padding=0, groups=n_embd)\n        self.convw = nn.Conv1d(n_embd, n_embd, kernel_size, stride=1, padding=kernel_size // 2, groups=n_embd)\n        self.convkw = nn.Conv1d(n_embd, n_embd, up_size, stride=1, padding=up_size // 2, groups=n_embd)\n        self.global_fc = nn.Conv1d(n_embd, n_embd, 1, stride=1, padding=0, groups=n_embd)\n\n        # input\n        if n_ds_stride > 1:\n            if downsample_type == 'max':\n                kernel_size, stride, padding = \\\n                    n_ds_stride + 1, n_ds_stride, (n_ds_stride + 1) // 2\n                self.downsample = nn.MaxPool1d(\n                    kernel_size, stride=stride, padding=padding)\n                self.stride = stride\n            elif downsample_type == 'avg':\n                self.downsample = nn.Sequential(nn.AvgPool1d(n_ds_stride, stride=n_ds_stride, padding=0),\n                                                nn.Conv1d(n_embd, n_embd, 1, 1, 0))\n                self.stride = n_ds_stride\n            else:\n                raise NotImplementedError(\"downsample type error\")\n        else:\n            self.downsample = nn.Identity()\n            self.stride = 1\n\n        # two layer mlp\n        if n_hidden is None:\n            n_hidden = 4 * n_embd  # default\n        if n_out is None:\n            n_out = n_embd\n\n        self.mlp = nn.Sequential(\n            nn.Conv1d(n_embd, n_hidden, 1, groups=group),\n            act_layer(),\n            nn.Conv1d(n_hidden, n_out, 1, groups=group),\n        )\n\n        # drop path\n        if path_pdrop > 0.0:\n            self.drop_path_out = AffineDropPath(n_embd, drop_prob=path_pdrop)\n            self.drop_path_mlp = AffineDropPath(n_out, drop_prob=path_pdrop)\n        else:\n            self.drop_path_out = nn.Identity()\n            self.drop_path_mlp = nn.Identity()\n\n        self.act = act_layer()\n        self.reset_params(init_conv_vars=init_conv_vars)\n\n    def reset_params(self, init_conv_vars=0):\n        torch.nn.init.normal_(self.psi.weight, 0, init_conv_vars)\n        torch.nn.init.normal_(self.fc.weight, 0, init_conv_vars)\n        torch.nn.init.normal_(self.convw.weight, 0, init_conv_vars)\n        torch.nn.init.normal_(self.convkw.weight, 0, init_conv_vars)\n        torch.nn.init.normal_(self.global_fc.weight, 0, init_conv_vars)\n        torch.nn.init.constant_(self.psi.bias, 0)\n        torch.nn.init.constant_(self.fc.bias, 0)\n        torch.nn.init.constant_(self.convw.bias, 0)\n        torch.nn.init.constant_(self.convkw.bias, 0)\n        torch.nn.init.constant_(self.global_fc.bias, 0)\n\n    def forward(self, x, mask):\n        # X shape: B, C, T\n        B, C, T = x.shape\n        x = self.downsample(x)\n        out_mask = F.interpolate(\n            mask.to(x.dtype),\n            size=torch.div(T, self.stride, rounding_mode='trunc'),\n            mode='nearest'\n        ).detach()\n\n        out = self.ln(x)\n        psi = self.psi(out)\n        fc = self.fc(out)\n        convw = self.convw(out)\n        convkw = self.convkw(out)\n        phi = torch.relu(self.global_fc(out.mean(dim=-1, keepdim=True)))\n        out = fc * phi + (convw + convkw) * psi + out\n\n        out = x * out_mask + self.drop_path_out(out)\n        # FFN\n        out = out + self.drop_path_mlp(self.mlp(self.gn(out)))\n\n        return out, out_mask.bool()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.213595Z","iopub.execute_input":"2025-10-04T16:20:11.213835Z","iopub.status.idle":"2025-10-04T16:20:11.230206Z","shell.execute_reply.started":"2025-10-04T16:20:11.213815Z","shell.execute_reply":"2025-10-04T16:20:11.229474Z"}},"outputs":[],"execution_count":12},{"id":"b2d23f6d","cell_type":"code","source":"class ConvBlock(nn.Module):\n    \"\"\"\n    A simple conv block similar to the basic block used in ResNet\n    \"\"\"\n    def __init__(\n        self,\n        n_embd,                # dimension of the input features\n        kernel_size=3,         # conv kernel size\n        n_ds_stride=1,         # downsampling stride for the current layer\n        expansion_factor=2,    # expansion factor of feat dims\n        n_out=None,            # output dimension, if None, set to input dim\n        act_layer=nn.ReLU,     # nonlinear activation used after conv, default ReLU\n    ):\n        super().__init__()\n        # must use odd sized kernel\n        assert (kernel_size % 2 == 1) and (kernel_size > 1)\n        padding = kernel_size // 2\n        if n_out is None:\n            n_out = n_embd\n\n         # 1x3 (strided) -> 1x3 (basic block in resnet)\n        width = n_embd * expansion_factor\n        self.conv1 = MaskedConv1D(\n            n_embd, width, kernel_size, n_ds_stride, padding=padding)\n        self.conv2 = MaskedConv1D(\n            width, n_out, kernel_size, 1, padding=padding)\n\n        # attach downsampling conv op\n        if n_ds_stride > 1:\n            # 1x1 strided conv (same as resnet)\n            self.downsample = MaskedConv1D(n_embd, n_out, 1, n_ds_stride)\n        else:\n            self.downsample = None\n\n        self.act = act_layer()\n\n    def forward(self, x, mask, pos_embd=None):\n        identity = x\n        out, out_mask = self.conv1(x, mask)\n        out = self.act(out)\n        out, out_mask = self.conv2(out, out_mask)\n\n        # downsampling\n        if self.downsample is not None:\n            identity, _ = self.downsample(x, mask)\n\n        # residual connection\n        out += identity\n        out = self.act(out)\n\n        return out, out_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.230984Z","iopub.execute_input":"2025-10-04T16:20:11.231520Z","iopub.status.idle":"2025-10-04T16:20:11.244656Z","shell.execute_reply.started":"2025-10-04T16:20:11.231498Z","shell.execute_reply":"2025-10-04T16:20:11.243985Z"}},"outputs":[],"execution_count":13},{"id":"76edede3","cell_type":"code","source":"# Backbone","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.245397Z","iopub.execute_input":"2025-10-04T16:20:11.245649Z","iopub.status.idle":"2025-10-04T16:20:11.257703Z","shell.execute_reply.started":"2025-10-04T16:20:11.245628Z","shell.execute_reply":"2025-10-04T16:20:11.256989Z"}},"outputs":[],"execution_count":14},{"id":"0079dfaa","cell_type":"code","source":"class ConvTransformerBackbone(nn.Module):\n    \"\"\"\n        A backbone that combines convolutions with transformers\n    \"\"\"\n    def __init__(\n        self,\n        n_in,                  # input feature dimension\n        n_embd,                # embedding dimension (after convolution)\n        n_head,                # number of head for self-attention in transformers\n        n_embd_ks,             # conv kernel size of the embedding network\n        max_len,               # max sequence length\n        arch = (2, 2, 5),      # (#convs, #stem transformers, #branch transformers)\n        mha_win_size = [-1]*6, # size of local window for mha\n        scale_factor = 2,      # dowsampling rate for the branch\n        with_ln = False,       # if to attach layernorm after conv\n        attn_pdrop = 0.0,      # dropout rate for the attention map\n        proj_pdrop = 0.0,      # dropout rate for the projection / MLP\n        path_pdrop = 0.0,      # droput rate for drop path\n        use_abs_pe = False,    # use absolute position embedding\n        use_rel_pe = False,    # use relative position embedding\n    ):\n        super().__init__()\n        assert len(arch) == 3\n        assert len(mha_win_size) == (1 + arch[2])\n        self.n_in = n_in\n        self.arch = arch\n        self.mha_win_size = mha_win_size\n        self.max_len = max_len\n        self.relu = nn.ReLU(inplace=True)\n        self.scale_factor = scale_factor\n        self.use_abs_pe = use_abs_pe\n        self.use_rel_pe = use_rel_pe\n\n        # feature projection\n        self.n_in = n_in\n        if isinstance(n_in, (list, tuple)):\n            assert isinstance(n_embd, (list, tuple)) and len(n_in) == len(n_embd)\n            self.proj = nn.ModuleList([\n                MaskedConv1D(c0, c1, 1) for c0, c1 in zip(n_in, n_embd)\n            ])\n            n_in = n_embd = sum(n_embd)\n        else:\n            self.proj = None\n\n        # embedding network using convs\n        self.embd = nn.ModuleList()\n        self.embd_norm = nn.ModuleList()\n        for idx in range(arch[0]):\n            n_in = n_embd if idx > 0 else n_in\n            self.embd.append(\n                MaskedConv1D(\n                    n_in, n_embd, n_embd_ks,\n                    stride=1, padding=n_embd_ks//2, bias=(not with_ln)\n                )\n            )\n            if with_ln:\n                self.embd_norm.append(LayerNorm(n_embd))\n            else:\n                self.embd_norm.append(nn.Identity())\n\n        # position embedding (1, C, T), rescaled by 1/sqrt(n_embd)\n        if self.use_abs_pe:\n            pos_embd = get_sinusoid_encoding(self.max_len, n_embd) / (n_embd**0.5)\n            self.register_buffer(\"pos_embd\", pos_embd, persistent=False)\n\n        # stem network using (vanilla) transformer\n        self.stem = nn.ModuleList()\n        for idx in range(arch[1]):\n            self.stem.append(\n                TransformerBlock(\n                    n_embd, n_head,\n                    n_ds_strides=(1, 1),\n                    attn_pdrop=attn_pdrop,\n                    proj_pdrop=proj_pdrop,\n                    path_pdrop=path_pdrop,\n                    mha_win_size=self.mha_win_size[0],\n                    use_rel_pe=self.use_rel_pe\n                )\n            )\n\n        # main branch using transformer with pooling\n        self.branch = nn.ModuleList()\n        for idx in range(arch[2]):\n            self.branch.append(\n                TransformerBlock(\n                    n_embd, n_head,\n                    n_ds_strides=(self.scale_factor, self.scale_factor),\n                    attn_pdrop=attn_pdrop,\n                    proj_pdrop=proj_pdrop,\n                    path_pdrop=path_pdrop,\n                    mha_win_size=self.mha_win_size[1 + idx],\n                    use_rel_pe=self.use_rel_pe\n                )\n            )\n\n        # init weights\n        self.apply(self.__init_weights__)\n\n    def __init_weights__(self, module):\n        # set nn.Linear/nn.Conv1d bias term to 0\n        if isinstance(module, (nn.Linear, nn.Conv1d)):\n            if module.bias is not None:\n                torch.nn.init.constant_(module.bias, 0.)\n\n    def forward(self, x, mask):\n        # x: batch size, feature channel, sequence length,\n        # mask: batch size, 1, sequence length (bool)\n        B, C, T = x.size()\n\n        # feature projection\n        if isinstance(self.n_in, (list, tuple)):\n            x = torch.cat(\n                [proj(s, mask)[0] \\\n                    for proj, s in zip(self.proj, x.split(self.n_in, dim=1))\n                ], dim=1\n            )\n\n        # embedding network\n        for idx in range(len(self.embd)):\n            x, mask = self.embd[idx](x, mask)\n            x = self.relu(self.embd_norm[idx](x))\n\n        # training: using fixed length position embeddings\n        if self.use_abs_pe and self.training:\n            assert T <= self.max_len, \"Reached max length.\"\n            pe = self.pos_embd\n            # add pe to x\n            x = x + pe[:, :, :T] * mask.to(x.dtype)\n\n        # inference: re-interpolate position embeddings for over-length sequences\n        if self.use_abs_pe and (not self.training):\n            if T >= self.max_len:\n                pe = F.interpolate(\n                    self.pos_embd, T, mode='linear', align_corners=False)\n            else:\n                pe = self.pos_embd\n            # add pe to x\n            x = x + pe[:, :, :T] * mask.to(x.dtype)\n\n        # stem transformer\n        for idx in range(len(self.stem)):\n            x, mask = self.stem[idx](x, mask)\n\n        # prep for outputs\n        out_feats = (x, )\n        out_masks = (mask, )\n\n        # main branch with downsampling\n        for idx in range(len(self.branch)):\n            x, mask = self.branch[idx](x, mask)\n            out_feats += (x, )\n            out_masks += (mask, )\n\n        return out_feats, out_masks\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.258526Z","iopub.execute_input":"2025-10-04T16:20:11.259059Z","iopub.status.idle":"2025-10-04T16:20:11.274419Z","shell.execute_reply.started":"2025-10-04T16:20:11.259043Z","shell.execute_reply":"2025-10-04T16:20:11.273750Z"}},"outputs":[],"execution_count":15},{"id":"1abba785","cell_type":"code","source":"class ConvBackbone(nn.Module):\n    \"\"\"\n        A backbone that with only conv\n    \"\"\"\n    def __init__(\n        self,\n        n_in,               # input feature dimension\n        n_embd,             # embedding dimension (after convolution)\n        n_embd_ks,          # conv kernel size of the embedding network\n        arch = (2, 2, 5),   # (#convs, #stem convs, #branch convs)\n        scale_factor = 2,   # dowsampling rate for the branch\n        with_ln=False,      # if to use layernorm\n    ):\n        super().__init__()\n        assert len(arch) == 3\n        self.n_in = n_in\n        self.arch = arch\n        self.relu = nn.ReLU(inplace=True)\n        self.scale_factor = scale_factor\n\n        # feature projection\n        self.n_in = n_in\n        if isinstance(n_in, (list, tuple)):\n            assert isinstance(n_embd, (list, tuple)) and len(n_in) == len(n_embd)\n            self.proj = nn.ModuleList([\n                MaskedConv1D(c0, c1, 1) for c0, c1 in zip(n_in, n_embd)\n            ])\n            n_in = n_embd = sum(n_embd)\n        else:\n            self.proj = None\n\n        # embedding network using convs\n        self.embd = nn.ModuleList()\n        self.embd_norm = nn.ModuleList()\n        for idx in range(arch[0]):\n            n_in = n_embd if idx > 0 else n_in\n            self.embd.append(\n                MaskedConv1D(\n                    n_in, n_embd, n_embd_ks,\n                    stride=1, padding=n_embd_ks//2, bias=(not with_ln)\n                )\n            )\n            if with_ln:\n                self.embd_norm.append(LayerNorm(n_embd))\n            else:\n                self.embd_norm.append(nn.Identity())\n\n        # stem network using convs\n        self.stem = nn.ModuleList()\n        for idx in range(arch[1]):\n            self.stem.append(ConvBlock(n_embd, 3, 1))\n\n        # main branch using convs with pooling\n        self.branch = nn.ModuleList()\n        for idx in range(arch[2]):\n            self.branch.append(ConvBlock(n_embd, 3, self.scale_factor))\n\n        # init weights\n        self.apply(self.__init_weights__)\n\n    def __init_weights__(self, module):\n        # set nn.Linear bias term to 0\n        if isinstance(module, (nn.Linear, nn.Conv1d)):\n            if module.bias is not None:\n                torch.nn.init.constant_(module.bias, 0.)\n\n    def forward(self, x, mask):\n        # x: batch size, feature channel, sequence length,\n        # mask: batch size, 1, sequence length (bool)\n        B, C, T = x.size()\n\n        # feature projection\n        if isinstance(self.n_in, (list, tuple)):\n            x = torch.cat(\n                [proj(s, mask)[0] \\\n                    for proj, s in zip(self.proj, x.split(self.n_in, dim=1))\n                ], dim=1\n            )\n\n        # embedding network\n        for idx in range(len(self.embd)):\n            x, mask = self.embd[idx](x, mask)\n            x = self.relu(self.embd_norm[idx](x))\n\n        # stem conv\n        for idx in range(len(self.stem)):\n            x, mask = self.stem[idx](x, mask)\n\n        # prep for outputs\n        out_feats = (x, )\n        out_masks = (mask, )\n\n        # main branch with downsampling\n        for idx in range(len(self.branch)):\n            x, mask = self.branch[idx](x, mask)\n            out_feats += (x, )\n            out_masks += (mask, )\n\n        return out_feats, out_masks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.275163Z","iopub.execute_input":"2025-10-04T16:20:11.275430Z","iopub.status.idle":"2025-10-04T16:20:11.291825Z","shell.execute_reply.started":"2025-10-04T16:20:11.275406Z","shell.execute_reply":"2025-10-04T16:20:11.291073Z"}},"outputs":[],"execution_count":16},{"id":"33d627bc-4d51-442d-8be5-ca8f1d59874b","cell_type":"code","source":"class SGPBackbone(nn.Module):\n    \"\"\"\n        A backbone that combines SGP layer with transformers\n    \"\"\"\n\n    def __init__(\n            self,\n            n_in,  # input feature dimension\n            n_embd,  # embedding dimension (after convolution)\n            sgp_mlp_dim,  # the numnber of dim in SGP\n            n_embd_ks,  # conv kernel size of the embedding network\n            max_len,  # max sequence length\n            arch=(2, 2, 5),  # (#convs, #stem transformers, #branch transformers)\n            scale_factor=2,  # dowsampling rate for the branch,\n            with_ln=False,  # if to attach layernorm after conv\n            path_pdrop=0.0,  # droput rate for drop path\n            downsample_type='max',  # how to downsample feature in FPN\n            sgp_win_size=[-1] * 6,  # size of local window for mha\n            k=1.5,  # the K in SGP\n            init_conv_vars=1,  # initialization of gaussian variance for the weight in SGP\n            use_abs_pe=False,  # use absolute position embedding\n    ):\n        super().__init__()\n        assert len(arch) == 3\n        assert len(sgp_win_size) == (1 + arch[2])\n        self.arch = arch\n        self.sgp_win_size = sgp_win_size\n        self.max_len = max_len\n        self.relu = nn.ReLU(inplace=True)\n        self.scale_factor = scale_factor\n        self.use_abs_pe = use_abs_pe\n\n        # position embedding (1, C, T), rescaled by 1/sqrt(n_embd)\n        if self.use_abs_pe:\n            pos_embd = get_sinusoid_encoding(self.max_len, n_embd) / (n_embd ** 0.5)\n            self.register_buffer(\"pos_embd\", pos_embd, persistent=False)\n\n        # embedding network using convs\n        self.embd = nn.ModuleList()\n        self.embd_norm = nn.ModuleList()\n        for idx in range(arch[0]):\n            if idx == 0:\n                in_channels = n_in\n            else:\n                in_channels = n_embd\n            self.embd.append(MaskedConv1D(\n                in_channels, n_embd, n_embd_ks,\n                stride=1, padding=n_embd_ks // 2, bias=(not with_ln)\n            )\n            )\n            if with_ln:\n                self.embd_norm.append(\n                    LayerNorm(n_embd)\n                )\n            else:\n                self.embd_norm.append(nn.Identity())\n\n        # stem network using (vanilla) transformer\n        self.stem = nn.ModuleList()\n        for idx in range(arch[1]):\n            self.stem.append(\n                SGPBlock(n_embd, 1, 1, n_hidden=sgp_mlp_dim, k=k, init_conv_vars=init_conv_vars))\n\n        # main branch using transformer with pooling\n        self.branch = nn.ModuleList()\n        for idx in range(arch[2]):\n            self.branch.append(SGPBlock(n_embd, self.sgp_win_size[1 + idx], self.scale_factor, path_pdrop=path_pdrop,\n                                        n_hidden=sgp_mlp_dim, downsample_type=downsample_type, k=k,\n                                        init_conv_vars=init_conv_vars))\n        # init weights\n        self.apply(self.__init_weights__)\n\n    def __init_weights__(self, module):\n        # set nn.Linear/nn.Conv1d bias term to 0\n        if isinstance(module, (nn.Linear, nn.Conv1d)):\n            if module.bias is not None:\n                torch.nn.init.constant_(module.bias, 0.)\n\n    def forward(self, x, mask):\n        # x: batch size, feature channel, sequence length,\n        # mask: batch size, 1, sequence length (bool)\n        B, C, T = x.size()\n\n        # embedding network\n        for idx in range(len(self.embd)):\n            x, mask = self.embd[idx](x, mask)\n            x = self.relu(self.embd_norm[idx](x))\n\n        # training: using fixed length position embeddings\n        if self.use_abs_pe and self.training:\n            assert T <= self.max_len, \"Reached max length.\"\n            pe = self.pos_embd\n            # add pe to x\n            x = x + pe[:, :, :T] * mask.to(x.dtype)\n\n        # inference: re-interpolate position embeddings for over-length sequences\n        if self.use_abs_pe and (not self.training):\n            if T >= self.max_len:\n                pe = F.interpolate(\n                    self.pos_embd, T, mode='linear', align_corners=False)\n            else:\n                pe = self.pos_embd\n            # add pe to x\n            x = x + pe[:, :, :T] * mask.to(x.dtype)\n\n        # stem network\n        for idx in range(len(self.stem)):\n            x, mask = self.stem[idx](x, mask)\n\n        # prep for outputs\n        out_feats = tuple()\n        out_masks = tuple()\n        # 1x resolution\n        out_feats += (x,)\n        out_masks += (mask,)\n\n        # main branch with downsampling\n        for idx in range(len(self.branch)):\n            x, mask = self.branch[idx](x, mask)\n            out_feats += (x,)\n            out_masks += (mask,)\n\n        return out_feats, out_masks\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.292690Z","iopub.execute_input":"2025-10-04T16:20:11.292956Z","iopub.status.idle":"2025-10-04T16:20:11.312570Z","shell.execute_reply.started":"2025-10-04T16:20:11.292935Z","shell.execute_reply":"2025-10-04T16:20:11.311612Z"}},"outputs":[],"execution_count":17},{"id":"f10bef55","cell_type":"code","source":"#necks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.313524Z","iopub.execute_input":"2025-10-04T16:20:11.314262Z","iopub.status.idle":"2025-10-04T16:20:11.324231Z","shell.execute_reply.started":"2025-10-04T16:20:11.314235Z","shell.execute_reply":"2025-10-04T16:20:11.323523Z"}},"outputs":[],"execution_count":18},{"id":"75a3a7fc","cell_type":"code","source":"class FPNIdentity(nn.Module):\n    def __init__(\n        self,\n        in_channels,      # input feature channels, len(in_channels) = #levels\n        out_channel,      # output feature channel\n        scale_factor=2.0, # downsampling rate between two fpn levels\n        start_level=0,    # start fpn level\n        end_level=-1,     # end fpn level\n        with_ln=True,     # if to apply layer norm at the end\n    ):\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.out_channel = out_channel\n        self.scale_factor = scale_factor\n\n        self.start_level = start_level\n        if end_level == -1:\n            self.end_level = len(in_channels)\n        else:\n            self.end_level = end_level\n        assert self.end_level <= len(in_channels)\n        assert (self.start_level >= 0) and (self.start_level < self.end_level)\n\n        self.fpn_norms = nn.ModuleList()\n        for i in range(self.start_level, self.end_level):\n            # check feat dims\n            assert self.in_channels[i] == self.out_channel\n            # layer norm for order (B C T)\n            if with_ln:\n                fpn_norm = LayerNorm(out_channel)\n            else:\n                fpn_norm = nn.Identity()\n            self.fpn_norms.append(fpn_norm)\n\n    def forward(self, inputs, fpn_masks):\n        # inputs must be a list / tuple\n        assert len(inputs) == len(self.in_channels)\n        assert len(fpn_masks) ==  len(self.in_channels)\n\n        # apply norms, fpn_masks will remain the same with 1x1 convs\n        fpn_feats = tuple()\n        new_fpn_masks = tuple()\n        for i in range(len(self.fpn_norms)):\n            x = self.fpn_norms[i](inputs[i + self.start_level])\n            fpn_feats += (x, )\n            new_fpn_masks += (fpn_masks[i + self.start_level], )\n\n        return fpn_feats, new_fpn_masks\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.324958Z","iopub.execute_input":"2025-10-04T16:20:11.325199Z","iopub.status.idle":"2025-10-04T16:20:11.333539Z","shell.execute_reply.started":"2025-10-04T16:20:11.325183Z","shell.execute_reply":"2025-10-04T16:20:11.332753Z"}},"outputs":[],"execution_count":19},{"id":"f6a548f5","cell_type":"code","source":"class FPN1D(nn.Module):\n    \"\"\"\n        Feature pyramid network\n    \"\"\"\n    def __init__(\n        self,\n        in_channels,      # input feature channels, len(in_channels) = # levels\n        out_channel,      # output feature channel\n        scale_factor=2.0, # downsampling rate between two fpn levels\n        start_level=0,    # start fpn level\n        end_level=-1,     # end fpn level\n        with_ln=True,     # if to apply layer norm at the end\n    ):\n        super().__init__()\n        assert isinstance(in_channels, list) or isinstance(in_channels, tuple)\n\n        self.in_channels = in_channels\n        self.out_channel = out_channel\n        self.scale_factor = scale_factor\n\n        self.start_level = start_level\n        if end_level == -1:\n            self.end_level = len(in_channels)\n        else:\n            self.end_level = end_level\n        assert self.end_level <= len(in_channels)\n        assert (self.start_level >= 0) and (self.start_level < self.end_level)\n\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n        self.fpn_norms = nn.ModuleList()\n        for i in range(self.start_level, self.end_level):\n            # disable bias if using layer norm\n            l_conv = MaskedConv1D(\n                in_channels[i], out_channel, 1, bias=(not with_ln)\n            )\n            # use depthwise conv here for efficiency\n            fpn_conv = MaskedConv1D(\n                out_channel, out_channel, 3,\n                padding=1, bias=(not with_ln), groups=out_channel\n            )\n            # layer norm for order (B C T)\n            if with_ln:\n                fpn_norm = LayerNorm(out_channel)\n            else:\n                fpn_norm = nn.Identity()\n\n            self.lateral_convs.append(l_conv)\n            self.fpn_convs.append(fpn_conv)\n            self.fpn_norms.append(fpn_norm)\n\n    def forward(self, inputs, fpn_masks):\n        # inputs must be a list / tuple\n        assert len(inputs) == len(self.in_channels)\n        assert len(fpn_masks) ==  len(self.in_channels)\n\n        # build laterals, fpn_masks will remain the same with 1x1 convs\n        laterals = []\n        for i in range(len(self.lateral_convs)):\n            x, _ = self.lateral_convs[i](\n                inputs[i + self.start_level], fpn_masks[i + self.start_level]\n            )\n            laterals.append(x)\n\n        # build top-down path\n        used_backbone_levels = len(laterals)\n        for i in range(used_backbone_levels - 1, 0, -1):\n            laterals[i - 1] += F.interpolate(\n                laterals[i], scale_factor=self.scale_factor, mode='nearest'\n            )\n\n        # fpn conv / norm -> outputs\n        # mask will remain the same\n        fpn_feats = tuple()\n        new_fpn_masks = tuple()\n        for i in range(used_backbone_levels):\n            x, new_mask = self.fpn_convs[i](\n                laterals[i], fpn_masks[i + self.start_level])\n            x = self.fpn_norms[i](x)\n            fpn_feats += (x, )\n            new_fpn_masks += (new_mask, )\n\n        return fpn_feats, new_fpn_masks\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.334311Z","iopub.execute_input":"2025-10-04T16:20:11.334557Z","iopub.status.idle":"2025-10-04T16:20:11.351657Z","shell.execute_reply.started":"2025-10-04T16:20:11.334537Z","shell.execute_reply":"2025-10-04T16:20:11.350946Z"}},"outputs":[],"execution_count":20},{"id":"b8f9c410","cell_type":"code","source":"# Generator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.352554Z","iopub.execute_input":"2025-10-04T16:20:11.352907Z","iopub.status.idle":"2025-10-04T16:20:11.363446Z","shell.execute_reply.started":"2025-10-04T16:20:11.352885Z","shell.execute_reply":"2025-10-04T16:20:11.362605Z"}},"outputs":[],"execution_count":21},{"id":"9025dd99","cell_type":"code","source":"class PointGenerator(nn.Module):\n    \"\"\"\n        A generator for temporal \"points\"\n        \n        max_seq_len can be much larger than the actual seq length\n    \"\"\"\n    def __init__(\n        self,\n        max_seq_len,        # max sequence length that the generator will buffer\n        fpn_strides,        # strides of fpn levels\n        regression_range,   # regression range (on feature grids)\n        use_offset=False    # if to align the points at grid centers\n    ):\n        super().__init__()\n        # sanity check, # fpn levels and length divisible\n        fpn_levels = len(fpn_strides)\n        assert len(regression_range) == fpn_levels\n\n        # save params\n        self.max_seq_len = max_seq_len\n        self.fpn_levels = fpn_levels\n        self.fpn_strides = fpn_strides\n        self.regression_range = regression_range\n        self.use_offset = use_offset\n\n        # generate all points and buffer the list\n        self.buffer_points = self._generate_points()\n\n    def _generate_points(self):\n        points_list = []\n        # loop over all points at each pyramid level\n        for l, stride in enumerate(self.fpn_strides):\n            reg_range = torch.as_tensor(\n                self.regression_range[l], dtype=torch.float)\n            fpn_stride = torch.as_tensor(stride, dtype=torch.float)\n            points = torch.arange(0, self.max_seq_len, stride)[:, None]\n            # add offset if necessary (not in our current model)\n            if self.use_offset:\n                points += 0.5 * stride\n            # pad the time stamp with additional regression range / stride\n            reg_range = reg_range[None].repeat(points.shape[0], 1)\n            fpn_stride = fpn_stride[None].repeat(points.shape[0], 1)\n            # size: T x 4 (ts, reg_range, stride)\n            points_list.append(torch.cat((points, reg_range, fpn_stride), dim=1))\n\n        return BufferList(points_list)\n\n    def forward(self, feats):\n        # feats will be a list of torch tensors\n        assert len(feats) == self.fpn_levels\n        pts_list = []\n        feat_lens = [feat.shape[-1] for feat in feats]\n        for feat_len, buffer_pts in zip(feat_lens, self.buffer_points):\n            assert feat_len <= buffer_pts.shape[0], \"Reached max buffer length for point generator\"\n            pts = buffer_pts[:feat_len, :]\n            pts_list.append(pts)\n        return pts_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.364143Z","iopub.execute_input":"2025-10-04T16:20:11.364406Z","iopub.status.idle":"2025-10-04T16:20:11.374914Z","shell.execute_reply.started":"2025-10-04T16:20:11.364384Z","shell.execute_reply":"2025-10-04T16:20:11.374266Z"}},"outputs":[],"execution_count":22},{"id":"ce21828d","cell_type":"code","source":"# Full model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.375641Z","iopub.execute_input":"2025-10-04T16:20:11.376036Z","iopub.status.idle":"2025-10-04T16:20:11.388425Z","shell.execute_reply.started":"2025-10-04T16:20:11.376021Z","shell.execute_reply":"2025-10-04T16:20:11.387733Z"}},"outputs":[],"execution_count":23},{"id":"bc491161-8fd7-4eed-8d9c-e4a50649270d","cell_type":"code","source":"#nms cpp version alternative\ndef nms_1d_cpu(segs, scores, iou_threshold):\n    \"\"\"\n    Pure Python implementation of 1D NMS matching the C++ version exactly\n    \n    Args:\n        segs: tensor of shape (N, 2) representing segments [start, end]\n        scores: tensor of shape (N,) representing confidence scores\n        iou_threshold: IoU threshold for NMS\n    \n    Returns:\n        indices of segments to keep\n    \"\"\"\n    if segs.numel() == 0:\n        return torch.empty((0,), dtype=torch.long, device=segs.device)\n    \n    # Extract x1, x2 coordinates\n    x1_t = segs[:, 0].contiguous()\n    x2_t = segs[:, 1].contiguous()\n    \n    # Compute areas\n    areas_t = x2_t - x1_t + 1e-6\n    \n    # Sort scores in descending order\n    order_t = torch.argsort(scores, descending=True)\n    \n    nsegs = segs.size(0)\n    select_t = torch.ones(nsegs, dtype=torch.bool, device=segs.device)\n    \n    # Convert to numpy-like access for easier translation\n    select = select_t\n    order = order_t\n    x1 = x1_t\n    x2 = x2_t\n    areas = areas_t\n    \n    # Main NMS loop - direct translation from C++\n    for _i in range(nsegs):\n        if not select[_i]:\n            continue\n        \n        i = order[_i]\n        ix1 = x1[i]\n        ix2 = x2[i]\n        iarea = areas[i]\n        \n        for _j in range(_i + 1, nsegs):\n            if not select[_j]:\n                continue\n            \n            j = order[_j]\n            xx1 = torch.max(ix1, x1[j])\n            xx2 = torch.min(ix2, x2[j])\n            \n            inter = torch.max(torch.tensor(0.0, device=segs.device), xx2 - xx1)\n            ovr = inter / (iarea + areas[j] - inter)\n            \n            if ovr >= iou_threshold:\n                select[_j] = False\n    \n    return order_t[select_t]\n\ndef softnms_1d_cpu(segs, scores, dets, iou_threshold, sigma, min_score, method):\n    \"\"\"\n    Pure Python implementation of 1D Soft NMS matching the C++ version exactly\n    \n    Args:\n        segs: tensor of shape (N, 2) representing segments [start, end]\n        scores: tensor of shape (N,) representing confidence scores\n        dets: tensor of shape (N, 3) to store results [x1, x2, score]\n        iou_threshold: IoU threshold for NMS\n        sigma: Gaussian sigma for soft NMS\n        min_score: minimum score threshold\n        method: 0=vanilla NMS, 1=linear, 2=gaussian\n    \n    Returns:\n        indices of segments to keep\n    \"\"\"\n    if segs.numel() == 0:\n        return torch.empty((0,), dtype=torch.long, device=segs.device)\n    \n    # Extract coordinates and clone scores\n    x1_t = segs[:, 0].contiguous()\n    x2_t = segs[:, 1].contiguous()\n    scores_t = scores.clone()\n    \n    # Compute areas\n    areas_t = x1_t - x1_t + 1e-6  # Initialize, will be computed properly\n    areas_t = x2_t - x1_t + 1e-6\n    \n    nsegs = segs.size(0)\n    \n    # Create indices tensor\n    inds_t = torch.arange(nsegs, dtype=torch.long, device=segs.device)\n    \n    # Work with cloned tensors to allow in-place modifications\n    x1 = x1_t.clone()\n    x2 = x2_t.clone()\n    sc = scores_t\n    areas = areas_t.clone()\n    inds = inds_t.clone()\n    \n    # Main soft NMS loop - direct translation from C++\n    for i in range(nsegs):\n        max_score = sc[i]\n        max_pos = i\n        \n        # Find segment with max score\n        for pos in range(i + 1, nsegs):\n            if max_score < sc[pos]:\n                max_score = sc[pos]\n                max_pos = pos\n        \n        # Swap current segment (i) with max score segment (max_pos)\n        # Store in dets\n        ix1 = x1[max_pos].clone()\n        ix2 = x2[max_pos].clone()\n        iscore = sc[max_pos].clone()\n        iarea = areas[max_pos].clone()\n        iind = inds[max_pos].clone()\n        \n        dets[i, 0] = ix1\n        dets[i, 1] = ix2\n        dets[i, 2] = iscore\n        \n        # Swap elements\n        x1[max_pos] = x1[i]\n        x2[max_pos] = x2[i]\n        sc[max_pos] = sc[i]\n        areas[max_pos] = areas[i]\n        inds[max_pos] = inds[i]\n        \n        x1[i] = ix1\n        x2[i] = ix2\n        sc[i] = iscore\n        areas[i] = iarea\n        inds[i] = iind\n        \n        # Apply soft NMS to remaining segments\n        pos = i + 1\n        while pos < nsegs:\n            xx1 = torch.max(ix1, x1[pos])\n            xx2 = torch.min(ix2, x2[pos])\n            \n            inter = torch.max(torch.tensor(0.0, device=segs.device), xx2 - xx1)\n            ovr = inter / (iarea + areas[pos] - inter)\n            \n            # Compute weight based on method\n            weight = 1.0\n            if method == 0:  # vanilla NMS\n                if ovr >= iou_threshold:\n                    weight = 0.0\n            elif method == 1:  # linear\n                if ovr >= iou_threshold:\n                    weight = 1.0 - ovr\n            elif method == 2:  # gaussian\n                weight = torch.exp(-(ovr * ovr) / sigma)\n            \n            sc[pos] *= weight\n            \n            # Remove segments with low scores by swapping with last element\n            if sc[pos] < min_score:\n                x1[pos] = x1[nsegs - 1]\n                x2[pos] = x2[nsegs - 1]\n                sc[pos] = sc[nsegs - 1]\n                areas[pos] = areas[nsegs - 1]\n                inds[pos] = inds[nsegs - 1]\n                nsegs -= 1\n                pos -= 1\n            \n            pos += 1\n    \n    return inds_t[:nsegs]\n\ndef nms_1d(segs, scores, iou_threshold):\n    \"\"\"\n    1D NMS interface matching the C++ version\n    \"\"\"\n    # Ensure tensors are on CPU and contiguous\n    if segs.device.type == 'cuda':\n        segs = segs.cpu()\n    if scores.device.type == 'cuda':\n        scores = scores.cpu()\n    \n    segs = segs.contiguous()\n    scores = scores.contiguous()\n    \n    return nms_1d_cpu(segs, scores, iou_threshold)\n\ndef softnms_1d(segs, scores, dets, iou_threshold, sigma, min_score, method):\n    \"\"\"\n    1D Soft NMS interface matching the C++ version\n    \"\"\"\n    # Ensure tensors are on CPU and contiguous\n    if segs.device.type == 'cuda':\n        segs = segs.cpu()\n    if scores.device.type == 'cuda':\n        scores = scores.cpu()\n    if dets.device.type == 'cuda':\n        dets = dets.cpu()\n    \n    segs = segs.contiguous()\n    scores = scores.contiguous()\n    dets = dets.contiguous()\n    \n    return softnms_1d_cpu(segs, scores, dets, iou_threshold, sigma, min_score, method)\n\n# Alternative optimized versions using vectorized operations\ndef nms_1d_vectorized(segs, scores, iou_threshold):\n    \"\"\"\n    Vectorized version of 1D NMS for better performance\n    \"\"\"\n    if segs.numel() == 0:\n        return torch.empty((0,), dtype=torch.long, device=segs.device)\n    \n    x1 = segs[:, 0]\n    x2 = segs[:, 1]\n    areas = x2 - x1 + 1e-6\n    \n    # Sort by scores in descending order\n    _, order = scores.sort(0, descending=True)\n    \n    keep = []\n    while order.numel() > 0:\n        i = order[0]\n        keep.append(i)\n        \n        if order.numel() == 1:\n            break\n            \n        # Compute IoU with remaining segments\n        xx1 = torch.maximum(x1[i], x1[order[1:]])\n        xx2 = torch.minimum(x2[i], x2[order[1:]])\n        \n        inter = torch.clamp(xx2 - xx1, min=0.0)\n        union = areas[i] + areas[order[1:]] - inter\n        iou = inter / union\n        \n        # Keep segments with IoU below threshold\n        inds = torch.where(iou <= iou_threshold)[0]\n        order = order[inds + 1]\n    \n    return torch.tensor(keep, dtype=torch.long, device=segs.device)\n\ndef softnms_1d_vectorized(segs, scores, iou_threshold, sigma=0.5, min_score=0.001, method=2):\n    \"\"\"\n    Vectorized version of 1D Soft NMS for better performance\n    \"\"\"\n    if segs.numel() == 0:\n        return torch.empty((0, 3), dtype=torch.float32, device=segs.device), torch.empty((0,), dtype=torch.long)\n    \n    x1 = segs[:, 0].clone()\n    x2 = segs[:, 1].clone()\n    scores_copy = scores.clone()\n    areas = x2 - x1 + 1e-6\n    \n    n_segs = segs.size(0)\n    indices = torch.arange(n_segs, dtype=torch.long, device=segs.device)\n    \n    dets = torch.zeros((n_segs, 3), dtype=torch.float32, device=segs.device)\n    \n    for i in range(n_segs):\n        # Find segment with maximum score\n        max_score, max_pos = scores_copy[i:].max(0)\n        max_pos = max_pos + i\n        \n        # Swap current segment with max score segment\n        if max_pos != i:\n            x1[i], x1[max_pos] = x1[max_pos].clone(), x1[i].clone()\n            x2[i], x2[max_pos] = x2[max_pos].clone(), x2[i].clone()\n            scores_copy[i], scores_copy[max_pos] = scores_copy[max_pos].clone(), scores_copy[i].clone()\n            areas[i], areas[max_pos] = areas[max_pos].clone(), areas[i].clone()\n            indices[i], indices[max_pos] = indices[max_pos].clone(), indices[i].clone()\n        \n        dets[i, 0] = x1[i]\n        dets[i, 1] = x2[i]\n        dets[i, 2] = scores_copy[i]\n        \n        # Apply soft NMS to remaining segments\n        if i < n_segs - 1:\n            xx1 = torch.maximum(x1[i], x1[i+1:])\n            xx2 = torch.minimum(x2[i], x2[i+1:])\n            \n            inter = torch.clamp(xx2 - xx1, min=0.0)\n            union = areas[i] + areas[i+1:] - inter\n            iou = inter / union\n            \n            # Apply weight based on method\n            if method == 0:  # vanilla NMS\n                weight = (iou < iou_threshold).float()\n            elif method == 1:  # linear\n                weight = torch.where(iou >= iou_threshold, 1 - iou, torch.ones_like(iou))\n            else:  # gaussian (method == 2)\n                weight = torch.exp(-(iou * iou) / sigma)\n            \n            scores_copy[i+1:] *= weight\n            \n            # Remove segments with score below threshold\n            keep_mask = scores_copy[i+1:] >= min_score\n            if not keep_mask.all():\n                valid_indices = torch.where(keep_mask)[0] + (i + 1)\n                remaining = len(valid_indices)\n                if remaining > 0:\n                    x1[i+1:i+1+remaining] = x1[valid_indices]\n                    x2[i+1:i+1+remaining] = x2[valid_indices]\n                    scores_copy[i+1:i+1+remaining] = scores_copy[valid_indices]\n                    areas[i+1:i+1+remaining] = areas[valid_indices]\n                    indices[i+1:i+1+remaining] = indices[valid_indices]\n                n_segs = i + 1 + remaining\n    \n    return dets[:n_segs], indices[:n_segs]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.389357Z","iopub.execute_input":"2025-10-04T16:20:11.389605Z","iopub.status.idle":"2025-10-04T16:20:11.426235Z","shell.execute_reply.started":"2025-10-04T16:20:11.389585Z","shell.execute_reply":"2025-10-04T16:20:11.425599Z"}},"outputs":[],"execution_count":24},{"id":"1883369b-4bff-49c4-a672-7b315e4298fd","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b8057f24-abcc-430d-bc40-7c0ef103aeb0","cell_type":"code","source":"class NMSop(torch.autograd.Function):\n    @staticmethod\n    def forward(\n        ctx, segs, scores, cls_idxs,\n        iou_threshold, min_score, max_num\n    ):\n        # vanilla nms will not change the score, so we can filter segs first\n        is_filtering_by_score = (min_score > 0)\n        if is_filtering_by_score:\n            valid_mask = scores > min_score\n            segs, scores = segs[valid_mask], scores[valid_mask]\n            cls_idxs = cls_idxs[valid_mask]\n            valid_inds = torch.nonzero(\n                valid_mask, as_tuple=False).squeeze(dim=1)\n\n        # nms op; return inds that is sorted by descending order\n        inds = nms_1d(segs, scores, iou_threshold)\n        # cap by max number\n        if max_num > 0:\n            inds = inds[:min(max_num, len(inds))]\n        # return the sorted segs / scores\n        sorted_segs = segs[inds]\n        sorted_scores = scores[inds]\n        sorted_cls_idxs = cls_idxs[inds]\n        return sorted_segs.clone(), sorted_scores.clone(), sorted_cls_idxs.clone()\n\n\nclass SoftNMSop(torch.autograd.Function):\n    @staticmethod\n    def forward(\n        ctx, segs, scores, cls_idxs,\n        iou_threshold, sigma, min_score, method, max_num\n    ):\n        # pre allocate memory for sorted results\n        dets = segs.new_empty((segs.size(0), 3), device='cpu')\n        # softnms op, return dets that stores the sorted segs / scores\n        inds = softnms_1d(segs, scores, dets, iou_threshold, sigma, min_score, method)\n        # cap by max number\n        if max_num > 0:\n            n_segs = min(len(inds), max_num)\n        else:\n            n_segs = len(inds)\n        sorted_segs = dets[:n_segs, :2]\n        sorted_scores = dets[:n_segs, 2]\n        sorted_cls_idxs = cls_idxs[inds]\n        sorted_cls_idxs = sorted_cls_idxs[:n_segs]\n        return sorted_segs.clone(), sorted_scores.clone(), sorted_cls_idxs.clone()\n\n\ndef seg_voting(nms_segs, all_segs, all_scores, iou_threshold, score_offset=1.5):\n    \"\"\"\n        blur localization results by incorporating side segs.\n        this is known as bounding box voting in object detection literature.\n        slightly boost the performance around iou_threshold\n    \"\"\"\n\n    # *_segs : N_i x 2, all_scores: N,\n    # apply offset\n    offset_scores = all_scores + score_offset\n\n    # computer overlap between nms and all segs\n    # construct the distance matrix of # N_nms x # N_all\n    num_nms_segs, num_all_segs = nms_segs.shape[0], all_segs.shape[0]\n    ex_nms_segs = nms_segs[:, None].expand(num_nms_segs, num_all_segs, 2)\n    ex_all_segs = all_segs[None, :].expand(num_nms_segs, num_all_segs, 2)\n\n    # compute intersection\n    left = torch.maximum(ex_nms_segs[:, :, 0], ex_all_segs[:, :, 0])\n    right = torch.minimum(ex_nms_segs[:, :, 1], ex_all_segs[:, :, 1])\n    inter = (right-left).clamp(min=0)\n\n    # lens of all segments\n    nms_seg_lens = ex_nms_segs[:, :, 1] - ex_nms_segs[:, :, 0]\n    all_seg_lens = ex_all_segs[:, :, 1] - ex_all_segs[:, :, 0]\n\n    # iou\n    iou = inter / (nms_seg_lens + all_seg_lens - inter)\n\n    # get neighbors (# N_nms x # N_all) / weights\n    seg_weights = (iou >= iou_threshold).to(all_scores.dtype) * all_scores[None, :] * iou\n    seg_weights /= torch.sum(seg_weights, dim=1, keepdim=True)\n    refined_segs = seg_weights @ all_segs\n\n    return refined_segs\n\ndef batched_nms(\n    segs,\n    scores,\n    cls_idxs,\n    iou_threshold,\n    min_score,\n    max_seg_num,\n    use_soft_nms=True,\n    multiclass=True,\n    sigma=0.5,\n    voting_thresh=0.75,\n):\n    # Based on Detectron2 implementation,\n    num_segs = segs.shape[0]\n    # corner case, no prediction outputs\n    if num_segs == 0:\n        return torch.zeros([0, 2]),\\\n               torch.zeros([0,]),\\\n               torch.zeros([0,], dtype=cls_idxs.dtype)\n\n    if multiclass:\n        # multiclass nms: apply nms on each class independently\n        new_segs, new_scores, new_cls_idxs = [], [], []\n        for class_id in torch.unique(cls_idxs):\n            curr_indices = torch.where(cls_idxs == class_id)[0]\n            # soft_nms vs nms\n            if use_soft_nms:\n                sorted_segs, sorted_scores, sorted_cls_idxs = SoftNMSop.apply(\n                    segs[curr_indices],\n                    scores[curr_indices],\n                    cls_idxs[curr_indices],\n                    iou_threshold,\n                    sigma,\n                    min_score,\n                    2,\n                    max_seg_num\n                )\n            else:\n                sorted_segs, sorted_scores, sorted_cls_idxs = NMSop.apply(\n                    segs[curr_indices],\n                    scores[curr_indices],\n                    cls_idxs[curr_indices],\n                    iou_threshold,\n                    min_score,\n                    max_seg_num\n                )\n            # disable seg voting for multiclass nms, no sufficient segs\n\n            # fill in the class index\n            new_segs.append(sorted_segs)\n            new_scores.append(sorted_scores)\n            new_cls_idxs.append(sorted_cls_idxs)\n\n        # cat the results\n        new_segs = torch.cat(new_segs)\n        new_scores = torch.cat(new_scores)\n        new_cls_idxs = torch.cat(new_cls_idxs)\n\n    else:\n        # class agnostic\n        if use_soft_nms:\n            new_segs, new_scores, new_cls_idxs = SoftNMSop.apply(\n                segs, scores, cls_idxs, iou_threshold,\n                sigma, min_score, 2, max_seg_num\n            )\n        else:\n            new_segs, new_scores, new_cls_idxs = NMSop.apply(\n                segs, scores, cls_idxs, iou_threshold,\n                min_score, max_seg_num\n            )\n        # seg voting\n        if voting_thresh > 0:\n            new_segs = seg_voting(\n                new_segs,\n                segs,\n                scores,\n                voting_thresh\n            )\n\n    # sort based on scores and return\n    # truncate the results based on max_seg_num\n    _, idxs = new_scores.sort(descending=True)\n    max_seg_num = min(max_seg_num, new_segs.shape[0])\n    # needed for multiclass NMS\n    new_segs = new_segs[idxs[:max_seg_num]]\n    new_scores = new_scores[idxs[:max_seg_num]]\n    new_cls_idxs = new_cls_idxs[idxs[:max_seg_num]]\n    return new_segs, new_scores, new_cls_idxs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.427019Z","iopub.execute_input":"2025-10-04T16:20:11.427269Z","iopub.status.idle":"2025-10-04T16:20:11.448590Z","shell.execute_reply.started":"2025-10-04T16:20:11.427248Z","shell.execute_reply":"2025-10-04T16:20:11.447726Z"}},"outputs":[],"execution_count":25},{"id":"307c7464","cell_type":"code","source":"class PtTransformer(nn.Module):\n    \"\"\"\n        Transformer based model for single stage action localization\n    \"\"\"\n    def __init__(\n        self,\n        backbone_type,         # a string defines which backbone we use\n        fpn_type,              # a string defines which fpn we use\n        backbone_arch,         # a tuple defines #layers in embed / stem / branch\n        scale_factor,          # scale factor between branch layers\n        input_dim,             # input feat dim\n        max_seq_len,           # max sequence length (used for training)\n        max_buffer_len_factor, # max buffer size (defined a factor of max_seq_len)\n        n_head,                # number of heads for self-attention in transformer\n        n_mha_win_size,        # window size for self attention; -1 to use full seq\n        embd_kernel_size,      # kernel size of the embedding network\n        embd_dim,              # output feat channel of the embedding network\n        embd_with_ln,          # attach layernorm to embedding network\n        fpn_dim,               # feature dim on FPN\n        fpn_with_ln,           # if to apply layer norm at the end of fpn\n        fpn_start_level,       # start level of fpn\n        head_dim,              # feature dim for head\n        regression_range,      # regression range on each level of FPN\n        head_num_layers,       # number of layers in the head (including the classifier)\n        head_kernel_size,      # kernel size for reg/cls heads\n        head_with_ln,          # attache layernorm to reg/cls heads\n        use_abs_pe,            # if to use abs position encoding\n        use_rel_pe,            # if to use rel position encoding\n        num_classes,           # number of action classes\n        train_cfg,             # other cfg for training\n        test_cfg,              # other cfg for testing\n        \n        n_sgp_win_size,        #new -  # window size w for sgp\n        downsample_type,       #new -  # how to downsample feature in FPN\n        sgp_mlp_dim,           #new -  # the numnber of dim in SGP\n        init_conv_vars,        #new -  # initialization of gaussian variance for the weight in SGP\n        k                      #new -  # the K in SGP\n\n        \n        \n    ):\n        super().__init__()\n         # re-distribute params to backbone / neck / head\n        self.fpn_strides = [scale_factor**i for i in range(\n            fpn_start_level, backbone_arch[-1]+1\n        )]\n        self.reg_range = regression_range\n        assert len(self.fpn_strides) == len(self.reg_range)\n        self.scale_factor = scale_factor\n        # #classes = num_classes + 1 (background) with last category as background\n        # e.g., num_classes = 10 -> 0, 1, ..., 9 as actions, 10 as background\n        self.num_classes = num_classes\n\n        if backbone_type == 'convTransformer':    \n            # check the feature pyramid and local attention window size\n            self.max_seq_len = max_seq_len\n            if isinstance(n_mha_win_size, int):\n                self.mha_win_size = [n_mha_win_size]*(1 + backbone_arch[-1])\n            else:\n                assert len(n_mha_win_size) == (1 + backbone_arch[-1])\n                self.mha_win_size = n_mha_win_size\n                \n            max_div_factor = 1\n            for l, (s, w) in enumerate(zip(self.fpn_strides, self.mha_win_size)):\n                stride = s * (w // 2) * 2 if w > 1 else s\n                assert max_seq_len % stride == 0, \"max_seq_len must be divisible by fpn stride and window size\"\n                if max_div_factor < stride:\n                    max_div_factor = stride\n            self.max_div_factor = max_div_factor\n\n        if backbone_type == 'SGP':\n            # check the feature pyramid and local attention window size\n            self.max_seq_len = max_seq_len\n            if isinstance(n_sgp_win_size, int):\n                self.sgp_win_size = [n_sgp_win_size] * len(self.fpn_strides)\n            else:\n                assert len(n_sgp_win_size) == len(self.fpn_strides)\n                self.sgp_win_size = n_sgp_win_size\n            max_div_factor = 1\n            for l, (s, w) in enumerate(zip(self.fpn_strides, self.sgp_win_size)):\n                stride = s * w if w > 1 else s\n                if max_div_factor < stride:\n                    max_div_factor = stride\n            self.max_div_factor = max_div_factor\n            \n\n        # training time config\n        self.train_center_sample = train_cfg['center_sample']\n        assert self.train_center_sample in ['radius', 'none']\n        self.train_center_sample_radius = train_cfg['center_sample_radius']\n        self.train_loss_weight = train_cfg['loss_weight']\n        self.train_cls_prior_prob = train_cfg['cls_prior_prob']\n        self.train_dropout = train_cfg['dropout']\n        self.train_droppath = train_cfg['droppath']\n        self.train_label_smoothing = train_cfg['label_smoothing']\n\n        # test time config\n        self.test_pre_nms_thresh = test_cfg['pre_nms_thresh']\n        self.test_pre_nms_topk = test_cfg['pre_nms_topk']\n        self.test_iou_threshold = test_cfg['iou_threshold']\n        self.test_min_score = test_cfg['min_score']\n        self.test_max_seg_num = test_cfg['max_seg_num']\n        self.test_nms_method = test_cfg['nms_method']\n        assert self.test_nms_method in ['soft', 'hard', 'none']\n        self.test_duration_thresh = test_cfg['duration_thresh']\n        self.test_multiclass_nms = test_cfg['multiclass_nms']\n        self.test_nms_sigma = test_cfg['nms_sigma']\n        self.test_voting_thresh = test_cfg['voting_thresh']\n\n        # we will need a better way to dispatch the params to backbones / necks\n        # backbone network: conv + transformer\n        assert backbone_type in ['convTransformer', 'conv', 'SGP']\n        if backbone_type == 'convTransformer':\n            self.backbone = ConvTransformerBackbone(\n                n_in=input_dim,\n                n_embd=embd_dim,\n                n_head=n_head,\n                n_embd_ks=embd_kernel_size,\n                max_len=max_seq_len,\n                arch=backbone_arch,\n                mha_win_size=self.mha_win_size,\n                scale_factor=scale_factor,\n                with_ln=embd_with_ln,\n                attn_pdrop=0.0,\n                proj_pdrop=self.train_dropout,\n                path_pdrop=self.train_droppath,\n                use_abs_pe=use_abs_pe,\n                use_rel_pe=use_rel_pe\n            )\n\n        elif backbone_type == 'conv':\n            self.backbone = ConvBackbone(  # Replace with actual conv backbone class name\n                n_in=input_dim,\n                n_embd=embd_dim,\n                n_embd_ks=embd_kernel_size,\n                arch=backbone_arch,\n                scale_factor=scale_factor,\n                with_ln=embd_with_ln\n            )\n\n        else: \n            self.backbone = SGPBackbone( \n                n_in=input_dim,\n                n_embd=embd_dim,\n                sgp_mlp_dim=sgp_mlp_dim, # new\n                n_embd_ks=embd_kernel_size,\n                max_len=max_seq_len,\n                arch=backbone_arch,\n                scale_factor=scale_factor,\n                with_ln=embd_with_ln,\n                path_pdrop=self.train_droppath,\n                downsample_type=downsample_type, #new\n                sgp_win_size=self.sgp_win_size, #new\n                use_abs_pe=use_abs_pe,\n                k=k, #new\n                init_conv_vars=init_conv_vars #new\n                \n            )\n        if isinstance(embd_dim, (list, tuple)):\n            embd_dim = sum(embd_dim)\n\n        # fpn network: convs\n        assert fpn_type in ['fpn', 'identity']\n        if fpn_type == 'fpn':\n            self.neck = FPN1D(\n                in_channels=[embd_dim] * (backbone_arch[-1] + 1),\n                out_channel=fpn_dim,\n                scale_factor=scale_factor,\n                start_level=fpn_start_level,\n                with_ln=fpn_with_ln\n            )\n        else:  # fpn_type == 'identity'\n            self.neck = FPNIdentity(\n                in_channels=[embd_dim] * (backbone_arch[-1] + 1),\n                out_channel=fpn_dim,\n                scale_factor=scale_factor,\n                start_level=fpn_start_level,\n                with_ln=fpn_with_ln\n            )\n        # location generator: points\n        self.point_generator = PointGenerator(  # Replace with actual class name\n            max_seq_len=max_seq_len * max_buffer_len_factor,\n            fpn_strides=self.fpn_strides,\n            regression_range=self.reg_range\n        )\n\n        # classfication and regerssion heads\n        self.cls_head = PtTransformerClsHead(\n            fpn_dim, head_dim, self.num_classes,\n            kernel_size=head_kernel_size,\n            prior_prob=self.train_cls_prior_prob,\n            with_ln=head_with_ln,\n            num_layers=head_num_layers,\n            empty_cls=train_cfg['head_empty_cls']\n        )\n        self.reg_head = PtTransformerRegHead(\n            fpn_dim, head_dim, len(self.fpn_strides),\n            kernel_size=head_kernel_size,\n            num_layers=head_num_layers,\n            with_ln=head_with_ln\n        )\n\n        # maintain an EMA of #foreground to stabilize the loss normalizer\n        # useful for small mini-batch training\n        self.loss_normalizer = train_cfg['init_loss_norm']\n        self.loss_normalizer_momentum = 0.9\n\n    @property\n    def device(self):\n        # a hacky way to get the device type\n        # will throw an error if parameters are on different devices\n        return list(set(p.device for p in self.parameters()))[0]\n\n    def forward(self, video_list):\n        # batch the video list into feats (B, C, T) and masks (B, 1, T)\n        batched_inputs, batched_masks = self.preprocessing(video_list)\n\n        # forward the network (backbone -> neck -> heads)\n        feats, masks = self.backbone(batched_inputs, batched_masks)\n        fpn_feats, fpn_masks = self.neck(feats, masks)\n\n        # compute the point coordinate along the FPN\n        # this is used for computing the GT or decode the final results\n        # points: List[T x 4] with length = # fpn levels\n        # (shared across all samples in the mini-batch)\n        points = self.point_generator(fpn_feats)\n\n        # out_cls: List[B, #cls + 1, T_i]\n        out_cls_logits = self.cls_head(fpn_feats, fpn_masks)\n        # out_offset: List[B, 2, T_i]\n        out_offsets = self.reg_head(fpn_feats, fpn_masks)\n\n        # permute the outputs\n        # out_cls: F List[B, #cls, T_i] -> F List[B, T_i, #cls]\n        out_cls_logits = [x.permute(0, 2, 1) for x in out_cls_logits]\n        # out_offset: F List[B, 2 (xC), T_i] -> F List[B, T_i, 2 (xC)]\n        out_offsets = [x.permute(0, 2, 1) for x in out_offsets]\n        # fpn_masks: F list[B, 1, T_i] -> F List[B, T_i]\n        fpn_masks = [x.squeeze(1) for x in fpn_masks]\n\n        # return loss during training\n        if self.training:\n            # generate segment/lable List[N x 2] / List[N] with length = B\n            assert video_list[0]['segments'] is not None, \"GT action labels does not exist\"\n            assert video_list[0]['labels'] is not None, \"GT action labels does not exist\"\n            gt_segments = [x['segments'].to(self.device) for x in video_list]\n            gt_labels = [x['labels'].to(self.device) for x in video_list]\n\n            # compute the gt labels for cls & reg\n            # list of prediction targets\n            gt_cls_labels, gt_offsets = self.label_points(\n                points, gt_segments, gt_labels)\n\n            # compute the loss and return\n            losses = self.losses(\n                fpn_masks,\n                out_cls_logits, out_offsets,\n                gt_cls_labels, gt_offsets\n            )\n            return losses\n\n        else:\n            # decode the actions (sigmoid / stride, etc)\n            results = self.inference(\n                video_list, points, fpn_masks,\n                out_cls_logits, out_offsets\n            )\n            return results\n\n    @torch.no_grad()\n    def preprocessing(self, video_list, padding_val=0.0):\n        \"\"\"\n            Generate batched features and masks from a list of dict items\n        \"\"\"\n        feats = [x['feats'] for x in video_list]\n        feats_lens = torch.as_tensor([feat.shape[-1] for feat in feats])\n        max_len = feats_lens.max(0).values.item()\n\n        if self.training:\n            assert max_len <= self.max_seq_len, \"Input length must be smaller than max_seq_len during training\"\n            # set max_len to self.max_seq_len\n            max_len = self.max_seq_len\n            # batch input shape B, C, T\n            batch_shape = [len(feats), feats[0].shape[0], max_len]\n            batched_inputs = feats[0].new_full(batch_shape, padding_val)\n            for feat, pad_feat in zip(feats, batched_inputs):\n                pad_feat[..., :feat.shape[-1]].copy_(feat)\n        else:\n            assert len(video_list) == 1, \"Only support batch_size = 1 during inference\"\n            # input length < self.max_seq_len, pad to max_seq_len\n            if max_len <= self.max_seq_len:\n                max_len = self.max_seq_len\n            else:\n                # pad the input to the next divisible size\n                stride = self.max_div_factor\n                max_len = (max_len + (stride - 1)) // stride * stride\n            padding_size = [0, max_len - feats_lens[0]]\n            batched_inputs = F.pad(\n                feats[0], padding_size, value=padding_val).unsqueeze(0)\n\n        # generate the mask\n        batched_masks = torch.arange(max_len)[None, :] < feats_lens[:, None]\n\n        # push to device\n        batched_inputs = batched_inputs.to(self.device)\n        batched_masks = batched_masks.unsqueeze(1).to(self.device)\n\n        return batched_inputs, batched_masks\n\n    @torch.no_grad()\n    def label_points(self, points, gt_segments, gt_labels):\n        # concat points on all fpn levels List[T x 4] -> F T x 4\n        # This is shared for all samples in the mini-batch\n        num_levels = len(points)\n        concat_points = torch.cat(points, dim=0)\n        gt_cls, gt_offset = [], []\n\n        # loop over each video sample\n        for gt_segment, gt_label in zip(gt_segments, gt_labels):\n            cls_targets, reg_targets = self.label_points_single_video(\n                concat_points, gt_segment, gt_label\n            )\n            # append to list (len = # images, each of size FT x C)\n            gt_cls.append(cls_targets)\n            gt_offset.append(reg_targets)\n\n        return gt_cls, gt_offset\n\n    @torch.no_grad()\n    def label_points_single_video(self, concat_points, gt_segment, gt_label):\n        # concat_points : F T x 4 (t, regression range, stride)\n        # gt_segment : N (#Events) x 2\n        # gt_label : N (#Events) x 1\n        num_pts = concat_points.shape[0]\n        num_gts = gt_segment.shape[0]\n\n        # corner case where current sample does not have actions\n        if num_gts == 0:\n            cls_targets = gt_segment.new_full((num_pts, self.num_classes), 0)\n            reg_targets = gt_segment.new_zeros((num_pts, 2))\n            return cls_targets, reg_targets\n\n        # compute the lengths of all segments -> F T x N\n        lens = gt_segment[:, 1] - gt_segment[:, 0]\n        lens = lens[None, :].repeat(num_pts, 1)\n\n        # compute the distance of every point to each segment boundary\n        # auto broadcasting for all reg target-> F T x N x2\n        gt_segs = gt_segment[None].expand(num_pts, num_gts, 2)\n        left = concat_points[:, 0, None] - gt_segs[:, :, 0]\n        right = gt_segs[:, :, 1] - concat_points[:, 0, None]\n        reg_targets = torch.stack((left, right), dim=-1)\n\n        if self.train_center_sample == 'radius':\n            # center of all segments F T x N\n            center_pts = 0.5 * (gt_segs[:, :, 0] + gt_segs[:, :, 1])\n            # center sampling based on stride radius\n            # compute the new boundaries:\n            # concat_points[:, 3] stores the stride\n            t_mins = \\\n                center_pts - concat_points[:, 3, None] * self.train_center_sample_radius\n            t_maxs = \\\n                center_pts + concat_points[:, 3, None] * self.train_center_sample_radius\n            # prevent t_mins / maxs from over-running the action boundary\n            # left: torch.maximum(t_mins, gt_segs[:, :, 0])\n            # right: torch.minimum(t_maxs, gt_segs[:, :, 1])\n            # F T x N (distance to the new boundary)\n            cb_dist_left = concat_points[:, 0, None] \\\n                           - torch.maximum(t_mins, gt_segs[:, :, 0])\n            cb_dist_right = torch.minimum(t_maxs, gt_segs[:, :, 1]) \\\n                            - concat_points[:, 0, None]\n            # F T x N x 2\n            center_seg = torch.stack(\n                (cb_dist_left, cb_dist_right), -1)\n            # F T x N\n            inside_gt_seg_mask = center_seg.min(-1)[0] > 0\n        else:\n            # inside an gt action\n            inside_gt_seg_mask = reg_targets.min(-1)[0] > 0\n\n        # limit the regression range for each location\n        max_regress_distance = reg_targets.max(-1)[0]\n        # F T x N\n        inside_regress_range = torch.logical_and(\n            (max_regress_distance >= concat_points[:, 1, None]),\n            (max_regress_distance <= concat_points[:, 2, None])\n        )\n\n        # if there are still more than one actions for one moment\n        # pick the one with the shortest duration (easiest to regress)\n        lens.masked_fill_(inside_gt_seg_mask==0, float('inf'))\n        lens.masked_fill_(inside_regress_range==0, float('inf'))\n        # F T x N -> F T\n        min_len, min_len_inds = lens.min(dim=1)\n\n        # corner case: multiple actions with very similar durations (e.g., THUMOS14)\n        min_len_mask = torch.logical_and(\n            (lens <= (min_len[:, None] + 1e-3)), (lens < float('inf'))\n        ).to(reg_targets.dtype)\n\n        # cls_targets: F T x C; reg_targets F T x 2\n        gt_label_one_hot = F.one_hot(\n            gt_label, self.num_classes\n        ).to(reg_targets.dtype)\n        cls_targets = min_len_mask @ gt_label_one_hot\n        # to prevent multiple GT actions with the same label and boundaries\n        cls_targets.clamp_(min=0.0, max=1.0)\n        # OK to use min_len_inds\n        reg_targets = reg_targets[range(num_pts), min_len_inds]\n        # normalization based on stride\n        reg_targets /= concat_points[:, 3, None]\n\n        return cls_targets, reg_targets\n\n    def losses(\n        self, fpn_masks,\n        out_cls_logits, out_offsets,\n        gt_cls_labels, gt_offsets\n    ):\n        # fpn_masks, out_*: F (List) [B, T_i, C]\n        # gt_* : B (list) [F T, C]\n        # fpn_masks -> (B, FT)\n        valid_mask = torch.cat(fpn_masks, dim=1)\n\n        # 1. classification loss\n        # stack the list -> (B, FT) -> (# Valid, )\n        gt_cls = torch.stack(gt_cls_labels)\n        pos_mask = torch.logical_and((gt_cls.sum(-1) > 0), valid_mask)\n\n        # cat the predicted offsets -> (B, FT, 2 (xC)) -> # (#Pos, 2 (xC))\n        pred_offsets = torch.cat(out_offsets, dim=1)[pos_mask]\n        gt_offsets = torch.stack(gt_offsets)[pos_mask]\n\n        # update the loss normalizer\n        num_pos = pos_mask.sum().item()\n        self.loss_normalizer = self.loss_normalizer_momentum * self.loss_normalizer + (\n            1 - self.loss_normalizer_momentum\n        ) * max(num_pos, 1)\n\n        # gt_cls is already one hot encoded now, simply masking out\n        gt_target = gt_cls[valid_mask]\n\n        # optinal label smoothing\n        gt_target *= 1 - self.train_label_smoothing\n        gt_target += self.train_label_smoothing / (self.num_classes + 1)\n\n        # focal loss\n        cls_loss = sigmoid_focal_loss(\n            torch.cat(out_cls_logits, dim=1)[valid_mask],\n            gt_target,\n            reduction='sum'\n        )\n        cls_loss /= self.loss_normalizer\n\n        # 2. regression using IoU/GIoU loss (defined on positive samples)\n        if num_pos == 0:\n            reg_loss = 0 * pred_offsets.sum()\n        else:\n            # giou loss defined on positive samples\n            reg_loss = ctr_diou_loss_1d(\n                pred_offsets,\n                gt_offsets,\n                reduction='sum'\n            )\n            reg_loss /= self.loss_normalizer\n\n        if self.train_loss_weight > 0:\n            loss_weight = self.train_loss_weight\n        else:\n            loss_weight = cls_loss.detach() / max(reg_loss.item(), 0.01)\n\n        # return a dict of losses\n        final_loss = cls_loss + reg_loss * loss_weight\n        return {'cls_loss'   : cls_loss,\n                'reg_loss'   : reg_loss,\n                'final_loss' : final_loss}\n\n    @torch.no_grad()\n    def inference(\n        self,\n        video_list,\n        points, fpn_masks,\n        out_cls_logits, out_offsets\n    ):\n        # video_list B (list) [dict]\n        # points F (list) [T_i, 4]\n        # fpn_masks, out_*: F (List) [B, T_i, C]\n        results = []\n\n        # 1: gather video meta information\n        vid_idxs = [x['video_id'] for x in video_list]\n        vid_fps = [x['fps'] for x in video_list]\n        vid_lens = [x['duration'] for x in video_list]\n        vid_ft_stride = [x['feat_stride'] for x in video_list]\n        vid_ft_nframes = [x['feat_num_frames'] for x in video_list]\n\n        # 2: inference on each single video and gather the results\n        # upto this point, all results use timestamps defined on feature grids\n        for idx, (vidx, fps, vlen, stride, nframes) in enumerate(\n            zip(vid_idxs, vid_fps, vid_lens, vid_ft_stride, vid_ft_nframes)\n        ):\n            # gather per-video outputs\n            cls_logits_per_vid = [x[idx] for x in out_cls_logits]\n            offsets_per_vid = [x[idx] for x in out_offsets]\n            fpn_masks_per_vid = [x[idx] for x in fpn_masks]\n            # inference on a single video (should always be the case)\n            results_per_vid = self.inference_single_video(\n                points, fpn_masks_per_vid,\n                cls_logits_per_vid, offsets_per_vid\n            )\n            # pass through video meta info\n            results_per_vid['video_id'] = vidx\n            results_per_vid['fps'] = fps\n            results_per_vid['duration'] = vlen\n            results_per_vid['feat_stride'] = stride\n            results_per_vid['feat_num_frames'] = nframes\n            results.append(results_per_vid)\n\n        # step 3: postprocssing\n        results = self.postprocessing(results)\n\n        return results\n\n    @torch.no_grad()\n    def inference_single_video(\n        self,\n        points,\n        fpn_masks,\n        out_cls_logits,\n        out_offsets,\n    ):\n        # points F (list) [T_i, 4]\n        # fpn_masks, out_*: F (List) [T_i, C]\n        segs_all = []\n        scores_all = []\n        cls_idxs_all = []\n\n        # loop over fpn levels\n        for cls_i, offsets_i, pts_i, mask_i in zip(\n                out_cls_logits, out_offsets, points, fpn_masks\n            ):\n            # sigmoid normalization for output logits\n            pred_prob = (cls_i.sigmoid() * mask_i.unsqueeze(-1)).flatten()\n\n            # Apply filtering to make NMS faster following detectron2\n            # 1. Keep seg with confidence score > a threshold\n            keep_idxs1 = (pred_prob > self.test_pre_nms_thresh)\n            pred_prob = pred_prob[keep_idxs1]\n            topk_idxs = keep_idxs1.nonzero(as_tuple=True)[0]\n\n            # 2. Keep top k top scoring boxes only\n            num_topk = min(self.test_pre_nms_topk, topk_idxs.size(0))\n            pred_prob, idxs = pred_prob.sort(descending=True)\n            pred_prob = pred_prob[:num_topk].clone()\n            topk_idxs = topk_idxs[idxs[:num_topk]].clone()\n\n            # fix a warning in pytorch 1.9\n            pt_idxs =  torch.div(\n                topk_idxs, self.num_classes, rounding_mode='floor'\n            )\n            cls_idxs = torch.fmod(topk_idxs, self.num_classes)\n\n            # 3. gather predicted offsets\n            offsets = offsets_i[pt_idxs]\n            pts = pts_i[pt_idxs]\n\n            # 4. compute predicted segments (denorm by stride for output offsets)\n            seg_left = pts[:, 0] - offsets[:, 0] * pts[:, 3]\n            seg_right = pts[:, 0] + offsets[:, 1] * pts[:, 3]\n            pred_segs = torch.stack((seg_left, seg_right), -1)\n\n            # 5. Keep seg with duration > a threshold (relative to feature grids)\n            seg_areas = seg_right - seg_left\n            keep_idxs2 = seg_areas > self.test_duration_thresh\n\n            # *_all : N (filtered # of segments) x 2 / 1\n            segs_all.append(pred_segs[keep_idxs2])\n            scores_all.append(pred_prob[keep_idxs2])\n            cls_idxs_all.append(cls_idxs[keep_idxs2])\n\n        # cat along the FPN levels (F N_i, C)\n        segs_all, scores_all, cls_idxs_all = [\n            torch.cat(x) for x in [segs_all, scores_all, cls_idxs_all]\n        ]\n        results = {'segments' : segs_all,\n                   'scores'   : scores_all,\n                   'labels'   : cls_idxs_all}\n\n        return results\n\n    @torch.no_grad()\n    def postprocessing(self, results):\n        # input : list of dictionary items\n        # (1) push to CPU; (2) NMS; (3) convert to actual time stamps\n        processed_results = []\n        for results_per_vid in results:\n            # unpack the meta info\n            vidx = results_per_vid['video_id']\n            fps = results_per_vid['fps']\n            vlen = results_per_vid['duration']\n            stride = results_per_vid['feat_stride']\n            nframes = results_per_vid['feat_num_frames']\n            # 1: unpack the results and move to CPU\n            segs = results_per_vid['segments'].detach().cpu()\n            scores = results_per_vid['scores'].detach().cpu()\n            labels = results_per_vid['labels'].detach().cpu()\n            if self.test_nms_method != 'none':\n                # 2: batched nms (only implemented on CPU)\n                segs, scores, labels = batched_nms(\n                    segs, scores, labels,\n                    self.test_iou_threshold,\n                    self.test_min_score,\n                    self.test_max_seg_num,\n                    use_soft_nms = (self.test_nms_method == 'soft'),\n                    multiclass = self.test_multiclass_nms,\n                    sigma = self.test_nms_sigma,\n                    voting_thresh = self.test_voting_thresh\n                )\n            # 3: convert from feature grids to seconds\n            if segs.shape[0] > 0:\n                segs = (segs * stride + 0.5 * nframes) / fps\n                # truncate all boundaries within [0, duration]\n                segs[segs<=0.0] *= 0.0\n                segs[segs>=vlen] = segs[segs>=vlen] * 0.0 + vlen\n            \n            # 4: repack the results\n            processed_results.append(\n                {'video_id' : vidx,\n                 'segments' : segs,\n                 'scores'   : scores,\n                 'labels'   : labels}\n            )\n\n        return processed_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.449737Z","iopub.execute_input":"2025-10-04T16:20:11.449988Z","iopub.status.idle":"2025-10-04T16:20:11.498679Z","shell.execute_reply.started":"2025-10-04T16:20:11.449968Z","shell.execute_reply":"2025-10-04T16:20:11.497896Z"}},"outputs":[],"execution_count":26},{"id":"b9c50061","cell_type":"code","source":"# Heads","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.499412Z","iopub.execute_input":"2025-10-04T16:20:11.499655Z","iopub.status.idle":"2025-10-04T16:20:11.512161Z","shell.execute_reply.started":"2025-10-04T16:20:11.499639Z","shell.execute_reply":"2025-10-04T16:20:11.511463Z"}},"outputs":[],"execution_count":27},{"id":"be54acb5","cell_type":"code","source":"class PtTransformerRegHead(nn.Module):\n    \"\"\"\n    Shared 1D Conv heads for regression\n    Simlar logic as PtTransformerClsHead with separated implementation for clarity\n    \"\"\"\n    def __init__(\n        self,\n        input_dim,\n        feat_dim,\n        fpn_levels,\n        num_layers=3,\n        kernel_size=3,\n        act_layer=nn.ReLU,\n        with_ln=False\n    ):\n        super().__init__()\n        self.fpn_levels = fpn_levels\n        self.act = act_layer()\n\n        # build the conv head\n        self.head = nn.ModuleList()\n        self.norm = nn.ModuleList()\n        for idx in range(num_layers-1):\n            if idx == 0:\n                in_dim = input_dim\n                out_dim = feat_dim\n            else:\n                in_dim = feat_dim\n                out_dim = feat_dim\n            self.head.append(\n                MaskedConv1D(\n                    in_dim, out_dim, kernel_size,\n                    stride=1,\n                    padding=kernel_size//2,\n                    bias=(not with_ln)\n                )\n            )\n            if with_ln:\n                self.norm.append(LayerNorm(out_dim))\n            else:\n                self.norm.append(nn.Identity())\n\n        self.scale = nn.ModuleList()\n        for idx in range(fpn_levels):\n            self.scale.append(Scale())\n\n        # segment regression\n        self.offset_head = MaskedConv1D(\n                feat_dim, 2, kernel_size,\n                stride=1, padding=kernel_size//2\n            )\n\n    def forward(self, fpn_feats, fpn_masks):\n        assert len(fpn_feats) == len(fpn_masks)\n        assert len(fpn_feats) == self.fpn_levels\n\n        # apply the classifier for each pyramid level\n        out_offsets = tuple()\n        for l, (cur_feat, cur_mask) in enumerate(zip(fpn_feats, fpn_masks)):\n            cur_out = cur_feat\n            for idx in range(len(self.head)):\n                cur_out, _ = self.head[idx](cur_out, cur_mask)\n                cur_out = self.act(self.norm[idx](cur_out))\n            cur_offsets, _ = self.offset_head(cur_out, cur_mask)\n            out_offsets += (F.relu(self.scale[l](cur_offsets)), )\n\n        # fpn_masks remains the same\n        return out_offsets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.517372Z","iopub.execute_input":"2025-10-04T16:20:11.517564Z","iopub.status.idle":"2025-10-04T16:20:11.526292Z","shell.execute_reply.started":"2025-10-04T16:20:11.517550Z","shell.execute_reply":"2025-10-04T16:20:11.525560Z"}},"outputs":[],"execution_count":28},{"id":"a7dafe3a","cell_type":"code","source":"class PtTransformerClsHead(nn.Module):\n    \"\"\"\n    1D Conv heads for classification\n    \"\"\"\n    def __init__(\n        self,\n        input_dim,\n        feat_dim,\n        num_classes,\n        prior_prob=0.01,\n        num_layers=3,\n        kernel_size=3,\n        act_layer=nn.ReLU,\n        with_ln=False,\n        empty_cls = []\n    ):\n        super().__init__()\n        self.act = act_layer()\n\n        # build the head\n        self.head = nn.ModuleList()\n        self.norm = nn.ModuleList()\n        for idx in range(num_layers-1):\n            if idx == 0:\n                in_dim = input_dim\n                out_dim = feat_dim\n            else:\n                in_dim = feat_dim\n                out_dim = feat_dim\n            self.head.append(\n                MaskedConv1D(\n                    in_dim, out_dim, kernel_size,\n                    stride=1,\n                    padding=kernel_size//2,\n                    bias=(not with_ln)\n                )\n            )\n            if with_ln:\n                self.norm.append(LayerNorm(out_dim))\n            else:\n                self.norm.append(nn.Identity())\n\n        # classifier\n        self.cls_head = MaskedConv1D(\n                feat_dim, num_classes, kernel_size,\n                stride=1, padding=kernel_size//2\n            )\n\n        # use prior in model initialization to improve stability\n        # this will overwrite other weight init\n        if prior_prob > 0:\n            bias_value = -(math.log((1 - prior_prob) / prior_prob))\n            torch.nn.init.constant_(self.cls_head.conv.bias, bias_value)\n\n        # a quick fix to empty categories:\n        # the weights assocaited with these categories will remain unchanged\n        # we set their bias to a large negative value to prevent their outputs\n        if len(empty_cls) > 0:\n            bias_value = -(math.log((1 - 1e-6) / 1e-6))\n            for idx in empty_cls:\n                torch.nn.init.constant_(self.cls_head.conv.bias[idx], bias_value)\n\n    def forward(self, fpn_feats, fpn_masks):\n        assert len(fpn_feats) == len(fpn_masks)\n\n        # apply the classifier for each pyramid level\n        out_logits = tuple()\n        for _, (cur_feat, cur_mask) in enumerate(zip(fpn_feats, fpn_masks)):\n            cur_out = cur_feat\n            for idx in range(len(self.head)):\n                cur_out, _ = self.head[idx](cur_out, cur_mask)\n                cur_out = self.act(self.norm[idx](cur_out))\n            cur_logits, _ = self.cls_head(cur_out, cur_mask)\n            out_logits += (cur_logits, )\n\n        # fpn_masks remains the same\n        return out_logits\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.527021Z","iopub.execute_input":"2025-10-04T16:20:11.527263Z","iopub.status.idle":"2025-10-04T16:20:11.540399Z","shell.execute_reply.started":"2025-10-04T16:20:11.527242Z","shell.execute_reply":"2025-10-04T16:20:11.539649Z"}},"outputs":[],"execution_count":29},{"id":"ed677433-824b-4658-bda4-480b5ca2a17b","cell_type":"code","source":"class MaskedConv1D(nn.Module):\n    \"\"\"\n    Masked 1D convolution. Interface remains the same as Conv1d.\n    Only support a sub set of 1d convs\n    \"\"\"\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        dilation=1,\n        groups=1,\n        bias=True,\n        padding_mode='zeros'\n    ):\n        super().__init__()\n        # element must be aligned\n        assert (kernel_size % 2 == 1) and (kernel_size // 2 == padding)\n        # stride\n        self.stride = stride\n        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size,\n                              stride, padding, dilation, groups, bias, padding_mode)\n        # zero out the bias term if it exists\n        if bias:\n            torch.nn.init.constant_(self.conv.bias, 0.)\n\n    def forward(self, x, mask):\n        # x: batch size, feature channel, sequence length,\n        # mask: batch size, 1, sequence length (bool)\n        B, C, T = x.size()\n        # input length must be divisible by stride\n        assert T % self.stride == 0\n\n        # conv\n        out_conv = self.conv(x)\n        # compute the mask\n        if self.stride > 1:\n            # downsample the mask using nearest neighbor\n            out_mask = F.interpolate(\n                mask.to(x.dtype), size=out_conv.size(-1), mode='nearest'\n            )\n        else:\n            # masking out the features\n            out_mask = mask.to(x.dtype)\n\n        # masking the output, stop grad to mask\n        out_conv = out_conv * out_mask.detach()\n        out_mask = out_mask.bool()\n        return out_conv, out_mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.540981Z","iopub.execute_input":"2025-10-04T16:20:11.541241Z","iopub.status.idle":"2025-10-04T16:20:11.555980Z","shell.execute_reply.started":"2025-10-04T16:20:11.541207Z","shell.execute_reply":"2025-10-04T16:20:11.555259Z"}},"outputs":[],"execution_count":30},{"id":"c88d8e7b-6b9a-428f-af5c-db7e283f70bb","cell_type":"code","source":"class LayerNorm(nn.Module):\n    \"\"\"\n    LayerNorm that supports inputs of size B, C, T\n    \"\"\"\n    def __init__(\n        self,\n        num_channels,\n        eps = 1e-5,\n        affine = True,\n        device = None,\n        dtype = None,\n    ):\n        super().__init__()\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        self.num_channels = num_channels\n        self.eps = eps\n        self.affine = affine\n\n        if self.affine:\n            self.weight = nn.Parameter(\n                torch.ones([1, num_channels, 1], **factory_kwargs))\n            self.bias = nn.Parameter(\n                torch.zeros([1, num_channels, 1], **factory_kwargs))\n        else:\n            self.register_parameter('weight', None)\n            self.register_parameter('bias', None)\n\n    def forward(self, x):\n        assert x.dim() == 3\n        assert x.shape[1] == self.num_channels\n\n        # normalization along C channels\n        mu = torch.mean(x, dim=1, keepdim=True)\n        res_x = x - mu\n        sigma = torch.mean(res_x**2, dim=1, keepdim=True)\n        out = res_x / torch.sqrt(sigma + self.eps)\n\n        # apply weight and bias\n        if self.affine:\n            out *= self.weight\n            out += self.bias\n\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.556847Z","iopub.execute_input":"2025-10-04T16:20:11.557102Z","iopub.status.idle":"2025-10-04T16:20:11.569545Z","shell.execute_reply.started":"2025-10-04T16:20:11.557081Z","shell.execute_reply":"2025-10-04T16:20:11.568774Z"}},"outputs":[],"execution_count":31},{"id":"94c2b069-015f-4289-bd19-8c92dad4336a","cell_type":"code","source":"class TransformerBlock(nn.Module):\n    \"\"\"\n    A simple (post layer norm) Transformer block\n    Modified from https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n    \"\"\"\n    def __init__(\n        self,\n        n_embd,                # dimension of the input features\n        n_head,                # number of attention heads\n        n_ds_strides=(1, 1),   # downsampling strides for q & x, k & v\n        n_out=None,            # output dimension, if None, set to input dim\n        n_hidden=None,         # dimension of the hidden layer in MLP\n        act_layer=nn.GELU,     # nonlinear activation used in MLP, default GELU\n        attn_pdrop=0.0,        # dropout rate for the attention map\n        proj_pdrop=0.0,        # dropout rate for the projection / MLP\n        path_pdrop=0.0,        # drop path rate\n        mha_win_size=-1,       # > 0 to use window mha\n        use_rel_pe=False       # if to add rel position encoding to attention\n    ):\n        super().__init__()\n        assert len(n_ds_strides) == 2\n        # layer norm for order (B C T)\n        self.ln1 = LayerNorm(n_embd)\n        self.ln2 = LayerNorm(n_embd)\n\n        # specify the attention module\n        if mha_win_size > 1:\n            self.attn = LocalMaskedMHCA(\n                n_embd,\n                n_head,\n                window_size=mha_win_size,\n                n_qx_stride=n_ds_strides[0],\n                n_kv_stride=n_ds_strides[1],\n                attn_pdrop=attn_pdrop,\n                proj_pdrop=proj_pdrop,\n                use_rel_pe=use_rel_pe  # only valid for local attention\n            )\n        else:\n            self.attn = MaskedMHCA(\n                n_embd,\n                n_head,\n                n_qx_stride=n_ds_strides[0],\n                n_kv_stride=n_ds_strides[1],\n                attn_pdrop=attn_pdrop,\n                proj_pdrop=proj_pdrop\n            )\n\n        # input\n        if n_ds_strides[0] > 1:\n            kernel_size, stride, padding = \\\n                n_ds_strides[0] + 1, n_ds_strides[0], (n_ds_strides[0] + 1)//2\n            self.pool_skip = nn.MaxPool1d(\n                kernel_size, stride=stride, padding=padding)\n        else:\n            self.pool_skip = nn.Identity()\n\n        # two layer mlp\n        if n_hidden is None:\n            n_hidden = 4 * n_embd  # default\n        if n_out is None:\n            n_out = n_embd\n        # ok to use conv1d here with stride=1\n        self.mlp = nn.Sequential(\n            nn.Conv1d(n_embd, n_hidden, 1),\n            act_layer(),\n            nn.Dropout(proj_pdrop, inplace=True),\n            nn.Conv1d(n_hidden, n_out, 1),\n            nn.Dropout(proj_pdrop, inplace=True),\n        )\n\n        # drop path\n        if path_pdrop > 0.0:\n            self.drop_path_attn = AffineDropPath(n_embd, drop_prob = path_pdrop)\n            self.drop_path_mlp = AffineDropPath(n_out, drop_prob = path_pdrop)\n        else:\n            self.drop_path_attn = nn.Identity()\n            self.drop_path_mlp = nn.Identity()\n\n    def forward(self, x, mask, pos_embd=None):\n        # pre-LN transformer: https://arxiv.org/pdf/2002.04745.pdf\n        out, out_mask = self.attn(self.ln1(x), mask)\n        out_mask_float = out_mask.to(out.dtype)\n        out = self.pool_skip(x) * out_mask_float + self.drop_path_attn(out)\n        # FFN\n        out = out + self.drop_path_mlp(self.mlp(self.ln2(out)) * out_mask_float)\n        # optionally add pos_embd to the output\n        if pos_embd is not None:\n            out += pos_embd * out_mask_float\n        return out, out_mask\n\n\nclass ConvBlock(nn.Module):\n    \"\"\"\n    A simple conv block similar to the basic block used in ResNet\n    \"\"\"\n    def __init__(\n        self,\n        n_embd,                # dimension of the input features\n        kernel_size=3,         # conv kernel size\n        n_ds_stride=1,         # downsampling stride for the current layer\n        expansion_factor=2,    # expansion factor of feat dims\n        n_out=None,            # output dimension, if None, set to input dim\n        act_layer=nn.ReLU,     # nonlinear activation used after conv, default ReLU\n    ):\n        super().__init__()\n        # must use odd sized kernel\n        assert (kernel_size % 2 == 1) and (kernel_size > 1)\n        padding = kernel_size // 2\n        if n_out is None:\n            n_out = n_embd\n\n         # 1x3 (strided) -> 1x3 (basic block in resnet)\n        width = n_embd * expansion_factor\n        self.conv1 = MaskedConv1D(\n            n_embd, width, kernel_size, n_ds_stride, padding=padding)\n        self.conv2 = MaskedConv1D(\n            width, n_out, kernel_size, 1, padding=padding)\n\n        # attach downsampling conv op\n        if n_ds_stride > 1:\n            # 1x1 strided conv (same as resnet)\n            self.downsample = MaskedConv1D(n_embd, n_out, 1, n_ds_stride)\n        else:\n            self.downsample = None\n\n        self.act = act_layer()\n\n    def forward(self, x, mask, pos_embd=None):\n        identity = x\n        out, out_mask = self.conv1(x, mask)\n        out = self.act(out)\n        out, out_mask = self.conv2(out, out_mask)\n\n        # downsampling\n        if self.downsample is not None:\n            identity, _ = self.downsample(x, mask)\n\n        # residual connection\n        out += identity\n        out = self.act(out)\n\n        return out, out_mask\n\n\n# drop path: from https://github.com/facebookresearch/SlowFast/blob/master/slowfast/models/common.py\nclass Scale(nn.Module):\n    \"\"\"\n    Multiply the output regression range by a learnable constant value\n    \"\"\"\n    def __init__(self, init_value=1.0):\n        \"\"\"\n        init_value : initial value for the scalar\n        \"\"\"\n        super().__init__()\n        self.scale = nn.Parameter(\n            torch.tensor(init_value, dtype=torch.float32),\n            requires_grad=True\n        )\n\n    def forward(self, x):\n        \"\"\"\n        input -> scale * input\n        \"\"\"\n        return x * self.scale\n\n\n# The follow code is modified from\n# https://github.com/facebookresearch/SlowFast/blob/master/slowfast/models/common.py\ndef drop_path(x, drop_prob=0.0, training=False):\n    \"\"\"\n    Stochastic Depth per sample.\n    \"\"\"\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (\n        x.ndim - 1\n    )  # work with diff dim tensors, not just 2D ConvNets\n    mask = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    mask.floor_()  # binarize\n    output = x.div(keep_prob) * mask\n    return output\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n\nclass AffineDropPath(nn.Module):\n    \"\"\"\n    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks) with a per channel scaling factor (and zero init)\n    See: https://arxiv.org/pdf/2103.17239.pdf\n    \"\"\"\n\n    def __init__(self, num_dim, drop_prob=0.0, init_scale_value=1e-4):\n        super().__init__()\n        self.scale = nn.Parameter(\n            init_scale_value * torch.ones((1, num_dim, 1)),\n            requires_grad=True\n        )\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(self.scale * x, self.drop_prob, self.training)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.570387Z","iopub.execute_input":"2025-10-04T16:20:11.570718Z","iopub.status.idle":"2025-10-04T16:20:11.592470Z","shell.execute_reply.started":"2025-10-04T16:20:11.570698Z","shell.execute_reply":"2025-10-04T16:20:11.591786Z"}},"outputs":[],"execution_count":32},{"id":"0aef23ed-fd91-4479-ac67-948d27236dfd","cell_type":"code","source":"class LocalMaskedMHCA(nn.Module):\n    \"\"\"\n    Local Multi Head Conv Attention with mask\n\n    Add a depthwise convolution within a standard MHA\n    The extra conv op can be used to\n    (1) encode relative position information (relacing position encoding);\n    (2) downsample the features if needed;\n    (3) match the feature channels\n\n    Note: With current implementation, the downsampled feature will be aligned\n    to every s+1 time step, where s is the downsampling stride. This allows us\n    to easily interpolate the corresponding positional embeddings.\n\n    The implementation is fairly tricky, code reference from\n    https://github.com/huggingface/transformers/blob/master/src/transformers/models/longformer/modeling_longformer.py\n    \"\"\"\n\n    def __init__(\n        self,\n        n_embd,          # dimension of the output features\n        n_head,          # number of heads in multi-head self-attention\n        window_size,     # size of the local attention window\n        n_qx_stride=1,   # dowsampling stride for query and input\n        n_kv_stride=1,   # downsampling stride for key and value\n        attn_pdrop=0.0,  # dropout rate for the attention map\n        proj_pdrop=0.0,  # dropout rate for projection op\n        use_rel_pe=False # use relative position encoding\n    ):\n        super().__init__()\n        assert n_embd % n_head == 0\n        self.n_embd = n_embd\n        self.n_head = n_head\n        self.n_channels = n_embd // n_head\n        self.scale = 1.0 / math.sqrt(self.n_channels)\n        self.window_size = window_size\n        self.window_overlap  = window_size // 2\n        # must use an odd window size\n        assert self.window_size > 1 and self.n_head >= 1\n        self.use_rel_pe = use_rel_pe\n\n        # conv/pooling operations\n        assert (n_qx_stride == 1) or (n_qx_stride % 2 == 0)\n        assert (n_kv_stride == 1) or (n_kv_stride % 2 == 0)\n        self.n_qx_stride = n_qx_stride\n        self.n_kv_stride = n_kv_stride\n\n        # query conv (depthwise)\n        kernel_size = self.n_qx_stride + 1 if self.n_qx_stride > 1 else 3\n        stride, padding = self.n_kv_stride, kernel_size // 2\n        self.query_conv = MaskedConv1D(\n            self.n_embd, self.n_embd, kernel_size,\n            stride=stride, padding=padding, groups=self.n_embd, bias=False\n        )\n        self.query_norm = LayerNorm(self.n_embd)\n\n        # key, value conv (depthwise)\n        kernel_size = self.n_kv_stride + 1 if self.n_kv_stride > 1 else 3\n        stride, padding = self.n_kv_stride, kernel_size // 2\n        self.key_conv = MaskedConv1D(\n            self.n_embd, self.n_embd, kernel_size,\n            stride=stride, padding=padding, groups=self.n_embd, bias=False\n        )\n        self.key_norm = LayerNorm(self.n_embd)\n        self.value_conv = MaskedConv1D(\n            self.n_embd, self.n_embd, kernel_size,\n            stride=stride, padding=padding, groups=self.n_embd, bias=False\n        )\n        self.value_norm = LayerNorm(self.n_embd)\n\n        # key, query, value projections for all heads\n        # it is OK to ignore masking, as the mask will be attached on the attention\n        self.key = nn.Conv1d(self.n_embd, self.n_embd, 1)\n        self.query = nn.Conv1d(self.n_embd, self.n_embd, 1)\n        self.value = nn.Conv1d(self.n_embd, self.n_embd, 1)\n\n        # regularization\n        self.attn_drop = nn.Dropout(attn_pdrop)\n        self.proj_drop = nn.Dropout(proj_pdrop)\n\n        # output projection\n        self.proj = nn.Conv1d(self.n_embd, self.n_embd, 1)\n\n        # relative position encoding\n        if self.use_rel_pe:\n            self.rel_pe = nn.Parameter(\n                torch.zeros(1, 1, self.n_head, self.window_size))\n            trunc_normal_(self.rel_pe, std=(2.0 / self.n_embd)**0.5)\n\n    @staticmethod\n    def _chunk(x, window_overlap):\n        \"\"\"convert into overlapping chunks. Chunk size = 2w, overlap size = w\"\"\"\n        # x: B x nh, T, hs\n        # non-overlapping chunks of size = 2w -> B x nh, T//2w, 2w, hs\n        x = x.view(\n            x.size(0),\n            x.size(1) // (window_overlap * 2),\n            window_overlap * 2,\n            x.size(2),\n        )\n\n        # use `as_strided` to make the chunks overlap with an overlap size = window_overlap\n        chunk_size = list(x.size())\n        chunk_size[1] = chunk_size[1] * 2 - 1\n        chunk_stride = list(x.stride())\n        chunk_stride[1] = chunk_stride[1] // 2\n\n        # B x nh, #chunks = T//w - 1, 2w, hs\n        return x.as_strided(size=chunk_size, stride=chunk_stride)\n\n    @staticmethod\n    def _pad_and_transpose_last_two_dims(x, padding):\n        \"\"\"pads rows and then flips rows and columns\"\"\"\n        # padding value is not important because it will be overwritten\n        x = nn.functional.pad(x, padding)\n        x = x.view(*x.size()[:-2], x.size(-1), x.size(-2))\n        return x\n\n    @staticmethod\n    def _mask_invalid_locations(input_tensor, affected_seq_len):\n        beginning_mask_2d = input_tensor.new_ones(affected_seq_len, affected_seq_len + 1).tril().flip(dims=[0])\n        beginning_mask = beginning_mask_2d[None, :, None, :]\n        ending_mask = beginning_mask.flip(dims=(1, 3))\n        beginning_input = input_tensor[:, :affected_seq_len, :, : affected_seq_len + 1]\n        beginning_mask = beginning_mask.expand(beginning_input.size())\n        # `== 1` converts to bool or uint8\n        beginning_input.masked_fill_(beginning_mask == 1, -float(\"inf\"))\n        ending_input = input_tensor[:, -affected_seq_len:, :, -(affected_seq_len + 1) :]\n        ending_mask = ending_mask.expand(ending_input.size())\n        # `== 1` converts to bool or uint8\n        ending_input.masked_fill_(ending_mask == 1, -float(\"inf\"))\n\n    @staticmethod\n    def _pad_and_diagonalize(x):\n        \"\"\"\n        shift every row 1 step right, converting columns into diagonals.\n        Example::\n              chunked_hidden_states: [ 0.4983,  2.6918, -0.0071,  1.0492,\n                                       -1.8348,  0.7672,  0.2986,  0.0285,\n                                       -0.7584,  0.4206, -0.0405,  0.1599,\n                                       2.0514, -1.1600,  0.5372,  0.2629 ]\n              window_overlap = num_rows = 4\n             (pad & diagonalize) =>\n             [ 0.4983,  2.6918, -0.0071,  1.0492, 0.0000,  0.0000,  0.0000\n               0.0000,  -1.8348,  0.7672,  0.2986,  0.0285, 0.0000,  0.0000\n               0.0000,  0.0000, -0.7584,  0.4206, -0.0405,  0.1599, 0.0000\n               0.0000,  0.0000,  0.0000, 2.0514, -1.1600,  0.5372,  0.2629 ]\n        \"\"\"\n        total_num_heads, num_chunks, window_overlap, hidden_dim = x.size()\n        # total_num_heads x num_chunks x window_overlap x (hidden_dim+window_overlap+1).\n        x = nn.functional.pad(\n            x, (0, window_overlap + 1)\n        )\n        # total_num_heads x num_chunks x window_overlap*window_overlap+window_overlap\n        x = x.view(total_num_heads, num_chunks, -1)\n        # total_num_heads x num_chunks x window_overlap*window_overlap\n        x = x[:, :, :-window_overlap]\n        x = x.view(\n            total_num_heads, num_chunks, window_overlap, window_overlap + hidden_dim\n        )\n        x = x[:, :, :, :-1]\n        return x\n\n    def _sliding_chunks_query_key_matmul(\n        self, query, key, num_heads, window_overlap\n    ):\n        \"\"\"\n        Matrix multiplication of query and key tensors using with a sliding window attention pattern. This implementation splits the input into overlapping chunks of size 2w with an overlap of size w (window_overlap)\n        \"\"\"\n        # query / key: B*nh, T, hs\n        bnh, seq_len, head_dim = query.size()\n        batch_size = bnh // num_heads\n        assert seq_len % (window_overlap * 2) == 0\n        assert query.size() == key.size()\n\n        chunks_count = seq_len // window_overlap - 1\n\n        # B * num_heads, head_dim, #chunks=(T//w - 1), 2w\n        chunk_query = self._chunk(query, window_overlap)\n        chunk_key = self._chunk(key, window_overlap)\n\n        # matrix multiplication\n        # bcxd: batch_size * num_heads x chunks x 2window_overlap x head_dim\n        # bcyd: batch_size * num_heads x chunks x 2window_overlap x head_dim\n        # bcxy: batch_size * num_heads x chunks x 2window_overlap x 2window_overlap\n        diagonal_chunked_attention_scores = torch.einsum(\n            \"bcxd,bcyd->bcxy\", (chunk_query, chunk_key))\n\n        # convert diagonals into columns\n        # B * num_heads, #chunks, 2w, 2w+1\n        diagonal_chunked_attention_scores = self._pad_and_transpose_last_two_dims(\n            diagonal_chunked_attention_scores, padding=(0, 0, 0, 1)\n        )\n\n        # allocate space for the overall attention matrix where the chunks are combined. The last dimension\n        # has (window_overlap * 2 + 1) columns. The first (window_overlap) columns are the window_overlap lower triangles (attention from a word to\n        # window_overlap previous words). The following column is attention score from each word to itself, then\n        # followed by window_overlap columns for the upper triangle.\n        diagonal_attention_scores = diagonal_chunked_attention_scores.new_empty(\n            (batch_size * num_heads, chunks_count + 1, window_overlap, window_overlap * 2 + 1)\n        )\n\n        # copy parts from diagonal_chunked_attention_scores into the combined matrix of attentions\n        # - copying the main diagonal and the upper triangle\n        diagonal_attention_scores[:, :-1, :, window_overlap:] = diagonal_chunked_attention_scores[\n            :, :, :window_overlap, : window_overlap + 1\n        ]\n        diagonal_attention_scores[:, -1, :, window_overlap:] = diagonal_chunked_attention_scores[\n            :, -1, window_overlap:, : window_overlap + 1\n        ]\n        # - copying the lower triangle\n        diagonal_attention_scores[:, 1:, :, :window_overlap] = diagonal_chunked_attention_scores[\n            :, :, -(window_overlap + 1) : -1, window_overlap + 1 :\n        ]\n\n        diagonal_attention_scores[:, 0, 1:window_overlap, 1:window_overlap] = diagonal_chunked_attention_scores[\n            :, 0, : window_overlap - 1, 1 - window_overlap :\n        ]\n\n        # separate batch_size and num_heads dimensions again\n        diagonal_attention_scores = diagonal_attention_scores.view(\n            batch_size, num_heads, seq_len, 2 * window_overlap + 1\n        ).transpose(2, 1)\n\n        self._mask_invalid_locations(diagonal_attention_scores, window_overlap)\n        return diagonal_attention_scores\n\n    def _sliding_chunks_matmul_attn_probs_value(\n        self, attn_probs, value, num_heads, window_overlap\n    ):\n        \"\"\"\n        Same as _sliding_chunks_query_key_matmul but for attn_probs and value tensors. Returned tensor will be of the\n        same shape as `attn_probs`\n        \"\"\"\n        bnh, seq_len, head_dim = value.size()\n        batch_size = bnh // num_heads\n        assert seq_len % (window_overlap * 2) == 0\n        assert attn_probs.size(3) == 2 * window_overlap + 1\n        chunks_count = seq_len // window_overlap - 1\n        # group batch_size and num_heads dimensions into one, then chunk seq_len into chunks of size 2 window overlap\n\n        chunked_attn_probs = attn_probs.transpose(1, 2).reshape(\n            batch_size * num_heads, seq_len // window_overlap, window_overlap, 2 * window_overlap + 1\n        )\n\n        # pad seq_len with w at the beginning of the sequence and another window overlap at the end\n        padded_value = nn.functional.pad(value, (0, 0, window_overlap, window_overlap), value=-1)\n\n        # chunk padded_value into chunks of size 3 window overlap and an overlap of size window overlap\n        chunked_value_size = (batch_size * num_heads, chunks_count + 1, 3 * window_overlap, head_dim)\n        chunked_value_stride = padded_value.stride()\n        chunked_value_stride = (\n            chunked_value_stride[0],\n            window_overlap * chunked_value_stride[1],\n            chunked_value_stride[1],\n            chunked_value_stride[2],\n        )\n        chunked_value = padded_value.as_strided(size=chunked_value_size, stride=chunked_value_stride)\n\n        chunked_attn_probs = self._pad_and_diagonalize(chunked_attn_probs)\n\n        context = torch.einsum(\"bcwd,bcdh->bcwh\", (chunked_attn_probs, chunked_value))\n        return context.view(batch_size, num_heads, seq_len, head_dim)\n\n    def forward(self, x, mask):\n        # x: batch size, feature channel, sequence length,\n        # mask: batch size, 1, sequence length (bool)\n        B, C, T = x.size()\n\n        # step 1: depth convolutions\n        # query conv -> (B, nh * hs, T')\n        q, qx_mask = self.query_conv(x, mask)\n        q = self.query_norm(q)\n        # key, value conv -> (B, nh * hs, T'')\n        k, kv_mask = self.key_conv(x, mask)\n        k = self.key_norm(k)\n        v, _ = self.value_conv(x, mask)\n        v = self.value_norm(v)\n\n        # step 2: query, key, value transforms & reshape\n        # projections\n        q = self.query(q)\n        k = self.key(k)\n        v = self.value(v)\n        # (B, nh * hs, T) -> (B, nh, T, hs)\n        q = q.view(B, self.n_head, self.n_channels, -1).transpose(2, 3)\n        k = k.view(B, self.n_head, self.n_channels, -1).transpose(2, 3)\n        v = v.view(B, self.n_head, self.n_channels, -1).transpose(2, 3)\n        # view as (B * nh, T, hs)\n        q = q.view(B * self.n_head, -1, self.n_channels).contiguous()\n        k = k.view(B * self.n_head, -1, self.n_channels).contiguous()\n        v = v.view(B * self.n_head, -1, self.n_channels).contiguous()\n\n        # step 3: compute local self-attention with rel pe and masking\n        q *= self.scale\n        # chunked query key attention -> B, T, nh, 2w+1 = window_size\n        att = self._sliding_chunks_query_key_matmul(\n            q, k, self.n_head, self.window_overlap)\n\n        # rel pe\n        if self.use_rel_pe:\n            att += self.rel_pe\n        # kv_mask -> B, T'', 1\n        inverse_kv_mask = torch.logical_not(\n            kv_mask[:, :, :, None].view(B, -1, 1))\n        # 0 for valid slot, -inf for masked ones\n        float_inverse_kv_mask = inverse_kv_mask.type_as(q).masked_fill(\n            inverse_kv_mask, -1e4)\n        # compute the diagonal mask (for each local window)\n        diagonal_mask = self._sliding_chunks_query_key_matmul(\n            float_inverse_kv_mask.new_ones(size=float_inverse_kv_mask.size()),\n            float_inverse_kv_mask,\n            1,\n            self.window_overlap\n        )\n        att += diagonal_mask\n\n        # ignore input masking for now\n        att = nn.functional.softmax(att, dim=-1)\n        # softmax sometimes inserts NaN if all positions are masked, replace them with 0\n        att = att.masked_fill(\n            torch.logical_not(kv_mask.squeeze(1)[:, :, None, None]), 0.0)\n        att = self.attn_drop(att)\n\n        # step 4: compute attention value product + output projection\n        # chunked attn value product -> B, nh, T, hs\n        out = self._sliding_chunks_matmul_attn_probs_value(\n            att, v, self.n_head, self.window_overlap)\n        # transpose to B, nh, hs, T -> B, nh*hs, T\n        out = out.transpose(2, 3).contiguous().view(B, C, -1)\n        # output projection + skip connection\n        out = self.proj_drop(self.proj(out)) * qx_mask.to(out.dtype)\n        return out, qx_mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.593273Z","iopub.execute_input":"2025-10-04T16:20:11.593501Z","iopub.status.idle":"2025-10-04T16:20:11.634862Z","shell.execute_reply.started":"2025-10-04T16:20:11.593481Z","shell.execute_reply":"2025-10-04T16:20:11.634028Z"}},"outputs":[],"execution_count":33},{"id":"93295964-137f-4dc4-bf07-1242147ad791","cell_type":"code","source":"class BufferList(nn.Module):\n    \"\"\"\n    Similar to nn.ParameterList, but for buffers\n\n    Taken from https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/anchor_generator.py\n    \"\"\"\n\n    def __init__(self, buffers):\n        super().__init__()\n        for i, buffer in enumerate(buffers):\n            # Use non-persistent buffer so the values are not saved in checkpoint\n            self.register_buffer(str(i), buffer, persistent=False)\n\n    def __len__(self):\n        return len(self._buffers)\n\n    def __iter__(self):\n        return iter(self._buffers.values())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.635777Z","iopub.execute_input":"2025-10-04T16:20:11.636172Z","iopub.status.idle":"2025-10-04T16:20:11.647642Z","shell.execute_reply.started":"2025-10-04T16:20:11.636150Z","shell.execute_reply":"2025-10-04T16:20:11.646912Z"}},"outputs":[],"execution_count":34},{"id":"eec4d061","cell_type":"code","source":"# model - direct instantiation\nmodel = PtTransformer(\n    backbone_type=backbone_type,\n    fpn_type=fpn_type,\n    backbone_arch=backbone_arch,\n    scale_factor=scale_factor,\n    input_dim=input_dim,\n    max_seq_len=max_seq_len,\n    max_buffer_len_factor=max_buffer_len_factor,\n    n_head=n_head,\n    n_mha_win_size=n_mha_win_size,\n    embd_kernel_size=embd_kernel_size,\n    embd_dim=embd_dim,\n    embd_with_ln=embd_with_ln,\n    fpn_dim=fpn_dim,\n    fpn_with_ln=fpn_with_ln,\n    fpn_start_level=fpn_start_level,\n    head_dim=head_dim,\n    regression_range=regression_range,\n    head_num_layers=head_num_layers,\n    head_kernel_size=head_kernel_size,\n    head_with_ln=head_with_ln,\n    use_abs_pe=use_abs_pe,\n    use_rel_pe=use_rel_pe,\n    num_classes=num_classes,\n    train_cfg=train_cfg,\n    test_cfg=test_cfg,\n\n    n_sgp_win_size=n_sgp_win_size,        #new -  # window size w for sgp\n    downsample_type=downsample_type,       #new -  # how to downsample feature in FPN\n    sgp_mlp_dim=sgp_mlp_dim,            #new -  # the numnber of dim in SGP\n    init_conv_vars=init_conv_vars,         #new -  # initialization of gaussian variance for the weight in SGP\n    k=k\n\n    \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.648424Z","iopub.execute_input":"2025-10-04T16:20:11.648687Z","iopub.status.idle":"2025-10-04T16:20:11.861328Z","shell.execute_reply.started":"2025-10-04T16:20:11.648662Z","shell.execute_reply":"2025-10-04T16:20:11.860564Z"}},"outputs":[],"execution_count":35},{"id":"52c5ae0f","cell_type":"code","source":"model = nn.DataParallel(model, device_ids=devices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:11.862107Z","iopub.execute_input":"2025-10-04T16:20:11.862333Z","iopub.status.idle":"2025-10-04T16:20:12.173882Z","shell.execute_reply.started":"2025-10-04T16:20:11.862311Z","shell.execute_reply":"2025-10-04T16:20:12.173268Z"}},"outputs":[],"execution_count":36},{"id":"cdd42c0d-49a9-49c7-bdfc-886cfe7aff83","cell_type":"code","source":"print(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:12.174682Z","iopub.execute_input":"2025-10-04T16:20:12.174912Z","iopub.status.idle":"2025-10-04T16:20:12.180086Z","shell.execute_reply.started":"2025-10-04T16:20:12.174896Z","shell.execute_reply":"2025-10-04T16:20:12.179521Z"}},"outputs":[{"name":"stdout","text":"DataParallel(\n  (module): PtTransformer(\n    (backbone): SGPBackbone(\n      (relu): ReLU(inplace=True)\n      (embd): ModuleList(\n        (0): MaskedConv1D(\n          (conv): Conv1d(2048, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n        )\n        (1): MaskedConv1D(\n          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n        )\n      )\n      (embd_norm): ModuleList(\n        (0-1): 2 x LayerNorm()\n      )\n      (stem): ModuleList(\n        (0-1): 2 x SGPBlock(\n          (ln): LayerNorm()\n          (gn): GroupNorm(16, 512, eps=1e-05, affine=True)\n          (psi): Conv1d(512, 512, kernel_size=(1,), stride=(1,), groups=512)\n          (fc): Conv1d(512, 512, kernel_size=(1,), stride=(1,), groups=512)\n          (convw): Conv1d(512, 512, kernel_size=(1,), stride=(1,), groups=512)\n          (convkw): Conv1d(512, 512, kernel_size=(11,), stride=(1,), padding=(5,), groups=512)\n          (global_fc): Conv1d(512, 512, kernel_size=(1,), stride=(1,), groups=512)\n          (downsample): Identity()\n          (mlp): Sequential(\n            (0): Conv1d(512, 768, kernel_size=(1,), stride=(1,))\n            (1): GELU(approximate='none')\n            (2): Conv1d(768, 512, kernel_size=(1,), stride=(1,))\n          )\n          (drop_path_out): Identity()\n          (drop_path_mlp): Identity()\n          (act): GELU(approximate='none')\n        )\n      )\n      (branch): ModuleList(\n        (0-4): 5 x SGPBlock(\n          (ln): LayerNorm()\n          (gn): GroupNorm(16, 512, eps=1e-05, affine=True)\n          (psi): Conv1d(512, 512, kernel_size=(1,), stride=(1,), groups=512)\n          (fc): Conv1d(512, 512, kernel_size=(1,), stride=(1,), groups=512)\n          (convw): Conv1d(512, 512, kernel_size=(1,), stride=(1,), groups=512)\n          (convkw): Conv1d(512, 512, kernel_size=(11,), stride=(1,), padding=(5,), groups=512)\n          (global_fc): Conv1d(512, 512, kernel_size=(1,), stride=(1,), groups=512)\n          (downsample): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n          (mlp): Sequential(\n            (0): Conv1d(512, 768, kernel_size=(1,), stride=(1,))\n            (1): GELU(approximate='none')\n            (2): Conv1d(768, 512, kernel_size=(1,), stride=(1,))\n          )\n          (drop_path_out): AffineDropPath()\n          (drop_path_mlp): AffineDropPath()\n          (act): GELU(approximate='none')\n        )\n      )\n    )\n    (neck): FPNIdentity(\n      (fpn_norms): ModuleList(\n        (0-5): 6 x LayerNorm()\n      )\n    )\n    (point_generator): PointGenerator(\n      (buffer_points): BufferList()\n    )\n    (cls_head): PtTransformerClsHead(\n      (act): ReLU()\n      (head): ModuleList(\n        (0-1): 2 x MaskedConv1D(\n          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n        )\n      )\n      (norm): ModuleList(\n        (0-1): 2 x LayerNorm()\n      )\n      (cls_head): MaskedConv1D(\n        (conv): Conv1d(512, 20, kernel_size=(3,), stride=(1,), padding=(1,))\n      )\n    )\n    (reg_head): PtTransformerRegHead(\n      (act): ReLU()\n      (head): ModuleList(\n        (0-1): 2 x MaskedConv1D(\n          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n        )\n      )\n      (norm): ModuleList(\n        (0-1): 2 x LayerNorm()\n      )\n      (scale): ModuleList(\n        (0-5): 6 x Scale()\n      )\n      (offset_head): MaskedConv1D(\n        (conv): Conv1d(512, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n      )\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":37},{"id":"07f40bf9","cell_type":"code","source":"opt_cfg = {\n    'type': opt_type,\n    'momentum': momentum,\n    'weight_decay': weight_decay,\n    'learning_rate': learning_rate,\n    'epochs': epochs,\n    'warmup': warmup,\n    'warmup_epochs': warmup_epochs,\n    'schedule_type': schedule_type,\n    'schedule_steps': schedule_steps,\n    'schedule_gamma': schedule_gamma\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:12.180847Z","iopub.execute_input":"2025-10-04T16:20:12.181084Z","iopub.status.idle":"2025-10-04T16:20:12.189876Z","shell.execute_reply.started":"2025-10-04T16:20:12.181063Z","shell.execute_reply":"2025-10-04T16:20:12.189348Z"}},"outputs":[],"execution_count":38},{"id":"1a8660d9","cell_type":"code","source":"from torch.optim.lr_scheduler import _LRScheduler\nclass LinearWarmupCosineAnnealingLR(_LRScheduler):\n    \"\"\"\n    Sets the learning rate of each parameter group to follow a linear warmup schedule\n    between warmup_start_lr and base_lr followed by a cosine annealing schedule between\n    base_lr and eta_min.\n\n    .. warning::\n        It is recommended to call :func:`.step()` for :class:`LinearWarmupCosineAnnealingLR`\n        after each iteration as calling it after each epoch will keep the starting lr at\n        warmup_start_lr for the first epoch which is 0 in most cases.\n\n    .. warning::\n        passing epoch to :func:`.step()` is being deprecated and comes with an EPOCH_DEPRECATION_WARNING.\n        It calls the :func:`_get_closed_form_lr()` method for this scheduler instead of\n        :func:`get_lr()`. Though this does not change the behavior of the scheduler, when passing\n        epoch param to :func:`.step()`, the user should call the :func:`.step()` function before calling\n        train and validation methods.\n\n    Example:\n        >>> layer = nn.Linear(10, 1)\n        >>> optimizer = Adam(layer.parameters(), lr=0.02)\n        >>> scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=10, max_epochs=40)\n        >>> #\n        >>> # the default case\n        >>> for epoch in range(40):\n        ...     # train(...)\n        ...     # validate(...)\n        ...     scheduler.step()\n        >>> #\n        >>> # passing epoch param case\n        >>> for epoch in range(40):\n        ...     scheduler.step(epoch)\n        ...     # train(...)\n        ...     # validate(...)\n    \"\"\"\n\n    def __init__(\n        self,\n        optimizer,\n        warmup_epochs,\n        max_epochs,\n        warmup_start_lr = 0.0,\n        eta_min = 1e-8,\n        last_epoch = -1,\n    ):\n        \"\"\"\n        Args:\n            optimizer (Optimizer): Wrapped optimizer.\n            warmup_epochs (int): Maximum number of iterations for linear warmup\n            max_epochs (int): Maximum number of iterations\n            warmup_start_lr (float): Learning rate to start the linear warmup. Default: 0.\n            eta_min (float): Minimum learning rate. Default: 0.\n            last_epoch (int): The index of last epoch. Default: -1.\n        \"\"\"\n        self.warmup_epochs = warmup_epochs\n        self.max_epochs = max_epochs\n        self.warmup_start_lr = warmup_start_lr\n        self.eta_min = eta_min\n\n        super(LinearWarmupCosineAnnealingLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        \"\"\"\n        Compute learning rate using chainable form of the scheduler\n        \"\"\"\n        if not self._get_lr_called_within_step:\n            warnings.warn(\n                \"To get the last learning rate computed by the scheduler, \"\n                \"please use `get_last_lr()`.\",\n                UserWarning,\n            )\n\n        if self.last_epoch == 0:\n            return [self.warmup_start_lr] * len(self.base_lrs)\n        elif self.last_epoch < self.warmup_epochs:\n            return [\n                group[\"lr\"] + (base_lr - self.warmup_start_lr) / (self.warmup_epochs - 1)\n                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n            ]\n        elif self.last_epoch == self.warmup_epochs:\n            return self.base_lrs\n        elif (self.last_epoch - 1 - self.max_epochs) % (2 * (self.max_epochs - self.warmup_epochs)) == 0:\n            return [\n                group[\"lr\"] + (base_lr - self.eta_min) *\n                (1 - math.cos(math.pi / (self.max_epochs - self.warmup_epochs))) / 2\n                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n            ]\n\n        return [\n            (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs))) /\n            (\n                1 +\n                math.cos(math.pi * (self.last_epoch - self.warmup_epochs - 1) / (self.max_epochs - self.warmup_epochs))\n            ) * (group[\"lr\"] - self.eta_min) + self.eta_min for group in self.optimizer.param_groups\n        ]\n\n    def _get_closed_form_lr(self):\n        \"\"\"\n        Called when epoch is passed as a param to the `step` function of the scheduler.\n        \"\"\"\n        if self.last_epoch < self.warmup_epochs:\n            return [\n                self.warmup_start_lr + self.last_epoch * (base_lr - self.warmup_start_lr) / (self.warmup_epochs - 1)\n                for base_lr in self.base_lrs\n            ]\n\n        return [\n            self.eta_min + 0.5 * (base_lr - self.eta_min) *\n            (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)))\n            for base_lr in self.base_lrs\n        ]\n\n\nclass LinearWarmupMultiStepLR(_LRScheduler):\n    \"\"\"\n    Sets the learning rate of each parameter group to follow a linear warmup schedule\n    between warmup_start_lr and base_lr followed by a multi-step schedule that decays\n    the learning rate of each parameter group by gamma once the\n    number of epoch reaches one of the milestones.\n\n    .. warning::\n        It is recommended to call :func:`.step()` for :class:`LinearWarmupCosineAnnealingLR`\n        after each iteration as calling it after each epoch will keep the starting lr at\n        warmup_start_lr for the first epoch which is 0 in most cases.\n\n    .. warning::\n        passing epoch to :func:`.step()` is being deprecated and comes with an EPOCH_DEPRECATION_WARNING.\n        It calls the :func:`_get_closed_form_lr()` method for this scheduler instead of\n        :func:`get_lr()`. Though this does not change the behavior of the scheduler, when passing\n        epoch param to :func:`.step()`, the user should call the :func:`.step()` function before calling\n        train and validation methods.\n    \"\"\"\n\n    def __init__(\n        self,\n        optimizer,\n        warmup_epochs,\n        milestones,\n        warmup_start_lr = 0.0,\n        gamma = 0.1,\n        last_epoch = -1,\n    ):\n        \"\"\"\n        Args:\n            optimizer (Optimizer): Wrapped optimizer.\n            warmup_epochs (int): Maximum number of iterations for linear warmup\n            max_epochs (int): Maximum number of iterations\n            milestones (list): List of epoch indices. Must be increasing.\n            warmup_start_lr (float): Learning rate to start the linear warmup. Default: 0.\n            gamma (float): Multiplicative factor of learning rate decay.\n            Default: 0.1.\n            last_epoch (int): The index of last epoch. Default: -1.\n        \"\"\"\n        self.warmup_epochs = warmup_epochs\n        self.warmup_start_lr = warmup_start_lr\n        self.milestones = Counter(milestones)\n        self.gamma = gamma\n\n        super(LinearWarmupMultiStepLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        \"\"\"\n        Compute learning rate using chainable form of the scheduler\n        \"\"\"\n        if not self._get_lr_called_within_step:\n            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n                          \"please use `get_last_lr()`.\", UserWarning)\n\n        if self.last_epoch == 0:\n            # starting warm up\n            return [self.warmup_start_lr] * len(self.base_lrs)\n        elif self.last_epoch < self.warmup_epochs:\n            # linear warm up (0 ~ self.warmup_epochs -1)\n            return [\n                group[\"lr\"] + (base_lr - self.warmup_start_lr) / (self.warmup_epochs - 1)\n                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n            ]\n        elif self.last_epoch == self.warmup_epochs:\n            # end of warm up (reset to base lrs)\n            return self.base_lrs\n        elif (self.last_epoch - self.warmup_epochs) not in self.milestones:\n            # in between the steps\n            return [group['lr'] for group in self.optimizer.param_groups]\n\n        return [\n            group['lr'] * self.gamma ** self.milestones[self.last_epoch - self.warmup_epochs]\n            for group in self.optimizer.param_groups\n        ]\n\n    def _get_closed_form_lr(self):\n        \"\"\"\n        Called when epoch is passed as a param to the `step` function of the scheduler.\n        \"\"\"\n        if self.last_epoch < self.warmup_epochs:\n            return [\n                self.warmup_start_lr + self.last_epoch * (base_lr - self.warmup_start_lr) / (self.warmup_epochs - 1)\n                for base_lr in self.base_lrs\n            ]\n\n        milestones = list(sorted(self.milestones.elements()))\n        return [base_lr * self.gamma ** bisect_right(milestones, self.last_epoch - self.warmup_epochs)\n                for base_lr in self.base_lrs]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:12.190583Z","iopub.execute_input":"2025-10-04T16:20:12.190805Z","iopub.status.idle":"2025-10-04T16:20:12.206201Z","shell.execute_reply.started":"2025-10-04T16:20:12.190784Z","shell.execute_reply":"2025-10-04T16:20:12.205502Z"}},"outputs":[],"execution_count":39},{"id":"a4005f7f","cell_type":"code","source":"def make_optimizer(model, optimizer_config):\n    \"\"\"create optimizer\n    return a supported optimizer\n    \"\"\"\n    # separate out all parameters that with / without weight decay\n    # see https://github.com/karpathy/minGPT/blob/master/mingpt/model.py#L134\n    decay = set()\n    no_decay = set()\n    whitelist_weight_modules = (torch.nn.Linear, torch.nn.Conv1d, MaskedConv1D)\n    blacklist_weight_modules = (LayerNorm, torch.nn.GroupNorm)\n\n    # loop over all modules / params\n    for mn, m in model.named_modules():\n        for pn, p in m.named_parameters():\n            fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n            if pn.endswith('bias'):\n                # all biases will not be decayed\n                no_decay.add(fpn)\n            elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n                # weights of whitelist modules will be weight decayed\n                decay.add(fpn)\n            elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n                # weights of blacklist modules will NOT be weight decayed\n                no_decay.add(fpn)\n            elif pn.endswith('scale') and isinstance(m, (Scale, AffineDropPath)):\n                # corner case of our scale layer\n                no_decay.add(fpn)\n            elif pn.endswith('rel_pe'):\n                # corner case for relative position encoding\n                no_decay.add(fpn)\n\n    # validate that we considered every parameter\n    param_dict = {pn: p for pn, p in model.named_parameters()}\n    inter_params = decay & no_decay\n    union_params = decay | no_decay\n    assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n    assert len(param_dict.keys() - union_params) == 0, \\\n        \"parameters %s were not separated into either decay/no_decay set!\" \\\n        % (str(param_dict.keys() - union_params), )\n\n    # create the pytorch optimizer object\n    optim_groups = [\n        {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": optimizer_config['weight_decay']},\n        {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n    ]\n\n    if optimizer_config[\"type\"] == \"SGD\":\n        optimizer = optim.SGD(\n            optim_groups,\n            lr=optimizer_config[\"learning_rate\"],\n            momentum=optimizer_config[\"momentum\"]\n        )\n    elif optimizer_config[\"type\"] == \"AdamW\":\n        optimizer = optim.AdamW(\n            optim_groups,\n            lr=optimizer_config[\"learning_rate\"]\n        )\n    else:\n        raise TypeError(\"Unsupported optimizer!\")\n\n    return optimizer\n\n\ndef make_scheduler(\n    optimizer,\n    optimizer_config,\n    num_iters_per_epoch,\n    last_epoch=-1\n):\n    \"\"\"create scheduler\n    return a supported scheduler\n    All scheduler returned by this function should step every iteration\n    \"\"\"\n    if optimizer_config[\"warmup\"]:\n        max_epochs = optimizer_config[\"epochs\"] + optimizer_config[\"warmup_epochs\"]\n        max_steps = max_epochs * num_iters_per_epoch\n\n        # get warmup params\n        warmup_epochs = optimizer_config[\"warmup_epochs\"]\n        warmup_steps = warmup_epochs * num_iters_per_epoch\n\n        # with linear warmup: call our custom schedulers\n        if optimizer_config[\"schedule_type\"] == \"cosine\":\n            # Cosine\n            scheduler = LinearWarmupCosineAnnealingLR(\n                optimizer,\n                warmup_steps,\n                max_steps,\n                last_epoch=last_epoch\n            )\n\n        elif optimizer_config[\"schedule_type\"] == \"multistep\":\n            # Multi step\n            steps = [num_iters_per_epoch * step for step in optimizer_config[\"schedule_steps\"]]\n            scheduler = LinearWarmupMultiStepLR(\n                optimizer,\n                warmup_steps,\n                steps,\n                gamma=optimizer_config[\"schedule_gamma\"],\n                last_epoch=last_epoch\n            )\n        else:\n            raise TypeError(\"Unsupported scheduler!\")\n\n    else:\n        max_epochs = optimizer_config[\"epochs\"]\n        max_steps = max_epochs * num_iters_per_epoch\n\n        # without warmup: call default schedulers\n        if optimizer_config[\"schedule_type\"] == \"cosine\":\n            # step per iteration\n            scheduler = optim.lr_scheduler.CosineAnnealingLR(\n                optimizer,\n                max_steps,\n                last_epoch=last_epoch\n            )\n\n        elif optimizer_config[\"schedule_type\"] == \"multistep\":\n            # step every some epochs\n            steps = [num_iters_per_epoch * step for step in optimizer_config[\"schedule_steps\"]]\n            scheduler = optim.lr_scheduler.MultiStepLR(\n                optimizer,\n                steps,\n                gamma=schedule_config[\"gamma\"],\n                last_epoch=last_epoch\n            )\n        else:\n            raise TypeError(\"Unsupported scheduler!\")\n\n    return scheduler\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:12.207008Z","iopub.execute_input":"2025-10-04T16:20:12.207185Z","iopub.status.idle":"2025-10-04T16:20:12.221512Z","shell.execute_reply.started":"2025-10-04T16:20:12.207164Z","shell.execute_reply":"2025-10-04T16:20:12.220797Z"}},"outputs":[],"execution_count":40},{"id":"cad3355e","cell_type":"code","source":"# optimizer\noptimizer = make_optimizer(model, opt_cfg)\n# schedule\nnum_iters_per_epoch = len(train_loader)\nscheduler = make_scheduler(optimizer, opt_cfg, num_iters_per_epoch)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:12.222208Z","iopub.execute_input":"2025-10-04T16:20:12.222462Z","iopub.status.idle":"2025-10-04T16:20:15.051271Z","shell.execute_reply.started":"2025-10-04T16:20:12.222440Z","shell.execute_reply":"2025-10-04T16:20:15.050653Z"}},"outputs":[],"execution_count":41},{"id":"275a76c6","cell_type":"code","source":"class ModelEma(torch.nn.Module):\n    def __init__(self, model, decay=0.999, device=None):\n        super().__init__()\n        # make a copy of the model for accumulating moving average of weights\n        self.module = deepcopy(model)\n        self.module.eval()\n        self.decay = decay\n        self.device = device  # perform ema on different device from model if set\n        if self.device is not None:\n            self.module.to(device=device)\n\n    def _update(self, model, update_fn):\n        with torch.no_grad():\n            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n                if self.device is not None:\n                    model_v = model_v.to(device=self.device)\n                ema_v.copy_(update_fn(ema_v, model_v))\n\n    def update(self, model):\n        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n\n    def set(self, model):\n        self._update(model, update_fn=lambda e, m: m)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:15.052096Z","iopub.execute_input":"2025-10-04T16:20:15.052717Z","iopub.status.idle":"2025-10-04T16:20:15.058990Z","shell.execute_reply.started":"2025-10-04T16:20:15.052689Z","shell.execute_reply":"2025-10-04T16:20:15.058266Z"}},"outputs":[],"execution_count":42},{"id":"e6d0f3e3","cell_type":"code","source":"# enable model EMA\nprint(\"Using model EMA ...\")\nmodel_ema = ModelEma(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:15.059898Z","iopub.execute_input":"2025-10-04T16:20:15.060180Z","iopub.status.idle":"2025-10-04T16:20:15.102120Z","shell.execute_reply.started":"2025-10-04T16:20:15.060154Z","shell.execute_reply":"2025-10-04T16:20:15.101573Z"}},"outputs":[{"name":"stdout","text":"Using model EMA ...\n","output_type":"stream"}],"execution_count":43},{"id":"fb5ad099","cell_type":"code","source":"\"\"\"4. Resume from model / Misc\"\"\"\n# resume from a checkpoint?\nif resume:\n    if os.path.isfile(resume):\n        # load ckpt, reset epoch / best rmse\n        checkpoint = torch.load(resume,\n            map_location = lambda storage, loc: storage.cuda(devices[0]))\n        start_epoch = checkpoint['epoch']\n        model.load_state_dict(checkpoint['state_dict'])\n        model_ema.module.load_state_dict(checkpoint['state_dict_ema'])\n        # also load the optimizer / scheduler if necessary\n        optimizer.load_state_dict(checkpoint['optimizer'])\n        scheduler.load_state_dict(checkpoint['scheduler'])\n        print(\"=> loaded checkpoint '{:s}' (epoch {:d}\".format(\n            resume, checkpoint['epoch']\n        ))\n        del checkpoint\n    else:\n        print(\"=> no checkpoint found at '{}'\".format(resume))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:15.102754Z","iopub.execute_input":"2025-10-04T16:20:15.102921Z","iopub.status.idle":"2025-10-04T16:20:15.107721Z","shell.execute_reply.started":"2025-10-04T16:20:15.102907Z","shell.execute_reply":"2025-10-04T16:20:15.107153Z"}},"outputs":[],"execution_count":44},{"id":"bac1528a","cell_type":"code","source":"# save the current parameters (instead of config)\nparams_dict = {\n    'dataset_name': dataset_name,\n    'train_split': train_split,\n    'val_split': val_split,\n    'model_name': model_name,\n    'backbone_type': backbone_type,\n    'fpn_type': fpn_type,\n    'learning_rate': learning_rate,\n    'batch_size': batch_size,\n    'epochs': epochs,\n    # ... add other important parameters as needed\n}\n\n# save the current config\nwith open(os.path.join(ckpt_folder, 'config.txt'), 'w') as fid:\n    pprint(params_dict, stream=fid)\n    fid.flush()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:15.108461Z","iopub.execute_input":"2025-10-04T16:20:15.109050Z","iopub.status.idle":"2025-10-04T16:20:15.117243Z","shell.execute_reply.started":"2025-10-04T16:20:15.109033Z","shell.execute_reply":"2025-10-04T16:20:15.116605Z"}},"outputs":[],"execution_count":45},{"id":"a28cf3a4-3732-4f14-9d9e-e96d75e2e9a8","cell_type":"code","source":"import copy\ndef truncate_feats(\n    data_dict,\n    max_seq_len,\n    trunc_thresh,\n    offset,\n    crop_ratio=None,\n    max_num_trials=200,\n    has_action=True,\n    no_trunc=False\n):\n    \"\"\"\n    Truncate feats and time stamps in a dict item\n\n    data_dict = {'video_id'        : str\n                 'feats'           : Tensor C x T\n                 'segments'        : Tensor N x 2 (in feature grid)\n                 'labels'          : Tensor N\n                 'fps'             : float\n                 'feat_stride'     : int\n                 'feat_num_frames' : in\n\n    \"\"\"\n    # get the meta info\n    feat_len = data_dict['feats'].shape[1]\n    num_segs = data_dict['segments'].shape[0]\n\n    # seq_len < max_seq_len\n    if feat_len <= max_seq_len:\n        # do nothing\n        if crop_ratio == None:\n            return data_dict\n        # randomly crop the seq by setting max_seq_len to a value in [l, r]\n        else:\n            max_seq_len = random.randint(\n                max(round(crop_ratio[0] * feat_len), 1),\n                min(round(crop_ratio[1] * feat_len), feat_len),\n            )\n            # # corner case\n            if feat_len == max_seq_len:\n                return data_dict\n\n    # otherwise, deep copy the dict\n    data_dict = copy.deepcopy(data_dict)\n\n    # try a few times till a valid truncation with at least one action\n    for _ in range(max_num_trials):\n\n        # sample a random truncation of the video feats\n        st = random.randint(0, feat_len - max_seq_len)\n        ed = st + max_seq_len\n        window = torch.as_tensor([st, ed], dtype=torch.float32)\n\n        # compute the intersection between the sampled window and all segments\n        window = window[None].repeat(num_segs, 1)\n        left = torch.maximum(window[:, 0] - offset, data_dict['segments'][:, 0])\n        right = torch.minimum(window[:, 1] + offset, data_dict['segments'][:, 1])\n        inter = (right - left).clamp(min=0)\n        area_segs = torch.abs(\n            data_dict['segments'][:, 1] - data_dict['segments'][:, 0])\n        inter_ratio = inter / area_segs\n\n        # only select those segments over the thresh\n        seg_idx = (inter_ratio >= trunc_thresh)\n\n        if no_trunc:\n            # with at least one action and not truncating any actions\n            seg_trunc_idx = torch.logical_and(\n                (inter_ratio > 0.0), (inter_ratio < 1.0)\n            )\n            if (seg_idx.sum().item() > 0) and (seg_trunc_idx.sum().item() == 0):\n                break\n        elif has_action:\n            # with at least one action\n            if seg_idx.sum().item() > 0:\n                break\n        else:\n            # without any constraints\n            break\n\n    # feats: C x T\n    data_dict['feats'] = data_dict['feats'][:, st:ed].clone()\n    # segments: N x 2 in feature grids\n    data_dict['segments'] = torch.stack((left[seg_idx], right[seg_idx]), dim=1)\n    # shift the time stamps due to truncation\n    data_dict['segments'] = data_dict['segments'] - st\n    # labels: N\n    data_dict['labels'] = data_dict['labels'][seg_idx].clone()\n\n    return data_dict\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:15.117955Z","iopub.execute_input":"2025-10-04T16:20:15.118202Z","iopub.status.idle":"2025-10-04T16:20:15.127990Z","shell.execute_reply.started":"2025-10-04T16:20:15.118180Z","shell.execute_reply":"2025-10-04T16:20:15.127478Z"}},"outputs":[],"execution_count":46},{"id":"faddc9a4","cell_type":"code","source":"def train_one_epoch(\n    train_loader,\n    model,\n    optimizer,\n    scheduler,\n    curr_epoch,\n    model_ema = None,\n    clip_grad_l2norm = -1,\n    tb_writer = None,\n    print_freq = 20\n):\n    \"\"\"Training the model for one epoch\"\"\"\n    # set up meters\n    batch_time = AverageMeter()\n    losses_tracker = {}\n    # number of iterations per epoch\n    num_iters = len(train_loader)\n    # switch to train mode\n    model.train()\n\n    # main training loop\n    print(\"\\n[Train]: Epoch {:d} started\".format(curr_epoch))\n    start = time.time()\n    for iter_idx, video_list in enumerate(train_loader, 0):\n        # zero out optim\n        optimizer.zero_grad(set_to_none=True)\n        # forward / backward the model\n        losses = model(video_list)\n        losses['final_loss'].backward()\n        # gradient cliping (to stabilize training if necessary)\n        if clip_grad_l2norm > 0.0:\n            torch.nn.utils.clip_grad_norm_(\n                model.parameters(),\n                clip_grad_l2norm\n            )\n        # step optimizer / scheduler\n        optimizer.step()\n        scheduler.step()\n\n        if model_ema is not None:\n            model_ema.update(model)\n\n        # printing (only check the stats when necessary to avoid extra cost)\n        if (iter_idx != 0) and (iter_idx % print_freq) == 0:\n            # measure elapsed time (sync all kernels)\n            torch.cuda.synchronize()\n            batch_time.update((time.time() - start) / print_freq)\n            start = time.time()\n\n            # track all losses\n            for key, value in losses.items():\n                # init meter if necessary\n                if key not in losses_tracker:\n                    losses_tracker[key] = AverageMeter()\n                # update\n                losses_tracker[key].update(value.item())\n\n            # log to tensor board\n            lr = scheduler.get_last_lr()[0]\n            global_step = curr_epoch * num_iters + iter_idx\n            if tb_writer is not None:\n                # learning rate (after stepping)\n                tb_writer.add_scalar(\n                    'train/learning_rate',\n                    lr,\n                    global_step\n                )\n                # all losses\n                tag_dict = {}\n                for key, value in losses_tracker.items():\n                    if key != \"final_loss\":\n                        tag_dict[key] = value.val\n                tb_writer.add_scalars(\n                    'train/all_losses',\n                    tag_dict,\n                    global_step\n                )\n                # final loss\n                tb_writer.add_scalar(\n                    'train/final_loss',\n                    losses_tracker['final_loss'].val,\n                    global_step\n                )\n\n            # print to terminal\n            block1 = 'Epoch: [{:03d}][{:05d}/{:05d}]'.format(\n                curr_epoch, iter_idx, num_iters\n            )\n            block2 = 'Time {:.2f} ({:.2f})'.format(\n                batch_time.val, batch_time.avg\n            )\n            block3 = 'Loss {:.2f} ({:.2f})\\n'.format(\n                losses_tracker['final_loss'].val,\n                losses_tracker['final_loss'].avg\n            )\n            block4 = ''\n            for key, value in losses_tracker.items():\n                if key != \"final_loss\":\n                    block4  += '\\t{:s} {:.2f} ({:.2f})'.format(\n                        key, value.val, value.avg\n                    )\n\n            print('\\t'.join([block1, block2, block3, block4]))\n\n    # finish up and print\n    lr = scheduler.get_last_lr()[0]\n    print(\"[Train]: Epoch {:d} finished with lr={:.8f}\\n\".format(curr_epoch, lr))\n    return\n\n\ndef valid_one_epoch(\n    val_loader,\n    model,\n    curr_epoch,\n    ext_score_file = None,\n    evaluator = None,\n    output_file = None,\n    tb_writer = None,\n    print_freq = 20\n):\n    \"\"\"Test the model on the validation set\"\"\"\n    # either evaluate the results or save the results\n    assert (evaluator is not None) or (output_file is not None)\n\n    # set up meters\n    batch_time = AverageMeter()\n    # switch to evaluate mode\n    model.eval()\n    # dict for results (for our evaluation code)\n    results = {\n        'video-id': [],\n        't-start' : [],\n        't-end': [],\n        'label': [],\n        'score': []\n    }\n\n    # loop over validation set\n    start = time.time()\n    for iter_idx, video_list in enumerate(val_loader, 0):\n        # forward the model (wo. grad)\n        with torch.no_grad():\n            output = model(video_list)\n\n            # unpack the results into ANet format\n            num_vids = len(output)\n            for vid_idx in range(num_vids):\n                if output[vid_idx]['segments'].shape[0] > 0:\n                    results['video-id'].extend(\n                        [output[vid_idx]['video_id']] *\n                        output[vid_idx]['segments'].shape[0]\n                    )\n                    results['t-start'].append(output[vid_idx]['segments'][:, 0])\n                    results['t-end'].append(output[vid_idx]['segments'][:, 1])\n                    results['label'].append(output[vid_idx]['labels'])\n                    results['score'].append(output[vid_idx]['scores'])\n\n        # printing\n        if (iter_idx != 0) and iter_idx % (print_freq) == 0:\n            # measure elapsed time (sync all kernels)\n            torch.cuda.synchronize()\n            batch_time.update((time.time() - start) / print_freq)\n            start = time.time()\n\n            # print timing\n            print('Test: [{0:05d}/{1:05d}]\\t'\n                  'Time {batch_time.val:.2f} ({batch_time.avg:.2f})'.format(\n                  iter_idx, len(val_loader), batch_time=batch_time))\n\n    # gather all stats and evaluate\n    results['t-start'] = torch.cat(results['t-start']).numpy()\n    results['t-end'] = torch.cat(results['t-end']).numpy()\n    results['label'] = torch.cat(results['label']).numpy()\n    results['score'] = torch.cat(results['score']).numpy()\n\n    if evaluator is not None:\n        if ext_score_file is not None and isinstance(ext_score_file, str):\n            results = postprocess_results(results, ext_score_file)\n        # call the evaluator\n        _, mAP, _ = evaluator.evaluate(results, verbose=True)\n    else:\n        # dump to a pickle file that can be directly used for evaluation\n        with open(output_file, \"wb\") as f:\n            pickle.dump(results, f)\n        mAP = 0.0\n\n    # log mAP to tb_writer\n    if tb_writer is not None:\n        tb_writer.add_scalar('validation/mAP', mAP, curr_epoch)\n\n    return mAP","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:15.129038Z","iopub.execute_input":"2025-10-04T16:20:15.129295Z","iopub.status.idle":"2025-10-04T16:20:15.145547Z","shell.execute_reply.started":"2025-10-04T16:20:15.129273Z","shell.execute_reply":"2025-10-04T16:20:15.144886Z"}},"outputs":[],"execution_count":47},{"id":"178e7669","cell_type":"code","source":"def save_checkpoint(state, is_best, file_folder,\n                    file_name='checkpoint.pth.tar'):\n    \"\"\"save checkpoint to file\"\"\"\n    if not os.path.exists(file_folder):\n        os.mkdir(file_folder)\n    torch.save(state, os.path.join(file_folder, file_name))\n    if is_best:\n        # skip the optimization / scheduler state\n        state.pop('optimizer', None)\n        state.pop('scheduler', None)\n        torch.save(state, os.path.join(file_folder, 'model_best.pth.tar'))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:15.146235Z","iopub.execute_input":"2025-10-04T16:20:15.146427Z","iopub.status.idle":"2025-10-04T16:20:15.157959Z","shell.execute_reply.started":"2025-10-04T16:20:15.146413Z","shell.execute_reply":"2025-10-04T16:20:15.157393Z"}},"outputs":[],"execution_count":48},{"id":"1c4662e7-56f6-4265-94f2-1a028eb0eb70","cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value.\n    Used to compute dataset stats from mini-batches\n    \"\"\"\n    def __init__(self):\n        self.initialized = False\n        self.val = None\n        self.avg = None\n        self.sum = None\n        self.count = 0.0\n\n    def initialize(self, val, n):\n        self.val = val\n        self.avg = val\n        self.sum = val * n\n        self.count = n\n        self.initialized = True\n\n    def update(self, val, n=1):\n        if not self.initialized:\n            self.initialize(val, n)\n        else:\n            self.add(val, n)\n\n    def add(self, val, n):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:15.158637Z","iopub.execute_input":"2025-10-04T16:20:15.158863Z","iopub.status.idle":"2025-10-04T16:20:15.167619Z","shell.execute_reply.started":"2025-10-04T16:20:15.158844Z","shell.execute_reply":"2025-10-04T16:20:15.167025Z"}},"outputs":[],"execution_count":49},{"id":"466e259f-67c4-465d-a034-dd4cce19e804","cell_type":"code","source":"@torch.jit.script\ndef sigmoid_focal_loss(\n    inputs: torch.Tensor,\n    targets: torch.Tensor,\n    alpha: float = 0.25,\n    gamma: float = 2.0,\n    reduction: str = \"none\",\n) -> torch.Tensor:\n    \"\"\"\n    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n    Taken from\n    https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py\n    # Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n\n    Args:\n        inputs: A float tensor of arbitrary shape.\n                The predictions for each example.\n        targets: A float tensor with the same shape as inputs. Stores the binary\n                 classification label for each element in inputs\n                (0 for the negative class and 1 for the positive class).\n        alpha: (optional) Weighting factor in range (0,1) to balance\n                positive vs negative examples. Default = 0.25.\n        gamma: Exponent of the modulating factor (1 - p_t) to\n               balance easy vs hard examples.\n        reduction: 'none' | 'mean' | 'sum'\n                 'none': No reduction will be applied to the output.\n                 'mean': The output will be averaged.\n                 'sum': The output will be summed.\n    Returns:\n        Loss tensor with the reduction option applied.\n    \"\"\"\n    inputs = inputs.float()\n    targets = targets.float()\n    p = torch.sigmoid(inputs)\n    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n    p_t = p * targets + (1 - p) * (1 - targets)\n    loss = ce_loss * ((1 - p_t) ** gamma)\n\n    if alpha >= 0:\n        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n        loss = alpha_t * loss\n\n    if reduction == \"mean\":\n        loss = loss.mean()\n    elif reduction == \"sum\":\n        loss = loss.sum()\n\n    return loss\n\n\n@torch.jit.script\ndef ctr_giou_loss_1d(\n    input_offsets: torch.Tensor,\n    target_offsets: torch.Tensor,\n    reduction: str = 'none',\n    eps: float = 1e-8,\n) -> torch.Tensor:\n    \"\"\"\n    Generalized Intersection over Union Loss (Hamid Rezatofighi et. al)\n    https://arxiv.org/abs/1902.09630\n\n    This is an implementation that assumes a 1D event is represented using\n    the same center point with different offsets, e.g.,\n    (t1, t2) = (c - o_1, c + o_2) with o_i >= 0\n\n    Reference code from\n    https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/giou_loss.py\n\n    Args:\n        input/target_offsets (Tensor): 1D offsets of size (N, 2)\n        reduction: 'none' | 'mean' | 'sum'\n                 'none': No reduction will be applied to the output.\n                 'mean': The output will be averaged.\n                 'sum': The output will be summed.\n        eps (float): small number to prevent division by zero\n    \"\"\"\n    input_offsets = input_offsets.float()\n    target_offsets = target_offsets.float()\n    # check all 1D events are valid\n    assert (input_offsets >= 0.0).all(), \"predicted offsets must be non-negative\"\n    assert (target_offsets >= 0.0).all(), \"GT offsets must be non-negative\"\n\n    lp, rp = input_offsets[:, 0], input_offsets[:, 1]\n    lg, rg = target_offsets[:, 0], target_offsets[:, 1]\n\n    # intersection key points\n    lkis = torch.min(lp, lg)\n    rkis = torch.min(rp, rg)\n\n    # iou\n    intsctk = rkis + lkis\n    unionk = (lp + rp) + (lg + rg) - intsctk\n    iouk = intsctk / unionk.clamp(min=eps)\n\n    # giou is reduced to iou in our setting, skip unnecessary steps\n    loss = 1.0 - iouk\n\n    if reduction == \"mean\":\n        loss = loss.mean() if loss.numel() > 0 else 0.0 * loss.sum()\n    elif reduction == \"sum\":\n        loss = loss.sum()\n\n    return loss\n\n@torch.jit.script\ndef ctr_diou_loss_1d(\n    input_offsets: torch.Tensor,\n    target_offsets: torch.Tensor,\n    reduction: str = 'none',\n    eps: float = 1e-8,\n) -> torch.Tensor:\n    \"\"\"\n    Distance-IoU Loss (Zheng et. al)\n    https://arxiv.org/abs/1911.08287\n\n    This is an implementation that assumes a 1D event is represented using\n    the same center point with different offsets, e.g.,\n    (t1, t2) = (c - o_1, c + o_2) with o_i >= 0\n\n    Reference code from\n    https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/giou_loss.py\n\n    Args:\n        input/target_offsets (Tensor): 1D offsets of size (N, 2)\n        reduction: 'none' | 'mean' | 'sum'\n                 'none': No reduction will be applied to the output.\n                 'mean': The output will be averaged.\n                 'sum': The output will be summed.\n        eps (float): small number to prevent division by zero\n    \"\"\"\n    input_offsets = input_offsets.float()\n    target_offsets = target_offsets.float()\n    # check all 1D events are valid\n    assert (input_offsets >= 0.0).all(), \"predicted offsets must be non-negative\"\n    assert (target_offsets >= 0.0).all(), \"GT offsets must be non-negative\"\n\n    lp, rp = input_offsets[:, 0], input_offsets[:, 1]\n    lg, rg = target_offsets[:, 0], target_offsets[:, 1]\n\n    # intersection key points\n    lkis = torch.min(lp, lg)\n    rkis = torch.min(rp, rg)\n\n    # iou\n    intsctk = rkis + lkis\n    unionk = (lp + rp) + (lg + rg) - intsctk\n    iouk = intsctk / unionk.clamp(min=eps)\n\n    # smallest enclosing box\n    lc = torch.max(lp, lg)\n    rc = torch.max(rp, rg)\n    len_c = lc + rc\n\n    # offset between centers\n    rho = 0.5 * (rp - lp - rg + lg)\n\n    # diou\n    loss = 1.0 - iouk + torch.square(rho / len_c.clamp(min=eps))\n\n    if reduction == \"mean\":\n        loss = loss.mean() if loss.numel() > 0 else 0.0 * loss.sum()\n    elif reduction == \"sum\":\n        loss = loss.sum()\n\n    return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:15.168352Z","iopub.execute_input":"2025-10-04T16:20:15.168563Z","iopub.status.idle":"2025-10-04T16:20:15.223837Z","shell.execute_reply.started":"2025-10-04T16:20:15.168540Z","shell.execute_reply":"2025-10-04T16:20:15.223334Z"}},"outputs":[],"execution_count":50},{"id":"8e7b3f59","cell_type":"code","source":"\"\"\"5. training loop\"\"\"\nprint(f\"\\nStart training model {model_name} ...\")\n\n# start training\nmax_epochs = epochs + warmup_epochs\nfor epoch in range(start_epoch, max_epochs):\n    # train for one epoch\n    train_one_epoch(\n        train_loader,\n        model,\n        optimizer,\n        scheduler,\n        epoch,\n        model_ema=model_ema,\n        clip_grad_l2norm=clip_grad_l2norm,\n        tb_writer=tb_writer,\n        print_freq=print_freq\n    )\n\n    # save ckpt once in a while\n    if (\n        ((epoch + 1) == max_epochs) or\n        ((ckpt_freq > 0) and ((epoch + 1) % ckpt_freq == 0))\n    ):\n        save_states = {\n            'epoch': epoch + 1,\n            'state_dict': model.state_dict(),\n            'scheduler': scheduler.state_dict(),\n            'optimizer': optimizer.state_dict(),\n        }\n\n        save_states['state_dict_ema'] = model_ema.module.state_dict()\n        save_checkpoint(\n            save_states,\n            False,\n            file_folder=ckpt_folder,\n            file_name='epoch_{:03d}.pth.tar'.format(epoch + 1)\n        )\n\n# wrap up\ntb_writer.close()\nprint(\"All done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:20:15.224560Z","iopub.execute_input":"2025-10-04T16:20:15.224768Z","iopub.status.idle":"2025-10-04T16:30:33.221973Z","shell.execute_reply.started":"2025-10-04T16:20:15.224752Z","shell.execute_reply":"2025-10-04T16:30:33.221270Z"}},"outputs":[{"name":"stdout","text":"\nStart training model LocPointTransformer ...\n\n[Train]: Epoch 0 started\nEpoch: [000][00010/00100]\tTime 0.65 (0.65)\tLoss 1.74 (1.74)\n\t\tcls_loss 1.14 (1.14)\treg_loss 0.60 (0.60)\nEpoch: [000][00020/00100]\tTime 0.17 (0.41)\tLoss 0.68 (1.21)\n\t\tcls_loss 0.41 (0.78)\treg_loss 0.27 (0.43)\nEpoch: [000][00030/00100]\tTime 0.16 (0.33)\tLoss 1.03 (1.15)\n\t\tcls_loss 0.68 (0.74)\treg_loss 0.35 (0.41)\nEpoch: [000][00040/00100]\tTime 0.17 (0.29)\tLoss 1.29 (1.18)\n\t\tcls_loss 0.92 (0.79)\treg_loss 0.36 (0.40)\nEpoch: [000][00050/00100]\tTime 0.17 (0.26)\tLoss 1.49 (1.24)\n\t\tcls_loss 1.10 (0.85)\treg_loss 0.39 (0.39)\nEpoch: [000][00060/00100]\tTime 0.17 (0.25)\tLoss 0.51 (1.12)\n\t\tcls_loss 0.37 (0.77)\treg_loss 0.14 (0.35)\nEpoch: [000][00070/00100]\tTime 0.17 (0.24)\tLoss 0.89 (1.09)\n\t\tcls_loss 0.67 (0.76)\treg_loss 0.23 (0.33)\nEpoch: [000][00080/00100]\tTime 0.17 (0.23)\tLoss 1.37 (1.13)\n\t\tcls_loss 0.98 (0.78)\treg_loss 0.39 (0.34)\nEpoch: [000][00090/00100]\tTime 0.16 (0.22)\tLoss 0.69 (1.08)\n\t\tcls_loss 0.51 (0.75)\treg_loss 0.19 (0.32)\n[Train]: Epoch 0 finished with lr=0.00002004\n\n\n[Train]: Epoch 1 started\nEpoch: [001][00010/00100]\tTime 0.21 (0.21)\tLoss 0.65 (0.65)\n\t\tcls_loss 0.45 (0.45)\treg_loss 0.20 (0.20)\nEpoch: [001][00020/00100]\tTime 0.17 (0.19)\tLoss 1.20 (0.92)\n\t\tcls_loss 0.82 (0.64)\treg_loss 0.37 (0.29)\nEpoch: [001][00030/00100]\tTime 0.17 (0.18)\tLoss 1.42 (1.09)\n\t\tcls_loss 0.87 (0.71)\treg_loss 0.56 (0.38)\nEpoch: [001][00040/00100]\tTime 0.17 (0.18)\tLoss 0.39 (0.92)\n\t\tcls_loss 0.29 (0.61)\treg_loss 0.11 (0.31)\nEpoch: [001][00050/00100]\tTime 0.16 (0.17)\tLoss 1.18 (0.97)\n\t\tcls_loss 0.78 (0.64)\treg_loss 0.40 (0.33)\nEpoch: [001][00060/00100]\tTime 0.16 (0.17)\tLoss 1.62 (1.08)\n\t\tcls_loss 0.98 (0.70)\treg_loss 0.64 (0.38)\nEpoch: [001][00070/00100]\tTime 0.17 (0.17)\tLoss 0.32 (0.97)\n\t\tcls_loss 0.20 (0.63)\treg_loss 0.12 (0.34)\nEpoch: [001][00080/00100]\tTime 0.16 (0.17)\tLoss 1.85 (1.08)\n\t\tcls_loss 1.18 (0.70)\treg_loss 0.67 (0.38)\nEpoch: [001][00090/00100]\tTime 0.16 (0.17)\tLoss 1.43 (1.12)\n\t\tcls_loss 0.83 (0.71)\treg_loss 0.60 (0.41)\n[Train]: Epoch 1 finished with lr=0.00004008\n\n\n[Train]: Epoch 2 started\nEpoch: [002][00010/00100]\tTime 0.20 (0.20)\tLoss 0.50 (0.50)\n\t\tcls_loss 0.32 (0.32)\treg_loss 0.18 (0.18)\nEpoch: [002][00020/00100]\tTime 0.17 (0.18)\tLoss 0.58 (0.54)\n\t\tcls_loss 0.41 (0.36)\treg_loss 0.17 (0.18)\nEpoch: [002][00030/00100]\tTime 0.17 (0.18)\tLoss 0.52 (0.53)\n\t\tcls_loss 0.38 (0.37)\treg_loss 0.14 (0.16)\nEpoch: [002][00040/00100]\tTime 0.17 (0.18)\tLoss 1.71 (0.83)\n\t\tcls_loss 0.96 (0.52)\treg_loss 0.75 (0.31)\nEpoch: [002][00050/00100]\tTime 0.17 (0.17)\tLoss 0.31 (0.72)\n\t\tcls_loss 0.22 (0.46)\treg_loss 0.09 (0.27)\nEpoch: [002][00060/00100]\tTime 0.16 (0.17)\tLoss 1.09 (0.79)\n\t\tcls_loss 0.64 (0.49)\treg_loss 0.45 (0.30)\nEpoch: [002][00070/00100]\tTime 0.17 (0.17)\tLoss 1.81 (0.93)\n\t\tcls_loss 1.26 (0.60)\treg_loss 0.55 (0.33)\nEpoch: [002][00080/00100]\tTime 0.17 (0.17)\tLoss 0.64 (0.90)\n\t\tcls_loss 0.40 (0.57)\treg_loss 0.25 (0.32)\nEpoch: [002][00090/00100]\tTime 0.17 (0.17)\tLoss 1.88 (1.00)\n\t\tcls_loss 1.35 (0.66)\treg_loss 0.53 (0.35)\n[Train]: Epoch 2 finished with lr=0.00006012\n\n\n[Train]: Epoch 3 started\nEpoch: [003][00010/00100]\tTime 0.22 (0.22)\tLoss 0.68 (0.68)\n\t\tcls_loss 0.47 (0.47)\treg_loss 0.21 (0.21)\nEpoch: [003][00020/00100]\tTime 0.17 (0.19)\tLoss 1.79 (1.23)\n\t\tcls_loss 1.01 (0.74)\treg_loss 0.78 (0.50)\nEpoch: [003][00030/00100]\tTime 0.17 (0.18)\tLoss 2.51 (1.66)\n\t\tcls_loss 1.60 (1.02)\treg_loss 0.91 (0.64)\nEpoch: [003][00040/00100]\tTime 0.17 (0.18)\tLoss 0.97 (1.49)\n\t\tcls_loss 0.72 (0.95)\treg_loss 0.25 (0.54)\nEpoch: [003][00050/00100]\tTime 0.17 (0.18)\tLoss 1.08 (1.41)\n\t\tcls_loss 0.63 (0.89)\treg_loss 0.45 (0.52)\nEpoch: [003][00060/00100]\tTime 0.17 (0.18)\tLoss 0.87 (1.32)\n\t\tcls_loss 0.54 (0.83)\treg_loss 0.33 (0.49)\nEpoch: [003][00070/00100]\tTime 0.17 (0.18)\tLoss 0.65 (1.22)\n\t\tcls_loss 0.40 (0.77)\treg_loss 0.25 (0.45)\nEpoch: [003][00080/00100]\tTime 0.17 (0.17)\tLoss 1.28 (1.23)\n\t\tcls_loss 0.86 (0.78)\treg_loss 0.42 (0.45)\nEpoch: [003][00090/00100]\tTime 0.17 (0.17)\tLoss 0.45 (1.14)\n\t\tcls_loss 0.36 (0.73)\treg_loss 0.09 (0.41)\n[Train]: Epoch 3 finished with lr=0.00008016\n\n\n[Train]: Epoch 4 started\nEpoch: [004][00010/00100]\tTime 0.21 (0.21)\tLoss 0.51 (0.51)\n\t\tcls_loss 0.32 (0.32)\treg_loss 0.19 (0.19)\nEpoch: [004][00020/00100]\tTime 0.17 (0.19)\tLoss 0.58 (0.55)\n\t\tcls_loss 0.37 (0.35)\treg_loss 0.21 (0.20)\nEpoch: [004][00030/00100]\tTime 0.17 (0.18)\tLoss 0.31 (0.47)\n\t\tcls_loss 0.24 (0.31)\treg_loss 0.07 (0.16)\nEpoch: [004][00040/00100]\tTime 0.17 (0.18)\tLoss 0.38 (0.45)\n\t\tcls_loss 0.28 (0.30)\treg_loss 0.10 (0.14)\nEpoch: [004][00050/00100]\tTime 0.17 (0.18)\tLoss 0.59 (0.47)\n\t\tcls_loss 0.37 (0.32)\treg_loss 0.22 (0.16)\nEpoch: [004][00060/00100]\tTime 0.17 (0.18)\tLoss 1.02 (0.57)\n\t\tcls_loss 0.60 (0.36)\treg_loss 0.42 (0.20)\nEpoch: [004][00070/00100]\tTime 0.17 (0.18)\tLoss 0.52 (0.56)\n\t\tcls_loss 0.31 (0.36)\treg_loss 0.20 (0.20)\nEpoch: [004][00080/00100]\tTime 0.17 (0.18)\tLoss 0.77 (0.58)\n\t\tcls_loss 0.45 (0.37)\treg_loss 0.32 (0.22)\nEpoch: [004][00090/00100]\tTime 0.17 (0.17)\tLoss 0.40 (0.56)\n\t\tcls_loss 0.27 (0.36)\treg_loss 0.12 (0.21)\n[Train]: Epoch 4 finished with lr=0.00010000\n\n\n[Train]: Epoch 5 started\nEpoch: [005][00010/00100]\tTime 0.21 (0.21)\tLoss 0.29 (0.29)\n\t\tcls_loss 0.17 (0.17)\treg_loss 0.12 (0.12)\nEpoch: [005][00020/00100]\tTime 0.17 (0.19)\tLoss 0.46 (0.38)\n\t\tcls_loss 0.32 (0.24)\treg_loss 0.14 (0.13)\nEpoch: [005][00030/00100]\tTime 0.17 (0.18)\tLoss 0.77 (0.51)\n\t\tcls_loss 0.45 (0.31)\treg_loss 0.32 (0.20)\nEpoch: [005][00040/00100]\tTime 0.18 (0.18)\tLoss 0.99 (0.63)\n\t\tcls_loss 0.73 (0.42)\treg_loss 0.26 (0.21)\nEpoch: [005][00050/00100]\tTime 0.17 (0.18)\tLoss 0.38 (0.58)\n\t\tcls_loss 0.28 (0.39)\treg_loss 0.11 (0.19)\nEpoch: [005][00060/00100]\tTime 0.18 (0.18)\tLoss 1.61 (0.75)\n\t\tcls_loss 0.96 (0.48)\treg_loss 0.65 (0.27)\nEpoch: [005][00070/00100]\tTime 0.17 (0.18)\tLoss 0.29 (0.69)\n\t\tcls_loss 0.17 (0.44)\treg_loss 0.12 (0.25)\nEpoch: [005][00080/00100]\tTime 0.17 (0.18)\tLoss 0.31 (0.64)\n\t\tcls_loss 0.21 (0.41)\treg_loss 0.10 (0.23)\nEpoch: [005][00090/00100]\tTime 0.17 (0.18)\tLoss 1.85 (0.77)\n\t\tcls_loss 1.27 (0.51)\treg_loss 0.58 (0.27)\n[Train]: Epoch 5 finished with lr=0.00009973\n\n\n[Train]: Epoch 6 started\nEpoch: [006][00010/00100]\tTime 0.21 (0.21)\tLoss 0.53 (0.53)\n\t\tcls_loss 0.38 (0.38)\treg_loss 0.14 (0.14)\nEpoch: [006][00020/00100]\tTime 0.18 (0.19)\tLoss 1.17 (0.85)\n\t\tcls_loss 0.75 (0.56)\treg_loss 0.43 (0.29)\nEpoch: [006][00030/00100]\tTime 0.18 (0.19)\tLoss 0.40 (0.70)\n\t\tcls_loss 0.23 (0.45)\treg_loss 0.17 (0.25)\nEpoch: [006][00040/00100]\tTime 0.18 (0.19)\tLoss 0.89 (0.75)\n\t\tcls_loss 0.50 (0.46)\treg_loss 0.39 (0.28)\nEpoch: [006][00050/00100]\tTime 0.17 (0.18)\tLoss 0.58 (0.71)\n\t\tcls_loss 0.34 (0.44)\treg_loss 0.24 (0.27)\nEpoch: [006][00060/00100]\tTime 0.17 (0.18)\tLoss 0.72 (0.71)\n\t\tcls_loss 0.42 (0.43)\treg_loss 0.31 (0.28)\nEpoch: [006][00070/00100]\tTime 0.18 (0.18)\tLoss 0.34 (0.66)\n\t\tcls_loss 0.22 (0.40)\treg_loss 0.12 (0.26)\nEpoch: [006][00080/00100]\tTime 0.17 (0.18)\tLoss 0.43 (0.63)\n\t\tcls_loss 0.32 (0.39)\treg_loss 0.11 (0.24)\nEpoch: [006][00090/00100]\tTime 0.17 (0.18)\tLoss 0.29 (0.59)\n\t\tcls_loss 0.17 (0.37)\treg_loss 0.11 (0.22)\n[Train]: Epoch 6 finished with lr=0.00009891\n\n\n[Train]: Epoch 7 started\nEpoch: [007][00010/00100]\tTime 0.21 (0.21)\tLoss 1.30 (1.30)\n\t\tcls_loss 0.89 (0.89)\treg_loss 0.41 (0.41)\nEpoch: [007][00020/00100]\tTime 0.17 (0.19)\tLoss 1.25 (1.27)\n\t\tcls_loss 0.68 (0.79)\treg_loss 0.56 (0.49)\nEpoch: [007][00030/00100]\tTime 0.18 (0.19)\tLoss 0.78 (1.11)\n\t\tcls_loss 0.56 (0.71)\treg_loss 0.22 (0.40)\nEpoch: [007][00040/00100]\tTime 0.17 (0.18)\tLoss 0.46 (0.94)\n\t\tcls_loss 0.24 (0.59)\treg_loss 0.22 (0.35)\nEpoch: [007][00050/00100]\tTime 0.17 (0.18)\tLoss 0.65 (0.89)\n\t\tcls_loss 0.41 (0.55)\treg_loss 0.25 (0.33)\nEpoch: [007][00060/00100]\tTime 0.17 (0.18)\tLoss 0.11 (0.76)\n\t\tcls_loss 0.06 (0.47)\treg_loss 0.05 (0.28)\nEpoch: [007][00070/00100]\tTime 0.17 (0.18)\tLoss 0.97 (0.79)\n\t\tcls_loss 0.54 (0.48)\treg_loss 0.43 (0.31)\nEpoch: [007][00080/00100]\tTime 0.17 (0.18)\tLoss 0.92 (0.80)\n\t\tcls_loss 0.63 (0.50)\treg_loss 0.28 (0.30)\nEpoch: [007][00090/00100]\tTime 0.18 (0.18)\tLoss 0.61 (0.78)\n\t\tcls_loss 0.42 (0.49)\treg_loss 0.19 (0.29)\n[Train]: Epoch 7 finished with lr=0.00009755\n\n\n[Train]: Epoch 8 started\nEpoch: [008][00010/00100]\tTime 0.24 (0.24)\tLoss 0.25 (0.25)\n\t\tcls_loss 0.15 (0.15)\treg_loss 0.10 (0.10)\nEpoch: [008][00020/00100]\tTime 0.17 (0.21)\tLoss 0.07 (0.16)\n\t\tcls_loss 0.04 (0.10)\treg_loss 0.03 (0.06)\nEpoch: [008][00030/00100]\tTime 0.17 (0.20)\tLoss 1.12 (0.48)\n\t\tcls_loss 0.75 (0.31)\treg_loss 0.38 (0.17)\nEpoch: [008][00040/00100]\tTime 0.17 (0.19)\tLoss 0.34 (0.45)\n\t\tcls_loss 0.20 (0.28)\treg_loss 0.14 (0.16)\nEpoch: [008][00050/00100]\tTime 0.18 (0.19)\tLoss 0.51 (0.46)\n\t\tcls_loss 0.37 (0.30)\treg_loss 0.14 (0.16)\nEpoch: [008][00060/00100]\tTime 0.18 (0.19)\tLoss 0.25 (0.42)\n\t\tcls_loss 0.14 (0.27)\treg_loss 0.11 (0.15)\nEpoch: [008][00070/00100]\tTime 0.17 (0.18)\tLoss 0.89 (0.49)\n\t\tcls_loss 0.55 (0.31)\treg_loss 0.35 (0.18)\nEpoch: [008][00080/00100]\tTime 0.17 (0.18)\tLoss 0.27 (0.46)\n\t\tcls_loss 0.19 (0.30)\treg_loss 0.08 (0.17)\nEpoch: [008][00090/00100]\tTime 0.17 (0.18)\tLoss 0.30 (0.45)\n\t\tcls_loss 0.18 (0.28)\treg_loss 0.13 (0.16)\n[Train]: Epoch 8 finished with lr=0.00009568\n\n\n[Train]: Epoch 9 started\nEpoch: [009][00010/00100]\tTime 0.21 (0.21)\tLoss 0.48 (0.48)\n\t\tcls_loss 0.24 (0.24)\treg_loss 0.24 (0.24)\nEpoch: [009][00020/00100]\tTime 0.17 (0.19)\tLoss 1.13 (0.81)\n\t\tcls_loss 0.67 (0.46)\treg_loss 0.46 (0.35)\nEpoch: [009][00030/00100]\tTime 0.18 (0.19)\tLoss 0.74 (0.78)\n\t\tcls_loss 0.39 (0.43)\treg_loss 0.35 (0.35)\nEpoch: [009][00040/00100]\tTime 0.17 (0.18)\tLoss 0.73 (0.77)\n\t\tcls_loss 0.45 (0.44)\treg_loss 0.28 (0.33)\nEpoch: [009][00050/00100]\tTime 0.17 (0.18)\tLoss 0.48 (0.71)\n\t\tcls_loss 0.30 (0.41)\treg_loss 0.18 (0.30)\nEpoch: [009][00060/00100]\tTime 0.17 (0.18)\tLoss 0.20 (0.63)\n\t\tcls_loss 0.14 (0.37)\treg_loss 0.05 (0.26)\nEpoch: [009][00070/00100]\tTime 0.17 (0.18)\tLoss 0.45 (0.60)\n\t\tcls_loss 0.24 (0.35)\treg_loss 0.20 (0.25)\nEpoch: [009][00080/00100]\tTime 0.17 (0.18)\tLoss 0.46 (0.58)\n\t\tcls_loss 0.29 (0.34)\treg_loss 0.17 (0.24)\nEpoch: [009][00090/00100]\tTime 0.17 (0.18)\tLoss 0.52 (0.58)\n\t\tcls_loss 0.36 (0.34)\treg_loss 0.16 (0.23)\n[Train]: Epoch 9 finished with lr=0.00009330\n\n\n[Train]: Epoch 10 started\nEpoch: [010][00010/00100]\tTime 0.21 (0.21)\tLoss 0.36 (0.36)\n\t\tcls_loss 0.26 (0.26)\treg_loss 0.10 (0.10)\nEpoch: [010][00020/00100]\tTime 0.17 (0.19)\tLoss 0.20 (0.28)\n\t\tcls_loss 0.13 (0.20)\treg_loss 0.07 (0.08)\nEpoch: [010][00030/00100]\tTime 0.17 (0.18)\tLoss 0.49 (0.35)\n\t\tcls_loss 0.28 (0.22)\treg_loss 0.21 (0.13)\nEpoch: [010][00040/00100]\tTime 0.17 (0.18)\tLoss 0.22 (0.32)\n\t\tcls_loss 0.11 (0.20)\treg_loss 0.10 (0.12)\nEpoch: [010][00050/00100]\tTime 0.18 (0.18)\tLoss 0.24 (0.30)\n\t\tcls_loss 0.15 (0.19)\treg_loss 0.09 (0.12)\nEpoch: [010][00060/00100]\tTime 0.18 (0.18)\tLoss 0.42 (0.32)\n\t\tcls_loss 0.26 (0.20)\treg_loss 0.16 (0.12)\nEpoch: [010][00070/00100]\tTime 0.18 (0.18)\tLoss 2.07 (0.57)\n\t\tcls_loss 1.26 (0.35)\treg_loss 0.81 (0.22)\nEpoch: [010][00080/00100]\tTime 0.17 (0.18)\tLoss 0.17 (0.52)\n\t\tcls_loss 0.09 (0.32)\treg_loss 0.08 (0.20)\nEpoch: [010][00090/00100]\tTime 0.17 (0.18)\tLoss 0.84 (0.56)\n\t\tcls_loss 0.42 (0.33)\treg_loss 0.41 (0.23)\n[Train]: Epoch 10 finished with lr=0.00009045\n\n\n[Train]: Epoch 11 started\nEpoch: [011][00010/00100]\tTime 0.20 (0.20)\tLoss 0.77 (0.77)\n\t\tcls_loss 0.44 (0.44)\treg_loss 0.33 (0.33)\nEpoch: [011][00020/00100]\tTime 0.17 (0.19)\tLoss 0.64 (0.71)\n\t\tcls_loss 0.37 (0.41)\treg_loss 0.27 (0.30)\nEpoch: [011][00030/00100]\tTime 0.17 (0.18)\tLoss 0.27 (0.56)\n\t\tcls_loss 0.15 (0.32)\treg_loss 0.12 (0.24)\nEpoch: [011][00040/00100]\tTime 0.18 (0.18)\tLoss 0.16 (0.46)\n\t\tcls_loss 0.11 (0.27)\treg_loss 0.05 (0.19)\nEpoch: [011][00050/00100]\tTime 0.17 (0.18)\tLoss 0.47 (0.46)\n\t\tcls_loss 0.28 (0.27)\treg_loss 0.20 (0.19)\nEpoch: [011][00060/00100]\tTime 0.18 (0.18)\tLoss 0.63 (0.49)\n\t\tcls_loss 0.32 (0.28)\treg_loss 0.32 (0.22)\nEpoch: [011][00070/00100]\tTime 0.17 (0.18)\tLoss 0.28 (0.46)\n\t\tcls_loss 0.17 (0.26)\treg_loss 0.11 (0.20)\nEpoch: [011][00080/00100]\tTime 0.18 (0.18)\tLoss 1.72 (0.62)\n\t\tcls_loss 0.96 (0.35)\treg_loss 0.76 (0.27)\nEpoch: [011][00090/00100]\tTime 0.18 (0.18)\tLoss 0.45 (0.60)\n\t\tcls_loss 0.27 (0.34)\treg_loss 0.18 (0.26)\n[Train]: Epoch 11 finished with lr=0.00008716\n\n\n[Train]: Epoch 12 started\nEpoch: [012][00010/00100]\tTime 0.21 (0.21)\tLoss 0.90 (0.90)\n\t\tcls_loss 0.58 (0.58)\treg_loss 0.32 (0.32)\nEpoch: [012][00020/00100]\tTime 0.17 (0.19)\tLoss 0.40 (0.65)\n\t\tcls_loss 0.26 (0.42)\treg_loss 0.14 (0.23)\nEpoch: [012][00030/00100]\tTime 0.18 (0.19)\tLoss 0.60 (0.63)\n\t\tcls_loss 0.33 (0.39)\treg_loss 0.27 (0.24)\nEpoch: [012][00040/00100]\tTime 0.17 (0.18)\tLoss 0.79 (0.67)\n\t\tcls_loss 0.47 (0.41)\treg_loss 0.32 (0.26)\nEpoch: [012][00050/00100]\tTime 0.18 (0.18)\tLoss 0.58 (0.65)\n\t\tcls_loss 0.33 (0.39)\treg_loss 0.25 (0.26)\nEpoch: [012][00060/00100]\tTime 0.17 (0.18)\tLoss 1.03 (0.72)\n\t\tcls_loss 0.52 (0.41)\treg_loss 0.52 (0.30)\nEpoch: [012][00070/00100]\tTime 0.18 (0.18)\tLoss 0.42 (0.67)\n\t\tcls_loss 0.30 (0.40)\treg_loss 0.13 (0.28)\nEpoch: [012][00080/00100]\tTime 0.17 (0.18)\tLoss 0.93 (0.71)\n\t\tcls_loss 0.49 (0.41)\treg_loss 0.44 (0.30)\nEpoch: [012][00090/00100]\tTime 0.17 (0.18)\tLoss 0.16 (0.65)\n\t\tcls_loss 0.09 (0.37)\treg_loss 0.07 (0.27)\n[Train]: Epoch 12 finished with lr=0.00008346\n\n\n[Train]: Epoch 13 started\nEpoch: [013][00010/00100]\tTime 0.21 (0.21)\tLoss 0.09 (0.09)\n\t\tcls_loss 0.05 (0.05)\treg_loss 0.04 (0.04)\nEpoch: [013][00020/00100]\tTime 0.17 (0.19)\tLoss 0.32 (0.21)\n\t\tcls_loss 0.22 (0.14)\treg_loss 0.10 (0.07)\nEpoch: [013][00030/00100]\tTime 0.17 (0.18)\tLoss 0.09 (0.17)\n\t\tcls_loss 0.05 (0.11)\treg_loss 0.04 (0.06)\nEpoch: [013][00040/00100]\tTime 0.17 (0.18)\tLoss 0.83 (0.33)\n\t\tcls_loss 0.40 (0.18)\treg_loss 0.43 (0.15)\nEpoch: [013][00050/00100]\tTime 0.18 (0.18)\tLoss 0.25 (0.32)\n\t\tcls_loss 0.16 (0.18)\treg_loss 0.09 (0.14)\nEpoch: [013][00060/00100]\tTime 0.18 (0.18)\tLoss 0.53 (0.35)\n\t\tcls_loss 0.38 (0.21)\treg_loss 0.15 (0.14)\nEpoch: [013][00070/00100]\tTime 0.17 (0.18)\tLoss 0.66 (0.39)\n\t\tcls_loss 0.36 (0.23)\treg_loss 0.29 (0.16)\nEpoch: [013][00080/00100]\tTime 0.17 (0.18)\tLoss 1.14 (0.49)\n\t\tcls_loss 0.68 (0.29)\treg_loss 0.46 (0.20)\nEpoch: [013][00090/00100]\tTime 0.17 (0.18)\tLoss 0.22 (0.46)\n\t\tcls_loss 0.11 (0.27)\treg_loss 0.11 (0.19)\n[Train]: Epoch 13 finished with lr=0.00007939\n\n\n[Train]: Epoch 14 started\nEpoch: [014][00010/00100]\tTime 0.22 (0.22)\tLoss 0.64 (0.64)\n\t\tcls_loss 0.30 (0.30)\treg_loss 0.34 (0.34)\nEpoch: [014][00020/00100]\tTime 0.17 (0.19)\tLoss 0.27 (0.46)\n\t\tcls_loss 0.16 (0.23)\treg_loss 0.12 (0.23)\nEpoch: [014][00030/00100]\tTime 0.17 (0.19)\tLoss 0.82 (0.58)\n\t\tcls_loss 0.50 (0.32)\treg_loss 0.32 (0.26)\nEpoch: [014][00040/00100]\tTime 0.17 (0.18)\tLoss 0.57 (0.58)\n\t\tcls_loss 0.29 (0.31)\treg_loss 0.28 (0.26)\nEpoch: [014][00050/00100]\tTime 0.17 (0.18)\tLoss 0.50 (0.56)\n\t\tcls_loss 0.32 (0.31)\treg_loss 0.18 (0.25)\nEpoch: [014][00060/00100]\tTime 0.17 (0.18)\tLoss 0.19 (0.50)\n\t\tcls_loss 0.12 (0.28)\treg_loss 0.08 (0.22)\nEpoch: [014][00070/00100]\tTime 0.17 (0.18)\tLoss 1.77 (0.68)\n\t\tcls_loss 0.91 (0.37)\treg_loss 0.86 (0.31)\nEpoch: [014][00080/00100]\tTime 0.18 (0.18)\tLoss 1.42 (0.77)\n\t\tcls_loss 0.78 (0.42)\treg_loss 0.63 (0.35)\nEpoch: [014][00090/00100]\tTime 0.17 (0.18)\tLoss 0.36 (0.73)\n\t\tcls_loss 0.19 (0.40)\treg_loss 0.17 (0.33)\n[Train]: Epoch 14 finished with lr=0.00007500\n\n\n[Train]: Epoch 15 started\nEpoch: [015][00010/00100]\tTime 0.21 (0.21)\tLoss 0.66 (0.66)\n\t\tcls_loss 0.37 (0.37)\treg_loss 0.29 (0.29)\nEpoch: [015][00020/00100]\tTime 0.18 (0.19)\tLoss 0.14 (0.40)\n\t\tcls_loss 0.08 (0.23)\treg_loss 0.06 (0.17)\nEpoch: [015][00030/00100]\tTime 0.17 (0.19)\tLoss 0.93 (0.58)\n\t\tcls_loss 0.49 (0.32)\treg_loss 0.44 (0.26)\nEpoch: [015][00040/00100]\tTime 0.17 (0.18)\tLoss 0.59 (0.58)\n\t\tcls_loss 0.33 (0.32)\treg_loss 0.26 (0.26)\nEpoch: [015][00050/00100]\tTime 0.17 (0.18)\tLoss 0.34 (0.53)\n\t\tcls_loss 0.18 (0.29)\treg_loss 0.16 (0.24)\nEpoch: [015][00060/00100]\tTime 0.17 (0.18)\tLoss 0.59 (0.54)\n\t\tcls_loss 0.36 (0.30)\treg_loss 0.22 (0.24)\nEpoch: [015][00070/00100]\tTime 0.17 (0.18)\tLoss 0.32 (0.51)\n\t\tcls_loss 0.18 (0.29)\treg_loss 0.14 (0.22)\nEpoch: [015][00080/00100]\tTime 0.18 (0.18)\tLoss 0.25 (0.48)\n\t\tcls_loss 0.14 (0.27)\treg_loss 0.11 (0.21)\nEpoch: [015][00090/00100]\tTime 0.17 (0.18)\tLoss 0.07 (0.43)\n\t\tcls_loss 0.03 (0.24)\treg_loss 0.04 (0.19)\n[Train]: Epoch 15 finished with lr=0.00007034\n\n\n[Train]: Epoch 16 started\nEpoch: [016][00010/00100]\tTime 0.22 (0.22)\tLoss 0.14 (0.14)\n\t\tcls_loss 0.08 (0.08)\treg_loss 0.07 (0.07)\nEpoch: [016][00020/00100]\tTime 0.17 (0.20)\tLoss 0.08 (0.11)\n\t\tcls_loss 0.05 (0.06)\treg_loss 0.03 (0.05)\nEpoch: [016][00030/00100]\tTime 0.18 (0.19)\tLoss 0.12 (0.12)\n\t\tcls_loss 0.08 (0.07)\treg_loss 0.04 (0.05)\nEpoch: [016][00040/00100]\tTime 0.17 (0.19)\tLoss 0.24 (0.15)\n\t\tcls_loss 0.15 (0.09)\treg_loss 0.09 (0.06)\nEpoch: [016][00050/00100]\tTime 0.17 (0.18)\tLoss 0.37 (0.19)\n\t\tcls_loss 0.21 (0.11)\treg_loss 0.16 (0.08)\nEpoch: [016][00060/00100]\tTime 0.17 (0.18)\tLoss 0.33 (0.22)\n\t\tcls_loss 0.16 (0.12)\treg_loss 0.17 (0.09)\nEpoch: [016][00070/00100]\tTime 0.17 (0.18)\tLoss 0.33 (0.23)\n\t\tcls_loss 0.18 (0.13)\treg_loss 0.15 (0.10)\nEpoch: [016][00080/00100]\tTime 0.17 (0.18)\tLoss 0.73 (0.29)\n\t\tcls_loss 0.39 (0.16)\treg_loss 0.35 (0.13)\nEpoch: [016][00090/00100]\tTime 0.18 (0.18)\tLoss 0.53 (0.32)\n\t\tcls_loss 0.27 (0.17)\treg_loss 0.25 (0.15)\n[Train]: Epoch 16 finished with lr=0.00006545\n\n\n[Train]: Epoch 17 started\nEpoch: [017][00010/00100]\tTime 0.21 (0.21)\tLoss 0.30 (0.30)\n\t\tcls_loss 0.18 (0.18)\treg_loss 0.12 (0.12)\nEpoch: [017][00020/00100]\tTime 0.17 (0.19)\tLoss 0.28 (0.29)\n\t\tcls_loss 0.17 (0.17)\treg_loss 0.12 (0.12)\nEpoch: [017][00030/00100]\tTime 0.18 (0.19)\tLoss 0.49 (0.36)\n\t\tcls_loss 0.31 (0.22)\treg_loss 0.18 (0.14)\nEpoch: [017][00040/00100]\tTime 0.17 (0.18)\tLoss 0.34 (0.35)\n\t\tcls_loss 0.17 (0.21)\treg_loss 0.17 (0.15)\nEpoch: [017][00050/00100]\tTime 0.18 (0.18)\tLoss 0.20 (0.32)\n\t\tcls_loss 0.12 (0.19)\treg_loss 0.08 (0.13)\nEpoch: [017][00060/00100]\tTime 0.17 (0.18)\tLoss 0.83 (0.41)\n\t\tcls_loss 0.57 (0.25)\treg_loss 0.26 (0.16)\nEpoch: [017][00070/00100]\tTime 0.17 (0.18)\tLoss 0.25 (0.38)\n\t\tcls_loss 0.12 (0.23)\treg_loss 0.12 (0.15)\nEpoch: [017][00080/00100]\tTime 0.17 (0.18)\tLoss 0.42 (0.39)\n\t\tcls_loss 0.22 (0.23)\treg_loss 0.20 (0.16)\nEpoch: [017][00090/00100]\tTime 0.17 (0.18)\tLoss 0.08 (0.36)\n\t\tcls_loss 0.04 (0.21)\treg_loss 0.04 (0.14)\n[Train]: Epoch 17 finished with lr=0.00006040\n\n\n[Train]: Epoch 18 started\nEpoch: [018][00010/00100]\tTime 0.21 (0.21)\tLoss 0.27 (0.27)\n\t\tcls_loss 0.15 (0.15)\treg_loss 0.12 (0.12)\nEpoch: [018][00020/00100]\tTime 0.17 (0.19)\tLoss 0.62 (0.45)\n\t\tcls_loss 0.35 (0.25)\treg_loss 0.27 (0.20)\nEpoch: [018][00030/00100]\tTime 0.17 (0.19)\tLoss 0.68 (0.52)\n\t\tcls_loss 0.37 (0.29)\treg_loss 0.30 (0.23)\nEpoch: [018][00040/00100]\tTime 0.17 (0.18)\tLoss 0.53 (0.53)\n\t\tcls_loss 0.30 (0.29)\treg_loss 0.23 (0.23)\nEpoch: [018][00050/00100]\tTime 0.18 (0.18)\tLoss 0.40 (0.50)\n\t\tcls_loss 0.24 (0.28)\treg_loss 0.17 (0.22)\nEpoch: [018][00060/00100]\tTime 0.18 (0.18)\tLoss 0.35 (0.48)\n\t\tcls_loss 0.18 (0.27)\treg_loss 0.17 (0.21)\nEpoch: [018][00070/00100]\tTime 0.18 (0.18)\tLoss 0.81 (0.52)\n\t\tcls_loss 0.46 (0.29)\treg_loss 0.35 (0.23)\nEpoch: [018][00080/00100]\tTime 0.18 (0.18)\tLoss 0.20 (0.48)\n\t\tcls_loss 0.12 (0.27)\treg_loss 0.07 (0.21)\nEpoch: [018][00090/00100]\tTime 0.17 (0.18)\tLoss 0.64 (0.50)\n\t\tcls_loss 0.40 (0.29)\treg_loss 0.23 (0.21)\n[Train]: Epoch 18 finished with lr=0.00005523\n\n\n[Train]: Epoch 19 started\nEpoch: [019][00010/00100]\tTime 0.22 (0.22)\tLoss 0.15 (0.15)\n\t\tcls_loss 0.09 (0.09)\treg_loss 0.06 (0.06)\nEpoch: [019][00020/00100]\tTime 0.18 (0.20)\tLoss 0.77 (0.46)\n\t\tcls_loss 0.53 (0.31)\treg_loss 0.24 (0.15)\nEpoch: [019][00030/00100]\tTime 0.18 (0.19)\tLoss 0.14 (0.35)\n\t\tcls_loss 0.07 (0.23)\treg_loss 0.07 (0.12)\nEpoch: [019][00040/00100]\tTime 0.18 (0.19)\tLoss 0.06 (0.28)\n\t\tcls_loss 0.02 (0.18)\treg_loss 0.04 (0.10)\nEpoch: [019][00050/00100]\tTime 0.17 (0.18)\tLoss 1.05 (0.43)\n\t\tcls_loss 0.49 (0.24)\treg_loss 0.56 (0.19)\nEpoch: [019][00060/00100]\tTime 0.17 (0.18)\tLoss 0.18 (0.39)\n\t\tcls_loss 0.11 (0.22)\treg_loss 0.07 (0.17)\nEpoch: [019][00070/00100]\tTime 0.17 (0.18)\tLoss 0.12 (0.35)\n\t\tcls_loss 0.08 (0.20)\treg_loss 0.04 (0.15)\nEpoch: [019][00080/00100]\tTime 0.17 (0.18)\tLoss 0.57 (0.38)\n\t\tcls_loss 0.36 (0.22)\treg_loss 0.20 (0.16)\nEpoch: [019][00090/00100]\tTime 0.18 (0.18)\tLoss 0.50 (0.39)\n\t\tcls_loss 0.25 (0.22)\treg_loss 0.25 (0.17)\n[Train]: Epoch 19 finished with lr=0.00005000\n\n\n[Train]: Epoch 20 started\nEpoch: [020][00010/00100]\tTime 0.21 (0.21)\tLoss 0.14 (0.14)\n\t\tcls_loss 0.07 (0.07)\treg_loss 0.07 (0.07)\nEpoch: [020][00020/00100]\tTime 0.17 (0.19)\tLoss 0.08 (0.11)\n\t\tcls_loss 0.04 (0.06)\treg_loss 0.03 (0.05)\nEpoch: [020][00030/00100]\tTime 0.17 (0.18)\tLoss 0.31 (0.18)\n\t\tcls_loss 0.16 (0.09)\treg_loss 0.15 (0.08)\nEpoch: [020][00040/00100]\tTime 0.18 (0.18)\tLoss 0.11 (0.16)\n\t\tcls_loss 0.05 (0.08)\treg_loss 0.06 (0.08)\nEpoch: [020][00050/00100]\tTime 0.17 (0.18)\tLoss 0.02 (0.13)\n\t\tcls_loss 0.01 (0.07)\treg_loss 0.01 (0.06)\nEpoch: [020][00060/00100]\tTime 0.18 (0.18)\tLoss 0.24 (0.15)\n\t\tcls_loss 0.13 (0.08)\treg_loss 0.11 (0.07)\nEpoch: [020][00070/00100]\tTime 0.17 (0.18)\tLoss 0.14 (0.15)\n\t\tcls_loss 0.10 (0.08)\treg_loss 0.04 (0.07)\nEpoch: [020][00080/00100]\tTime 0.17 (0.18)\tLoss 0.09 (0.14)\n\t\tcls_loss 0.05 (0.08)\treg_loss 0.04 (0.06)\nEpoch: [020][00090/00100]\tTime 0.17 (0.18)\tLoss 0.28 (0.16)\n\t\tcls_loss 0.15 (0.08)\treg_loss 0.13 (0.07)\n[Train]: Epoch 20 finished with lr=0.00004478\n\n\n[Train]: Epoch 21 started\nEpoch: [021][00010/00100]\tTime 0.20 (0.20)\tLoss 0.22 (0.22)\n\t\tcls_loss 0.12 (0.12)\treg_loss 0.09 (0.09)\nEpoch: [021][00020/00100]\tTime 0.17 (0.18)\tLoss 0.04 (0.13)\n\t\tcls_loss 0.02 (0.07)\treg_loss 0.02 (0.06)\nEpoch: [021][00030/00100]\tTime 0.17 (0.18)\tLoss 0.41 (0.22)\n\t\tcls_loss 0.26 (0.13)\treg_loss 0.15 (0.09)\nEpoch: [021][00040/00100]\tTime 0.17 (0.18)\tLoss 0.96 (0.41)\n\t\tcls_loss 0.68 (0.27)\treg_loss 0.28 (0.13)\nEpoch: [021][00050/00100]\tTime 0.17 (0.18)\tLoss 0.13 (0.35)\n\t\tcls_loss 0.08 (0.23)\treg_loss 0.05 (0.12)\nEpoch: [021][00060/00100]\tTime 0.17 (0.18)\tLoss 0.12 (0.31)\n\t\tcls_loss 0.07 (0.21)\treg_loss 0.05 (0.11)\nEpoch: [021][00070/00100]\tTime 0.17 (0.18)\tLoss 1.26 (0.45)\n\t\tcls_loss 0.70 (0.28)\treg_loss 0.56 (0.17)\nEpoch: [021][00080/00100]\tTime 0.17 (0.18)\tLoss 0.04 (0.40)\n\t\tcls_loss 0.02 (0.24)\treg_loss 0.02 (0.15)\nEpoch: [021][00090/00100]\tTime 0.17 (0.18)\tLoss 0.57 (0.42)\n\t\tcls_loss 0.29 (0.25)\treg_loss 0.28 (0.17)\n[Train]: Epoch 21 finished with lr=0.00003961\n\n\n[Train]: Epoch 22 started\nEpoch: [022][00010/00100]\tTime 0.20 (0.20)\tLoss 0.10 (0.10)\n\t\tcls_loss 0.05 (0.05)\treg_loss 0.05 (0.05)\nEpoch: [022][00020/00100]\tTime 0.17 (0.19)\tLoss 0.15 (0.13)\n\t\tcls_loss 0.07 (0.06)\treg_loss 0.08 (0.07)\nEpoch: [022][00030/00100]\tTime 0.17 (0.18)\tLoss 0.29 (0.18)\n\t\tcls_loss 0.15 (0.09)\treg_loss 0.14 (0.09)\nEpoch: [022][00040/00100]\tTime 0.17 (0.18)\tLoss 0.24 (0.20)\n\t\tcls_loss 0.13 (0.10)\treg_loss 0.11 (0.10)\nEpoch: [022][00050/00100]\tTime 0.18 (0.18)\tLoss 0.13 (0.19)\n\t\tcls_loss 0.07 (0.10)\treg_loss 0.06 (0.09)\nEpoch: [022][00060/00100]\tTime 0.18 (0.18)\tLoss 0.07 (0.17)\n\t\tcls_loss 0.03 (0.08)\treg_loss 0.04 (0.08)\nEpoch: [022][00070/00100]\tTime 0.17 (0.18)\tLoss 0.23 (0.18)\n\t\tcls_loss 0.11 (0.09)\treg_loss 0.11 (0.09)\nEpoch: [022][00080/00100]\tTime 0.17 (0.18)\tLoss 0.35 (0.20)\n\t\tcls_loss 0.17 (0.10)\treg_loss 0.18 (0.10)\nEpoch: [022][00090/00100]\tTime 0.17 (0.18)\tLoss 0.25 (0.20)\n\t\tcls_loss 0.13 (0.10)\treg_loss 0.11 (0.10)\n[Train]: Epoch 22 finished with lr=0.00003456\n\n\n[Train]: Epoch 23 started\nEpoch: [023][00010/00100]\tTime 0.21 (0.21)\tLoss 0.08 (0.08)\n\t\tcls_loss 0.05 (0.05)\treg_loss 0.03 (0.03)\nEpoch: [023][00020/00100]\tTime 0.17 (0.19)\tLoss 0.35 (0.21)\n\t\tcls_loss 0.18 (0.12)\treg_loss 0.17 (0.10)\nEpoch: [023][00030/00100]\tTime 0.17 (0.19)\tLoss 0.07 (0.17)\n\t\tcls_loss 0.04 (0.09)\treg_loss 0.02 (0.07)\nEpoch: [023][00040/00100]\tTime 0.17 (0.18)\tLoss 0.50 (0.25)\n\t\tcls_loss 0.27 (0.14)\treg_loss 0.22 (0.11)\nEpoch: [023][00050/00100]\tTime 0.18 (0.18)\tLoss 0.52 (0.30)\n\t\tcls_loss 0.35 (0.18)\treg_loss 0.17 (0.12)\nEpoch: [023][00060/00100]\tTime 0.17 (0.18)\tLoss 0.29 (0.30)\n\t\tcls_loss 0.15 (0.18)\treg_loss 0.14 (0.13)\nEpoch: [023][00070/00100]\tTime 0.17 (0.18)\tLoss 0.49 (0.33)\n\t\tcls_loss 0.28 (0.19)\treg_loss 0.21 (0.14)\nEpoch: [023][00080/00100]\tTime 0.17 (0.18)\tLoss 1.14 (0.43)\n\t\tcls_loss 0.50 (0.23)\treg_loss 0.65 (0.20)\nEpoch: [023][00090/00100]\tTime 0.17 (0.18)\tLoss 0.14 (0.40)\n\t\tcls_loss 0.07 (0.21)\treg_loss 0.07 (0.19)\n[Train]: Epoch 23 finished with lr=0.00002967\n\n\n[Train]: Epoch 24 started\nEpoch: [024][00010/00100]\tTime 0.21 (0.21)\tLoss 0.09 (0.09)\n\t\tcls_loss 0.06 (0.06)\treg_loss 0.03 (0.03)\nEpoch: [024][00020/00100]\tTime 0.18 (0.19)\tLoss 0.26 (0.18)\n\t\tcls_loss 0.12 (0.09)\treg_loss 0.14 (0.09)\nEpoch: [024][00030/00100]\tTime 0.17 (0.19)\tLoss 0.35 (0.23)\n\t\tcls_loss 0.17 (0.12)\treg_loss 0.17 (0.12)\nEpoch: [024][00040/00100]\tTime 0.17 (0.18)\tLoss 0.10 (0.20)\n\t\tcls_loss 0.05 (0.10)\treg_loss 0.05 (0.10)\nEpoch: [024][00050/00100]\tTime 0.17 (0.18)\tLoss 0.41 (0.24)\n\t\tcls_loss 0.22 (0.12)\treg_loss 0.19 (0.12)\nEpoch: [024][00060/00100]\tTime 0.17 (0.18)\tLoss 0.15 (0.23)\n\t\tcls_loss 0.08 (0.12)\treg_loss 0.07 (0.11)\nEpoch: [024][00070/00100]\tTime 0.17 (0.18)\tLoss 0.08 (0.20)\n\t\tcls_loss 0.05 (0.11)\treg_loss 0.03 (0.10)\nEpoch: [024][00080/00100]\tTime 0.17 (0.18)\tLoss 0.12 (0.19)\n\t\tcls_loss 0.07 (0.10)\treg_loss 0.05 (0.09)\nEpoch: [024][00090/00100]\tTime 0.18 (0.18)\tLoss 0.17 (0.19)\n\t\tcls_loss 0.09 (0.10)\treg_loss 0.07 (0.09)\n[Train]: Epoch 24 finished with lr=0.00002501\n\n\n[Train]: Epoch 25 started\nEpoch: [025][00010/00100]\tTime 0.21 (0.21)\tLoss 0.71 (0.71)\n\t\tcls_loss 0.40 (0.40)\treg_loss 0.31 (0.31)\nEpoch: [025][00020/00100]\tTime 0.17 (0.19)\tLoss 0.27 (0.49)\n\t\tcls_loss 0.15 (0.28)\treg_loss 0.12 (0.22)\nEpoch: [025][00030/00100]\tTime 0.17 (0.18)\tLoss 0.14 (0.37)\n\t\tcls_loss 0.08 (0.21)\treg_loss 0.05 (0.16)\nEpoch: [025][00040/00100]\tTime 0.17 (0.18)\tLoss 0.17 (0.32)\n\t\tcls_loss 0.09 (0.18)\treg_loss 0.08 (0.14)\nEpoch: [025][00050/00100]\tTime 0.17 (0.18)\tLoss 0.50 (0.36)\n\t\tcls_loss 0.26 (0.20)\treg_loss 0.24 (0.16)\nEpoch: [025][00060/00100]\tTime 0.17 (0.18)\tLoss 0.37 (0.36)\n\t\tcls_loss 0.20 (0.20)\treg_loss 0.17 (0.16)\nEpoch: [025][00070/00100]\tTime 0.17 (0.18)\tLoss 0.81 (0.43)\n\t\tcls_loss 0.43 (0.23)\treg_loss 0.38 (0.19)\nEpoch: [025][00080/00100]\tTime 0.17 (0.18)\tLoss 0.16 (0.39)\n\t\tcls_loss 0.11 (0.22)\treg_loss 0.05 (0.18)\nEpoch: [025][00090/00100]\tTime 0.18 (0.18)\tLoss 0.29 (0.38)\n\t\tcls_loss 0.15 (0.21)\treg_loss 0.14 (0.17)\n[Train]: Epoch 25 finished with lr=0.00002062\n\n\n[Train]: Epoch 26 started\nEpoch: [026][00010/00100]\tTime 0.20 (0.20)\tLoss 0.13 (0.13)\n\t\tcls_loss 0.08 (0.08)\treg_loss 0.05 (0.05)\nEpoch: [026][00020/00100]\tTime 0.17 (0.19)\tLoss 0.07 (0.10)\n\t\tcls_loss 0.03 (0.05)\treg_loss 0.05 (0.05)\nEpoch: [026][00030/00100]\tTime 0.17 (0.18)\tLoss 0.40 (0.20)\n\t\tcls_loss 0.20 (0.10)\treg_loss 0.21 (0.10)\nEpoch: [026][00040/00100]\tTime 0.18 (0.18)\tLoss 0.13 (0.18)\n\t\tcls_loss 0.05 (0.09)\treg_loss 0.07 (0.09)\nEpoch: [026][00050/00100]\tTime 0.18 (0.18)\tLoss 0.22 (0.19)\n\t\tcls_loss 0.12 (0.10)\treg_loss 0.09 (0.09)\nEpoch: [026][00060/00100]\tTime 0.18 (0.18)\tLoss 1.25 (0.37)\n\t\tcls_loss 0.61 (0.18)\treg_loss 0.63 (0.18)\nEpoch: [026][00070/00100]\tTime 0.17 (0.18)\tLoss 0.10 (0.33)\n\t\tcls_loss 0.07 (0.17)\treg_loss 0.03 (0.16)\nEpoch: [026][00080/00100]\tTime 0.17 (0.18)\tLoss 0.04 (0.29)\n\t\tcls_loss 0.02 (0.15)\treg_loss 0.02 (0.14)\nEpoch: [026][00090/00100]\tTime 0.17 (0.18)\tLoss 0.29 (0.29)\n\t\tcls_loss 0.14 (0.15)\treg_loss 0.15 (0.15)\n[Train]: Epoch 26 finished with lr=0.00001655\n\n\n[Train]: Epoch 27 started\nEpoch: [027][00010/00100]\tTime 0.21 (0.21)\tLoss 0.41 (0.41)\n\t\tcls_loss 0.21 (0.21)\treg_loss 0.19 (0.19)\nEpoch: [027][00020/00100]\tTime 0.17 (0.19)\tLoss 0.29 (0.35)\n\t\tcls_loss 0.14 (0.17)\treg_loss 0.15 (0.17)\nEpoch: [027][00030/00100]\tTime 0.17 (0.19)\tLoss 0.58 (0.43)\n\t\tcls_loss 0.31 (0.22)\treg_loss 0.27 (0.21)\nEpoch: [027][00040/00100]\tTime 0.18 (0.18)\tLoss 0.35 (0.41)\n\t\tcls_loss 0.19 (0.21)\treg_loss 0.16 (0.20)\nEpoch: [027][00050/00100]\tTime 0.17 (0.18)\tLoss 0.43 (0.41)\n\t\tcls_loss 0.23 (0.22)\treg_loss 0.20 (0.20)\nEpoch: [027][00060/00100]\tTime 0.17 (0.18)\tLoss 0.09 (0.36)\n\t\tcls_loss 0.05 (0.19)\treg_loss 0.04 (0.17)\nEpoch: [027][00070/00100]\tTime 0.17 (0.18)\tLoss 0.23 (0.34)\n\t\tcls_loss 0.11 (0.18)\treg_loss 0.11 (0.16)\nEpoch: [027][00080/00100]\tTime 0.17 (0.18)\tLoss 0.03 (0.30)\n\t\tcls_loss 0.01 (0.16)\treg_loss 0.01 (0.14)\nEpoch: [027][00090/00100]\tTime 0.17 (0.18)\tLoss 0.09 (0.28)\n\t\tcls_loss 0.05 (0.14)\treg_loss 0.05 (0.13)\n[Train]: Epoch 27 finished with lr=0.00001285\n\n\n[Train]: Epoch 28 started\nEpoch: [028][00010/00100]\tTime 0.22 (0.22)\tLoss 0.17 (0.17)\n\t\tcls_loss 0.09 (0.09)\treg_loss 0.08 (0.08)\nEpoch: [028][00020/00100]\tTime 0.17 (0.19)\tLoss 0.22 (0.19)\n\t\tcls_loss 0.11 (0.10)\treg_loss 0.11 (0.10)\nEpoch: [028][00030/00100]\tTime 0.18 (0.19)\tLoss 0.04 (0.14)\n\t\tcls_loss 0.02 (0.07)\treg_loss 0.02 (0.07)\nEpoch: [028][00040/00100]\tTime 0.17 (0.18)\tLoss 0.38 (0.20)\n\t\tcls_loss 0.18 (0.10)\treg_loss 0.20 (0.10)\nEpoch: [028][00050/00100]\tTime 0.17 (0.18)\tLoss 0.09 (0.18)\n\t\tcls_loss 0.04 (0.09)\treg_loss 0.05 (0.09)\nEpoch: [028][00060/00100]\tTime 0.17 (0.18)\tLoss 0.51 (0.24)\n\t\tcls_loss 0.27 (0.12)\treg_loss 0.24 (0.12)\nEpoch: [028][00070/00100]\tTime 0.18 (0.18)\tLoss 0.16 (0.22)\n\t\tcls_loss 0.09 (0.11)\treg_loss 0.08 (0.11)\nEpoch: [028][00080/00100]\tTime 0.17 (0.18)\tLoss 0.10 (0.21)\n\t\tcls_loss 0.06 (0.11)\treg_loss 0.05 (0.10)\nEpoch: [028][00090/00100]\tTime 0.18 (0.18)\tLoss 0.05 (0.19)\n\t\tcls_loss 0.03 (0.10)\treg_loss 0.03 (0.09)\n[Train]: Epoch 28 finished with lr=0.00000956\n\n\n[Train]: Epoch 29 started\nEpoch: [029][00010/00100]\tTime 0.22 (0.22)\tLoss 0.21 (0.21)\n\t\tcls_loss 0.12 (0.12)\treg_loss 0.10 (0.10)\nEpoch: [029][00020/00100]\tTime 0.17 (0.19)\tLoss 0.08 (0.14)\n\t\tcls_loss 0.05 (0.08)\treg_loss 0.03 (0.06)\nEpoch: [029][00030/00100]\tTime 0.17 (0.19)\tLoss 0.18 (0.16)\n\t\tcls_loss 0.08 (0.08)\treg_loss 0.10 (0.08)\nEpoch: [029][00040/00100]\tTime 0.17 (0.18)\tLoss 0.43 (0.23)\n\t\tcls_loss 0.21 (0.11)\treg_loss 0.22 (0.11)\nEpoch: [029][00050/00100]\tTime 0.17 (0.18)\tLoss 0.10 (0.20)\n\t\tcls_loss 0.05 (0.10)\treg_loss 0.05 (0.10)\nEpoch: [029][00060/00100]\tTime 0.17 (0.18)\tLoss 0.28 (0.21)\n\t\tcls_loss 0.16 (0.11)\treg_loss 0.12 (0.10)\nEpoch: [029][00070/00100]\tTime 0.17 (0.18)\tLoss 0.12 (0.20)\n\t\tcls_loss 0.07 (0.10)\treg_loss 0.05 (0.10)\nEpoch: [029][00080/00100]\tTime 0.18 (0.18)\tLoss 0.27 (0.21)\n\t\tcls_loss 0.15 (0.11)\treg_loss 0.13 (0.10)\nEpoch: [029][00090/00100]\tTime 0.17 (0.18)\tLoss 0.02 (0.19)\n\t\tcls_loss 0.01 (0.10)\treg_loss 0.01 (0.09)\n[Train]: Epoch 29 finished with lr=0.00000671\n\n\n[Train]: Epoch 30 started\nEpoch: [030][00010/00100]\tTime 0.21 (0.21)\tLoss 0.07 (0.07)\n\t\tcls_loss 0.03 (0.03)\treg_loss 0.03 (0.03)\nEpoch: [030][00020/00100]\tTime 0.17 (0.19)\tLoss 0.15 (0.11)\n\t\tcls_loss 0.10 (0.07)\treg_loss 0.05 (0.04)\nEpoch: [030][00030/00100]\tTime 0.17 (0.19)\tLoss 0.28 (0.16)\n\t\tcls_loss 0.13 (0.09)\treg_loss 0.15 (0.08)\nEpoch: [030][00040/00100]\tTime 0.17 (0.18)\tLoss 0.16 (0.16)\n\t\tcls_loss 0.10 (0.09)\treg_loss 0.06 (0.07)\nEpoch: [030][00050/00100]\tTime 0.17 (0.18)\tLoss 0.36 (0.20)\n\t\tcls_loss 0.19 (0.11)\treg_loss 0.18 (0.09)\nEpoch: [030][00060/00100]\tTime 0.17 (0.18)\tLoss 0.18 (0.20)\n\t\tcls_loss 0.10 (0.11)\treg_loss 0.09 (0.09)\nEpoch: [030][00070/00100]\tTime 0.17 (0.18)\tLoss 0.50 (0.24)\n\t\tcls_loss 0.27 (0.13)\treg_loss 0.22 (0.11)\nEpoch: [030][00080/00100]\tTime 0.17 (0.18)\tLoss 0.04 (0.22)\n\t\tcls_loss 0.02 (0.12)\treg_loss 0.02 (0.10)\nEpoch: [030][00090/00100]\tTime 0.17 (0.18)\tLoss 0.41 (0.24)\n\t\tcls_loss 0.21 (0.13)\treg_loss 0.20 (0.11)\n[Train]: Epoch 30 finished with lr=0.00000433\n\n\n[Train]: Epoch 31 started\nEpoch: [031][00010/00100]\tTime 0.23 (0.23)\tLoss 0.09 (0.09)\n\t\tcls_loss 0.05 (0.05)\treg_loss 0.05 (0.05)\nEpoch: [031][00020/00100]\tTime 0.17 (0.20)\tLoss 0.02 (0.06)\n\t\tcls_loss 0.01 (0.03)\treg_loss 0.01 (0.03)\nEpoch: [031][00030/00100]\tTime 0.18 (0.19)\tLoss 0.84 (0.32)\n\t\tcls_loss 0.43 (0.16)\treg_loss 0.41 (0.16)\nEpoch: [031][00040/00100]\tTime 0.17 (0.19)\tLoss 0.31 (0.32)\n\t\tcls_loss 0.15 (0.16)\treg_loss 0.16 (0.16)\nEpoch: [031][00050/00100]\tTime 0.17 (0.19)\tLoss 0.32 (0.32)\n\t\tcls_loss 0.17 (0.16)\treg_loss 0.15 (0.15)\nEpoch: [031][00060/00100]\tTime 0.18 (0.18)\tLoss 0.03 (0.27)\n\t\tcls_loss 0.02 (0.14)\treg_loss 0.01 (0.13)\nEpoch: [031][00070/00100]\tTime 0.17 (0.18)\tLoss 0.17 (0.26)\n\t\tcls_loss 0.10 (0.13)\treg_loss 0.07 (0.12)\nEpoch: [031][00080/00100]\tTime 0.18 (0.18)\tLoss 0.24 (0.25)\n\t\tcls_loss 0.13 (0.13)\treg_loss 0.11 (0.12)\nEpoch: [031][00090/00100]\tTime 0.17 (0.18)\tLoss 0.11 (0.24)\n\t\tcls_loss 0.07 (0.13)\treg_loss 0.05 (0.11)\n[Train]: Epoch 31 finished with lr=0.00000246\n\n\n[Train]: Epoch 32 started\nEpoch: [032][00010/00100]\tTime 0.20 (0.20)\tLoss 0.07 (0.07)\n\t\tcls_loss 0.04 (0.04)\treg_loss 0.04 (0.04)\nEpoch: [032][00020/00100]\tTime 0.17 (0.19)\tLoss 0.30 (0.19)\n\t\tcls_loss 0.17 (0.11)\treg_loss 0.13 (0.08)\nEpoch: [032][00030/00100]\tTime 0.17 (0.18)\tLoss 0.13 (0.17)\n\t\tcls_loss 0.07 (0.09)\treg_loss 0.06 (0.07)\nEpoch: [032][00040/00100]\tTime 0.18 (0.18)\tLoss 0.08 (0.15)\n\t\tcls_loss 0.04 (0.08)\treg_loss 0.04 (0.07)\nEpoch: [032][00050/00100]\tTime 0.18 (0.18)\tLoss 0.19 (0.16)\n\t\tcls_loss 0.09 (0.08)\treg_loss 0.10 (0.07)\nEpoch: [032][00060/00100]\tTime 0.17 (0.18)\tLoss 0.69 (0.25)\n\t\tcls_loss 0.46 (0.15)\treg_loss 0.23 (0.10)\nEpoch: [032][00070/00100]\tTime 0.17 (0.18)\tLoss 0.18 (0.24)\n\t\tcls_loss 0.12 (0.14)\treg_loss 0.06 (0.09)\nEpoch: [032][00080/00100]\tTime 0.18 (0.18)\tLoss 0.21 (0.23)\n\t\tcls_loss 0.10 (0.14)\treg_loss 0.11 (0.10)\nEpoch: [032][00090/00100]\tTime 0.18 (0.18)\tLoss 0.33 (0.24)\n\t\tcls_loss 0.19 (0.14)\treg_loss 0.13 (0.10)\n[Train]: Epoch 32 finished with lr=0.00000110\n\n\n[Train]: Epoch 33 started\nEpoch: [033][00010/00100]\tTime 0.22 (0.22)\tLoss 0.04 (0.04)\n\t\tcls_loss 0.02 (0.02)\treg_loss 0.02 (0.02)\nEpoch: [033][00020/00100]\tTime 0.17 (0.20)\tLoss 0.07 (0.05)\n\t\tcls_loss 0.03 (0.02)\treg_loss 0.03 (0.03)\nEpoch: [033][00030/00100]\tTime 0.17 (0.19)\tLoss 0.20 (0.10)\n\t\tcls_loss 0.10 (0.05)\treg_loss 0.10 (0.05)\nEpoch: [033][00040/00100]\tTime 0.17 (0.18)\tLoss 0.16 (0.11)\n\t\tcls_loss 0.07 (0.06)\treg_loss 0.09 (0.06)\nEpoch: [033][00050/00100]\tTime 0.18 (0.18)\tLoss 0.03 (0.10)\n\t\tcls_loss 0.02 (0.05)\treg_loss 0.01 (0.05)\nEpoch: [033][00060/00100]\tTime 0.17 (0.18)\tLoss 0.10 (0.10)\n\t\tcls_loss 0.04 (0.05)\treg_loss 0.06 (0.05)\nEpoch: [033][00070/00100]\tTime 0.17 (0.18)\tLoss 0.19 (0.11)\n\t\tcls_loss 0.10 (0.05)\treg_loss 0.09 (0.06)\nEpoch: [033][00080/00100]\tTime 0.18 (0.18)\tLoss 0.07 (0.11)\n\t\tcls_loss 0.03 (0.05)\treg_loss 0.04 (0.05)\nEpoch: [033][00090/00100]\tTime 0.17 (0.18)\tLoss 0.20 (0.12)\n\t\tcls_loss 0.12 (0.06)\treg_loss 0.08 (0.06)\n[Train]: Epoch 33 finished with lr=0.00000028\n\n\n[Train]: Epoch 34 started\nEpoch: [034][00010/00100]\tTime 0.21 (0.21)\tLoss 0.14 (0.14)\n\t\tcls_loss 0.08 (0.08)\treg_loss 0.06 (0.06)\nEpoch: [034][00020/00100]\tTime 0.22 (0.21)\tLoss 0.08 (0.11)\n\t\tcls_loss 0.04 (0.06)\treg_loss 0.03 (0.05)\nEpoch: [034][00030/00100]\tTime 0.17 (0.20)\tLoss 0.05 (0.09)\n\t\tcls_loss 0.02 (0.05)\treg_loss 0.02 (0.04)\nEpoch: [034][00040/00100]\tTime 0.18 (0.19)\tLoss 0.23 (0.12)\n\t\tcls_loss 0.12 (0.07)\treg_loss 0.11 (0.06)\nEpoch: [034][00050/00100]\tTime 0.17 (0.19)\tLoss 0.33 (0.17)\n\t\tcls_loss 0.17 (0.09)\treg_loss 0.16 (0.08)\nEpoch: [034][00060/00100]\tTime 0.17 (0.19)\tLoss 0.08 (0.15)\n\t\tcls_loss 0.04 (0.08)\treg_loss 0.04 (0.07)\nEpoch: [034][00070/00100]\tTime 0.17 (0.19)\tLoss 0.17 (0.15)\n\t\tcls_loss 0.09 (0.08)\treg_loss 0.08 (0.07)\nEpoch: [034][00080/00100]\tTime 0.17 (0.18)\tLoss 0.21 (0.16)\n\t\tcls_loss 0.11 (0.08)\treg_loss 0.10 (0.08)\nEpoch: [034][00090/00100]\tTime 0.18 (0.18)\tLoss 0.43 (0.19)\n\t\tcls_loss 0.21 (0.10)\treg_loss 0.22 (0.09)\n[Train]: Epoch 34 finished with lr=0.00000001\n\nAll done!\n","output_type":"stream"}],"execution_count":51},{"id":"3dc637e2-78f4-4e10-87b8-c5f41c536778","cell_type":"code","source":"print(\"Error)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.223008Z","iopub.execute_input":"2025-10-04T16:30:33.223337Z","iopub.status.idle":"2025-10-04T16:30:33.229229Z","shell.execute_reply.started":"2025-10-04T16:30:33.223307Z","shell.execute_reply":"2025-10-04T16:30:33.228159Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_36/1218826395.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print(\"Error)\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"],"ename":"SyntaxError","evalue":"unterminated string literal (detected at line 1) (1218826395.py, line 1)","output_type":"error"}],"execution_count":52},{"id":"182b9699-4bea-460a-ba45-1c994660b3f4","cell_type":"code","source":"!mv \"/kaggle/working/ckpt/thumos_i3d_2025-10-04 14:17:07\" /kaggle/working/ckpt/thumos_i3d_2025-10-04_14-17-07\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.229847Z","iopub.status.idle":"2025-10-04T16:30:33.230175Z","shell.execute_reply.started":"2025-10-04T16:30:33.230016Z","shell.execute_reply":"2025-10-04T16:30:33.230032Z"}},"outputs":[],"execution_count":null},{"id":"676ff250-5c2a-4268-82b5-465f9f950c73","cell_type":"code","source":"!kaggle datasets list | head\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.231235Z","iopub.status.idle":"2025-10-04T16:30:33.231507Z","shell.execute_reply.started":"2025-10-04T16:30:33.231391Z","shell.execute_reply":"2025-10-04T16:30:33.231406Z"}},"outputs":[],"execution_count":null},{"id":"0c808a68-3c0e-4576-bfd5-c89ec82e6ddd","cell_type":"code","source":"mkdir -p /root/.kaggle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.233071Z","iopub.status.idle":"2025-10-04T16:30:33.233387Z","shell.execute_reply.started":"2025-10-04T16:30:33.233250Z","shell.execute_reply":"2025-10-04T16:30:33.233265Z"}},"outputs":[],"execution_count":null},{"id":"364f9d38-560d-4959-a2a5-b14ff829e51c","cell_type":"code","source":"cp /kaggle/input/kaggle-credentials/kaggle.json /root/.kaggle/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.234653Z","iopub.status.idle":"2025-10-04T16:30:33.235007Z","shell.execute_reply.started":"2025-10-04T16:30:33.234851Z","shell.execute_reply":"2025-10-04T16:30:33.234868Z"}},"outputs":[],"execution_count":null},{"id":"625f6a88-4235-43b5-889d-f5711bc7a193","cell_type":"code","source":"!chmod 600 /root/.kaggle/kaggle.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.236946Z","iopub.status.idle":"2025-10-04T16:30:33.237195Z","shell.execute_reply.started":"2025-10-04T16:30:33.237078Z","shell.execute_reply":"2025-10-04T16:30:33.237089Z"}},"outputs":[],"execution_count":null},{"id":"9dcc0b63-a7b3-42c7-944d-fe7b6c3cdf86","cell_type":"code","source":"!kaggle datasets init -p /kaggle/working/ckpt/thumos_i3d_2025-10-04_14-17-07","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.238485Z","iopub.status.idle":"2025-10-04T16:30:33.238786Z","shell.execute_reply.started":"2025-10-04T16:30:33.238634Z","shell.execute_reply":"2025-10-04T16:30:33.238647Z"}},"outputs":[],"execution_count":null},{"id":"da8d3543-4368-4371-8cce-af4291ce2868","cell_type":"code","source":"/kaggle/working/ckpt/thumos_i3d_2025-10-04_14-17-07/dataset-metadata.json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.239410Z","iopub.status.idle":"2025-10-04T16:30:33.239605Z","shell.execute_reply.started":"2025-10-04T16:30:33.239511Z","shell.execute_reply":"2025-10-04T16:30:33.239520Z"}},"outputs":[],"execution_count":null},{"id":"2631d5bd-550a-4d4d-a26e-b0b88005cc29","cell_type":"code","source":"%%bash\ncat > \"/kaggle/working/ckpt/thumos_i3d_2025-10-04_14-17-07/dataset-metadata.json\" <<'EOF'\n{\n  \"title\": \"thisara_121212121212332323343_232323233\",\n  \"id\": \"thisaraweerakoon/thumos-i3d-epoch-35-2025-10-04\",\n  \"licenses\": [{\"name\": \"CC0-1.0\"}],\n  \"description\": \"Model checkpoint trained on THUMOS14 (I3D features) — Epoch 35 from SGP backbone with FPN1D\"\n}\nEOF\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.240818Z","iopub.status.idle":"2025-10-04T16:30:33.241352Z","shell.execute_reply.started":"2025-10-04T16:30:33.241191Z","shell.execute_reply":"2025-10-04T16:30:33.241207Z"}},"outputs":[],"execution_count":null},{"id":"50bd09bb-9a91-4913-846e-3606031417cd","cell_type":"code","source":"!kaggle datasets create -p \"/kaggle/working/ckpt/thumos_i3d_2025-10-04_14-17-07\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.242300Z","iopub.status.idle":"2025-10-04T16:30:33.242599Z","shell.execute_reply.started":"2025-10-04T16:30:33.242449Z","shell.execute_reply":"2025-10-04T16:30:33.242463Z"}},"outputs":[],"execution_count":null},{"id":"ede6cbbe-13f0-4bd3-99ef-5f6b83d05294","cell_type":"code","source":"class ANETdetection(object):\n    \"\"\"THUMOS14 detection evaluation\"\"\"\n    def __init__(self, json_file, split_file, tiou_thresholds=np.linspace(0.1, 0.5, 5)):\n        self.tiou_thresholds = tiou_thresholds\n        self.ap = None\n        \n        # Load ground truth\n        with open(json_file, 'r') as f:\n            data = json.load(f)\n            \n        self.ground_truth = {}\n        for vid_id, vid_info in data['database'].items():\n            if vid_info['subset'].lower() == split_file:\n                self.ground_truth[vid_id] = {\n                    'annotations': vid_info['annotations'],\n                    'duration': vid_info['duration']\n                }\n                \n    def evaluate(self, predictions):\n        \"\"\"Evaluate predictions\"\"\"\n        # Convert predictions to required format\n        prediction_by_label = {}\n        \n        for result in predictions:\n            vid_id = result['video_id']\n            segments = result['segments'].numpy() if hasattr(result['segments'], 'numpy') else result['segments']\n            scores = result['scores'].numpy() if hasattr(result['scores'], 'numpy') else result['scores']\n            labels = result['labels'].numpy() if hasattr(result['labels'], 'numpy') else result['labels']\n            \n            for seg, score, label in zip(segments, scores, labels):\n                label_id = int(label)\n                if label_id not in prediction_by_label:\n                    prediction_by_label[label_id] = []\n                    \n                prediction_by_label[label_id].append({\n                    'video-id': vid_id,\n                    'score': float(score),\n                    't-start': float(seg[0]),\n                    't-end': float(seg[1])\n                })\n                \n        # Compute AP for each label and IoU threshold\n        ap_values = []\n        for label_id in prediction_by_label:\n            for tiou in self.tiou_thresholds:\n                ap = self._compute_ap(prediction_by_label[label_id], label_id, tiou)\n                ap_values.append(ap)\n                \n        # Compute mean AP\n        self.ap = np.array(ap_values).reshape(len(prediction_by_label), len(self.tiou_thresholds))\n        mAP = np.mean(self.ap)\n        \n        return mAP\n        \n    def _compute_ap(self, predictions, label_id, tiou_threshold):\n        \"\"\"Compute AP for a specific label and IoU threshold\"\"\"\n        # Get ground truth for this label\n        gt_segments = []\n        for vid_id, vid_info in self.ground_truth.items():\n            for ann in vid_info['annotations']:\n                if ann['label_id'] == label_id:\n                    gt_segments.append({\n                        'video-id': vid_id,\n                        't-start': ann['segment'][0],\n                        't-end': ann['segment'][1]\n                    })\n                    \n        if len(gt_segments) == 0:\n            return 0.0\n            \n        # Sort predictions by score\n        predictions = sorted(predictions, key=lambda x: x['score'], reverse=True)\n        \n        # Compute precision and recall\n        tp = np.zeros(len(predictions))\n        fp = np.zeros(len(predictions))\n        matched_gt = set()\n        \n        for i, pred in enumerate(predictions):\n            # Find best matching ground truth\n            best_iou = 0.0\n            best_gt_idx = -1\n            \n            for j, gt in enumerate(gt_segments):\n                if pred['video-id'] == gt['video-id']:\n                    iou = self._compute_iou(pred, gt)\n                    if iou > best_iou:\n                        best_iou = iou\n                        best_gt_idx = j\n                        \n            # Check if match is valid\n            if best_iou >= tiou_threshold and best_gt_idx not in matched_gt:\n                tp[i] = 1\n                matched_gt.add(best_gt_idx)\n            else:\n                fp[i] = 1\n                \n        # Compute precision and recall\n        tp_cumsum = np.cumsum(tp)\n        fp_cumsum = np.cumsum(fp)\n        recall = tp_cumsum / len(gt_segments)\n        precision = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-8)\n        \n        # Compute AP\n        ap = self._compute_ap_from_pr(precision, recall)\n        return ap\n        \n    def _compute_iou(self, pred, gt):\n        \"\"\"Compute IoU between prediction and ground truth\"\"\"\n        start_i = max(pred['t-start'], gt['t-start'])\n        end_i = min(pred['t-end'], gt['t-end'])\n        \n        if end_i <= start_i:\n            return 0.0\n            \n        intersection = end_i - start_i\n        union = (pred['t-end'] - pred['t-start']) + (gt['t-end'] - gt['t-start']) - intersection\n        \n        return intersection / union\n        \n    def _compute_ap_from_pr(self, precision, recall):\n        \"\"\"Compute AP from precision-recall curve\"\"\"\n        # Add boundary points\n        mrec = np.concatenate(([0.], recall, [1.]))\n        mpre = np.concatenate(([0.], precision, [0.]))\n        \n        # Compute envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n            \n        # Find points where recall changes\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n        \n        # Compute AP\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n        return ap\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.243836Z","iopub.status.idle":"2025-10-04T16:30:33.244126Z","shell.execute_reply.started":"2025-10-04T16:30:33.243995Z","shell.execute_reply":"2025-10-04T16:30:33.244010Z"}},"outputs":[],"execution_count":null},{"id":"dc036b7e-233f-49f0-83b4-bedaf2c28e85","cell_type":"code","source":"# Modified from official EPIC-Kitchens action detection evaluation code\n# see https://github.com/epic-kitchens/C2-Action-Detection/blob/master/EvaluationCode/evaluate_detection_json_ek100.py\nimport os\nimport json\nimport pandas as pd\nimport numpy as np\nfrom joblib import Parallel, delayed\nfrom typing import List\nfrom typing import Tuple\nfrom typing import Dict\n\n\ndef remove_duplicate_annotations(ants, tol=1e-3):\n    # remove duplicate / very short annotations (same category and starting/ending time)\n    valid_events = []\n    for event in ants:\n        s, e, l = event['segment'][0], event['segment'][1], event['label_id']\n        if (e - s) >= tol:\n            valid = True\n        else:\n            valid = False\n        for p_event in valid_events:\n            if ((abs(s-p_event['segment'][0]) <= tol)\n                and (abs(e-p_event['segment'][1]) <= tol)\n                and (l == p_event['label_id'])\n            ):\n                valid = False\n                break\n        if valid:\n            valid_events.append(event)\n    return valid_events\n\n\ndef load_gt_seg_from_json(json_file, split=None, label='label_id', label_offset=0):\n    # load json file\n    with open(json_file, \"r\", encoding=\"utf8\") as f:\n        json_db = json.load(f)\n    json_db = json_db['database']\n\n    vids, starts, stops, labels = [], [], [], []\n    for k, v in json_db.items():\n\n        # filter based on split\n        if (split is not None) and v['subset'].lower() != split:\n            continue\n        # remove duplicated instances\n        ants = remove_duplicate_annotations(v['annotations'])\n        # video id\n        vids += [k] * len(ants)\n        # for each event, grab the start/end time and label\n        for event in ants:\n            starts += [float(event['segment'][0])]\n            stops += [float(event['segment'][1])]\n            if isinstance(event[label], (Tuple, List)):\n                # offset the labels by label_offset\n                label_id = 0\n                for i, x in enumerate(event[label][::-1]):\n                    label_id += label_offset**i + int(x)\n            else:\n                # load label_id directly\n                label_id = int(event[label])\n            labels += [label_id]\n\n    # move to pd dataframe\n    gt_base = pd.DataFrame({\n        'video-id' : vids,\n        't-start' : starts,\n        't-end': stops,\n        'label': labels\n    })\n\n    return gt_base\n\n\ndef load_pred_seg_from_json(json_file, label='label_id', label_offset=0):\n    # load json file\n    with open(json_file, \"r\", encoding=\"utf8\") as f:\n        json_db = json.load(f)\n    json_db = json_db['database']\n\n    vids, starts, stops, labels, scores = [], [], [], [], []\n    for k, v, in json_db.items():\n        # video id\n        vids += [k] * len(v)\n        # for each event\n        for event in v:\n            starts += [float(event['segment'][0])]\n            stops += [float(event['segment'][1])]\n            if isinstance(event[label], (Tuple, List)):\n                # offset the labels by label_offset\n                label_id = 0\n                for i, x in enumerate(event[label][::-1]):\n                    label_id += label_offset**i + int(x)\n            else:\n                # load label_id directly\n                label_id = int(event[label])\n            labels += [label_id]\n            scores += [float(event['scores'])]\n\n    # move to pd dataframe\n    pred_base = pd.DataFrame({\n        'video-id' : vids,\n        't-start' : starts,\n        't-end': stops,\n        'label': labels,\n        'score': scores\n    })\n\n    return pred_base\n\n\nclass ANETdetection(object):\n    \"\"\"Adapted from https://github.com/activitynet/ActivityNet/blob/master/Evaluation/eval_detection.py\"\"\"\n\n    def __init__(\n        self,\n        ant_file,\n        split=None,\n        tiou_thresholds=np.linspace(0.1, 0.5, 5),\n        top_k=(1, 5),\n        label='label_id',\n        label_offset=0,\n        num_workers=8,\n        dataset_name=None,\n    ):\n\n        self.tiou_thresholds = tiou_thresholds\n        self.top_k = top_k\n        self.ap = None\n        self.num_workers = num_workers\n        if dataset_name is not None:\n            self.dataset_name = dataset_name\n        else:\n            self.dataset_name = os.path.basename(ant_file).replace('.json', '')\n\n        # Import ground truth and predictions\n        self.split = split\n        self.ground_truth = load_gt_seg_from_json(\n            ant_file, split=self.split, label=label, label_offset=label_offset)\n\n        # remove labels that does not exists in gt\n        self.activity_index = {j: i for i, j in enumerate(sorted(self.ground_truth['label'].unique()))}\n        self.ground_truth['label']=self.ground_truth['label'].replace(self.activity_index)\n\n    def _get_predictions_with_label(self, prediction_by_label, label_name, cidx):\n        \"\"\"Get all predicitons of the given label. Return empty DataFrame if there\n        is no predcitions with the given label.\n        \"\"\"\n        try:\n            res = prediction_by_label.get_group(cidx).reset_index(drop=True)\n            return res\n        except:\n            print('Warning: No predictions of label \\'%s\\' were provdied.' % label_name)\n            return pd.DataFrame()\n\n    def wrapper_compute_average_precision(self, preds):\n        \"\"\"Computes average precision for each class in the subset.\n        \"\"\"\n        ap = np.zeros((len(self.tiou_thresholds), len(self.activity_index)))\n\n        # Adaptation to query faster\n        ground_truth_by_label = self.ground_truth.groupby('label')\n        prediction_by_label = preds.groupby('label')\n\n        results = Parallel(n_jobs=self.num_workers)(\n            delayed(compute_average_precision_detection)(\n                ground_truth=ground_truth_by_label.get_group(cidx).reset_index(drop=True),\n                prediction=self._get_predictions_with_label(prediction_by_label, label_name, cidx),\n                tiou_thresholds=self.tiou_thresholds,\n            ) for label_name, cidx in self.activity_index.items())\n\n        for i, cidx in enumerate(self.activity_index.values()):\n            ap[:,cidx] = results[i]\n\n        return ap\n\n    def wrapper_compute_topkx_recall(self, preds):\n        \"\"\"Computes Top-kx recall for each class in the subset.\n        \"\"\"\n        recall = np.zeros((len(self.tiou_thresholds), len(self.top_k), len(self.activity_index)))\n\n        # Adaptation to query faster\n        ground_truth_by_label = self.ground_truth.groupby('label')\n        prediction_by_label = preds.groupby('label')\n\n        results = Parallel(n_jobs=self.num_workers)(\n            delayed(compute_topkx_recall_detection)(\n                ground_truth=ground_truth_by_label.get_group(cidx).reset_index(drop=True),\n                prediction=self._get_predictions_with_label(prediction_by_label, label_name, cidx),\n                tiou_thresholds=self.tiou_thresholds,\n                top_k=self.top_k,\n            ) for label_name, cidx in self.activity_index.items())\n\n        for i, cidx in enumerate(self.activity_index.values()):\n            recall[...,cidx] = results[i]\n\n        return recall\n\n    def evaluate(self, preds, verbose=True):\n        \"\"\"Evaluates a prediction file. For the detection task we measure the\n        interpolated mean average precision to measure the performance of a\n        method.\n        preds can be (1) a pd.DataFrame; or (2) a json file where the data will be loaded;\n        or (3) a python dict item with numpy arrays as the values\n        \"\"\"\n\n        if isinstance(preds, pd.DataFrame):\n            assert 'label' in preds\n        elif isinstance(preds, str) and os.path.isfile(preds):\n            preds = load_pred_seg_from_json(preds)\n        elif isinstance(preds, Dict):\n            # move to pd dataframe\n            # did not check dtype here, can accept both numpy / pytorch tensors\n            preds = pd.DataFrame({\n                'video-id' : preds['video-id'],\n                't-start' : preds['t-start'].tolist(),\n                't-end': preds['t-end'].tolist(),\n                'label': preds['label'].tolist(),\n                'score': preds['score'].tolist()\n            })\n        # always reset ap\n        self.ap = None\n\n        # make the label ids consistent\n        preds['label'] = preds['label'].replace(self.activity_index)\n\n        # compute mAP\n        self.ap = self.wrapper_compute_average_precision(preds)\n        self.recall = self.wrapper_compute_topkx_recall(preds)\n        mAP = self.ap.mean(axis=1)\n        mRecall = self.recall.mean(axis=2)\n        average_mAP = mAP.mean()\n\n        # print results\n        if verbose:\n            # print the results\n            print('[RESULTS] Action detection results on {:s}.'.format(\n                self.dataset_name)\n            )\n            block = ''\n            for tiou, tiou_mAP, tiou_mRecall in zip(self.tiou_thresholds, mAP, mRecall):\n                block += '\\n|tIoU = {:.2f}: '.format(tiou)\n                block += 'mAP = {:>4.2f} (%) '.format(tiou_mAP*100)\n                for idx, k in enumerate(self.top_k):\n                    block += 'Recall@{:d}x = {:>4.2f} (%) '.format(k, tiou_mRecall[idx]*100)\n            print(block)\n            print('Average mAP: {:>4.2f} (%)'.format(average_mAP*100))\n\n        # return the results\n        return mAP, average_mAP, mRecall\n\n\ndef compute_average_precision_detection(\n    ground_truth,\n    prediction,\n    tiou_thresholds=np.linspace(0.1, 0.5, 5)\n):\n    \"\"\"Compute average precision (detection task) between ground truth and\n    predictions data frames. If multiple predictions occurs for the same\n    predicted segment, only the one with highest score is matches as\n    true positive. This code is greatly inspired by Pascal VOC devkit.\n    Parameters\n    ----------\n    ground_truth : df\n        Data frame containing the ground truth instances.\n        Required fields: ['video-id', 't-start', 't-end']\n    prediction : df\n        Data frame containing the prediction instances.\n        Required fields: ['video-id, 't-start', 't-end', 'score']\n    tiou_thresholds : 1darray, optional\n        Temporal intersection over union threshold.\n    Outputs\n    -------\n    ap : float\n        Average precision score.\n    \"\"\"\n    ap = np.zeros(len(tiou_thresholds))\n    if prediction.empty:\n        return ap\n\n    npos = float(len(ground_truth))\n    lock_gt = np.ones((len(tiou_thresholds),len(ground_truth))) * -1\n    # Sort predictions by decreasing score order.\n    sort_idx = prediction['score'].values.argsort()[::-1]\n    prediction = prediction.loc[sort_idx].reset_index(drop=True)\n\n    # Initialize true positive and false positive vectors.\n    tp = np.zeros((len(tiou_thresholds), len(prediction)))\n    fp = np.zeros((len(tiou_thresholds), len(prediction)))\n\n    # Adaptation to query faster\n    ground_truth_gbvn = ground_truth.groupby('video-id')\n\n    # Assigning true positive to truly ground truth instances.\n    for idx, this_pred in prediction.iterrows():\n\n        try:\n            # Check if there is at least one ground truth in the video associated.\n            ground_truth_videoid = ground_truth_gbvn.get_group(this_pred['video-id'])\n        except Exception as e:\n            fp[:, idx] = 1\n            continue\n\n        this_gt = ground_truth_videoid.reset_index()\n        tiou_arr = segment_iou(this_pred[['t-start', 't-end']].values,\n                               this_gt[['t-start', 't-end']].values)\n        # We would like to retrieve the predictions with highest tiou score.\n        tiou_sorted_idx = tiou_arr.argsort()[::-1]\n        for tidx, tiou_thr in enumerate(tiou_thresholds):\n            for jdx in tiou_sorted_idx:\n                if tiou_arr[jdx] < tiou_thr:\n                    fp[tidx, idx] = 1\n                    break\n                if lock_gt[tidx, this_gt.loc[jdx]['index']] >= 0:\n                    continue\n                # Assign as true positive after the filters above.\n                tp[tidx, idx] = 1\n                lock_gt[tidx, this_gt.loc[jdx]['index']] = idx\n                break\n\n            if fp[tidx, idx] == 0 and tp[tidx, idx] == 0:\n                fp[tidx, idx] = 1\n\n    tp_cumsum = np.cumsum(tp, axis=1).astype(np.float)\n    fp_cumsum = np.cumsum(fp, axis=1).astype(np.float)\n    recall_cumsum = tp_cumsum / npos\n\n    precision_cumsum = tp_cumsum / (tp_cumsum + fp_cumsum)\n\n    for tidx in range(len(tiou_thresholds)):\n        ap[tidx] = interpolated_prec_rec(precision_cumsum[tidx,:], recall_cumsum[tidx,:])\n\n    return ap\n\n\ndef compute_topkx_recall_detection(\n    ground_truth,\n    prediction,\n    tiou_thresholds=np.linspace(0.1, 0.5, 5),\n    top_k=(1, 5),\n):\n    \"\"\"Compute recall (detection task) between ground truth and\n    predictions data frames. If multiple predictions occurs for the same\n    predicted segment, only the one with highest score is matches as\n    true positive. This code is greatly inspired by Pascal VOC devkit.\n    Parameters\n    ----------\n    ground_truth : df\n        Data frame containing the ground truth instances.\n        Required fields: ['video-id', 't-start', 't-end']\n    prediction : df\n        Data frame containing the prediction instances.\n        Required fields: ['video-id, 't-start', 't-end', 'score']\n    tiou_thresholds : 1darray, optional\n        Temporal intersection over union threshold.\n    top_k: tuple, optional\n        Top-kx results of a action category where x stands for the number of \n        instances for the action category in the video.\n    Outputs\n    -------\n    recall : float\n        Recall score.\n    \"\"\"\n    if prediction.empty:\n        return np.zeros((len(tiou_thresholds), len(top_k)))\n\n    # Initialize true positive vectors.\n    tp = np.zeros((len(tiou_thresholds), len(top_k)))\n    n_gts = 0\n\n    # Adaptation to query faster\n    ground_truth_gbvn = ground_truth.groupby('video-id')\n    prediction_gbvn = prediction.groupby('video-id')\n\n    for videoid, _ in ground_truth_gbvn.groups.items():\n        ground_truth_videoid = ground_truth_gbvn.get_group(videoid)\n        n_gts += len(ground_truth_videoid)\n        try:\n            prediction_videoid = prediction_gbvn.get_group(videoid)\n        except Exception as e:\n            continue\n\n        this_gt = ground_truth_videoid.reset_index()\n        this_pred = prediction_videoid.reset_index()\n\n        # Sort predictions by decreasing score order.\n        score_sort_idx = this_pred['score'].values.argsort()[::-1]\n        top_kx_idx = score_sort_idx[:max(top_k) * len(this_gt)]\n        tiou_arr = k_segment_iou(this_pred[['t-start', 't-end']].values[top_kx_idx],\n                                 this_gt[['t-start', 't-end']].values)\n            \n        for tidx, tiou_thr in enumerate(tiou_thresholds):\n            for kidx, k in enumerate(top_k):\n                tiou = tiou_arr[:k * len(this_gt)]\n                tp[tidx, kidx] += ((tiou >= tiou_thr).sum(axis=0) > 0).sum()\n\n    recall = tp / n_gts\n\n    return recall\n\n\ndef k_segment_iou(target_segments, candidate_segments):\n    return np.stack(\n        [segment_iou(target_segment, candidate_segments) \\\n            for target_segment in target_segments]\n    )\n\n\ndef segment_iou(target_segment, candidate_segments):\n    \"\"\"Compute the temporal intersection over union between a\n    target segment and all the test segments.\n    Parameters\n    ----------\n    target_segment : 1d array\n        Temporal target segment containing [starting, ending] times.\n    candidate_segments : 2d array\n        Temporal candidate segments containing N x [starting, ending] times.\n    Outputs\n    -------\n    tiou : 1d array\n        Temporal intersection over union score of the N's candidate segments.\n    \"\"\"\n    tt1 = np.maximum(target_segment[0], candidate_segments[:, 0])\n    tt2 = np.minimum(target_segment[1], candidate_segments[:, 1])\n    # Intersection including Non-negative overlap score.\n    segments_intersection = (tt2 - tt1).clip(0)\n    # Segment union.\n    segments_union = (candidate_segments[:, 1] - candidate_segments[:, 0]) \\\n                     + (target_segment[1] - target_segment[0]) - segments_intersection\n    # Compute overlap as the ratio of the intersection\n    # over union of two segments.\n    tIoU = segments_intersection.astype(float) / segments_union\n    return tIoU\n\n\ndef interpolated_prec_rec(prec, rec):\n    \"\"\"Interpolated AP - VOCdevkit from VOC 2011.\n    \"\"\"\n    mprec = np.hstack([[0], prec, [0]])\n    mrec = np.hstack([[0], rec, [1]])\n    for i in range(len(mprec) - 1)[::-1]:\n        mprec[i] = max(mprec[i], mprec[i + 1])\n    idx = np.where(mrec[1::] != mrec[0:-1])[0] + 1\n    ap = np.sum((mrec[idx] - mrec[idx - 1]) * mprec[idx])\n    return ap\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.245341Z","iopub.status.idle":"2025-10-04T16:30:33.245564Z","shell.execute_reply.started":"2025-10-04T16:30:33.245456Z","shell.execute_reply":"2025-10-04T16:30:33.245465Z"}},"outputs":[],"execution_count":null},{"id":"f96f1c0f-8900-4c5f-b75e-69b105d7a8bf","cell_type":"code","source":"ckpt_file = \"/kaggle/working/ckpt/thumos_i3d_2025-09-30 04:17:37/epoch_001.pth.tar\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.246593Z","iopub.status.idle":"2025-10-04T16:30:33.246875Z","shell.execute_reply.started":"2025-10-04T16:30:33.246728Z","shell.execute_reply":"2025-10-04T16:30:33.246745Z"}},"outputs":[],"execution_count":null},{"id":"f4f795fb-bda9-4b47-a575-820f33473679","cell_type":"code","source":"_ = fix_random_seed(0, include_cuda=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.247802Z","iopub.status.idle":"2025-10-04T16:30:33.248089Z","shell.execute_reply.started":"2025-10-04T16:30:33.247942Z","shell.execute_reply":"2025-10-04T16:30:33.247954Z"}},"outputs":[],"execution_count":null},{"id":"7d1b3d11-2e5d-4485-97a1-75b5e5adf11b","cell_type":"code","source":"val_dataset = THUMOS14Dataset(\n    is_training=False,\n    split=val_split,\n    feat_folder=feat_folder,\n    json_file=json_file,\n    feat_stride=feat_stride,\n    num_frames=num_frames,\n    default_fps=default_fps,\n    downsample_rate=downsample_rate,\n    max_seq_len=max_seq_len,\n    trunc_thresh=trunc_thresh,\n    crop_ratio=crop_ratio,\n    input_dim=input_dim,\n    num_classes=num_classes,\n    file_prefix=file_prefix,\n    file_ext=file_ext,\n    force_upsampling=force_upsampling\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.249308Z","iopub.status.idle":"2025-10-04T16:30:33.249615Z","shell.execute_reply.started":"2025-10-04T16:30:33.249461Z","shell.execute_reply":"2025-10-04T16:30:33.249477Z"}},"outputs":[],"execution_count":null},{"id":"ddc8406e-e9e4-409c-bb3b-5690049a1ba2","cell_type":"code","source":"is_training = False\n\nval_loader = torch.utils.data.DataLoader(\n    val_dataset,\n    batch_size=1,\n    num_workers=num_workers,\n    collate_fn=trivial_batch_collator,\n    worker_init_fn=(worker_init_reset_seed if is_training else None),\n    shuffle=is_training,\n    drop_last=is_training,\n    generator=None,\n    persistent_workers=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.250644Z","iopub.status.idle":"2025-10-04T16:30:33.250936Z","shell.execute_reply.started":"2025-10-04T16:30:33.250793Z","shell.execute_reply":"2025-10-04T16:30:33.250808Z"}},"outputs":[],"execution_count":null},{"id":"e2579b6c-fd37-4fd5-bc0e-e621116e8e04","cell_type":"code","source":"model = nn.DataParallel(model, device_ids=devices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.252303Z","iopub.status.idle":"2025-10-04T16:30:33.252503Z","shell.execute_reply.started":"2025-10-04T16:30:33.252408Z","shell.execute_reply":"2025-10-04T16:30:33.252416Z"}},"outputs":[],"execution_count":null},{"id":"9dd2e098-e479-4c89-8c98-af9aeb0677c6","cell_type":"code","source":"'''\n\"\"\"4. load ckpt\"\"\"\nprint(\"=> loading checkpoint '{}'\".format(ckpt_file))\n# load ckpt, reset epoch / best rmse\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ncheckpoint = torch.load(\n    ckpt_file,\n    map_location = device\n)\n# load ema model instead\nprint(\"Loading from EMA model ...\")\n#print(checkpoint)\nmodel.load_state_dict(checkpoint['state_dict_ema'])\ndel checkpoint\n'''\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.253494Z","iopub.status.idle":"2025-10-04T16:30:33.253812Z","shell.execute_reply.started":"2025-10-04T16:30:33.253635Z","shell.execute_reply":"2025-10-04T16:30:33.253648Z"}},"outputs":[],"execution_count":null},{"id":"69de8bde-90e1-44ee-8ee1-8b489033a4c1","cell_type":"code","source":"print(\"=> loading checkpoint '{}'\".format(ckpt_file))\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ncheckpoint = torch.load(ckpt_file, map_location=device)\n\nprint(\"Loading from EMA model ...\")\n\n# Get state dict\nif 'state_dict_ema' in checkpoint:\n    state_dict = checkpoint['state_dict_ema']\nelse:\n    state_dict = checkpoint['state_dict']\n\n# Remove ALL DataParallel wrappers and create clean model\nfrom collections import OrderedDict\nclean_state_dict = OrderedDict()\nfor k, v in state_dict.items():\n    name = k\n    # Remove all 'module.' prefixes to get the base parameter name\n    while name.startswith('module.'):\n        name = name[7:]\n    clean_state_dict[name] = v\n\n# Get the base model (unwrap all DataParallel layers)\nbase_model = model\nwhile hasattr(base_model, 'module'):\n    base_model = base_model.module\n\n# Move to device\nbase_model = base_model.to(device)\n\nprint(f\"Loading {len(clean_state_dict)} parameters...\")\nprint(f\"Base model expects keys like: {list(base_model.state_dict().keys())[0]}\")\nprint(f\"Clean state dict provides keys like: {list(clean_state_dict.keys())[0]}\")\n\n# Load into clean model\nbase_model.load_state_dict(clean_state_dict)\n\n# Use the clean model for evaluation (no DataParallel needed for eval)\nmodel = base_model\n\ndel checkpoint\n\nprint(\"✅ Model loaded successfully!\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.254732Z","iopub.status.idle":"2025-10-04T16:30:33.255025Z","shell.execute_reply.started":"2025-10-04T16:30:33.254861Z","shell.execute_reply":"2025-10-04T16:30:33.254874Z"}},"outputs":[],"execution_count":null},{"id":"f1e0b651-3b24-4e29-8ee9-dc9ee0f93123","cell_type":"code","source":"# set up evaluator\ndet_eval, output_file = None, None\n\n\nval_db_vars = val_dataset.get_attributes()\ndet_eval = ANETdetection(\n    val_dataset.json_file,\n    val_dataset.split[0],\n    tiou_thresholds = val_db_vars['tiou_thresholds']\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.256694Z","iopub.status.idle":"2025-10-04T16:30:33.256896Z","shell.execute_reply.started":"2025-10-04T16:30:33.256800Z","shell.execute_reply":"2025-10-04T16:30:33.256809Z"}},"outputs":[],"execution_count":null},{"id":"81e2471c-98e6-44b5-bcbb-31acf74aac25","cell_type":"code","source":"\"\"\"5. Test the model\"\"\"\nprint(\"\\nStart testing model {:s} ...\".format(model_name))\nstart = time.time()\nmAP = valid_one_epoch(\n    val_loader,\n    model,\n    -1,\n    evaluator=det_eval,\n    output_file=output_file,\n    ext_score_file=ext_score_file,\n    tb_writer=None\n)\nend = time.time()\nprint(\"All done! Total time: {:0.2f} sec\".format(end - start))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.258531Z","iopub.status.idle":"2025-10-04T16:30:33.258758Z","shell.execute_reply.started":"2025-10-04T16:30:33.258652Z","shell.execute_reply":"2025-10-04T16:30:33.258661Z"}},"outputs":[],"execution_count":null},{"id":"3e898a26-ab1e-4554-9bda-cc6ef8a2ed24","cell_type":"code","source":"print(\"Hello\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:30:33.259607Z","iopub.status.idle":"2025-10-04T16:30:33.259801Z","shell.execute_reply.started":"2025-10-04T16:30:33.259708Z","shell.execute_reply":"2025-10-04T16:30:33.259717Z"}},"outputs":[],"execution_count":null},{"id":"d7e018f4-4e3f-40ac-9ac3-9c4d9a9cedfe","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"26f84aed-cb9d-4218-a0d4-33d815e4f3b6","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8dd3fada-c59f-4f97-92d0-66698f71e84e","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"9bbd7f74-8b92-4e15-a3bf-5f4632c1acf3","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ef25c74a-d692-4d0c-b32d-d8dfb536ce48","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1cbc8b72-6f33-4ced-861e-2fef015351af","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}