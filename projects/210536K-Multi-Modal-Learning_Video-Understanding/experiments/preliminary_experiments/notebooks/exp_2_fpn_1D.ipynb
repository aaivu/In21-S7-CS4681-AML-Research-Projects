{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1acab503",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:57.714715Z",
     "iopub.status.busy": "2025-09-30T08:45:57.713919Z",
     "iopub.status.idle": "2025-09-30T08:45:57.719490Z",
     "shell.execute_reply": "2025-09-30T08:45:57.718728Z",
     "shell.execute_reply.started": "2025-09-30T08:45:57.714678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# python imports\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# for visualization\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c2e74b30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:57.721111Z",
     "iopub.status.busy": "2025-09-30T08:45:57.720796Z",
     "iopub.status.idle": "2025-09-30T08:45:57.731419Z",
     "shell.execute_reply": "2025-09-30T08:45:57.730768Z",
     "shell.execute_reply.started": "2025-09-30T08:45:57.721094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0f011ed9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:57.732334Z",
     "iopub.status.busy": "2025-09-30T08:45:57.732089Z",
     "iopub.status.idle": "2025-09-30T08:45:57.741515Z",
     "shell.execute_reply": "2025-09-30T08:45:57.740901Z",
     "shell.execute_reply.started": "2025-09-30T08:45:57.732300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training parameters - directly set values\n",
    "init_rand_seed = 1234567891\n",
    "dataset_name = \"thumos\"\n",
    "devices = ['cuda:0']  # or [0] depending on your setup\n",
    "train_split = ['validation']\n",
    "val_split = ['test']\n",
    "model_name = \"LocPointTransformer\"\n",
    "output_folder = \"./ckpt/\"\n",
    "\n",
    "# Dataset parameters\n",
    "json_file = \"/kaggle/input/thumos/thumos/annotations/thumos14.json\"\n",
    "feat_folder = \"/kaggle/input/thumos/thumos/i3d_features\"\n",
    "file_prefix = None\n",
    "file_ext = \".npy\"\n",
    "feat_stride = 4\n",
    "num_frames = 16\n",
    "default_fps = None\n",
    "input_dim = 2048\n",
    "num_classes = 20\n",
    "downsample_rate = 1\n",
    "max_seq_len = 2304\n",
    "trunc_thresh = 0.5\n",
    "crop_ratio = [0.9, 1.0]\n",
    "force_upsampling = False\n",
    "\n",
    "# Loader parameters\n",
    "batch_size = 2\n",
    "num_workers = 4\n",
    "\n",
    "# Model architecture parameters\n",
    "backbone_type = 'convTransformer'\n",
    "fpn_type = \"fpn\"\n",
    "backbone_arch = (2, 2, 5)\n",
    "scale_factor = 2\n",
    "regression_range = [(0, 4), (4, 8), (8, 16), (16, 32), (32, 64), (64, 10000)]\n",
    "n_head = 4\n",
    "n_mha_win_size = 19\n",
    "embd_kernel_size = 3\n",
    "embd_dim = 512\n",
    "embd_with_ln = True\n",
    "fpn_dim = 512\n",
    "fpn_with_ln = True\n",
    "fpn_start_level = 0\n",
    "head_dim = 512\n",
    "head_kernel_size = 3\n",
    "head_num_layers = 3\n",
    "head_with_ln = True\n",
    "max_buffer_len_factor = 6.0\n",
    "use_abs_pe = False\n",
    "use_rel_pe = False\n",
    "\n",
    "# Training configuration\n",
    "center_sample = \"radius\"\n",
    "center_sample_radius = 1.5\n",
    "loss_weight = 1.0\n",
    "cls_prior_prob = 0.01\n",
    "init_loss_norm = 100\n",
    "clip_grad_l2norm = 1.0\n",
    "head_empty_cls = []\n",
    "dropout = 0.0\n",
    "droppath = 0.1\n",
    "label_smoothing = 0.0\n",
    "\n",
    "# Test configuration\n",
    "pre_nms_thresh = 0.001\n",
    "pre_nms_topk = 2000\n",
    "iou_threshold = 0.1\n",
    "min_score = 0.001\n",
    "max_seg_num = 200\n",
    "nms_method = 'soft'\n",
    "nms_sigma = 0.5\n",
    "duration_thresh = 0.05\n",
    "multiclass_nms = True\n",
    "ext_score_file = \"/kaggle/input/thumos/thumos/annotations/thumos14_cls_scores.pkl\"\n",
    "voting_thresh = 0.7\n",
    "\n",
    "# Optimizer parameters\n",
    "opt_type = \"AdamW\"\n",
    "momentum = 0.9\n",
    "weight_decay = 0.05\n",
    "learning_rate = 0.0001\n",
    "epochs = 30\n",
    "warmup = True\n",
    "warmup_epochs = 5\n",
    "schedule_type = \"cosine\"\n",
    "schedule_steps = []\n",
    "schedule_gamma = 0.1\n",
    "\n",
    "# Other training parameters\n",
    "start_epoch = 0\n",
    "print_freq = 10\n",
    "ckpt_freq = 5\n",
    "output = \"\"\n",
    "resume = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "631ab31f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:57.742410Z",
     "iopub.status.busy": "2025-09-30T08:45:57.742171Z",
     "iopub.status.idle": "2025-09-30T08:45:57.755006Z",
     "shell.execute_reply": "2025-09-30T08:45:57.754285Z",
     "shell.execute_reply.started": "2025-09-30T08:45:57.742388Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fix_random_seed(seed, include_cuda=True):\n",
    "    rng_generator = torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    if include_cuda:\n",
    "        # training: disable cudnn benchmark to ensure the reproducibility\n",
    "        cudnn.enabled = True\n",
    "        cudnn.benchmark = False\n",
    "        cudnn.deterministic = True\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # this is needed for CUDA >= 10.2\n",
    "        os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    else:\n",
    "        cudnn.enabled = True\n",
    "        cudnn.benchmark = True\n",
    "    return rng_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "65e89a35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:57.757152Z",
     "iopub.status.busy": "2025-09-30T08:45:57.756525Z",
     "iopub.status.idle": "2025-09-30T08:45:57.772838Z",
     "shell.execute_reply": "2025-09-30T08:45:57.772205Z",
     "shell.execute_reply.started": "2025-09-30T08:45:57.757127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# prep for output folder (based on time stamp)\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "if len(output) == 0:\n",
    "    ts = datetime.datetime.fromtimestamp(int(time.time()))\n",
    "    ckpt_folder = os.path.join(output_folder, f'thumos_i3d_{str(ts)}')\n",
    "else:\n",
    "    ckpt_folder = os.path.join(output_folder, f'thumos_i3d_{str(output)}')\n",
    "\n",
    "if not os.path.exists(ckpt_folder):\n",
    "    os.mkdir(ckpt_folder)\n",
    "\n",
    "# tensorboard writer\n",
    "tb_writer = SummaryWriter(os.path.join(ckpt_folder, 'logs'))\n",
    "\n",
    "# fix the random seeds (this will fix everything)\n",
    "rng_generator = fix_random_seed(init_rand_seed, include_cuda=True)\n",
    "\n",
    "# re-scale learning rate / # workers based on number of GPUs\n",
    "learning_rate *= len(devices)\n",
    "num_workers *= len(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2832ac8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:57.773888Z",
     "iopub.status.busy": "2025-09-30T08:45:57.773611Z",
     "iopub.status.idle": "2025-09-30T08:45:57.777869Z",
     "shell.execute_reply": "2025-09-30T08:45:57.777300Z",
     "shell.execute_reply.started": "2025-09-30T08:45:57.773871Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dataset - Thumos14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "88eeb20b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:57.896281Z",
     "iopub.status.busy": "2025-09-30T08:45:57.896087Z",
     "iopub.status.idle": "2025-09-30T08:45:57.910350Z",
     "shell.execute_reply": "2025-09-30T08:45:57.909836Z",
     "shell.execute_reply.started": "2025-09-30T08:45:57.896267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class THUMOS14Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        is_training,     # if in training mode\n",
    "        split,           # split, a tuple/list allowing concat of subsets\n",
    "        feat_folder,     # folder for features\n",
    "        json_file,       # json file for annotations\n",
    "        feat_stride,     # temporal stride of the feats\n",
    "        num_frames,      # number of frames for each feat\n",
    "        default_fps,     # default fps\n",
    "        downsample_rate, # downsample rate for feats\n",
    "        max_seq_len,     # maximum sequence length during training\n",
    "        trunc_thresh,    # threshold for truncate an action segment\n",
    "        crop_ratio,      # a tuple (e.g., (0.9, 1.0)) for random cropping\n",
    "        input_dim,       # input feat dim\n",
    "        num_classes,     # number of action categories\n",
    "        file_prefix,     # feature file prefix if any\n",
    "        file_ext,        # feature file extension if any\n",
    "        force_upsampling # force to upsample to max_seq_len\n",
    "    ):\n",
    "        # file path\n",
    "        assert os.path.exists(feat_folder) and os.path.exists(json_file)\n",
    "        assert isinstance(split, tuple) or isinstance(split, list)\n",
    "        assert crop_ratio == None or len(crop_ratio) == 2\n",
    "        self.feat_folder = feat_folder\n",
    "        if file_prefix is not None:\n",
    "            self.file_prefix = file_prefix\n",
    "        else:\n",
    "            self.file_prefix = ''\n",
    "        self.file_ext = file_ext\n",
    "        self.json_file = json_file\n",
    "\n",
    "        # split / training mode\n",
    "        self.split = split\n",
    "        self.is_training = is_training\n",
    "\n",
    "        # features meta info\n",
    "        self.feat_stride = feat_stride\n",
    "        self.num_frames = num_frames\n",
    "        self.input_dim = input_dim\n",
    "        self.default_fps = default_fps\n",
    "        self.downsample_rate = downsample_rate\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.trunc_thresh = trunc_thresh\n",
    "        self.num_classes = num_classes\n",
    "        self.label_dict = None\n",
    "        self.crop_ratio = crop_ratio\n",
    "\n",
    "        # load database and select the subset\n",
    "        dict_db, label_dict = self._load_json_db(self.json_file)\n",
    "        assert len(label_dict) == num_classes\n",
    "        self.data_list = dict_db\n",
    "        self.label_dict = label_dict\n",
    "\n",
    "        # dataset specific attributes\n",
    "        self.db_attributes = {\n",
    "            'dataset_name': 'thumos-14',\n",
    "            'tiou_thresholds': np.linspace(0.3, 0.7, 5),\n",
    "            # we will mask out cliff diving\n",
    "            'empty_label_ids': [],\n",
    "        }\n",
    "\n",
    "    def get_attributes(self):\n",
    "        return self.db_attributes\n",
    "\n",
    "    def _load_json_db(self, json_file):\n",
    "        # load database and select the subset\n",
    "        with open(json_file, 'r') as fid:\n",
    "            json_data = json.load(fid)\n",
    "        json_db = json_data['database']\n",
    "\n",
    "        # if label_dict is not available\n",
    "        if self.label_dict is None:\n",
    "            label_dict = {}\n",
    "            for key, value in json_db.items():\n",
    "                for act in value['annotations']:\n",
    "                    label_dict[act['label']] = act['label_id']\n",
    "\n",
    "        # fill in the db (immutable afterwards)\n",
    "        dict_db = tuple()\n",
    "        for key, value in json_db.items():\n",
    "            # skip the video if not in the split\n",
    "            if value['subset'].lower() not in self.split:\n",
    "                continue\n",
    "            # or does not have the feature file\n",
    "            feat_file = os.path.join(self.feat_folder,\n",
    "                                     self.file_prefix + key + self.file_ext)\n",
    "            if not os.path.exists(feat_file):\n",
    "                continue\n",
    "\n",
    "            # get fps if available\n",
    "            if self.default_fps is not None:\n",
    "                fps = self.default_fps\n",
    "            elif 'fps' in value:\n",
    "                fps = value['fps']\n",
    "            else:\n",
    "                assert False, \"Unknown video FPS.\"\n",
    "\n",
    "            # get video duration if available\n",
    "            if 'duration' in value:\n",
    "                duration = value['duration']\n",
    "            else:\n",
    "                duration = 1e8\n",
    "\n",
    "            # get annotations if available\n",
    "            if ('annotations' in value) and (len(value['annotations']) > 0):\n",
    "                # a fun fact of THUMOS: cliffdiving (4) is a subset of diving (7)\n",
    "                # our code can now handle this corner case\n",
    "                segments, labels = [], []\n",
    "                for act in value['annotations']:\n",
    "                    segments.append(act['segment'])\n",
    "                    labels.append([label_dict[act['label']]])\n",
    "\n",
    "                segments = np.asarray(segments, dtype=np.float32)\n",
    "                labels = np.squeeze(np.asarray(labels, dtype=np.int64), axis=1)\n",
    "            else:\n",
    "                segments = None\n",
    "                labels = None\n",
    "            dict_db += ({'id': key,\n",
    "                         'fps' : fps,\n",
    "                         'duration' : duration,\n",
    "                         'segments' : segments,\n",
    "                         'labels' : labels\n",
    "            }, )\n",
    "\n",
    "        return dict_db, label_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # directly return a (truncated) data point (so it is very fast!)\n",
    "        # auto batching will be disabled in the subsequent dataloader\n",
    "        # instead the model will need to decide how to batch / preporcess the data\n",
    "        video_item = self.data_list[idx]\n",
    "\n",
    "        # load features\n",
    "        filename = os.path.join(self.feat_folder,\n",
    "                                self.file_prefix + video_item['id'] + self.file_ext)\n",
    "        feats = np.load(filename).astype(np.float32)\n",
    "\n",
    "        # deal with downsampling (= increased feat stride)\n",
    "        feats = feats[::self.downsample_rate, :]\n",
    "        feat_stride = self.feat_stride * self.downsample_rate\n",
    "        feat_offset = 0.5 * self.num_frames / feat_stride\n",
    "        # T x C -> C x T\n",
    "        feats = torch.from_numpy(np.ascontiguousarray(feats.transpose()))\n",
    "\n",
    "        # convert time stamp (in second) into temporal feature grids\n",
    "        # ok to have small negative values here\n",
    "        if video_item['segments'] is not None:\n",
    "            segments = torch.from_numpy(\n",
    "                video_item['segments'] * video_item['fps'] / feat_stride - feat_offset\n",
    "            )\n",
    "            labels = torch.from_numpy(video_item['labels'])\n",
    "        else:\n",
    "            segments, labels = None, None\n",
    "\n",
    "        # return a data dict\n",
    "        data_dict = {'video_id'        : video_item['id'],\n",
    "                     'feats'           : feats,      # C x T\n",
    "                     'segments'        : segments,   # N x 2\n",
    "                     'labels'          : labels,     # N\n",
    "                     'fps'             : video_item['fps'],\n",
    "                     'duration'        : video_item['duration'],\n",
    "                     'feat_stride'     : feat_stride,\n",
    "                     'feat_num_frames' : self.num_frames}\n",
    "\n",
    "        # truncate the features during training\n",
    "        if self.is_training and (segments is not None):\n",
    "            data_dict = truncate_feats(\n",
    "                data_dict, self.max_seq_len, self.trunc_thresh, feat_offset, self.crop_ratio\n",
    "            )\n",
    "\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "23faa37c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:57.911827Z",
     "iopub.status.busy": "2025-09-30T08:45:57.911638Z",
     "iopub.status.idle": "2025-09-30T08:45:58.062172Z",
     "shell.execute_reply": "2025-09-30T08:45:58.061495Z",
     "shell.execute_reply.started": "2025-09-30T08:45:57.911813Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"2. create dataset / dataloader directly\"\"\"\n",
    "train_dataset = THUMOS14Dataset(\n",
    "    is_training=True,\n",
    "    split=train_split,\n",
    "    feat_folder=feat_folder,\n",
    "    json_file=json_file,\n",
    "    feat_stride=feat_stride,\n",
    "    num_frames=num_frames,\n",
    "    default_fps=default_fps,\n",
    "    downsample_rate=downsample_rate,\n",
    "    max_seq_len=max_seq_len,\n",
    "    trunc_thresh=trunc_thresh,\n",
    "    crop_ratio=crop_ratio,\n",
    "    input_dim=input_dim,\n",
    "    num_classes=num_classes,\n",
    "    file_prefix=file_prefix,\n",
    "    file_ext=file_ext,\n",
    "    force_upsampling=force_upsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "54f5b645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.063108Z",
     "iopub.status.busy": "2025-09-30T08:45:58.062887Z",
     "iopub.status.idle": "2025-09-30T08:45:58.066605Z",
     "shell.execute_reply": "2025-09-30T08:45:58.065935Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.063089Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# update head_empty_cls based on dataset attributes\n",
    "train_db_vars = train_dataset.get_attributes()\n",
    "head_empty_cls = train_db_vars['empty_label_ids']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c63ffcb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.067430Z",
     "iopub.status.busy": "2025-09-30T08:45:58.067212Z",
     "iopub.status.idle": "2025-09-30T08:45:58.078145Z",
     "shell.execute_reply": "2025-09-30T08:45:58.077435Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.067411Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def trivial_batch_collator(batch):\n",
    "    \"\"\"\n",
    "        A batch collator that does nothing\n",
    "    \"\"\"\n",
    "    return batch\n",
    "\n",
    "def worker_init_reset_seed(worker_id):\n",
    "    \"\"\"\n",
    "        Reset random seed for each worker\n",
    "    \"\"\"\n",
    "    seed = torch.initial_seed() % 2 ** 31\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "is_training = True\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=trivial_batch_collator,\n",
    "    worker_init_fn=(worker_init_reset_seed if is_training else None),\n",
    "    shuffle=is_training,\n",
    "    drop_last=is_training,\n",
    "    generator=rng_generator,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6de2cad1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.080731Z",
     "iopub.status.busy": "2025-09-30T08:45:58.080505Z",
     "iopub.status.idle": "2025-09-30T08:45:58.087028Z",
     "shell.execute_reply": "2025-09-30T08:45:58.086489Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.080712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create train_cfg and test_cfg dictionaries\n",
    "train_cfg = {\n",
    "    'center_sample': center_sample,\n",
    "    'center_sample_radius': center_sample_radius,\n",
    "    'loss_weight': loss_weight,\n",
    "    'cls_prior_prob': cls_prior_prob,\n",
    "    'init_loss_norm': init_loss_norm,\n",
    "    'clip_grad_l2norm': clip_grad_l2norm,\n",
    "    'head_empty_cls': head_empty_cls,\n",
    "    'dropout': dropout,\n",
    "    'droppath': droppath,\n",
    "    'label_smoothing': label_smoothing\n",
    "}\n",
    "\n",
    "test_cfg = {\n",
    "    'pre_nms_thresh': pre_nms_thresh,\n",
    "    'pre_nms_topk': pre_nms_topk,\n",
    "    'iou_threshold': iou_threshold,\n",
    "    'min_score': min_score,\n",
    "    'max_seg_num': max_seg_num,\n",
    "    'nms_method': nms_method,\n",
    "    'nms_sigma': nms_sigma,\n",
    "    'duration_thresh': duration_thresh,\n",
    "    'multiclass_nms': multiclass_nms,\n",
    "    'ext_score_file': ext_score_file,\n",
    "    'voting_thresh': voting_thresh\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b2d23f6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.088031Z",
     "iopub.status.busy": "2025-09-30T08:45:58.087737Z",
     "iopub.status.idle": "2025-09-30T08:45:58.099564Z",
     "shell.execute_reply": "2025-09-30T08:45:58.098868Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.088015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple conv block similar to the basic block used in ResNet\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embd,                # dimension of the input features\n",
    "        kernel_size=3,         # conv kernel size\n",
    "        n_ds_stride=1,         # downsampling stride for the current layer\n",
    "        expansion_factor=2,    # expansion factor of feat dims\n",
    "        n_out=None,            # output dimension, if None, set to input dim\n",
    "        act_layer=nn.ReLU,     # nonlinear activation used after conv, default ReLU\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # must use odd sized kernel\n",
    "        assert (kernel_size % 2 == 1) and (kernel_size > 1)\n",
    "        padding = kernel_size // 2\n",
    "        if n_out is None:\n",
    "            n_out = n_embd\n",
    "\n",
    "         # 1x3 (strided) -> 1x3 (basic block in resnet)\n",
    "        width = n_embd * expansion_factor\n",
    "        self.conv1 = MaskedConv1D(\n",
    "            n_embd, width, kernel_size, n_ds_stride, padding=padding)\n",
    "        self.conv2 = MaskedConv1D(\n",
    "            width, n_out, kernel_size, 1, padding=padding)\n",
    "\n",
    "        # attach downsampling conv op\n",
    "        if n_ds_stride > 1:\n",
    "            # 1x1 strided conv (same as resnet)\n",
    "            self.downsample = MaskedConv1D(n_embd, n_out, 1, n_ds_stride)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "        self.act = act_layer()\n",
    "\n",
    "    def forward(self, x, mask, pos_embd=None):\n",
    "        identity = x\n",
    "        out, out_mask = self.conv1(x, mask)\n",
    "        out = self.act(out)\n",
    "        out, out_mask = self.conv2(out, out_mask)\n",
    "\n",
    "        # downsampling\n",
    "        if self.downsample is not None:\n",
    "            identity, _ = self.downsample(x, mask)\n",
    "\n",
    "        # residual connection\n",
    "        out += identity\n",
    "        out = self.act(out)\n",
    "\n",
    "        return out, out_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "76edede3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.100527Z",
     "iopub.status.busy": "2025-09-30T08:45:58.100274Z",
     "iopub.status.idle": "2025-09-30T08:45:58.111893Z",
     "shell.execute_reply": "2025-09-30T08:45:58.111175Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.100505Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0079dfaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.112817Z",
     "iopub.status.busy": "2025-09-30T08:45:58.112624Z",
     "iopub.status.idle": "2025-09-30T08:45:58.127252Z",
     "shell.execute_reply": "2025-09-30T08:45:58.126624Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.112802Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ConvTransformerBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "        A backbone that combines convolutions with transformers\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in,                  # input feature dimension\n",
    "        n_embd,                # embedding dimension (after convolution)\n",
    "        n_head,                # number of head for self-attention in transformers\n",
    "        n_embd_ks,             # conv kernel size of the embedding network\n",
    "        max_len,               # max sequence length\n",
    "        arch = (2, 2, 5),      # (#convs, #stem transformers, #branch transformers)\n",
    "        mha_win_size = [-1]*6, # size of local window for mha\n",
    "        scale_factor = 2,      # dowsampling rate for the branch\n",
    "        with_ln = False,       # if to attach layernorm after conv\n",
    "        attn_pdrop = 0.0,      # dropout rate for the attention map\n",
    "        proj_pdrop = 0.0,      # dropout rate for the projection / MLP\n",
    "        path_pdrop = 0.0,      # droput rate for drop path\n",
    "        use_abs_pe = False,    # use absolute position embedding\n",
    "        use_rel_pe = False,    # use relative position embedding\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(arch) == 3\n",
    "        assert len(mha_win_size) == (1 + arch[2])\n",
    "        self.n_in = n_in\n",
    "        self.arch = arch\n",
    "        self.mha_win_size = mha_win_size\n",
    "        self.max_len = max_len\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.scale_factor = scale_factor\n",
    "        self.use_abs_pe = use_abs_pe\n",
    "        self.use_rel_pe = use_rel_pe\n",
    "\n",
    "        # feature projection\n",
    "        self.n_in = n_in\n",
    "        if isinstance(n_in, (list, tuple)):\n",
    "            assert isinstance(n_embd, (list, tuple)) and len(n_in) == len(n_embd)\n",
    "            self.proj = nn.ModuleList([\n",
    "                MaskedConv1D(c0, c1, 1) for c0, c1 in zip(n_in, n_embd)\n",
    "            ])\n",
    "            n_in = n_embd = sum(n_embd)\n",
    "        else:\n",
    "            self.proj = None\n",
    "\n",
    "        # embedding network using convs\n",
    "        self.embd = nn.ModuleList()\n",
    "        self.embd_norm = nn.ModuleList()\n",
    "        for idx in range(arch[0]):\n",
    "            n_in = n_embd if idx > 0 else n_in\n",
    "            self.embd.append(\n",
    "                MaskedConv1D(\n",
    "                    n_in, n_embd, n_embd_ks,\n",
    "                    stride=1, padding=n_embd_ks//2, bias=(not with_ln)\n",
    "                )\n",
    "            )\n",
    "            if with_ln:\n",
    "                self.embd_norm.append(LayerNorm(n_embd))\n",
    "            else:\n",
    "                self.embd_norm.append(nn.Identity())\n",
    "\n",
    "        # position embedding (1, C, T), rescaled by 1/sqrt(n_embd)\n",
    "        if self.use_abs_pe:\n",
    "            pos_embd = get_sinusoid_encoding(self.max_len, n_embd) / (n_embd**0.5)\n",
    "            self.register_buffer(\"pos_embd\", pos_embd, persistent=False)\n",
    "\n",
    "        # stem network using (vanilla) transformer\n",
    "        self.stem = nn.ModuleList()\n",
    "        for idx in range(arch[1]):\n",
    "            self.stem.append(\n",
    "                TransformerBlock(\n",
    "                    n_embd, n_head,\n",
    "                    n_ds_strides=(1, 1),\n",
    "                    attn_pdrop=attn_pdrop,\n",
    "                    proj_pdrop=proj_pdrop,\n",
    "                    path_pdrop=path_pdrop,\n",
    "                    mha_win_size=self.mha_win_size[0],\n",
    "                    use_rel_pe=self.use_rel_pe\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # main branch using transformer with pooling\n",
    "        self.branch = nn.ModuleList()\n",
    "        for idx in range(arch[2]):\n",
    "            self.branch.append(\n",
    "                TransformerBlock(\n",
    "                    n_embd, n_head,\n",
    "                    n_ds_strides=(self.scale_factor, self.scale_factor),\n",
    "                    attn_pdrop=attn_pdrop,\n",
    "                    proj_pdrop=proj_pdrop,\n",
    "                    path_pdrop=path_pdrop,\n",
    "                    mha_win_size=self.mha_win_size[1 + idx],\n",
    "                    use_rel_pe=self.use_rel_pe\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # init weights\n",
    "        self.apply(self.__init_weights__)\n",
    "\n",
    "    def __init_weights__(self, module):\n",
    "        # set nn.Linear/nn.Conv1d bias term to 0\n",
    "        if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.constant_(module.bias, 0.)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x: batch size, feature channel, sequence length,\n",
    "        # mask: batch size, 1, sequence length (bool)\n",
    "        B, C, T = x.size()\n",
    "\n",
    "        # feature projection\n",
    "        if isinstance(self.n_in, (list, tuple)):\n",
    "            x = torch.cat(\n",
    "                [proj(s, mask)[0] \\\n",
    "                    for proj, s in zip(self.proj, x.split(self.n_in, dim=1))\n",
    "                ], dim=1\n",
    "            )\n",
    "\n",
    "        # embedding network\n",
    "        for idx in range(len(self.embd)):\n",
    "            x, mask = self.embd[idx](x, mask)\n",
    "            x = self.relu(self.embd_norm[idx](x))\n",
    "\n",
    "        # training: using fixed length position embeddings\n",
    "        if self.use_abs_pe and self.training:\n",
    "            assert T <= self.max_len, \"Reached max length.\"\n",
    "            pe = self.pos_embd\n",
    "            # add pe to x\n",
    "            x = x + pe[:, :, :T] * mask.to(x.dtype)\n",
    "\n",
    "        # inference: re-interpolate position embeddings for over-length sequences\n",
    "        if self.use_abs_pe and (not self.training):\n",
    "            if T >= self.max_len:\n",
    "                pe = F.interpolate(\n",
    "                    self.pos_embd, T, mode='linear', align_corners=False)\n",
    "            else:\n",
    "                pe = self.pos_embd\n",
    "            # add pe to x\n",
    "            x = x + pe[:, :, :T] * mask.to(x.dtype)\n",
    "\n",
    "        # stem transformer\n",
    "        for idx in range(len(self.stem)):\n",
    "            x, mask = self.stem[idx](x, mask)\n",
    "\n",
    "        # prep for outputs\n",
    "        out_feats = (x, )\n",
    "        out_masks = (mask, )\n",
    "\n",
    "        # main branch with downsampling\n",
    "        for idx in range(len(self.branch)):\n",
    "            x, mask = self.branch[idx](x, mask)\n",
    "            out_feats += (x, )\n",
    "            out_masks += (mask, )\n",
    "\n",
    "        return out_feats, out_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1abba785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.128253Z",
     "iopub.status.busy": "2025-09-30T08:45:58.128038Z",
     "iopub.status.idle": "2025-09-30T08:45:58.145577Z",
     "shell.execute_reply": "2025-09-30T08:45:58.144906Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.128230Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ConvBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "        A backbone that with only conv\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in,               # input feature dimension\n",
    "        n_embd,             # embedding dimension (after convolution)\n",
    "        n_embd_ks,          # conv kernel size of the embedding network\n",
    "        arch = (2, 2, 5),   # (#convs, #stem convs, #branch convs)\n",
    "        scale_factor = 2,   # dowsampling rate for the branch\n",
    "        with_ln=False,      # if to use layernorm\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(arch) == 3\n",
    "        self.n_in = n_in\n",
    "        self.arch = arch\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "        # feature projection\n",
    "        self.n_in = n_in\n",
    "        if isinstance(n_in, (list, tuple)):\n",
    "            assert isinstance(n_embd, (list, tuple)) and len(n_in) == len(n_embd)\n",
    "            self.proj = nn.ModuleList([\n",
    "                MaskedConv1D(c0, c1, 1) for c0, c1 in zip(n_in, n_embd)\n",
    "            ])\n",
    "            n_in = n_embd = sum(n_embd)\n",
    "        else:\n",
    "            self.proj = None\n",
    "\n",
    "        # embedding network using convs\n",
    "        self.embd = nn.ModuleList()\n",
    "        self.embd_norm = nn.ModuleList()\n",
    "        for idx in range(arch[0]):\n",
    "            n_in = n_embd if idx > 0 else n_in\n",
    "            self.embd.append(\n",
    "                MaskedConv1D(\n",
    "                    n_in, n_embd, n_embd_ks,\n",
    "                    stride=1, padding=n_embd_ks//2, bias=(not with_ln)\n",
    "                )\n",
    "            )\n",
    "            if with_ln:\n",
    "                self.embd_norm.append(LayerNorm(n_embd))\n",
    "            else:\n",
    "                self.embd_norm.append(nn.Identity())\n",
    "\n",
    "        # stem network using convs\n",
    "        self.stem = nn.ModuleList()\n",
    "        for idx in range(arch[1]):\n",
    "            self.stem.append(ConvBlock(n_embd, 3, 1))\n",
    "\n",
    "        # main branch using convs with pooling\n",
    "        self.branch = nn.ModuleList()\n",
    "        for idx in range(arch[2]):\n",
    "            self.branch.append(ConvBlock(n_embd, 3, self.scale_factor))\n",
    "\n",
    "        # init weights\n",
    "        self.apply(self.__init_weights__)\n",
    "\n",
    "    def __init_weights__(self, module):\n",
    "        # set nn.Linear bias term to 0\n",
    "        if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.constant_(module.bias, 0.)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x: batch size, feature channel, sequence length,\n",
    "        # mask: batch size, 1, sequence length (bool)\n",
    "        B, C, T = x.size()\n",
    "\n",
    "        # feature projection\n",
    "        if isinstance(self.n_in, (list, tuple)):\n",
    "            x = torch.cat(\n",
    "                [proj(s, mask)[0] \\\n",
    "                    for proj, s in zip(self.proj, x.split(self.n_in, dim=1))\n",
    "                ], dim=1\n",
    "            )\n",
    "\n",
    "        # embedding network\n",
    "        for idx in range(len(self.embd)):\n",
    "            x, mask = self.embd[idx](x, mask)\n",
    "            x = self.relu(self.embd_norm[idx](x))\n",
    "\n",
    "        # stem conv\n",
    "        for idx in range(len(self.stem)):\n",
    "            x, mask = self.stem[idx](x, mask)\n",
    "\n",
    "        # prep for outputs\n",
    "        out_feats = (x, )\n",
    "        out_masks = (mask, )\n",
    "\n",
    "        # main branch with downsampling\n",
    "        for idx in range(len(self.branch)):\n",
    "            x, mask = self.branch[idx](x, mask)\n",
    "            out_feats += (x, )\n",
    "            out_masks += (mask, )\n",
    "\n",
    "        return out_feats, out_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f10bef55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.146492Z",
     "iopub.status.busy": "2025-09-30T08:45:58.146261Z",
     "iopub.status.idle": "2025-09-30T08:45:58.157638Z",
     "shell.execute_reply": "2025-09-30T08:45:58.156910Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.146470Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#necks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "75a3a7fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.158535Z",
     "iopub.status.busy": "2025-09-30T08:45:58.158321Z",
     "iopub.status.idle": "2025-09-30T08:45:58.169605Z",
     "shell.execute_reply": "2025-09-30T08:45:58.168900Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.158521Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FPNIdentity(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,      # input feature channels, len(in_channels) = #levels\n",
    "        out_channel,      # output feature channel\n",
    "        scale_factor=2.0, # downsampling rate between two fpn levels\n",
    "        start_level=0,    # start fpn level\n",
    "        end_level=-1,     # end fpn level\n",
    "        with_ln=True,     # if to apply layer norm at the end\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channel = out_channel\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "        self.start_level = start_level\n",
    "        if end_level == -1:\n",
    "            self.end_level = len(in_channels)\n",
    "        else:\n",
    "            self.end_level = end_level\n",
    "        assert self.end_level <= len(in_channels)\n",
    "        assert (self.start_level >= 0) and (self.start_level < self.end_level)\n",
    "\n",
    "        self.fpn_norms = nn.ModuleList()\n",
    "        for i in range(self.start_level, self.end_level):\n",
    "            # check feat dims\n",
    "            assert self.in_channels[i] == self.out_channel\n",
    "            # layer norm for order (B C T)\n",
    "            if with_ln:\n",
    "                fpn_norm = LayerNorm(out_channel)\n",
    "            else:\n",
    "                fpn_norm = nn.Identity()\n",
    "            self.fpn_norms.append(fpn_norm)\n",
    "\n",
    "    def forward(self, inputs, fpn_masks):\n",
    "        # inputs must be a list / tuple\n",
    "        assert len(inputs) == len(self.in_channels)\n",
    "        assert len(fpn_masks) ==  len(self.in_channels)\n",
    "\n",
    "        # apply norms, fpn_masks will remain the same with 1x1 convs\n",
    "        fpn_feats = tuple()\n",
    "        new_fpn_masks = tuple()\n",
    "        for i in range(len(self.fpn_norms)):\n",
    "            x = self.fpn_norms[i](inputs[i + self.start_level])\n",
    "            fpn_feats += (x, )\n",
    "            new_fpn_masks += (fpn_masks[i + self.start_level], )\n",
    "\n",
    "        return fpn_feats, new_fpn_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f6a548f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.170490Z",
     "iopub.status.busy": "2025-09-30T08:45:58.170246Z",
     "iopub.status.idle": "2025-09-30T08:45:58.183987Z",
     "shell.execute_reply": "2025-09-30T08:45:58.183451Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.170451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FPN1D(nn.Module):\n",
    "    \"\"\"\n",
    "        Feature pyramid network\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,      # input feature channels, len(in_channels) = # levels\n",
    "        out_channel,      # output feature channel\n",
    "        scale_factor=2.0, # downsampling rate between two fpn levels\n",
    "        start_level=0,    # start fpn level\n",
    "        end_level=-1,     # end fpn level\n",
    "        with_ln=True,     # if to apply layer norm at the end\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert isinstance(in_channels, list) or isinstance(in_channels, tuple)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channel = out_channel\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "        self.start_level = start_level\n",
    "        if end_level == -1:\n",
    "            self.end_level = len(in_channels)\n",
    "        else:\n",
    "            self.end_level = end_level\n",
    "        assert self.end_level <= len(in_channels)\n",
    "        assert (self.start_level >= 0) and (self.start_level < self.end_level)\n",
    "\n",
    "        self.lateral_convs = nn.ModuleList()\n",
    "        self.fpn_convs = nn.ModuleList()\n",
    "        self.fpn_norms = nn.ModuleList()\n",
    "        for i in range(self.start_level, self.end_level):\n",
    "            # disable bias if using layer norm\n",
    "            l_conv = MaskedConv1D(\n",
    "                in_channels[i], out_channel, 1, bias=(not with_ln)\n",
    "            )\n",
    "            # use depthwise conv here for efficiency\n",
    "            fpn_conv = MaskedConv1D(\n",
    "                out_channel, out_channel, 3,\n",
    "                padding=1, bias=(not with_ln), groups=out_channel\n",
    "            )\n",
    "            # layer norm for order (B C T)\n",
    "            if with_ln:\n",
    "                fpn_norm = LayerNorm(out_channel)\n",
    "            else:\n",
    "                fpn_norm = nn.Identity()\n",
    "\n",
    "            self.lateral_convs.append(l_conv)\n",
    "            self.fpn_convs.append(fpn_conv)\n",
    "            self.fpn_norms.append(fpn_norm)\n",
    "\n",
    "    def forward(self, inputs, fpn_masks):\n",
    "        # inputs must be a list / tuple\n",
    "        assert len(inputs) == len(self.in_channels)\n",
    "        assert len(fpn_masks) ==  len(self.in_channels)\n",
    "\n",
    "        # build laterals, fpn_masks will remain the same with 1x1 convs\n",
    "        laterals = []\n",
    "        for i in range(len(self.lateral_convs)):\n",
    "            x, _ = self.lateral_convs[i](\n",
    "                inputs[i + self.start_level], fpn_masks[i + self.start_level]\n",
    "            )\n",
    "            laterals.append(x)\n",
    "\n",
    "        # build top-down path\n",
    "        used_backbone_levels = len(laterals)\n",
    "        for i in range(used_backbone_levels - 1, 0, -1):\n",
    "            laterals[i - 1] += F.interpolate(\n",
    "                laterals[i], scale_factor=self.scale_factor, mode='nearest'\n",
    "            )\n",
    "\n",
    "        # fpn conv / norm -> outputs\n",
    "        # mask will remain the same\n",
    "        fpn_feats = tuple()\n",
    "        new_fpn_masks = tuple()\n",
    "        for i in range(used_backbone_levels):\n",
    "            x, new_mask = self.fpn_convs[i](\n",
    "                laterals[i], fpn_masks[i + self.start_level])\n",
    "            x = self.fpn_norms[i](x)\n",
    "            fpn_feats += (x, )\n",
    "            new_fpn_masks += (new_mask, )\n",
    "\n",
    "        return fpn_feats, new_fpn_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b8f9c410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.184866Z",
     "iopub.status.busy": "2025-09-30T08:45:58.184638Z",
     "iopub.status.idle": "2025-09-30T08:45:58.196164Z",
     "shell.execute_reply": "2025-09-30T08:45:58.195605Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.184844Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9025dd99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.198609Z",
     "iopub.status.busy": "2025-09-30T08:45:58.198119Z",
     "iopub.status.idle": "2025-09-30T08:45:58.211130Z",
     "shell.execute_reply": "2025-09-30T08:45:58.210432Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.198591Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PointGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "        A generator for temporal \"points\"\n",
    "        \n",
    "        max_seq_len can be much larger than the actual seq length\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_seq_len,        # max sequence length that the generator will buffer\n",
    "        fpn_strides,        # strides of fpn levels\n",
    "        regression_range,   # regression range (on feature grids)\n",
    "        use_offset=False    # if to align the points at grid centers\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # sanity check, # fpn levels and length divisible\n",
    "        fpn_levels = len(fpn_strides)\n",
    "        assert len(regression_range) == fpn_levels\n",
    "\n",
    "        # save params\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.fpn_levels = fpn_levels\n",
    "        self.fpn_strides = fpn_strides\n",
    "        self.regression_range = regression_range\n",
    "        self.use_offset = use_offset\n",
    "\n",
    "        # generate all points and buffer the list\n",
    "        self.buffer_points = self._generate_points()\n",
    "\n",
    "    def _generate_points(self):\n",
    "        points_list = []\n",
    "        # loop over all points at each pyramid level\n",
    "        for l, stride in enumerate(self.fpn_strides):\n",
    "            reg_range = torch.as_tensor(\n",
    "                self.regression_range[l], dtype=torch.float)\n",
    "            fpn_stride = torch.as_tensor(stride, dtype=torch.float)\n",
    "            points = torch.arange(0, self.max_seq_len, stride)[:, None]\n",
    "            # add offset if necessary (not in our current model)\n",
    "            if self.use_offset:\n",
    "                points += 0.5 * stride\n",
    "            # pad the time stamp with additional regression range / stride\n",
    "            reg_range = reg_range[None].repeat(points.shape[0], 1)\n",
    "            fpn_stride = fpn_stride[None].repeat(points.shape[0], 1)\n",
    "            # size: T x 4 (ts, reg_range, stride)\n",
    "            points_list.append(torch.cat((points, reg_range, fpn_stride), dim=1))\n",
    "\n",
    "        return BufferList(points_list)\n",
    "\n",
    "    def forward(self, feats):\n",
    "        # feats will be a list of torch tensors\n",
    "        assert len(feats) == self.fpn_levels\n",
    "        pts_list = []\n",
    "        feat_lens = [feat.shape[-1] for feat in feats]\n",
    "        for feat_len, buffer_pts in zip(feat_lens, self.buffer_points):\n",
    "            assert feat_len <= buffer_pts.shape[0], \"Reached max buffer length for point generator\"\n",
    "            pts = buffer_pts[:feat_len, :]\n",
    "            pts_list.append(pts)\n",
    "        return pts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ce21828d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.212078Z",
     "iopub.status.busy": "2025-09-30T08:45:58.211816Z",
     "iopub.status.idle": "2025-09-30T08:45:58.224895Z",
     "shell.execute_reply": "2025-09-30T08:45:58.224247Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.212058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bc491161-8fd7-4eed-8d9c-e4a50649270d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.225781Z",
     "iopub.status.busy": "2025-09-30T08:45:58.225562Z",
     "iopub.status.idle": "2025-09-30T08:45:58.251573Z",
     "shell.execute_reply": "2025-09-30T08:45:58.250976Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.225760Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#nms cpp version alternative\n",
    "def nms_1d_cpu(segs, scores, iou_threshold):\n",
    "    \"\"\"\n",
    "    Pure Python implementation of 1D NMS matching the C++ version exactly\n",
    "    \n",
    "    Args:\n",
    "        segs: tensor of shape (N, 2) representing segments [start, end]\n",
    "        scores: tensor of shape (N,) representing confidence scores\n",
    "        iou_threshold: IoU threshold for NMS\n",
    "    \n",
    "    Returns:\n",
    "        indices of segments to keep\n",
    "    \"\"\"\n",
    "    if segs.numel() == 0:\n",
    "        return torch.empty((0,), dtype=torch.long, device=segs.device)\n",
    "    \n",
    "    # Extract x1, x2 coordinates\n",
    "    x1_t = segs[:, 0].contiguous()\n",
    "    x2_t = segs[:, 1].contiguous()\n",
    "    \n",
    "    # Compute areas\n",
    "    areas_t = x2_t - x1_t + 1e-6\n",
    "    \n",
    "    # Sort scores in descending order\n",
    "    order_t = torch.argsort(scores, descending=True)\n",
    "    \n",
    "    nsegs = segs.size(0)\n",
    "    select_t = torch.ones(nsegs, dtype=torch.bool, device=segs.device)\n",
    "    \n",
    "    # Convert to numpy-like access for easier translation\n",
    "    select = select_t\n",
    "    order = order_t\n",
    "    x1 = x1_t\n",
    "    x2 = x2_t\n",
    "    areas = areas_t\n",
    "    \n",
    "    # Main NMS loop - direct translation from C++\n",
    "    for _i in range(nsegs):\n",
    "        if not select[_i]:\n",
    "            continue\n",
    "        \n",
    "        i = order[_i]\n",
    "        ix1 = x1[i]\n",
    "        ix2 = x2[i]\n",
    "        iarea = areas[i]\n",
    "        \n",
    "        for _j in range(_i + 1, nsegs):\n",
    "            if not select[_j]:\n",
    "                continue\n",
    "            \n",
    "            j = order[_j]\n",
    "            xx1 = torch.max(ix1, x1[j])\n",
    "            xx2 = torch.min(ix2, x2[j])\n",
    "            \n",
    "            inter = torch.max(torch.tensor(0.0, device=segs.device), xx2 - xx1)\n",
    "            ovr = inter / (iarea + areas[j] - inter)\n",
    "            \n",
    "            if ovr >= iou_threshold:\n",
    "                select[_j] = False\n",
    "    \n",
    "    return order_t[select_t]\n",
    "\n",
    "def softnms_1d_cpu(segs, scores, dets, iou_threshold, sigma, min_score, method):\n",
    "    \"\"\"\n",
    "    Pure Python implementation of 1D Soft NMS matching the C++ version exactly\n",
    "    \n",
    "    Args:\n",
    "        segs: tensor of shape (N, 2) representing segments [start, end]\n",
    "        scores: tensor of shape (N,) representing confidence scores\n",
    "        dets: tensor of shape (N, 3) to store results [x1, x2, score]\n",
    "        iou_threshold: IoU threshold for NMS\n",
    "        sigma: Gaussian sigma for soft NMS\n",
    "        min_score: minimum score threshold\n",
    "        method: 0=vanilla NMS, 1=linear, 2=gaussian\n",
    "    \n",
    "    Returns:\n",
    "        indices of segments to keep\n",
    "    \"\"\"\n",
    "    if segs.numel() == 0:\n",
    "        return torch.empty((0,), dtype=torch.long, device=segs.device)\n",
    "    \n",
    "    # Extract coordinates and clone scores\n",
    "    x1_t = segs[:, 0].contiguous()\n",
    "    x2_t = segs[:, 1].contiguous()\n",
    "    scores_t = scores.clone()\n",
    "    \n",
    "    # Compute areas\n",
    "    areas_t = x1_t - x1_t + 1e-6  # Initialize, will be computed properly\n",
    "    areas_t = x2_t - x1_t + 1e-6\n",
    "    \n",
    "    nsegs = segs.size(0)\n",
    "    \n",
    "    # Create indices tensor\n",
    "    inds_t = torch.arange(nsegs, dtype=torch.long, device=segs.device)\n",
    "    \n",
    "    # Work with cloned tensors to allow in-place modifications\n",
    "    x1 = x1_t.clone()\n",
    "    x2 = x2_t.clone()\n",
    "    sc = scores_t\n",
    "    areas = areas_t.clone()\n",
    "    inds = inds_t.clone()\n",
    "    \n",
    "    # Main soft NMS loop - direct translation from C++\n",
    "    for i in range(nsegs):\n",
    "        max_score = sc[i]\n",
    "        max_pos = i\n",
    "        \n",
    "        # Find segment with max score\n",
    "        for pos in range(i + 1, nsegs):\n",
    "            if max_score < sc[pos]:\n",
    "                max_score = sc[pos]\n",
    "                max_pos = pos\n",
    "        \n",
    "        # Swap current segment (i) with max score segment (max_pos)\n",
    "        # Store in dets\n",
    "        ix1 = x1[max_pos].clone()\n",
    "        ix2 = x2[max_pos].clone()\n",
    "        iscore = sc[max_pos].clone()\n",
    "        iarea = areas[max_pos].clone()\n",
    "        iind = inds[max_pos].clone()\n",
    "        \n",
    "        dets[i, 0] = ix1\n",
    "        dets[i, 1] = ix2\n",
    "        dets[i, 2] = iscore\n",
    "        \n",
    "        # Swap elements\n",
    "        x1[max_pos] = x1[i]\n",
    "        x2[max_pos] = x2[i]\n",
    "        sc[max_pos] = sc[i]\n",
    "        areas[max_pos] = areas[i]\n",
    "        inds[max_pos] = inds[i]\n",
    "        \n",
    "        x1[i] = ix1\n",
    "        x2[i] = ix2\n",
    "        sc[i] = iscore\n",
    "        areas[i] = iarea\n",
    "        inds[i] = iind\n",
    "        \n",
    "        # Apply soft NMS to remaining segments\n",
    "        pos = i + 1\n",
    "        while pos < nsegs:\n",
    "            xx1 = torch.max(ix1, x1[pos])\n",
    "            xx2 = torch.min(ix2, x2[pos])\n",
    "            \n",
    "            inter = torch.max(torch.tensor(0.0, device=segs.device), xx2 - xx1)\n",
    "            ovr = inter / (iarea + areas[pos] - inter)\n",
    "            \n",
    "            # Compute weight based on method\n",
    "            weight = 1.0\n",
    "            if method == 0:  # vanilla NMS\n",
    "                if ovr >= iou_threshold:\n",
    "                    weight = 0.0\n",
    "            elif method == 1:  # linear\n",
    "                if ovr >= iou_threshold:\n",
    "                    weight = 1.0 - ovr\n",
    "            elif method == 2:  # gaussian\n",
    "                weight = torch.exp(-(ovr * ovr) / sigma)\n",
    "            \n",
    "            sc[pos] *= weight\n",
    "            \n",
    "            # Remove segments with low scores by swapping with last element\n",
    "            if sc[pos] < min_score:\n",
    "                x1[pos] = x1[nsegs - 1]\n",
    "                x2[pos] = x2[nsegs - 1]\n",
    "                sc[pos] = sc[nsegs - 1]\n",
    "                areas[pos] = areas[nsegs - 1]\n",
    "                inds[pos] = inds[nsegs - 1]\n",
    "                nsegs -= 1\n",
    "                pos -= 1\n",
    "            \n",
    "            pos += 1\n",
    "    \n",
    "    return inds_t[:nsegs]\n",
    "\n",
    "def nms_1d(segs, scores, iou_threshold):\n",
    "    \"\"\"\n",
    "    1D NMS interface matching the C++ version\n",
    "    \"\"\"\n",
    "    # Ensure tensors are on CPU and contiguous\n",
    "    if segs.device.type == 'cuda':\n",
    "        segs = segs.cpu()\n",
    "    if scores.device.type == 'cuda':\n",
    "        scores = scores.cpu()\n",
    "    \n",
    "    segs = segs.contiguous()\n",
    "    scores = scores.contiguous()\n",
    "    \n",
    "    return nms_1d_cpu(segs, scores, iou_threshold)\n",
    "\n",
    "def softnms_1d(segs, scores, dets, iou_threshold, sigma, min_score, method):\n",
    "    \"\"\"\n",
    "    1D Soft NMS interface matching the C++ version\n",
    "    \"\"\"\n",
    "    # Ensure tensors are on CPU and contiguous\n",
    "    if segs.device.type == 'cuda':\n",
    "        segs = segs.cpu()\n",
    "    if scores.device.type == 'cuda':\n",
    "        scores = scores.cpu()\n",
    "    if dets.device.type == 'cuda':\n",
    "        dets = dets.cpu()\n",
    "    \n",
    "    segs = segs.contiguous()\n",
    "    scores = scores.contiguous()\n",
    "    dets = dets.contiguous()\n",
    "    \n",
    "    return softnms_1d_cpu(segs, scores, dets, iou_threshold, sigma, min_score, method)\n",
    "\n",
    "# Alternative optimized versions using vectorized operations\n",
    "def nms_1d_vectorized(segs, scores, iou_threshold):\n",
    "    \"\"\"\n",
    "    Vectorized version of 1D NMS for better performance\n",
    "    \"\"\"\n",
    "    if segs.numel() == 0:\n",
    "        return torch.empty((0,), dtype=torch.long, device=segs.device)\n",
    "    \n",
    "    x1 = segs[:, 0]\n",
    "    x2 = segs[:, 1]\n",
    "    areas = x2 - x1 + 1e-6\n",
    "    \n",
    "    # Sort by scores in descending order\n",
    "    _, order = scores.sort(0, descending=True)\n",
    "    \n",
    "    keep = []\n",
    "    while order.numel() > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        \n",
    "        if order.numel() == 1:\n",
    "            break\n",
    "            \n",
    "        # Compute IoU with remaining segments\n",
    "        xx1 = torch.maximum(x1[i], x1[order[1:]])\n",
    "        xx2 = torch.minimum(x2[i], x2[order[1:]])\n",
    "        \n",
    "        inter = torch.clamp(xx2 - xx1, min=0.0)\n",
    "        union = areas[i] + areas[order[1:]] - inter\n",
    "        iou = inter / union\n",
    "        \n",
    "        # Keep segments with IoU below threshold\n",
    "        inds = torch.where(iou <= iou_threshold)[0]\n",
    "        order = order[inds + 1]\n",
    "    \n",
    "    return torch.tensor(keep, dtype=torch.long, device=segs.device)\n",
    "\n",
    "def softnms_1d_vectorized(segs, scores, iou_threshold, sigma=0.5, min_score=0.001, method=2):\n",
    "    \"\"\"\n",
    "    Vectorized version of 1D Soft NMS for better performance\n",
    "    \"\"\"\n",
    "    if segs.numel() == 0:\n",
    "        return torch.empty((0, 3), dtype=torch.float32, device=segs.device), torch.empty((0,), dtype=torch.long)\n",
    "    \n",
    "    x1 = segs[:, 0].clone()\n",
    "    x2 = segs[:, 1].clone()\n",
    "    scores_copy = scores.clone()\n",
    "    areas = x2 - x1 + 1e-6\n",
    "    \n",
    "    n_segs = segs.size(0)\n",
    "    indices = torch.arange(n_segs, dtype=torch.long, device=segs.device)\n",
    "    \n",
    "    dets = torch.zeros((n_segs, 3), dtype=torch.float32, device=segs.device)\n",
    "    \n",
    "    for i in range(n_segs):\n",
    "        # Find segment with maximum score\n",
    "        max_score, max_pos = scores_copy[i:].max(0)\n",
    "        max_pos = max_pos + i\n",
    "        \n",
    "        # Swap current segment with max score segment\n",
    "        if max_pos != i:\n",
    "            x1[i], x1[max_pos] = x1[max_pos].clone(), x1[i].clone()\n",
    "            x2[i], x2[max_pos] = x2[max_pos].clone(), x2[i].clone()\n",
    "            scores_copy[i], scores_copy[max_pos] = scores_copy[max_pos].clone(), scores_copy[i].clone()\n",
    "            areas[i], areas[max_pos] = areas[max_pos].clone(), areas[i].clone()\n",
    "            indices[i], indices[max_pos] = indices[max_pos].clone(), indices[i].clone()\n",
    "        \n",
    "        dets[i, 0] = x1[i]\n",
    "        dets[i, 1] = x2[i]\n",
    "        dets[i, 2] = scores_copy[i]\n",
    "        \n",
    "        # Apply soft NMS to remaining segments\n",
    "        if i < n_segs - 1:\n",
    "            xx1 = torch.maximum(x1[i], x1[i+1:])\n",
    "            xx2 = torch.minimum(x2[i], x2[i+1:])\n",
    "            \n",
    "            inter = torch.clamp(xx2 - xx1, min=0.0)\n",
    "            union = areas[i] + areas[i+1:] - inter\n",
    "            iou = inter / union\n",
    "            \n",
    "            # Apply weight based on method\n",
    "            if method == 0:  # vanilla NMS\n",
    "                weight = (iou < iou_threshold).float()\n",
    "            elif method == 1:  # linear\n",
    "                weight = torch.where(iou >= iou_threshold, 1 - iou, torch.ones_like(iou))\n",
    "            else:  # gaussian (method == 2)\n",
    "                weight = torch.exp(-(iou * iou) / sigma)\n",
    "            \n",
    "            scores_copy[i+1:] *= weight\n",
    "            \n",
    "            # Remove segments with score below threshold\n",
    "            keep_mask = scores_copy[i+1:] >= min_score\n",
    "            if not keep_mask.all():\n",
    "                valid_indices = torch.where(keep_mask)[0] + (i + 1)\n",
    "                remaining = len(valid_indices)\n",
    "                if remaining > 0:\n",
    "                    x1[i+1:i+1+remaining] = x1[valid_indices]\n",
    "                    x2[i+1:i+1+remaining] = x2[valid_indices]\n",
    "                    scores_copy[i+1:i+1+remaining] = scores_copy[valid_indices]\n",
    "                    areas[i+1:i+1+remaining] = areas[valid_indices]\n",
    "                    indices[i+1:i+1+remaining] = indices[valid_indices]\n",
    "                n_segs = i + 1 + remaining\n",
    "    \n",
    "    return dets[:n_segs], indices[:n_segs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883369b-4bff-49c4-a672-7b315e4298fd",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b8057f24-abcc-430d-bc40-7c0ef103aeb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.252367Z",
     "iopub.status.busy": "2025-09-30T08:45:58.252176Z",
     "iopub.status.idle": "2025-09-30T08:45:58.270962Z",
     "shell.execute_reply": "2025-09-30T08:45:58.270346Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.252349Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NMSop(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx, segs, scores, cls_idxs,\n",
    "        iou_threshold, min_score, max_num\n",
    "    ):\n",
    "        # vanilla nms will not change the score, so we can filter segs first\n",
    "        is_filtering_by_score = (min_score > 0)\n",
    "        if is_filtering_by_score:\n",
    "            valid_mask = scores > min_score\n",
    "            segs, scores = segs[valid_mask], scores[valid_mask]\n",
    "            cls_idxs = cls_idxs[valid_mask]\n",
    "            valid_inds = torch.nonzero(\n",
    "                valid_mask, as_tuple=False).squeeze(dim=1)\n",
    "\n",
    "        # nms op; return inds that is sorted by descending order\n",
    "        inds = nms_1d(segs, scores, iou_threshold)\n",
    "        # cap by max number\n",
    "        if max_num > 0:\n",
    "            inds = inds[:min(max_num, len(inds))]\n",
    "        # return the sorted segs / scores\n",
    "        sorted_segs = segs[inds]\n",
    "        sorted_scores = scores[inds]\n",
    "        sorted_cls_idxs = cls_idxs[inds]\n",
    "        return sorted_segs.clone(), sorted_scores.clone(), sorted_cls_idxs.clone()\n",
    "\n",
    "\n",
    "class SoftNMSop(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx, segs, scores, cls_idxs,\n",
    "        iou_threshold, sigma, min_score, method, max_num\n",
    "    ):\n",
    "        # pre allocate memory for sorted results\n",
    "        dets = segs.new_empty((segs.size(0), 3), device='cpu')\n",
    "        # softnms op, return dets that stores the sorted segs / scores\n",
    "        inds = softnms_1d(segs, scores, dets, iou_threshold, sigma, min_score, method)\n",
    "        # cap by max number\n",
    "        if max_num > 0:\n",
    "            n_segs = min(len(inds), max_num)\n",
    "        else:\n",
    "            n_segs = len(inds)\n",
    "        sorted_segs = dets[:n_segs, :2]\n",
    "        sorted_scores = dets[:n_segs, 2]\n",
    "        sorted_cls_idxs = cls_idxs[inds]\n",
    "        sorted_cls_idxs = sorted_cls_idxs[:n_segs]\n",
    "        return sorted_segs.clone(), sorted_scores.clone(), sorted_cls_idxs.clone()\n",
    "\n",
    "\n",
    "def seg_voting(nms_segs, all_segs, all_scores, iou_threshold, score_offset=1.5):\n",
    "    \"\"\"\n",
    "        blur localization results by incorporating side segs.\n",
    "        this is known as bounding box voting in object detection literature.\n",
    "        slightly boost the performance around iou_threshold\n",
    "    \"\"\"\n",
    "\n",
    "    # *_segs : N_i x 2, all_scores: N,\n",
    "    # apply offset\n",
    "    offset_scores = all_scores + score_offset\n",
    "\n",
    "    # computer overlap between nms and all segs\n",
    "    # construct the distance matrix of # N_nms x # N_all\n",
    "    num_nms_segs, num_all_segs = nms_segs.shape[0], all_segs.shape[0]\n",
    "    ex_nms_segs = nms_segs[:, None].expand(num_nms_segs, num_all_segs, 2)\n",
    "    ex_all_segs = all_segs[None, :].expand(num_nms_segs, num_all_segs, 2)\n",
    "\n",
    "    # compute intersection\n",
    "    left = torch.maximum(ex_nms_segs[:, :, 0], ex_all_segs[:, :, 0])\n",
    "    right = torch.minimum(ex_nms_segs[:, :, 1], ex_all_segs[:, :, 1])\n",
    "    inter = (right-left).clamp(min=0)\n",
    "\n",
    "    # lens of all segments\n",
    "    nms_seg_lens = ex_nms_segs[:, :, 1] - ex_nms_segs[:, :, 0]\n",
    "    all_seg_lens = ex_all_segs[:, :, 1] - ex_all_segs[:, :, 0]\n",
    "\n",
    "    # iou\n",
    "    iou = inter / (nms_seg_lens + all_seg_lens - inter)\n",
    "\n",
    "    # get neighbors (# N_nms x # N_all) / weights\n",
    "    seg_weights = (iou >= iou_threshold).to(all_scores.dtype) * all_scores[None, :] * iou\n",
    "    seg_weights /= torch.sum(seg_weights, dim=1, keepdim=True)\n",
    "    refined_segs = seg_weights @ all_segs\n",
    "\n",
    "    return refined_segs\n",
    "\n",
    "def batched_nms(\n",
    "    segs,\n",
    "    scores,\n",
    "    cls_idxs,\n",
    "    iou_threshold,\n",
    "    min_score,\n",
    "    max_seg_num,\n",
    "    use_soft_nms=True,\n",
    "    multiclass=True,\n",
    "    sigma=0.5,\n",
    "    voting_thresh=0.75,\n",
    "):\n",
    "    # Based on Detectron2 implementation,\n",
    "    num_segs = segs.shape[0]\n",
    "    # corner case, no prediction outputs\n",
    "    if num_segs == 0:\n",
    "        return torch.zeros([0, 2]),\\\n",
    "               torch.zeros([0,]),\\\n",
    "               torch.zeros([0,], dtype=cls_idxs.dtype)\n",
    "\n",
    "    if multiclass:\n",
    "        # multiclass nms: apply nms on each class independently\n",
    "        new_segs, new_scores, new_cls_idxs = [], [], []\n",
    "        for class_id in torch.unique(cls_idxs):\n",
    "            curr_indices = torch.where(cls_idxs == class_id)[0]\n",
    "            # soft_nms vs nms\n",
    "            if use_soft_nms:\n",
    "                sorted_segs, sorted_scores, sorted_cls_idxs = SoftNMSop.apply(\n",
    "                    segs[curr_indices],\n",
    "                    scores[curr_indices],\n",
    "                    cls_idxs[curr_indices],\n",
    "                    iou_threshold,\n",
    "                    sigma,\n",
    "                    min_score,\n",
    "                    2,\n",
    "                    max_seg_num\n",
    "                )\n",
    "            else:\n",
    "                sorted_segs, sorted_scores, sorted_cls_idxs = NMSop.apply(\n",
    "                    segs[curr_indices],\n",
    "                    scores[curr_indices],\n",
    "                    cls_idxs[curr_indices],\n",
    "                    iou_threshold,\n",
    "                    min_score,\n",
    "                    max_seg_num\n",
    "                )\n",
    "            # disable seg voting for multiclass nms, no sufficient segs\n",
    "\n",
    "            # fill in the class index\n",
    "            new_segs.append(sorted_segs)\n",
    "            new_scores.append(sorted_scores)\n",
    "            new_cls_idxs.append(sorted_cls_idxs)\n",
    "\n",
    "        # cat the results\n",
    "        new_segs = torch.cat(new_segs)\n",
    "        new_scores = torch.cat(new_scores)\n",
    "        new_cls_idxs = torch.cat(new_cls_idxs)\n",
    "\n",
    "    else:\n",
    "        # class agnostic\n",
    "        if use_soft_nms:\n",
    "            new_segs, new_scores, new_cls_idxs = SoftNMSop.apply(\n",
    "                segs, scores, cls_idxs, iou_threshold,\n",
    "                sigma, min_score, 2, max_seg_num\n",
    "            )\n",
    "        else:\n",
    "            new_segs, new_scores, new_cls_idxs = NMSop.apply(\n",
    "                segs, scores, cls_idxs, iou_threshold,\n",
    "                min_score, max_seg_num\n",
    "            )\n",
    "        # seg voting\n",
    "        if voting_thresh > 0:\n",
    "            new_segs = seg_voting(\n",
    "                new_segs,\n",
    "                segs,\n",
    "                scores,\n",
    "                voting_thresh\n",
    "            )\n",
    "\n",
    "    # sort based on scores and return\n",
    "    # truncate the results based on max_seg_num\n",
    "    _, idxs = new_scores.sort(descending=True)\n",
    "    max_seg_num = min(max_seg_num, new_segs.shape[0])\n",
    "    # needed for multiclass NMS\n",
    "    new_segs = new_segs[idxs[:max_seg_num]]\n",
    "    new_scores = new_scores[idxs[:max_seg_num]]\n",
    "    new_cls_idxs = new_cls_idxs[idxs[:max_seg_num]]\n",
    "    return new_segs, new_scores, new_cls_idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "307c7464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.272154Z",
     "iopub.status.busy": "2025-09-30T08:45:58.271920Z",
     "iopub.status.idle": "2025-09-30T08:45:58.312539Z",
     "shell.execute_reply": "2025-09-30T08:45:58.311850Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.272139Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PtTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "        Transformer based model for single stage action localization\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone_type,         # a string defines which backbone we use\n",
    "        fpn_type,              # a string defines which fpn we use\n",
    "        backbone_arch,         # a tuple defines #layers in embed / stem / branch\n",
    "        scale_factor,          # scale factor between branch layers\n",
    "        input_dim,             # input feat dim\n",
    "        max_seq_len,           # max sequence length (used for training)\n",
    "        max_buffer_len_factor, # max buffer size (defined a factor of max_seq_len)\n",
    "        n_head,                # number of heads for self-attention in transformer\n",
    "        n_mha_win_size,        # window size for self attention; -1 to use full seq\n",
    "        embd_kernel_size,      # kernel size of the embedding network\n",
    "        embd_dim,              # output feat channel of the embedding network\n",
    "        embd_with_ln,          # attach layernorm to embedding network\n",
    "        fpn_dim,               # feature dim on FPN\n",
    "        fpn_with_ln,           # if to apply layer norm at the end of fpn\n",
    "        fpn_start_level,       # start level of fpn\n",
    "        head_dim,              # feature dim for head\n",
    "        regression_range,      # regression range on each level of FPN\n",
    "        head_num_layers,       # number of layers in the head (including the classifier)\n",
    "        head_kernel_size,      # kernel size for reg/cls heads\n",
    "        head_with_ln,          # attache layernorm to reg/cls heads\n",
    "        use_abs_pe,            # if to use abs position encoding\n",
    "        use_rel_pe,            # if to use rel position encoding\n",
    "        num_classes,           # number of action classes\n",
    "        train_cfg,             # other cfg for training\n",
    "        test_cfg               # other cfg for testing\n",
    "    ):\n",
    "        super().__init__()\n",
    "         # re-distribute params to backbone / neck / head\n",
    "        self.fpn_strides = [scale_factor**i for i in range(\n",
    "            fpn_start_level, backbone_arch[-1]+1\n",
    "        )]\n",
    "        self.reg_range = regression_range\n",
    "        assert len(self.fpn_strides) == len(self.reg_range)\n",
    "        self.scale_factor = scale_factor\n",
    "        # #classes = num_classes + 1 (background) with last category as background\n",
    "        # e.g., num_classes = 10 -> 0, 1, ..., 9 as actions, 10 as background\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # check the feature pyramid and local attention window size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        if isinstance(n_mha_win_size, int):\n",
    "            self.mha_win_size = [n_mha_win_size]*(1 + backbone_arch[-1])\n",
    "        else:\n",
    "            assert len(n_mha_win_size) == (1 + backbone_arch[-1])\n",
    "            self.mha_win_size = n_mha_win_size\n",
    "        max_div_factor = 1\n",
    "        for l, (s, w) in enumerate(zip(self.fpn_strides, self.mha_win_size)):\n",
    "            stride = s * (w // 2) * 2 if w > 1 else s\n",
    "            assert max_seq_len % stride == 0, \"max_seq_len must be divisible by fpn stride and window size\"\n",
    "            if max_div_factor < stride:\n",
    "                max_div_factor = stride\n",
    "        self.max_div_factor = max_div_factor\n",
    "\n",
    "        # training time config\n",
    "        self.train_center_sample = train_cfg['center_sample']\n",
    "        assert self.train_center_sample in ['radius', 'none']\n",
    "        self.train_center_sample_radius = train_cfg['center_sample_radius']\n",
    "        self.train_loss_weight = train_cfg['loss_weight']\n",
    "        self.train_cls_prior_prob = train_cfg['cls_prior_prob']\n",
    "        self.train_dropout = train_cfg['dropout']\n",
    "        self.train_droppath = train_cfg['droppath']\n",
    "        self.train_label_smoothing = train_cfg['label_smoothing']\n",
    "\n",
    "        # test time config\n",
    "        self.test_pre_nms_thresh = test_cfg['pre_nms_thresh']\n",
    "        self.test_pre_nms_topk = test_cfg['pre_nms_topk']\n",
    "        self.test_iou_threshold = test_cfg['iou_threshold']\n",
    "        self.test_min_score = test_cfg['min_score']\n",
    "        self.test_max_seg_num = test_cfg['max_seg_num']\n",
    "        self.test_nms_method = test_cfg['nms_method']\n",
    "        assert self.test_nms_method in ['soft', 'hard', 'none']\n",
    "        self.test_duration_thresh = test_cfg['duration_thresh']\n",
    "        self.test_multiclass_nms = test_cfg['multiclass_nms']\n",
    "        self.test_nms_sigma = test_cfg['nms_sigma']\n",
    "        self.test_voting_thresh = test_cfg['voting_thresh']\n",
    "\n",
    "        # we will need a better way to dispatch the params to backbones / necks\n",
    "        # backbone network: conv + transformer\n",
    "        assert backbone_type in ['convTransformer', 'conv']\n",
    "        if backbone_type == 'convTransformer':\n",
    "            self.backbone = ConvTransformerBackbone(\n",
    "                n_in=input_dim,\n",
    "                n_embd=embd_dim,\n",
    "                n_head=n_head,\n",
    "                n_embd_ks=embd_kernel_size,\n",
    "                max_len=max_seq_len,\n",
    "                arch=backbone_arch,\n",
    "                mha_win_size=self.mha_win_size,\n",
    "                scale_factor=scale_factor,\n",
    "                with_ln=embd_with_ln,\n",
    "                attn_pdrop=0.0,\n",
    "                proj_pdrop=self.train_dropout,\n",
    "                path_pdrop=self.train_droppath,\n",
    "                use_abs_pe=use_abs_pe,\n",
    "                use_rel_pe=use_rel_pe\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.backbone = ConvBackbone(  # Replace with actual conv backbone class name\n",
    "                n_in=input_dim,\n",
    "                n_embd=embd_dim,\n",
    "                n_embd_ks=embd_kernel_size,\n",
    "                arch=backbone_arch,\n",
    "                scale_factor=scale_factor,\n",
    "                with_ln=embd_with_ln\n",
    "            )\n",
    "        if isinstance(embd_dim, (list, tuple)):\n",
    "            embd_dim = sum(embd_dim)\n",
    "\n",
    "        # fpn network: convs\n",
    "        assert fpn_type in ['fpn', 'identity']\n",
    "        if fpn_type == 'fpn':\n",
    "            self.neck = FPN1D(\n",
    "                in_channels=[embd_dim] * (backbone_arch[-1] + 1),\n",
    "                out_channel=fpn_dim,\n",
    "                scale_factor=scale_factor,\n",
    "                start_level=fpn_start_level,\n",
    "                with_ln=fpn_with_ln\n",
    "            )\n",
    "        else:  # fpn_type == 'identity'\n",
    "            self.neck = FPNIdentity(\n",
    "                in_channels=[embd_dim] * (backbone_arch[-1] + 1),\n",
    "                out_channel=fpn_dim,\n",
    "                scale_factor=scale_factor,\n",
    "                start_level=fpn_start_level,\n",
    "                with_ln=fpn_with_ln\n",
    "            )\n",
    "        # location generator: points\n",
    "        self.point_generator = PointGenerator(  # Replace with actual class name\n",
    "            max_seq_len=max_seq_len * max_buffer_len_factor,\n",
    "            fpn_strides=self.fpn_strides,\n",
    "            regression_range=self.reg_range\n",
    "        )\n",
    "\n",
    "        # classfication and regerssion heads\n",
    "        self.cls_head = PtTransformerClsHead(\n",
    "            fpn_dim, head_dim, self.num_classes,\n",
    "            kernel_size=head_kernel_size,\n",
    "            prior_prob=self.train_cls_prior_prob,\n",
    "            with_ln=head_with_ln,\n",
    "            num_layers=head_num_layers,\n",
    "            empty_cls=train_cfg['head_empty_cls']\n",
    "        )\n",
    "        self.reg_head = PtTransformerRegHead(\n",
    "            fpn_dim, head_dim, len(self.fpn_strides),\n",
    "            kernel_size=head_kernel_size,\n",
    "            num_layers=head_num_layers,\n",
    "            with_ln=head_with_ln\n",
    "        )\n",
    "\n",
    "        # maintain an EMA of #foreground to stabilize the loss normalizer\n",
    "        # useful for small mini-batch training\n",
    "        self.loss_normalizer = train_cfg['init_loss_norm']\n",
    "        self.loss_normalizer_momentum = 0.9\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        # a hacky way to get the device type\n",
    "        # will throw an error if parameters are on different devices\n",
    "        return list(set(p.device for p in self.parameters()))[0]\n",
    "\n",
    "    def forward(self, video_list):\n",
    "        # batch the video list into feats (B, C, T) and masks (B, 1, T)\n",
    "        batched_inputs, batched_masks = self.preprocessing(video_list)\n",
    "\n",
    "        # forward the network (backbone -> neck -> heads)\n",
    "        feats, masks = self.backbone(batched_inputs, batched_masks)\n",
    "        fpn_feats, fpn_masks = self.neck(feats, masks)\n",
    "\n",
    "        # compute the point coordinate along the FPN\n",
    "        # this is used for computing the GT or decode the final results\n",
    "        # points: List[T x 4] with length = # fpn levels\n",
    "        # (shared across all samples in the mini-batch)\n",
    "        points = self.point_generator(fpn_feats)\n",
    "\n",
    "        # out_cls: List[B, #cls + 1, T_i]\n",
    "        out_cls_logits = self.cls_head(fpn_feats, fpn_masks)\n",
    "        # out_offset: List[B, 2, T_i]\n",
    "        out_offsets = self.reg_head(fpn_feats, fpn_masks)\n",
    "\n",
    "        # permute the outputs\n",
    "        # out_cls: F List[B, #cls, T_i] -> F List[B, T_i, #cls]\n",
    "        out_cls_logits = [x.permute(0, 2, 1) for x in out_cls_logits]\n",
    "        # out_offset: F List[B, 2 (xC), T_i] -> F List[B, T_i, 2 (xC)]\n",
    "        out_offsets = [x.permute(0, 2, 1) for x in out_offsets]\n",
    "        # fpn_masks: F list[B, 1, T_i] -> F List[B, T_i]\n",
    "        fpn_masks = [x.squeeze(1) for x in fpn_masks]\n",
    "\n",
    "        # return loss during training\n",
    "        if self.training:\n",
    "            # generate segment/lable List[N x 2] / List[N] with length = B\n",
    "            assert video_list[0]['segments'] is not None, \"GT action labels does not exist\"\n",
    "            assert video_list[0]['labels'] is not None, \"GT action labels does not exist\"\n",
    "            gt_segments = [x['segments'].to(self.device) for x in video_list]\n",
    "            gt_labels = [x['labels'].to(self.device) for x in video_list]\n",
    "\n",
    "            # compute the gt labels for cls & reg\n",
    "            # list of prediction targets\n",
    "            gt_cls_labels, gt_offsets = self.label_points(\n",
    "                points, gt_segments, gt_labels)\n",
    "\n",
    "            # compute the loss and return\n",
    "            losses = self.losses(\n",
    "                fpn_masks,\n",
    "                out_cls_logits, out_offsets,\n",
    "                gt_cls_labels, gt_offsets\n",
    "            )\n",
    "            return losses\n",
    "\n",
    "        else:\n",
    "            # decode the actions (sigmoid / stride, etc)\n",
    "            results = self.inference(\n",
    "                video_list, points, fpn_masks,\n",
    "                out_cls_logits, out_offsets\n",
    "            )\n",
    "            return results\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def preprocessing(self, video_list, padding_val=0.0):\n",
    "        \"\"\"\n",
    "            Generate batched features and masks from a list of dict items\n",
    "        \"\"\"\n",
    "        feats = [x['feats'] for x in video_list]\n",
    "        feats_lens = torch.as_tensor([feat.shape[-1] for feat in feats])\n",
    "        max_len = feats_lens.max(0).values.item()\n",
    "\n",
    "        if self.training:\n",
    "            assert max_len <= self.max_seq_len, \"Input length must be smaller than max_seq_len during training\"\n",
    "            # set max_len to self.max_seq_len\n",
    "            max_len = self.max_seq_len\n",
    "            # batch input shape B, C, T\n",
    "            batch_shape = [len(feats), feats[0].shape[0], max_len]\n",
    "            batched_inputs = feats[0].new_full(batch_shape, padding_val)\n",
    "            for feat, pad_feat in zip(feats, batched_inputs):\n",
    "                pad_feat[..., :feat.shape[-1]].copy_(feat)\n",
    "        else:\n",
    "            assert len(video_list) == 1, \"Only support batch_size = 1 during inference\"\n",
    "            # input length < self.max_seq_len, pad to max_seq_len\n",
    "            if max_len <= self.max_seq_len:\n",
    "                max_len = self.max_seq_len\n",
    "            else:\n",
    "                # pad the input to the next divisible size\n",
    "                stride = self.max_div_factor\n",
    "                max_len = (max_len + (stride - 1)) // stride * stride\n",
    "            padding_size = [0, max_len - feats_lens[0]]\n",
    "            batched_inputs = F.pad(\n",
    "                feats[0], padding_size, value=padding_val).unsqueeze(0)\n",
    "\n",
    "        # generate the mask\n",
    "        batched_masks = torch.arange(max_len)[None, :] < feats_lens[:, None]\n",
    "\n",
    "        # push to device\n",
    "        batched_inputs = batched_inputs.to(self.device)\n",
    "        batched_masks = batched_masks.unsqueeze(1).to(self.device)\n",
    "\n",
    "        return batched_inputs, batched_masks\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def label_points(self, points, gt_segments, gt_labels):\n",
    "        # concat points on all fpn levels List[T x 4] -> F T x 4\n",
    "        # This is shared for all samples in the mini-batch\n",
    "        num_levels = len(points)\n",
    "        concat_points = torch.cat(points, dim=0)\n",
    "        gt_cls, gt_offset = [], []\n",
    "\n",
    "        # loop over each video sample\n",
    "        for gt_segment, gt_label in zip(gt_segments, gt_labels):\n",
    "            cls_targets, reg_targets = self.label_points_single_video(\n",
    "                concat_points, gt_segment, gt_label\n",
    "            )\n",
    "            # append to list (len = # images, each of size FT x C)\n",
    "            gt_cls.append(cls_targets)\n",
    "            gt_offset.append(reg_targets)\n",
    "\n",
    "        return gt_cls, gt_offset\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def label_points_single_video(self, concat_points, gt_segment, gt_label):\n",
    "        # concat_points : F T x 4 (t, regression range, stride)\n",
    "        # gt_segment : N (#Events) x 2\n",
    "        # gt_label : N (#Events) x 1\n",
    "        num_pts = concat_points.shape[0]\n",
    "        num_gts = gt_segment.shape[0]\n",
    "\n",
    "        # corner case where current sample does not have actions\n",
    "        if num_gts == 0:\n",
    "            cls_targets = gt_segment.new_full((num_pts, self.num_classes), 0)\n",
    "            reg_targets = gt_segment.new_zeros((num_pts, 2))\n",
    "            return cls_targets, reg_targets\n",
    "\n",
    "        # compute the lengths of all segments -> F T x N\n",
    "        lens = gt_segment[:, 1] - gt_segment[:, 0]\n",
    "        lens = lens[None, :].repeat(num_pts, 1)\n",
    "\n",
    "        # compute the distance of every point to each segment boundary\n",
    "        # auto broadcasting for all reg target-> F T x N x2\n",
    "        gt_segs = gt_segment[None].expand(num_pts, num_gts, 2)\n",
    "        left = concat_points[:, 0, None] - gt_segs[:, :, 0]\n",
    "        right = gt_segs[:, :, 1] - concat_points[:, 0, None]\n",
    "        reg_targets = torch.stack((left, right), dim=-1)\n",
    "\n",
    "        if self.train_center_sample == 'radius':\n",
    "            # center of all segments F T x N\n",
    "            center_pts = 0.5 * (gt_segs[:, :, 0] + gt_segs[:, :, 1])\n",
    "            # center sampling based on stride radius\n",
    "            # compute the new boundaries:\n",
    "            # concat_points[:, 3] stores the stride\n",
    "            t_mins = \\\n",
    "                center_pts - concat_points[:, 3, None] * self.train_center_sample_radius\n",
    "            t_maxs = \\\n",
    "                center_pts + concat_points[:, 3, None] * self.train_center_sample_radius\n",
    "            # prevent t_mins / maxs from over-running the action boundary\n",
    "            # left: torch.maximum(t_mins, gt_segs[:, :, 0])\n",
    "            # right: torch.minimum(t_maxs, gt_segs[:, :, 1])\n",
    "            # F T x N (distance to the new boundary)\n",
    "            cb_dist_left = concat_points[:, 0, None] \\\n",
    "                           - torch.maximum(t_mins, gt_segs[:, :, 0])\n",
    "            cb_dist_right = torch.minimum(t_maxs, gt_segs[:, :, 1]) \\\n",
    "                            - concat_points[:, 0, None]\n",
    "            # F T x N x 2\n",
    "            center_seg = torch.stack(\n",
    "                (cb_dist_left, cb_dist_right), -1)\n",
    "            # F T x N\n",
    "            inside_gt_seg_mask = center_seg.min(-1)[0] > 0\n",
    "        else:\n",
    "            # inside an gt action\n",
    "            inside_gt_seg_mask = reg_targets.min(-1)[0] > 0\n",
    "\n",
    "        # limit the regression range for each location\n",
    "        max_regress_distance = reg_targets.max(-1)[0]\n",
    "        # F T x N\n",
    "        inside_regress_range = torch.logical_and(\n",
    "            (max_regress_distance >= concat_points[:, 1, None]),\n",
    "            (max_regress_distance <= concat_points[:, 2, None])\n",
    "        )\n",
    "\n",
    "        # if there are still more than one actions for one moment\n",
    "        # pick the one with the shortest duration (easiest to regress)\n",
    "        lens.masked_fill_(inside_gt_seg_mask==0, float('inf'))\n",
    "        lens.masked_fill_(inside_regress_range==0, float('inf'))\n",
    "        # F T x N -> F T\n",
    "        min_len, min_len_inds = lens.min(dim=1)\n",
    "\n",
    "        # corner case: multiple actions with very similar durations (e.g., THUMOS14)\n",
    "        min_len_mask = torch.logical_and(\n",
    "            (lens <= (min_len[:, None] + 1e-3)), (lens < float('inf'))\n",
    "        ).to(reg_targets.dtype)\n",
    "\n",
    "        # cls_targets: F T x C; reg_targets F T x 2\n",
    "        gt_label_one_hot = F.one_hot(\n",
    "            gt_label, self.num_classes\n",
    "        ).to(reg_targets.dtype)\n",
    "        cls_targets = min_len_mask @ gt_label_one_hot\n",
    "        # to prevent multiple GT actions with the same label and boundaries\n",
    "        cls_targets.clamp_(min=0.0, max=1.0)\n",
    "        # OK to use min_len_inds\n",
    "        reg_targets = reg_targets[range(num_pts), min_len_inds]\n",
    "        # normalization based on stride\n",
    "        reg_targets /= concat_points[:, 3, None]\n",
    "\n",
    "        return cls_targets, reg_targets\n",
    "\n",
    "    def losses(\n",
    "        self, fpn_masks,\n",
    "        out_cls_logits, out_offsets,\n",
    "        gt_cls_labels, gt_offsets\n",
    "    ):\n",
    "        # fpn_masks, out_*: F (List) [B, T_i, C]\n",
    "        # gt_* : B (list) [F T, C]\n",
    "        # fpn_masks -> (B, FT)\n",
    "        valid_mask = torch.cat(fpn_masks, dim=1)\n",
    "\n",
    "        # 1. classification loss\n",
    "        # stack the list -> (B, FT) -> (# Valid, )\n",
    "        gt_cls = torch.stack(gt_cls_labels)\n",
    "        pos_mask = torch.logical_and((gt_cls.sum(-1) > 0), valid_mask)\n",
    "\n",
    "        # cat the predicted offsets -> (B, FT, 2 (xC)) -> # (#Pos, 2 (xC))\n",
    "        pred_offsets = torch.cat(out_offsets, dim=1)[pos_mask]\n",
    "        gt_offsets = torch.stack(gt_offsets)[pos_mask]\n",
    "\n",
    "        # update the loss normalizer\n",
    "        num_pos = pos_mask.sum().item()\n",
    "        self.loss_normalizer = self.loss_normalizer_momentum * self.loss_normalizer + (\n",
    "            1 - self.loss_normalizer_momentum\n",
    "        ) * max(num_pos, 1)\n",
    "\n",
    "        # gt_cls is already one hot encoded now, simply masking out\n",
    "        gt_target = gt_cls[valid_mask]\n",
    "\n",
    "        # optinal label smoothing\n",
    "        gt_target *= 1 - self.train_label_smoothing\n",
    "        gt_target += self.train_label_smoothing / (self.num_classes + 1)\n",
    "\n",
    "        # focal loss\n",
    "        cls_loss = sigmoid_focal_loss(\n",
    "            torch.cat(out_cls_logits, dim=1)[valid_mask],\n",
    "            gt_target,\n",
    "            reduction='sum'\n",
    "        )\n",
    "        cls_loss /= self.loss_normalizer\n",
    "\n",
    "        # 2. regression using IoU/GIoU loss (defined on positive samples)\n",
    "        if num_pos == 0:\n",
    "            reg_loss = 0 * pred_offsets.sum()\n",
    "        else:\n",
    "            # giou loss defined on positive samples\n",
    "            reg_loss = ctr_diou_loss_1d(\n",
    "                pred_offsets,\n",
    "                gt_offsets,\n",
    "                reduction='sum'\n",
    "            )\n",
    "            reg_loss /= self.loss_normalizer\n",
    "\n",
    "        if self.train_loss_weight > 0:\n",
    "            loss_weight = self.train_loss_weight\n",
    "        else:\n",
    "            loss_weight = cls_loss.detach() / max(reg_loss.item(), 0.01)\n",
    "\n",
    "        # return a dict of losses\n",
    "        final_loss = cls_loss + reg_loss * loss_weight\n",
    "        return {'cls_loss'   : cls_loss,\n",
    "                'reg_loss'   : reg_loss,\n",
    "                'final_loss' : final_loss}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference(\n",
    "        self,\n",
    "        video_list,\n",
    "        points, fpn_masks,\n",
    "        out_cls_logits, out_offsets\n",
    "    ):\n",
    "        # video_list B (list) [dict]\n",
    "        # points F (list) [T_i, 4]\n",
    "        # fpn_masks, out_*: F (List) [B, T_i, C]\n",
    "        results = []\n",
    "\n",
    "        # 1: gather video meta information\n",
    "        vid_idxs = [x['video_id'] for x in video_list]\n",
    "        vid_fps = [x['fps'] for x in video_list]\n",
    "        vid_lens = [x['duration'] for x in video_list]\n",
    "        vid_ft_stride = [x['feat_stride'] for x in video_list]\n",
    "        vid_ft_nframes = [x['feat_num_frames'] for x in video_list]\n",
    "\n",
    "        # 2: inference on each single video and gather the results\n",
    "        # upto this point, all results use timestamps defined on feature grids\n",
    "        for idx, (vidx, fps, vlen, stride, nframes) in enumerate(\n",
    "            zip(vid_idxs, vid_fps, vid_lens, vid_ft_stride, vid_ft_nframes)\n",
    "        ):\n",
    "            # gather per-video outputs\n",
    "            cls_logits_per_vid = [x[idx] for x in out_cls_logits]\n",
    "            offsets_per_vid = [x[idx] for x in out_offsets]\n",
    "            fpn_masks_per_vid = [x[idx] for x in fpn_masks]\n",
    "            # inference on a single video (should always be the case)\n",
    "            results_per_vid = self.inference_single_video(\n",
    "                points, fpn_masks_per_vid,\n",
    "                cls_logits_per_vid, offsets_per_vid\n",
    "            )\n",
    "            # pass through video meta info\n",
    "            results_per_vid['video_id'] = vidx\n",
    "            results_per_vid['fps'] = fps\n",
    "            results_per_vid['duration'] = vlen\n",
    "            results_per_vid['feat_stride'] = stride\n",
    "            results_per_vid['feat_num_frames'] = nframes\n",
    "            results.append(results_per_vid)\n",
    "\n",
    "        # step 3: postprocssing\n",
    "        results = self.postprocessing(results)\n",
    "\n",
    "        return results\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def inference_single_video(\n",
    "        self,\n",
    "        points,\n",
    "        fpn_masks,\n",
    "        out_cls_logits,\n",
    "        out_offsets,\n",
    "    ):\n",
    "        # points F (list) [T_i, 4]\n",
    "        # fpn_masks, out_*: F (List) [T_i, C]\n",
    "        segs_all = []\n",
    "        scores_all = []\n",
    "        cls_idxs_all = []\n",
    "\n",
    "        # loop over fpn levels\n",
    "        for cls_i, offsets_i, pts_i, mask_i in zip(\n",
    "                out_cls_logits, out_offsets, points, fpn_masks\n",
    "            ):\n",
    "            # sigmoid normalization for output logits\n",
    "            pred_prob = (cls_i.sigmoid() * mask_i.unsqueeze(-1)).flatten()\n",
    "\n",
    "            # Apply filtering to make NMS faster following detectron2\n",
    "            # 1. Keep seg with confidence score > a threshold\n",
    "            keep_idxs1 = (pred_prob > self.test_pre_nms_thresh)\n",
    "            pred_prob = pred_prob[keep_idxs1]\n",
    "            topk_idxs = keep_idxs1.nonzero(as_tuple=True)[0]\n",
    "\n",
    "            # 2. Keep top k top scoring boxes only\n",
    "            num_topk = min(self.test_pre_nms_topk, topk_idxs.size(0))\n",
    "            pred_prob, idxs = pred_prob.sort(descending=True)\n",
    "            pred_prob = pred_prob[:num_topk].clone()\n",
    "            topk_idxs = topk_idxs[idxs[:num_topk]].clone()\n",
    "\n",
    "            # fix a warning in pytorch 1.9\n",
    "            pt_idxs =  torch.div(\n",
    "                topk_idxs, self.num_classes, rounding_mode='floor'\n",
    "            )\n",
    "            cls_idxs = torch.fmod(topk_idxs, self.num_classes)\n",
    "\n",
    "            # 3. gather predicted offsets\n",
    "            offsets = offsets_i[pt_idxs]\n",
    "            pts = pts_i[pt_idxs]\n",
    "\n",
    "            # 4. compute predicted segments (denorm by stride for output offsets)\n",
    "            seg_left = pts[:, 0] - offsets[:, 0] * pts[:, 3]\n",
    "            seg_right = pts[:, 0] + offsets[:, 1] * pts[:, 3]\n",
    "            pred_segs = torch.stack((seg_left, seg_right), -1)\n",
    "\n",
    "            # 5. Keep seg with duration > a threshold (relative to feature grids)\n",
    "            seg_areas = seg_right - seg_left\n",
    "            keep_idxs2 = seg_areas > self.test_duration_thresh\n",
    "\n",
    "            # *_all : N (filtered # of segments) x 2 / 1\n",
    "            segs_all.append(pred_segs[keep_idxs2])\n",
    "            scores_all.append(pred_prob[keep_idxs2])\n",
    "            cls_idxs_all.append(cls_idxs[keep_idxs2])\n",
    "\n",
    "        # cat along the FPN levels (F N_i, C)\n",
    "        segs_all, scores_all, cls_idxs_all = [\n",
    "            torch.cat(x) for x in [segs_all, scores_all, cls_idxs_all]\n",
    "        ]\n",
    "        results = {'segments' : segs_all,\n",
    "                   'scores'   : scores_all,\n",
    "                   'labels'   : cls_idxs_all}\n",
    "\n",
    "        return results\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def postprocessing(self, results):\n",
    "        # input : list of dictionary items\n",
    "        # (1) push to CPU; (2) NMS; (3) convert to actual time stamps\n",
    "        processed_results = []\n",
    "        for results_per_vid in results:\n",
    "            # unpack the meta info\n",
    "            vidx = results_per_vid['video_id']\n",
    "            fps = results_per_vid['fps']\n",
    "            vlen = results_per_vid['duration']\n",
    "            stride = results_per_vid['feat_stride']\n",
    "            nframes = results_per_vid['feat_num_frames']\n",
    "            # 1: unpack the results and move to CPU\n",
    "            segs = results_per_vid['segments'].detach().cpu()\n",
    "            scores = results_per_vid['scores'].detach().cpu()\n",
    "            labels = results_per_vid['labels'].detach().cpu()\n",
    "            if self.test_nms_method != 'none':\n",
    "                # 2: batched nms (only implemented on CPU)\n",
    "                segs, scores, labels = batched_nms(\n",
    "                    segs, scores, labels,\n",
    "                    self.test_iou_threshold,\n",
    "                    self.test_min_score,\n",
    "                    self.test_max_seg_num,\n",
    "                    use_soft_nms = (self.test_nms_method == 'soft'),\n",
    "                    multiclass = self.test_multiclass_nms,\n",
    "                    sigma = self.test_nms_sigma,\n",
    "                    voting_thresh = self.test_voting_thresh\n",
    "                )\n",
    "            # 3: convert from feature grids to seconds\n",
    "            if segs.shape[0] > 0:\n",
    "                segs = (segs * stride + 0.5 * nframes) / fps\n",
    "                # truncate all boundaries within [0, duration]\n",
    "                segs[segs<=0.0] *= 0.0\n",
    "                segs[segs>=vlen] = segs[segs>=vlen] * 0.0 + vlen\n",
    "            \n",
    "            # 4: repack the results\n",
    "            processed_results.append(\n",
    "                {'video_id' : vidx,\n",
    "                 'segments' : segs,\n",
    "                 'scores'   : scores,\n",
    "                 'labels'   : labels}\n",
    "            )\n",
    "\n",
    "        return processed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b9c50061",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.313412Z",
     "iopub.status.busy": "2025-09-30T08:45:58.313181Z",
     "iopub.status.idle": "2025-09-30T08:45:58.325428Z",
     "shell.execute_reply": "2025-09-30T08:45:58.324800Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.313393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "be54acb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.326246Z",
     "iopub.status.busy": "2025-09-30T08:45:58.326034Z",
     "iopub.status.idle": "2025-09-30T08:45:58.339361Z",
     "shell.execute_reply": "2025-09-30T08:45:58.338789Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.326231Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PtTransformerRegHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Shared 1D Conv heads for regression\n",
    "    Simlar logic as PtTransformerClsHead with separated implementation for clarity\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        feat_dim,\n",
    "        fpn_levels,\n",
    "        num_layers=3,\n",
    "        kernel_size=3,\n",
    "        act_layer=nn.ReLU,\n",
    "        with_ln=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.fpn_levels = fpn_levels\n",
    "        self.act = act_layer()\n",
    "\n",
    "        # build the conv head\n",
    "        self.head = nn.ModuleList()\n",
    "        self.norm = nn.ModuleList()\n",
    "        for idx in range(num_layers-1):\n",
    "            if idx == 0:\n",
    "                in_dim = input_dim\n",
    "                out_dim = feat_dim\n",
    "            else:\n",
    "                in_dim = feat_dim\n",
    "                out_dim = feat_dim\n",
    "            self.head.append(\n",
    "                MaskedConv1D(\n",
    "                    in_dim, out_dim, kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=kernel_size//2,\n",
    "                    bias=(not with_ln)\n",
    "                )\n",
    "            )\n",
    "            if with_ln:\n",
    "                self.norm.append(LayerNorm(out_dim))\n",
    "            else:\n",
    "                self.norm.append(nn.Identity())\n",
    "\n",
    "        self.scale = nn.ModuleList()\n",
    "        for idx in range(fpn_levels):\n",
    "            self.scale.append(Scale())\n",
    "\n",
    "        # segment regression\n",
    "        self.offset_head = MaskedConv1D(\n",
    "                feat_dim, 2, kernel_size,\n",
    "                stride=1, padding=kernel_size//2\n",
    "            )\n",
    "\n",
    "    def forward(self, fpn_feats, fpn_masks):\n",
    "        assert len(fpn_feats) == len(fpn_masks)\n",
    "        assert len(fpn_feats) == self.fpn_levels\n",
    "\n",
    "        # apply the classifier for each pyramid level\n",
    "        out_offsets = tuple()\n",
    "        for l, (cur_feat, cur_mask) in enumerate(zip(fpn_feats, fpn_masks)):\n",
    "            cur_out = cur_feat\n",
    "            for idx in range(len(self.head)):\n",
    "                cur_out, _ = self.head[idx](cur_out, cur_mask)\n",
    "                cur_out = self.act(self.norm[idx](cur_out))\n",
    "            cur_offsets, _ = self.offset_head(cur_out, cur_mask)\n",
    "            out_offsets += (F.relu(self.scale[l](cur_offsets)), )\n",
    "\n",
    "        # fpn_masks remains the same\n",
    "        return out_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a7dafe3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.341763Z",
     "iopub.status.busy": "2025-09-30T08:45:58.341392Z",
     "iopub.status.idle": "2025-09-30T08:45:58.352162Z",
     "shell.execute_reply": "2025-09-30T08:45:58.351497Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.341746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PtTransformerClsHead(nn.Module):\n",
    "    \"\"\"\n",
    "    1D Conv heads for classification\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        feat_dim,\n",
    "        num_classes,\n",
    "        prior_prob=0.01,\n",
    "        num_layers=3,\n",
    "        kernel_size=3,\n",
    "        act_layer=nn.ReLU,\n",
    "        with_ln=False,\n",
    "        empty_cls = []\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.act = act_layer()\n",
    "\n",
    "        # build the head\n",
    "        self.head = nn.ModuleList()\n",
    "        self.norm = nn.ModuleList()\n",
    "        for idx in range(num_layers-1):\n",
    "            if idx == 0:\n",
    "                in_dim = input_dim\n",
    "                out_dim = feat_dim\n",
    "            else:\n",
    "                in_dim = feat_dim\n",
    "                out_dim = feat_dim\n",
    "            self.head.append(\n",
    "                MaskedConv1D(\n",
    "                    in_dim, out_dim, kernel_size,\n",
    "                    stride=1,\n",
    "                    padding=kernel_size//2,\n",
    "                    bias=(not with_ln)\n",
    "                )\n",
    "            )\n",
    "            if with_ln:\n",
    "                self.norm.append(LayerNorm(out_dim))\n",
    "            else:\n",
    "                self.norm.append(nn.Identity())\n",
    "\n",
    "        # classifier\n",
    "        self.cls_head = MaskedConv1D(\n",
    "                feat_dim, num_classes, kernel_size,\n",
    "                stride=1, padding=kernel_size//2\n",
    "            )\n",
    "\n",
    "        # use prior in model initialization to improve stability\n",
    "        # this will overwrite other weight init\n",
    "        if prior_prob > 0:\n",
    "            bias_value = -(math.log((1 - prior_prob) / prior_prob))\n",
    "            torch.nn.init.constant_(self.cls_head.conv.bias, bias_value)\n",
    "\n",
    "        # a quick fix to empty categories:\n",
    "        # the weights assocaited with these categories will remain unchanged\n",
    "        # we set their bias to a large negative value to prevent their outputs\n",
    "        if len(empty_cls) > 0:\n",
    "            bias_value = -(math.log((1 - 1e-6) / 1e-6))\n",
    "            for idx in empty_cls:\n",
    "                torch.nn.init.constant_(self.cls_head.conv.bias[idx], bias_value)\n",
    "\n",
    "    def forward(self, fpn_feats, fpn_masks):\n",
    "        assert len(fpn_feats) == len(fpn_masks)\n",
    "\n",
    "        # apply the classifier for each pyramid level\n",
    "        out_logits = tuple()\n",
    "        for _, (cur_feat, cur_mask) in enumerate(zip(fpn_feats, fpn_masks)):\n",
    "            cur_out = cur_feat\n",
    "            for idx in range(len(self.head)):\n",
    "                cur_out, _ = self.head[idx](cur_out, cur_mask)\n",
    "                cur_out = self.act(self.norm[idx](cur_out))\n",
    "            cur_logits, _ = self.cls_head(cur_out, cur_mask)\n",
    "            out_logits += (cur_logits, )\n",
    "\n",
    "        # fpn_masks remains the same\n",
    "        return out_logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ed677433-824b-4658-bda4-480b5ca2a17b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.353081Z",
     "iopub.status.busy": "2025-09-30T08:45:58.352857Z",
     "iopub.status.idle": "2025-09-30T08:45:58.364833Z",
     "shell.execute_reply": "2025-09-30T08:45:58.364167Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.353059Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MaskedConv1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked 1D convolution. Interface remains the same as Conv1d.\n",
    "    Only support a sub set of 1d convs\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        bias=True,\n",
    "        padding_mode='zeros'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # element must be aligned\n",
    "        assert (kernel_size % 2 == 1) and (kernel_size // 2 == padding)\n",
    "        # stride\n",
    "        self.stride = stride\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size,\n",
    "                              stride, padding, dilation, groups, bias, padding_mode)\n",
    "        # zero out the bias term if it exists\n",
    "        if bias:\n",
    "            torch.nn.init.constant_(self.conv.bias, 0.)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x: batch size, feature channel, sequence length,\n",
    "        # mask: batch size, 1, sequence length (bool)\n",
    "        B, C, T = x.size()\n",
    "        # input length must be divisible by stride\n",
    "        assert T % self.stride == 0\n",
    "\n",
    "        # conv\n",
    "        out_conv = self.conv(x)\n",
    "        # compute the mask\n",
    "        if self.stride > 1:\n",
    "            # downsample the mask using nearest neighbor\n",
    "            out_mask = F.interpolate(\n",
    "                mask.to(x.dtype), size=out_conv.size(-1), mode='nearest'\n",
    "            )\n",
    "        else:\n",
    "            # masking out the features\n",
    "            out_mask = mask.to(x.dtype)\n",
    "\n",
    "        # masking the output, stop grad to mask\n",
    "        out_conv = out_conv * out_mask.detach()\n",
    "        out_mask = out_mask.bool()\n",
    "        return out_conv, out_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c88d8e7b-6b9a-428f-af5c-db7e283f70bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.365836Z",
     "iopub.status.busy": "2025-09-30T08:45:58.365610Z",
     "iopub.status.idle": "2025-09-30T08:45:58.376428Z",
     "shell.execute_reply": "2025-09-30T08:45:58.375862Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.365807Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    LayerNorm that supports inputs of size B, C, T\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels,\n",
    "        eps = 1e-5,\n",
    "        affine = True,\n",
    "        device = None,\n",
    "        dtype = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        self.num_channels = num_channels\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "\n",
    "        if self.affine:\n",
    "            self.weight = nn.Parameter(\n",
    "                torch.ones([1, num_channels, 1], **factory_kwargs))\n",
    "            self.bias = nn.Parameter(\n",
    "                torch.zeros([1, num_channels, 1], **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.dim() == 3\n",
    "        assert x.shape[1] == self.num_channels\n",
    "\n",
    "        # normalization along C channels\n",
    "        mu = torch.mean(x, dim=1, keepdim=True)\n",
    "        res_x = x - mu\n",
    "        sigma = torch.mean(res_x**2, dim=1, keepdim=True)\n",
    "        out = res_x / torch.sqrt(sigma + self.eps)\n",
    "\n",
    "        # apply weight and bias\n",
    "        if self.affine:\n",
    "            out *= self.weight\n",
    "            out += self.bias\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "94c2b069-015f-4289-bd19-8c92dad4336a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.377440Z",
     "iopub.status.busy": "2025-09-30T08:45:58.377146Z",
     "iopub.status.idle": "2025-09-30T08:45:58.395010Z",
     "shell.execute_reply": "2025-09-30T08:45:58.394301Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.377424Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple (post layer norm) Transformer block\n",
    "    Modified from https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embd,                # dimension of the input features\n",
    "        n_head,                # number of attention heads\n",
    "        n_ds_strides=(1, 1),   # downsampling strides for q & x, k & v\n",
    "        n_out=None,            # output dimension, if None, set to input dim\n",
    "        n_hidden=None,         # dimension of the hidden layer in MLP\n",
    "        act_layer=nn.GELU,     # nonlinear activation used in MLP, default GELU\n",
    "        attn_pdrop=0.0,        # dropout rate for the attention map\n",
    "        proj_pdrop=0.0,        # dropout rate for the projection / MLP\n",
    "        path_pdrop=0.0,        # drop path rate\n",
    "        mha_win_size=-1,       # > 0 to use window mha\n",
    "        use_rel_pe=False       # if to add rel position encoding to attention\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(n_ds_strides) == 2\n",
    "        # layer norm for order (B C T)\n",
    "        self.ln1 = LayerNorm(n_embd)\n",
    "        self.ln2 = LayerNorm(n_embd)\n",
    "\n",
    "        # specify the attention module\n",
    "        if mha_win_size > 1:\n",
    "            self.attn = LocalMaskedMHCA(\n",
    "                n_embd,\n",
    "                n_head,\n",
    "                window_size=mha_win_size,\n",
    "                n_qx_stride=n_ds_strides[0],\n",
    "                n_kv_stride=n_ds_strides[1],\n",
    "                attn_pdrop=attn_pdrop,\n",
    "                proj_pdrop=proj_pdrop,\n",
    "                use_rel_pe=use_rel_pe  # only valid for local attention\n",
    "            )\n",
    "        else:\n",
    "            self.attn = MaskedMHCA(\n",
    "                n_embd,\n",
    "                n_head,\n",
    "                n_qx_stride=n_ds_strides[0],\n",
    "                n_kv_stride=n_ds_strides[1],\n",
    "                attn_pdrop=attn_pdrop,\n",
    "                proj_pdrop=proj_pdrop\n",
    "            )\n",
    "\n",
    "        # input\n",
    "        if n_ds_strides[0] > 1:\n",
    "            kernel_size, stride, padding = \\\n",
    "                n_ds_strides[0] + 1, n_ds_strides[0], (n_ds_strides[0] + 1)//2\n",
    "            self.pool_skip = nn.MaxPool1d(\n",
    "                kernel_size, stride=stride, padding=padding)\n",
    "        else:\n",
    "            self.pool_skip = nn.Identity()\n",
    "\n",
    "        # two layer mlp\n",
    "        if n_hidden is None:\n",
    "            n_hidden = 4 * n_embd  # default\n",
    "        if n_out is None:\n",
    "            n_out = n_embd\n",
    "        # ok to use conv1d here with stride=1\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv1d(n_embd, n_hidden, 1),\n",
    "            act_layer(),\n",
    "            nn.Dropout(proj_pdrop, inplace=True),\n",
    "            nn.Conv1d(n_hidden, n_out, 1),\n",
    "            nn.Dropout(proj_pdrop, inplace=True),\n",
    "        )\n",
    "\n",
    "        # drop path\n",
    "        if path_pdrop > 0.0:\n",
    "            self.drop_path_attn = AffineDropPath(n_embd, drop_prob = path_pdrop)\n",
    "            self.drop_path_mlp = AffineDropPath(n_out, drop_prob = path_pdrop)\n",
    "        else:\n",
    "            self.drop_path_attn = nn.Identity()\n",
    "            self.drop_path_mlp = nn.Identity()\n",
    "\n",
    "    def forward(self, x, mask, pos_embd=None):\n",
    "        # pre-LN transformer: https://arxiv.org/pdf/2002.04745.pdf\n",
    "        out, out_mask = self.attn(self.ln1(x), mask)\n",
    "        out_mask_float = out_mask.to(out.dtype)\n",
    "        out = self.pool_skip(x) * out_mask_float + self.drop_path_attn(out)\n",
    "        # FFN\n",
    "        out = out + self.drop_path_mlp(self.mlp(self.ln2(out)) * out_mask_float)\n",
    "        # optionally add pos_embd to the output\n",
    "        if pos_embd is not None:\n",
    "            out += pos_embd * out_mask_float\n",
    "        return out, out_mask\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple conv block similar to the basic block used in ResNet\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embd,                # dimension of the input features\n",
    "        kernel_size=3,         # conv kernel size\n",
    "        n_ds_stride=1,         # downsampling stride for the current layer\n",
    "        expansion_factor=2,    # expansion factor of feat dims\n",
    "        n_out=None,            # output dimension, if None, set to input dim\n",
    "        act_layer=nn.ReLU,     # nonlinear activation used after conv, default ReLU\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # must use odd sized kernel\n",
    "        assert (kernel_size % 2 == 1) and (kernel_size > 1)\n",
    "        padding = kernel_size // 2\n",
    "        if n_out is None:\n",
    "            n_out = n_embd\n",
    "\n",
    "         # 1x3 (strided) -> 1x3 (basic block in resnet)\n",
    "        width = n_embd * expansion_factor\n",
    "        self.conv1 = MaskedConv1D(\n",
    "            n_embd, width, kernel_size, n_ds_stride, padding=padding)\n",
    "        self.conv2 = MaskedConv1D(\n",
    "            width, n_out, kernel_size, 1, padding=padding)\n",
    "\n",
    "        # attach downsampling conv op\n",
    "        if n_ds_stride > 1:\n",
    "            # 1x1 strided conv (same as resnet)\n",
    "            self.downsample = MaskedConv1D(n_embd, n_out, 1, n_ds_stride)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "        self.act = act_layer()\n",
    "\n",
    "    def forward(self, x, mask, pos_embd=None):\n",
    "        identity = x\n",
    "        out, out_mask = self.conv1(x, mask)\n",
    "        out = self.act(out)\n",
    "        out, out_mask = self.conv2(out, out_mask)\n",
    "\n",
    "        # downsampling\n",
    "        if self.downsample is not None:\n",
    "            identity, _ = self.downsample(x, mask)\n",
    "\n",
    "        # residual connection\n",
    "        out += identity\n",
    "        out = self.act(out)\n",
    "\n",
    "        return out, out_mask\n",
    "\n",
    "\n",
    "# drop path: from https://github.com/facebookresearch/SlowFast/blob/master/slowfast/models/common.py\n",
    "class Scale(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiply the output regression range by a learnable constant value\n",
    "    \"\"\"\n",
    "    def __init__(self, init_value=1.0):\n",
    "        \"\"\"\n",
    "        init_value : initial value for the scalar\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(\n",
    "            torch.tensor(init_value, dtype=torch.float32),\n",
    "            requires_grad=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        input -> scale * input\n",
    "        \"\"\"\n",
    "        return x * self.scale\n",
    "\n",
    "\n",
    "# The follow code is modified from\n",
    "# https://github.com/facebookresearch/SlowFast/blob/master/slowfast/models/common.py\n",
    "def drop_path(x, drop_prob=0.0, training=False):\n",
    "    \"\"\"\n",
    "    Stochastic Depth per sample.\n",
    "    \"\"\"\n",
    "    if drop_prob == 0.0 or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (\n",
    "        x.ndim - 1\n",
    "    )  # work with diff dim tensors, not just 2D ConvNets\n",
    "    mask = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    mask.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * mask\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class AffineDropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks) with a per channel scaling factor (and zero init)\n",
    "    See: https://arxiv.org/pdf/2103.17239.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_dim, drop_prob=0.0, init_scale_value=1e-4):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(\n",
    "            init_scale_value * torch.ones((1, num_dim, 1)),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(self.scale * x, self.drop_prob, self.training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0aef23ed-fd91-4479-ac67-948d27236dfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.396308Z",
     "iopub.status.busy": "2025-09-30T08:45:58.396082Z",
     "iopub.status.idle": "2025-09-30T08:45:58.424570Z",
     "shell.execute_reply": "2025-09-30T08:45:58.423721Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.396288Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LocalMaskedMHCA(nn.Module):\n",
    "    \"\"\"\n",
    "    Local Multi Head Conv Attention with mask\n",
    "\n",
    "    Add a depthwise convolution within a standard MHA\n",
    "    The extra conv op can be used to\n",
    "    (1) encode relative position information (relacing position encoding);\n",
    "    (2) downsample the features if needed;\n",
    "    (3) match the feature channels\n",
    "\n",
    "    Note: With current implementation, the downsampled feature will be aligned\n",
    "    to every s+1 time step, where s is the downsampling stride. This allows us\n",
    "    to easily interpolate the corresponding positional embeddings.\n",
    "\n",
    "    The implementation is fairly tricky, code reference from\n",
    "    https://github.com/huggingface/transformers/blob/master/src/transformers/models/longformer/modeling_longformer.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embd,          # dimension of the output features\n",
    "        n_head,          # number of heads in multi-head self-attention\n",
    "        window_size,     # size of the local attention window\n",
    "        n_qx_stride=1,   # dowsampling stride for query and input\n",
    "        n_kv_stride=1,   # downsampling stride for key and value\n",
    "        attn_pdrop=0.0,  # dropout rate for the attention map\n",
    "        proj_pdrop=0.0,  # dropout rate for projection op\n",
    "        use_rel_pe=False # use relative position encoding\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.n_channels = n_embd // n_head\n",
    "        self.scale = 1.0 / math.sqrt(self.n_channels)\n",
    "        self.window_size = window_size\n",
    "        self.window_overlap  = window_size // 2\n",
    "        # must use an odd window size\n",
    "        assert self.window_size > 1 and self.n_head >= 1\n",
    "        self.use_rel_pe = use_rel_pe\n",
    "\n",
    "        # conv/pooling operations\n",
    "        assert (n_qx_stride == 1) or (n_qx_stride % 2 == 0)\n",
    "        assert (n_kv_stride == 1) or (n_kv_stride % 2 == 0)\n",
    "        self.n_qx_stride = n_qx_stride\n",
    "        self.n_kv_stride = n_kv_stride\n",
    "\n",
    "        # query conv (depthwise)\n",
    "        kernel_size = self.n_qx_stride + 1 if self.n_qx_stride > 1 else 3\n",
    "        stride, padding = self.n_kv_stride, kernel_size // 2\n",
    "        self.query_conv = MaskedConv1D(\n",
    "            self.n_embd, self.n_embd, kernel_size,\n",
    "            stride=stride, padding=padding, groups=self.n_embd, bias=False\n",
    "        )\n",
    "        self.query_norm = LayerNorm(self.n_embd)\n",
    "\n",
    "        # key, value conv (depthwise)\n",
    "        kernel_size = self.n_kv_stride + 1 if self.n_kv_stride > 1 else 3\n",
    "        stride, padding = self.n_kv_stride, kernel_size // 2\n",
    "        self.key_conv = MaskedConv1D(\n",
    "            self.n_embd, self.n_embd, kernel_size,\n",
    "            stride=stride, padding=padding, groups=self.n_embd, bias=False\n",
    "        )\n",
    "        self.key_norm = LayerNorm(self.n_embd)\n",
    "        self.value_conv = MaskedConv1D(\n",
    "            self.n_embd, self.n_embd, kernel_size,\n",
    "            stride=stride, padding=padding, groups=self.n_embd, bias=False\n",
    "        )\n",
    "        self.value_norm = LayerNorm(self.n_embd)\n",
    "\n",
    "        # key, query, value projections for all heads\n",
    "        # it is OK to ignore masking, as the mask will be attached on the attention\n",
    "        self.key = nn.Conv1d(self.n_embd, self.n_embd, 1)\n",
    "        self.query = nn.Conv1d(self.n_embd, self.n_embd, 1)\n",
    "        self.value = nn.Conv1d(self.n_embd, self.n_embd, 1)\n",
    "\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(attn_pdrop)\n",
    "        self.proj_drop = nn.Dropout(proj_pdrop)\n",
    "\n",
    "        # output projection\n",
    "        self.proj = nn.Conv1d(self.n_embd, self.n_embd, 1)\n",
    "\n",
    "        # relative position encoding\n",
    "        if self.use_rel_pe:\n",
    "            self.rel_pe = nn.Parameter(\n",
    "                torch.zeros(1, 1, self.n_head, self.window_size))\n",
    "            trunc_normal_(self.rel_pe, std=(2.0 / self.n_embd)**0.5)\n",
    "\n",
    "    @staticmethod\n",
    "    def _chunk(x, window_overlap):\n",
    "        \"\"\"convert into overlapping chunks. Chunk size = 2w, overlap size = w\"\"\"\n",
    "        # x: B x nh, T, hs\n",
    "        # non-overlapping chunks of size = 2w -> B x nh, T//2w, 2w, hs\n",
    "        x = x.view(\n",
    "            x.size(0),\n",
    "            x.size(1) // (window_overlap * 2),\n",
    "            window_overlap * 2,\n",
    "            x.size(2),\n",
    "        )\n",
    "\n",
    "        # use `as_strided` to make the chunks overlap with an overlap size = window_overlap\n",
    "        chunk_size = list(x.size())\n",
    "        chunk_size[1] = chunk_size[1] * 2 - 1\n",
    "        chunk_stride = list(x.stride())\n",
    "        chunk_stride[1] = chunk_stride[1] // 2\n",
    "\n",
    "        # B x nh, #chunks = T//w - 1, 2w, hs\n",
    "        return x.as_strided(size=chunk_size, stride=chunk_stride)\n",
    "\n",
    "    @staticmethod\n",
    "    def _pad_and_transpose_last_two_dims(x, padding):\n",
    "        \"\"\"pads rows and then flips rows and columns\"\"\"\n",
    "        # padding value is not important because it will be overwritten\n",
    "        x = nn.functional.pad(x, padding)\n",
    "        x = x.view(*x.size()[:-2], x.size(-1), x.size(-2))\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _mask_invalid_locations(input_tensor, affected_seq_len):\n",
    "        beginning_mask_2d = input_tensor.new_ones(affected_seq_len, affected_seq_len + 1).tril().flip(dims=[0])\n",
    "        beginning_mask = beginning_mask_2d[None, :, None, :]\n",
    "        ending_mask = beginning_mask.flip(dims=(1, 3))\n",
    "        beginning_input = input_tensor[:, :affected_seq_len, :, : affected_seq_len + 1]\n",
    "        beginning_mask = beginning_mask.expand(beginning_input.size())\n",
    "        # `== 1` converts to bool or uint8\n",
    "        beginning_input.masked_fill_(beginning_mask == 1, -float(\"inf\"))\n",
    "        ending_input = input_tensor[:, -affected_seq_len:, :, -(affected_seq_len + 1) :]\n",
    "        ending_mask = ending_mask.expand(ending_input.size())\n",
    "        # `== 1` converts to bool or uint8\n",
    "        ending_input.masked_fill_(ending_mask == 1, -float(\"inf\"))\n",
    "\n",
    "    @staticmethod\n",
    "    def _pad_and_diagonalize(x):\n",
    "        \"\"\"\n",
    "        shift every row 1 step right, converting columns into diagonals.\n",
    "        Example::\n",
    "              chunked_hidden_states: [ 0.4983,  2.6918, -0.0071,  1.0492,\n",
    "                                       -1.8348,  0.7672,  0.2986,  0.0285,\n",
    "                                       -0.7584,  0.4206, -0.0405,  0.1599,\n",
    "                                       2.0514, -1.1600,  0.5372,  0.2629 ]\n",
    "              window_overlap = num_rows = 4\n",
    "             (pad & diagonalize) =>\n",
    "             [ 0.4983,  2.6918, -0.0071,  1.0492, 0.0000,  0.0000,  0.0000\n",
    "               0.0000,  -1.8348,  0.7672,  0.2986,  0.0285, 0.0000,  0.0000\n",
    "               0.0000,  0.0000, -0.7584,  0.4206, -0.0405,  0.1599, 0.0000\n",
    "               0.0000,  0.0000,  0.0000, 2.0514, -1.1600,  0.5372,  0.2629 ]\n",
    "        \"\"\"\n",
    "        total_num_heads, num_chunks, window_overlap, hidden_dim = x.size()\n",
    "        # total_num_heads x num_chunks x window_overlap x (hidden_dim+window_overlap+1).\n",
    "        x = nn.functional.pad(\n",
    "            x, (0, window_overlap + 1)\n",
    "        )\n",
    "        # total_num_heads x num_chunks x window_overlap*window_overlap+window_overlap\n",
    "        x = x.view(total_num_heads, num_chunks, -1)\n",
    "        # total_num_heads x num_chunks x window_overlap*window_overlap\n",
    "        x = x[:, :, :-window_overlap]\n",
    "        x = x.view(\n",
    "            total_num_heads, num_chunks, window_overlap, window_overlap + hidden_dim\n",
    "        )\n",
    "        x = x[:, :, :, :-1]\n",
    "        return x\n",
    "\n",
    "    def _sliding_chunks_query_key_matmul(\n",
    "        self, query, key, num_heads, window_overlap\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Matrix multiplication of query and key tensors using with a sliding window attention pattern. This implementation splits the input into overlapping chunks of size 2w with an overlap of size w (window_overlap)\n",
    "        \"\"\"\n",
    "        # query / key: B*nh, T, hs\n",
    "        bnh, seq_len, head_dim = query.size()\n",
    "        batch_size = bnh // num_heads\n",
    "        assert seq_len % (window_overlap * 2) == 0\n",
    "        assert query.size() == key.size()\n",
    "\n",
    "        chunks_count = seq_len // window_overlap - 1\n",
    "\n",
    "        # B * num_heads, head_dim, #chunks=(T//w - 1), 2w\n",
    "        chunk_query = self._chunk(query, window_overlap)\n",
    "        chunk_key = self._chunk(key, window_overlap)\n",
    "\n",
    "        # matrix multiplication\n",
    "        # bcxd: batch_size * num_heads x chunks x 2window_overlap x head_dim\n",
    "        # bcyd: batch_size * num_heads x chunks x 2window_overlap x head_dim\n",
    "        # bcxy: batch_size * num_heads x chunks x 2window_overlap x 2window_overlap\n",
    "        diagonal_chunked_attention_scores = torch.einsum(\n",
    "            \"bcxd,bcyd->bcxy\", (chunk_query, chunk_key))\n",
    "\n",
    "        # convert diagonals into columns\n",
    "        # B * num_heads, #chunks, 2w, 2w+1\n",
    "        diagonal_chunked_attention_scores = self._pad_and_transpose_last_two_dims(\n",
    "            diagonal_chunked_attention_scores, padding=(0, 0, 0, 1)\n",
    "        )\n",
    "\n",
    "        # allocate space for the overall attention matrix where the chunks are combined. The last dimension\n",
    "        # has (window_overlap * 2 + 1) columns. The first (window_overlap) columns are the window_overlap lower triangles (attention from a word to\n",
    "        # window_overlap previous words). The following column is attention score from each word to itself, then\n",
    "        # followed by window_overlap columns for the upper triangle.\n",
    "        diagonal_attention_scores = diagonal_chunked_attention_scores.new_empty(\n",
    "            (batch_size * num_heads, chunks_count + 1, window_overlap, window_overlap * 2 + 1)\n",
    "        )\n",
    "\n",
    "        # copy parts from diagonal_chunked_attention_scores into the combined matrix of attentions\n",
    "        # - copying the main diagonal and the upper triangle\n",
    "        diagonal_attention_scores[:, :-1, :, window_overlap:] = diagonal_chunked_attention_scores[\n",
    "            :, :, :window_overlap, : window_overlap + 1\n",
    "        ]\n",
    "        diagonal_attention_scores[:, -1, :, window_overlap:] = diagonal_chunked_attention_scores[\n",
    "            :, -1, window_overlap:, : window_overlap + 1\n",
    "        ]\n",
    "        # - copying the lower triangle\n",
    "        diagonal_attention_scores[:, 1:, :, :window_overlap] = diagonal_chunked_attention_scores[\n",
    "            :, :, -(window_overlap + 1) : -1, window_overlap + 1 :\n",
    "        ]\n",
    "\n",
    "        diagonal_attention_scores[:, 0, 1:window_overlap, 1:window_overlap] = diagonal_chunked_attention_scores[\n",
    "            :, 0, : window_overlap - 1, 1 - window_overlap :\n",
    "        ]\n",
    "\n",
    "        # separate batch_size and num_heads dimensions again\n",
    "        diagonal_attention_scores = diagonal_attention_scores.view(\n",
    "            batch_size, num_heads, seq_len, 2 * window_overlap + 1\n",
    "        ).transpose(2, 1)\n",
    "\n",
    "        self._mask_invalid_locations(diagonal_attention_scores, window_overlap)\n",
    "        return diagonal_attention_scores\n",
    "\n",
    "    def _sliding_chunks_matmul_attn_probs_value(\n",
    "        self, attn_probs, value, num_heads, window_overlap\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Same as _sliding_chunks_query_key_matmul but for attn_probs and value tensors. Returned tensor will be of the\n",
    "        same shape as `attn_probs`\n",
    "        \"\"\"\n",
    "        bnh, seq_len, head_dim = value.size()\n",
    "        batch_size = bnh // num_heads\n",
    "        assert seq_len % (window_overlap * 2) == 0\n",
    "        assert attn_probs.size(3) == 2 * window_overlap + 1\n",
    "        chunks_count = seq_len // window_overlap - 1\n",
    "        # group batch_size and num_heads dimensions into one, then chunk seq_len into chunks of size 2 window overlap\n",
    "\n",
    "        chunked_attn_probs = attn_probs.transpose(1, 2).reshape(\n",
    "            batch_size * num_heads, seq_len // window_overlap, window_overlap, 2 * window_overlap + 1\n",
    "        )\n",
    "\n",
    "        # pad seq_len with w at the beginning of the sequence and another window overlap at the end\n",
    "        padded_value = nn.functional.pad(value, (0, 0, window_overlap, window_overlap), value=-1)\n",
    "\n",
    "        # chunk padded_value into chunks of size 3 window overlap and an overlap of size window overlap\n",
    "        chunked_value_size = (batch_size * num_heads, chunks_count + 1, 3 * window_overlap, head_dim)\n",
    "        chunked_value_stride = padded_value.stride()\n",
    "        chunked_value_stride = (\n",
    "            chunked_value_stride[0],\n",
    "            window_overlap * chunked_value_stride[1],\n",
    "            chunked_value_stride[1],\n",
    "            chunked_value_stride[2],\n",
    "        )\n",
    "        chunked_value = padded_value.as_strided(size=chunked_value_size, stride=chunked_value_stride)\n",
    "\n",
    "        chunked_attn_probs = self._pad_and_diagonalize(chunked_attn_probs)\n",
    "\n",
    "        context = torch.einsum(\"bcwd,bcdh->bcwh\", (chunked_attn_probs, chunked_value))\n",
    "        return context.view(batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x: batch size, feature channel, sequence length,\n",
    "        # mask: batch size, 1, sequence length (bool)\n",
    "        B, C, T = x.size()\n",
    "\n",
    "        # step 1: depth convolutions\n",
    "        # query conv -> (B, nh * hs, T')\n",
    "        q, qx_mask = self.query_conv(x, mask)\n",
    "        q = self.query_norm(q)\n",
    "        # key, value conv -> (B, nh * hs, T'')\n",
    "        k, kv_mask = self.key_conv(x, mask)\n",
    "        k = self.key_norm(k)\n",
    "        v, _ = self.value_conv(x, mask)\n",
    "        v = self.value_norm(v)\n",
    "\n",
    "        # step 2: query, key, value transforms & reshape\n",
    "        # projections\n",
    "        q = self.query(q)\n",
    "        k = self.key(k)\n",
    "        v = self.value(v)\n",
    "        # (B, nh * hs, T) -> (B, nh, T, hs)\n",
    "        q = q.view(B, self.n_head, self.n_channels, -1).transpose(2, 3)\n",
    "        k = k.view(B, self.n_head, self.n_channels, -1).transpose(2, 3)\n",
    "        v = v.view(B, self.n_head, self.n_channels, -1).transpose(2, 3)\n",
    "        # view as (B * nh, T, hs)\n",
    "        q = q.view(B * self.n_head, -1, self.n_channels).contiguous()\n",
    "        k = k.view(B * self.n_head, -1, self.n_channels).contiguous()\n",
    "        v = v.view(B * self.n_head, -1, self.n_channels).contiguous()\n",
    "\n",
    "        # step 3: compute local self-attention with rel pe and masking\n",
    "        q *= self.scale\n",
    "        # chunked query key attention -> B, T, nh, 2w+1 = window_size\n",
    "        att = self._sliding_chunks_query_key_matmul(\n",
    "            q, k, self.n_head, self.window_overlap)\n",
    "\n",
    "        # rel pe\n",
    "        if self.use_rel_pe:\n",
    "            att += self.rel_pe\n",
    "        # kv_mask -> B, T'', 1\n",
    "        inverse_kv_mask = torch.logical_not(\n",
    "            kv_mask[:, :, :, None].view(B, -1, 1))\n",
    "        # 0 for valid slot, -inf for masked ones\n",
    "        float_inverse_kv_mask = inverse_kv_mask.type_as(q).masked_fill(\n",
    "            inverse_kv_mask, -1e4)\n",
    "        # compute the diagonal mask (for each local window)\n",
    "        diagonal_mask = self._sliding_chunks_query_key_matmul(\n",
    "            float_inverse_kv_mask.new_ones(size=float_inverse_kv_mask.size()),\n",
    "            float_inverse_kv_mask,\n",
    "            1,\n",
    "            self.window_overlap\n",
    "        )\n",
    "        att += diagonal_mask\n",
    "\n",
    "        # ignore input masking for now\n",
    "        att = nn.functional.softmax(att, dim=-1)\n",
    "        # softmax sometimes inserts NaN if all positions are masked, replace them with 0\n",
    "        att = att.masked_fill(\n",
    "            torch.logical_not(kv_mask.squeeze(1)[:, :, None, None]), 0.0)\n",
    "        att = self.attn_drop(att)\n",
    "\n",
    "        # step 4: compute attention value product + output projection\n",
    "        # chunked attn value product -> B, nh, T, hs\n",
    "        out = self._sliding_chunks_matmul_attn_probs_value(\n",
    "            att, v, self.n_head, self.window_overlap)\n",
    "        # transpose to B, nh, hs, T -> B, nh*hs, T\n",
    "        out = out.transpose(2, 3).contiguous().view(B, C, -1)\n",
    "        # output projection + skip connection\n",
    "        out = self.proj_drop(self.proj(out)) * qx_mask.to(out.dtype)\n",
    "        return out, qx_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "93295964-137f-4dc4-bf07-1242147ad791",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.426058Z",
     "iopub.status.busy": "2025-09-30T08:45:58.425362Z",
     "iopub.status.idle": "2025-09-30T08:45:58.437779Z",
     "shell.execute_reply": "2025-09-30T08:45:58.437220Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.426033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BufferList(nn.Module):\n",
    "    \"\"\"\n",
    "    Similar to nn.ParameterList, but for buffers\n",
    "\n",
    "    Taken from https://github.com/facebookresearch/detectron2/blob/master/detectron2/modeling/anchor_generator.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buffers):\n",
    "        super().__init__()\n",
    "        for i, buffer in enumerate(buffers):\n",
    "            # Use non-persistent buffer so the values are not saved in checkpoint\n",
    "            self.register_buffer(str(i), buffer, persistent=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._buffers)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._buffers.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "eec4d061",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.438778Z",
     "iopub.status.busy": "2025-09-30T08:45:58.438427Z",
     "iopub.status.idle": "2025-09-30T08:45:58.697514Z",
     "shell.execute_reply": "2025-09-30T08:45:58.696709Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.438753Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model - direct instantiation\n",
    "model = PtTransformer(\n",
    "    backbone_type=backbone_type,\n",
    "    fpn_type=fpn_type,\n",
    "    backbone_arch=backbone_arch,\n",
    "    scale_factor=scale_factor,\n",
    "    input_dim=input_dim,\n",
    "    max_seq_len=max_seq_len,\n",
    "    max_buffer_len_factor=max_buffer_len_factor,\n",
    "    n_head=n_head,\n",
    "    n_mha_win_size=n_mha_win_size,\n",
    "    embd_kernel_size=embd_kernel_size,\n",
    "    embd_dim=embd_dim,\n",
    "    embd_with_ln=embd_with_ln,\n",
    "    fpn_dim=fpn_dim,\n",
    "    fpn_with_ln=fpn_with_ln,\n",
    "    fpn_start_level=fpn_start_level,\n",
    "    head_dim=head_dim,\n",
    "    regression_range=regression_range,\n",
    "    head_num_layers=head_num_layers,\n",
    "    head_kernel_size=head_kernel_size,\n",
    "    head_with_ln=head_with_ln,\n",
    "    use_abs_pe=use_abs_pe,\n",
    "    use_rel_pe=use_rel_pe,\n",
    "    num_classes=num_classes,\n",
    "    train_cfg=train_cfg,\n",
    "    test_cfg=test_cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cdd42c0d-49a9-49c7-bdfc-886cfe7aff83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.698738Z",
     "iopub.status.busy": "2025-09-30T08:45:58.698508Z",
     "iopub.status.idle": "2025-09-30T08:45:58.703982Z",
     "shell.execute_reply": "2025-09-30T08:45:58.703220Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.698720Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PtTransformer(\n",
      "  (backbone): ConvTransformerBackbone(\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (embd): ModuleList(\n",
      "      (0): MaskedConv1D(\n",
      "        (conv): Conv1d(2048, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      )\n",
      "      (1): MaskedConv1D(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (embd_norm): ModuleList(\n",
      "      (0-1): 2 x LayerNorm()\n",
      "    )\n",
      "    (stem): ModuleList(\n",
      "      (0-1): 2 x TransformerBlock(\n",
      "        (ln1): LayerNorm()\n",
      "        (ln2): LayerNorm()\n",
      "        (attn): LocalMaskedMHCA(\n",
      "          (query_conv): MaskedConv1D(\n",
      "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)\n",
      "          )\n",
      "          (query_norm): LayerNorm()\n",
      "          (key_conv): MaskedConv1D(\n",
      "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)\n",
      "          )\n",
      "          (key_norm): LayerNorm()\n",
      "          (value_conv): MaskedConv1D(\n",
      "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)\n",
      "          )\n",
      "          (value_norm): LayerNorm()\n",
      "          (key): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "          (query): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "          (value): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (pool_skip): Identity()\n",
      "        (mlp): Sequential(\n",
      "          (0): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=True)\n",
      "          (3): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
      "          (4): Dropout(p=0.0, inplace=True)\n",
      "        )\n",
      "        (drop_path_attn): AffineDropPath()\n",
      "        (drop_path_mlp): AffineDropPath()\n",
      "      )\n",
      "    )\n",
      "    (branch): ModuleList(\n",
      "      (0-4): 5 x TransformerBlock(\n",
      "        (ln1): LayerNorm()\n",
      "        (ln2): LayerNorm()\n",
      "        (attn): LocalMaskedMHCA(\n",
      "          (query_conv): MaskedConv1D(\n",
      "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,), groups=512, bias=False)\n",
      "          )\n",
      "          (query_norm): LayerNorm()\n",
      "          (key_conv): MaskedConv1D(\n",
      "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,), groups=512, bias=False)\n",
      "          )\n",
      "          (key_norm): LayerNorm()\n",
      "          (value_conv): MaskedConv1D(\n",
      "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,), groups=512, bias=False)\n",
      "          )\n",
      "          (value_norm): LayerNorm()\n",
      "          (key): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "          (query): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "          (value): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (pool_skip): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (mlp): Sequential(\n",
      "          (0): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.0, inplace=True)\n",
      "          (3): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
      "          (4): Dropout(p=0.0, inplace=True)\n",
      "        )\n",
      "        (drop_path_attn): AffineDropPath()\n",
      "        (drop_path_mlp): AffineDropPath()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (neck): FPN1D(\n",
      "    (lateral_convs): ModuleList(\n",
      "      (0-5): 6 x MaskedConv1D(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (fpn_convs): ModuleList(\n",
      "      (0-5): 6 x MaskedConv1D(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), groups=512, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (fpn_norms): ModuleList(\n",
      "      (0-5): 6 x LayerNorm()\n",
      "    )\n",
      "  )\n",
      "  (point_generator): PointGenerator(\n",
      "    (buffer_points): BufferList()\n",
      "  )\n",
      "  (cls_head): PtTransformerClsHead(\n",
      "    (act): ReLU()\n",
      "    (head): ModuleList(\n",
      "      (0-1): 2 x MaskedConv1D(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): ModuleList(\n",
      "      (0-1): 2 x LayerNorm()\n",
      "    )\n",
      "    (cls_head): MaskedConv1D(\n",
      "      (conv): Conv1d(512, 20, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "  )\n",
      "  (reg_head): PtTransformerRegHead(\n",
      "    (act): ReLU()\n",
      "    (head): ModuleList(\n",
      "      (0-1): 2 x MaskedConv1D(\n",
      "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): ModuleList(\n",
      "      (0-1): 2 x LayerNorm()\n",
      "    )\n",
      "    (scale): ModuleList(\n",
      "      (0-5): 6 x Scale()\n",
      "    )\n",
      "    (offset_head): MaskedConv1D(\n",
      "      (conv): Conv1d(512, 2, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "52c5ae0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.705437Z",
     "iopub.status.busy": "2025-09-30T08:45:58.704688Z",
     "iopub.status.idle": "2025-09-30T08:45:58.763445Z",
     "shell.execute_reply": "2025-09-30T08:45:58.762904Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.705413Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = nn.DataParallel(model, device_ids=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "07f40bf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.764381Z",
     "iopub.status.busy": "2025-09-30T08:45:58.764148Z",
     "iopub.status.idle": "2025-09-30T08:45:58.768065Z",
     "shell.execute_reply": "2025-09-30T08:45:58.767421Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.764357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "opt_cfg = {\n",
    "    'type': opt_type,\n",
    "    'momentum': momentum,\n",
    "    'weight_decay': weight_decay,\n",
    "    'learning_rate': learning_rate,\n",
    "    'epochs': epochs,\n",
    "    'warmup': warmup,\n",
    "    'warmup_epochs': warmup_epochs,\n",
    "    'schedule_type': schedule_type,\n",
    "    'schedule_steps': schedule_steps,\n",
    "    'schedule_gamma': schedule_gamma\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1a8660d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.769000Z",
     "iopub.status.busy": "2025-09-30T08:45:58.768764Z",
     "iopub.status.idle": "2025-09-30T08:45:58.784982Z",
     "shell.execute_reply": "2025-09-30T08:45:58.784416Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.768978Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "class LinearWarmupCosineAnnealingLR(_LRScheduler):\n",
    "    \"\"\"\n",
    "    Sets the learning rate of each parameter group to follow a linear warmup schedule\n",
    "    between warmup_start_lr and base_lr followed by a cosine annealing schedule between\n",
    "    base_lr and eta_min.\n",
    "\n",
    "    .. warning::\n",
    "        It is recommended to call :func:`.step()` for :class:`LinearWarmupCosineAnnealingLR`\n",
    "        after each iteration as calling it after each epoch will keep the starting lr at\n",
    "        warmup_start_lr for the first epoch which is 0 in most cases.\n",
    "\n",
    "    .. warning::\n",
    "        passing epoch to :func:`.step()` is being deprecated and comes with an EPOCH_DEPRECATION_WARNING.\n",
    "        It calls the :func:`_get_closed_form_lr()` method for this scheduler instead of\n",
    "        :func:`get_lr()`. Though this does not change the behavior of the scheduler, when passing\n",
    "        epoch param to :func:`.step()`, the user should call the :func:`.step()` function before calling\n",
    "        train and validation methods.\n",
    "\n",
    "    Example:\n",
    "        >>> layer = nn.Linear(10, 1)\n",
    "        >>> optimizer = Adam(layer.parameters(), lr=0.02)\n",
    "        >>> scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=10, max_epochs=40)\n",
    "        >>> #\n",
    "        >>> # the default case\n",
    "        >>> for epoch in range(40):\n",
    "        ...     # train(...)\n",
    "        ...     # validate(...)\n",
    "        ...     scheduler.step()\n",
    "        >>> #\n",
    "        >>> # passing epoch param case\n",
    "        >>> for epoch in range(40):\n",
    "        ...     scheduler.step(epoch)\n",
    "        ...     # train(...)\n",
    "        ...     # validate(...)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer,\n",
    "        warmup_epochs,\n",
    "        max_epochs,\n",
    "        warmup_start_lr = 0.0,\n",
    "        eta_min = 1e-8,\n",
    "        last_epoch = -1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            optimizer (Optimizer): Wrapped optimizer.\n",
    "            warmup_epochs (int): Maximum number of iterations for linear warmup\n",
    "            max_epochs (int): Maximum number of iterations\n",
    "            warmup_start_lr (float): Learning rate to start the linear warmup. Default: 0.\n",
    "            eta_min (float): Minimum learning rate. Default: 0.\n",
    "            last_epoch (int): The index of last epoch. Default: -1.\n",
    "        \"\"\"\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.max_epochs = max_epochs\n",
    "        self.warmup_start_lr = warmup_start_lr\n",
    "        self.eta_min = eta_min\n",
    "\n",
    "        super(LinearWarmupCosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"\n",
    "        Compute learning rate using chainable form of the scheduler\n",
    "        \"\"\"\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\n",
    "                \"To get the last learning rate computed by the scheduler, \"\n",
    "                \"please use `get_last_lr()`.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "\n",
    "        if self.last_epoch == 0:\n",
    "            return [self.warmup_start_lr] * len(self.base_lrs)\n",
    "        elif self.last_epoch < self.warmup_epochs:\n",
    "            return [\n",
    "                group[\"lr\"] + (base_lr - self.warmup_start_lr) / (self.warmup_epochs - 1)\n",
    "                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n",
    "            ]\n",
    "        elif self.last_epoch == self.warmup_epochs:\n",
    "            return self.base_lrs\n",
    "        elif (self.last_epoch - 1 - self.max_epochs) % (2 * (self.max_epochs - self.warmup_epochs)) == 0:\n",
    "            return [\n",
    "                group[\"lr\"] + (base_lr - self.eta_min) *\n",
    "                (1 - math.cos(math.pi / (self.max_epochs - self.warmup_epochs))) / 2\n",
    "                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n",
    "            ]\n",
    "\n",
    "        return [\n",
    "            (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs))) /\n",
    "            (\n",
    "                1 +\n",
    "                math.cos(math.pi * (self.last_epoch - self.warmup_epochs - 1) / (self.max_epochs - self.warmup_epochs))\n",
    "            ) * (group[\"lr\"] - self.eta_min) + self.eta_min for group in self.optimizer.param_groups\n",
    "        ]\n",
    "\n",
    "    def _get_closed_form_lr(self):\n",
    "        \"\"\"\n",
    "        Called when epoch is passed as a param to the `step` function of the scheduler.\n",
    "        \"\"\"\n",
    "        if self.last_epoch < self.warmup_epochs:\n",
    "            return [\n",
    "                self.warmup_start_lr + self.last_epoch * (base_lr - self.warmup_start_lr) / (self.warmup_epochs - 1)\n",
    "                for base_lr in self.base_lrs\n",
    "            ]\n",
    "\n",
    "        return [\n",
    "            self.eta_min + 0.5 * (base_lr - self.eta_min) *\n",
    "            (1 + math.cos(math.pi * (self.last_epoch - self.warmup_epochs) / (self.max_epochs - self.warmup_epochs)))\n",
    "            for base_lr in self.base_lrs\n",
    "        ]\n",
    "\n",
    "\n",
    "class LinearWarmupMultiStepLR(_LRScheduler):\n",
    "    \"\"\"\n",
    "    Sets the learning rate of each parameter group to follow a linear warmup schedule\n",
    "    between warmup_start_lr and base_lr followed by a multi-step schedule that decays\n",
    "    the learning rate of each parameter group by gamma once the\n",
    "    number of epoch reaches one of the milestones.\n",
    "\n",
    "    .. warning::\n",
    "        It is recommended to call :func:`.step()` for :class:`LinearWarmupCosineAnnealingLR`\n",
    "        after each iteration as calling it after each epoch will keep the starting lr at\n",
    "        warmup_start_lr for the first epoch which is 0 in most cases.\n",
    "\n",
    "    .. warning::\n",
    "        passing epoch to :func:`.step()` is being deprecated and comes with an EPOCH_DEPRECATION_WARNING.\n",
    "        It calls the :func:`_get_closed_form_lr()` method for this scheduler instead of\n",
    "        :func:`get_lr()`. Though this does not change the behavior of the scheduler, when passing\n",
    "        epoch param to :func:`.step()`, the user should call the :func:`.step()` function before calling\n",
    "        train and validation methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer,\n",
    "        warmup_epochs,\n",
    "        milestones,\n",
    "        warmup_start_lr = 0.0,\n",
    "        gamma = 0.1,\n",
    "        last_epoch = -1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            optimizer (Optimizer): Wrapped optimizer.\n",
    "            warmup_epochs (int): Maximum number of iterations for linear warmup\n",
    "            max_epochs (int): Maximum number of iterations\n",
    "            milestones (list): List of epoch indices. Must be increasing.\n",
    "            warmup_start_lr (float): Learning rate to start the linear warmup. Default: 0.\n",
    "            gamma (float): Multiplicative factor of learning rate decay.\n",
    "            Default: 0.1.\n",
    "            last_epoch (int): The index of last epoch. Default: -1.\n",
    "        \"\"\"\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.warmup_start_lr = warmup_start_lr\n",
    "        self.milestones = Counter(milestones)\n",
    "        self.gamma = gamma\n",
    "\n",
    "        super(LinearWarmupMultiStepLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"\n",
    "        Compute learning rate using chainable form of the scheduler\n",
    "        \"\"\"\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "        if self.last_epoch == 0:\n",
    "            # starting warm up\n",
    "            return [self.warmup_start_lr] * len(self.base_lrs)\n",
    "        elif self.last_epoch < self.warmup_epochs:\n",
    "            # linear warm up (0 ~ self.warmup_epochs -1)\n",
    "            return [\n",
    "                group[\"lr\"] + (base_lr - self.warmup_start_lr) / (self.warmup_epochs - 1)\n",
    "                for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups)\n",
    "            ]\n",
    "        elif self.last_epoch == self.warmup_epochs:\n",
    "            # end of warm up (reset to base lrs)\n",
    "            return self.base_lrs\n",
    "        elif (self.last_epoch - self.warmup_epochs) not in self.milestones:\n",
    "            # in between the steps\n",
    "            return [group['lr'] for group in self.optimizer.param_groups]\n",
    "\n",
    "        return [\n",
    "            group['lr'] * self.gamma ** self.milestones[self.last_epoch - self.warmup_epochs]\n",
    "            for group in self.optimizer.param_groups\n",
    "        ]\n",
    "\n",
    "    def _get_closed_form_lr(self):\n",
    "        \"\"\"\n",
    "        Called when epoch is passed as a param to the `step` function of the scheduler.\n",
    "        \"\"\"\n",
    "        if self.last_epoch < self.warmup_epochs:\n",
    "            return [\n",
    "                self.warmup_start_lr + self.last_epoch * (base_lr - self.warmup_start_lr) / (self.warmup_epochs - 1)\n",
    "                for base_lr in self.base_lrs\n",
    "            ]\n",
    "\n",
    "        milestones = list(sorted(self.milestones.elements()))\n",
    "        return [base_lr * self.gamma ** bisect_right(milestones, self.last_epoch - self.warmup_epochs)\n",
    "                for base_lr in self.base_lrs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a4005f7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.785968Z",
     "iopub.status.busy": "2025-09-30T08:45:58.785680Z",
     "iopub.status.idle": "2025-09-30T08:45:58.801249Z",
     "shell.execute_reply": "2025-09-30T08:45:58.800625Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.785943Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_optimizer(model, optimizer_config):\n",
    "    \"\"\"create optimizer\n",
    "    return a supported optimizer\n",
    "    \"\"\"\n",
    "    # separate out all parameters that with / without weight decay\n",
    "    # see https://github.com/karpathy/minGPT/blob/master/mingpt/model.py#L134\n",
    "    decay = set()\n",
    "    no_decay = set()\n",
    "    whitelist_weight_modules = (torch.nn.Linear, torch.nn.Conv1d, MaskedConv1D)\n",
    "    blacklist_weight_modules = (LayerNorm, torch.nn.GroupNorm)\n",
    "\n",
    "    # loop over all modules / params\n",
    "    for mn, m in model.named_modules():\n",
    "        for pn, p in m.named_parameters():\n",
    "            fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "            if pn.endswith('bias'):\n",
    "                # all biases will not be decayed\n",
    "                no_decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                # weights of whitelist modules will be weight decayed\n",
    "                decay.add(fpn)\n",
    "            elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                # weights of blacklist modules will NOT be weight decayed\n",
    "                no_decay.add(fpn)\n",
    "            elif pn.endswith('scale') and isinstance(m, (Scale, AffineDropPath)):\n",
    "                # corner case of our scale layer\n",
    "                no_decay.add(fpn)\n",
    "            elif pn.endswith('rel_pe'):\n",
    "                # corner case for relative position encoding\n",
    "                no_decay.add(fpn)\n",
    "\n",
    "    # validate that we considered every parameter\n",
    "    param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "    inter_params = decay & no_decay\n",
    "    union_params = decay | no_decay\n",
    "    assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "    assert len(param_dict.keys() - union_params) == 0, \\\n",
    "        \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "        % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "    # create the pytorch optimizer object\n",
    "    optim_groups = [\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": optimizer_config['weight_decay']},\n",
    "        {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "    if optimizer_config[\"type\"] == \"SGD\":\n",
    "        optimizer = optim.SGD(\n",
    "            optim_groups,\n",
    "            lr=optimizer_config[\"learning_rate\"],\n",
    "            momentum=optimizer_config[\"momentum\"]\n",
    "        )\n",
    "    elif optimizer_config[\"type\"] == \"AdamW\":\n",
    "        optimizer = optim.AdamW(\n",
    "            optim_groups,\n",
    "            lr=optimizer_config[\"learning_rate\"]\n",
    "        )\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported optimizer!\")\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def make_scheduler(\n",
    "    optimizer,\n",
    "    optimizer_config,\n",
    "    num_iters_per_epoch,\n",
    "    last_epoch=-1\n",
    "):\n",
    "    \"\"\"create scheduler\n",
    "    return a supported scheduler\n",
    "    All scheduler returned by this function should step every iteration\n",
    "    \"\"\"\n",
    "    if optimizer_config[\"warmup\"]:\n",
    "        max_epochs = optimizer_config[\"epochs\"] + optimizer_config[\"warmup_epochs\"]\n",
    "        max_steps = max_epochs * num_iters_per_epoch\n",
    "\n",
    "        # get warmup params\n",
    "        warmup_epochs = optimizer_config[\"warmup_epochs\"]\n",
    "        warmup_steps = warmup_epochs * num_iters_per_epoch\n",
    "\n",
    "        # with linear warmup: call our custom schedulers\n",
    "        if optimizer_config[\"schedule_type\"] == \"cosine\":\n",
    "            # Cosine\n",
    "            scheduler = LinearWarmupCosineAnnealingLR(\n",
    "                optimizer,\n",
    "                warmup_steps,\n",
    "                max_steps,\n",
    "                last_epoch=last_epoch\n",
    "            )\n",
    "\n",
    "        elif optimizer_config[\"schedule_type\"] == \"multistep\":\n",
    "            # Multi step\n",
    "            steps = [num_iters_per_epoch * step for step in optimizer_config[\"schedule_steps\"]]\n",
    "            scheduler = LinearWarmupMultiStepLR(\n",
    "                optimizer,\n",
    "                warmup_steps,\n",
    "                steps,\n",
    "                gamma=optimizer_config[\"schedule_gamma\"],\n",
    "                last_epoch=last_epoch\n",
    "            )\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported scheduler!\")\n",
    "\n",
    "    else:\n",
    "        max_epochs = optimizer_config[\"epochs\"]\n",
    "        max_steps = max_epochs * num_iters_per_epoch\n",
    "\n",
    "        # without warmup: call default schedulers\n",
    "        if optimizer_config[\"schedule_type\"] == \"cosine\":\n",
    "            # step per iteration\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                max_steps,\n",
    "                last_epoch=last_epoch\n",
    "            )\n",
    "\n",
    "        elif optimizer_config[\"schedule_type\"] == \"multistep\":\n",
    "            # step every some epochs\n",
    "            steps = [num_iters_per_epoch * step for step in optimizer_config[\"schedule_steps\"]]\n",
    "            scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "                optimizer,\n",
    "                steps,\n",
    "                gamma=schedule_config[\"gamma\"],\n",
    "                last_epoch=last_epoch\n",
    "            )\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported scheduler!\")\n",
    "\n",
    "    return scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cad3355e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:45:58.802250Z",
     "iopub.status.busy": "2025-09-30T08:45:58.802013Z",
     "iopub.status.idle": "2025-09-30T08:46:01.430928Z",
     "shell.execute_reply": "2025-09-30T08:46:01.430337Z",
     "shell.execute_reply.started": "2025-09-30T08:45:58.802226Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = make_optimizer(model, opt_cfg)\n",
    "# schedule\n",
    "num_iters_per_epoch = len(train_loader)\n",
    "scheduler = make_scheduler(optimizer, opt_cfg, num_iters_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "275a76c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:46:01.431989Z",
     "iopub.status.busy": "2025-09-30T08:46:01.431593Z",
     "iopub.status.idle": "2025-09-30T08:46:01.438015Z",
     "shell.execute_reply": "2025-09-30T08:46:01.437340Z",
     "shell.execute_reply.started": "2025-09-30T08:46:01.431964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ModelEma(torch.nn.Module):\n",
    "    def __init__(self, model, decay=0.999, device=None):\n",
    "        super().__init__()\n",
    "        # make a copy of the model for accumulating moving average of weights\n",
    "        self.module = deepcopy(model)\n",
    "        self.module.eval()\n",
    "        self.decay = decay\n",
    "        self.device = device  # perform ema on different device from model if set\n",
    "        if self.device is not None:\n",
    "            self.module.to(device=device)\n",
    "\n",
    "    def _update(self, model, update_fn):\n",
    "        with torch.no_grad():\n",
    "            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n",
    "                if self.device is not None:\n",
    "                    model_v = model_v.to(device=self.device)\n",
    "                ema_v.copy_(update_fn(ema_v, model_v))\n",
    "\n",
    "    def update(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n",
    "\n",
    "    def set(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e6d0f3e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:46:01.438878Z",
     "iopub.status.busy": "2025-09-30T08:46:01.438646Z",
     "iopub.status.idle": "2025-09-30T08:46:01.498315Z",
     "shell.execute_reply": "2025-09-30T08:46:01.497792Z",
     "shell.execute_reply.started": "2025-09-30T08:46:01.438858Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model EMA ...\n"
     ]
    }
   ],
   "source": [
    "# enable model EMA\n",
    "print(\"Using model EMA ...\")\n",
    "model_ema = ModelEma(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fb5ad099",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:46:01.499215Z",
     "iopub.status.busy": "2025-09-30T08:46:01.499015Z",
     "iopub.status.idle": "2025-09-30T08:46:01.503974Z",
     "shell.execute_reply": "2025-09-30T08:46:01.503262Z",
     "shell.execute_reply.started": "2025-09-30T08:46:01.499199Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"4. Resume from model / Misc\"\"\"\n",
    "# resume from a checkpoint?\n",
    "if resume:\n",
    "    if os.path.isfile(resume):\n",
    "        # load ckpt, reset epoch / best rmse\n",
    "        checkpoint = torch.load(resume,\n",
    "            map_location = lambda storage, loc: storage.cuda(devices[0]))\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        model_ema.module.load_state_dict(checkpoint['state_dict_ema'])\n",
    "        # also load the optimizer / scheduler if necessary\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        print(\"=> loaded checkpoint '{:s}' (epoch {:d}\".format(\n",
    "            resume, checkpoint['epoch']\n",
    "        ))\n",
    "        del checkpoint\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(resume))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bac1528a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:46:01.504895Z",
     "iopub.status.busy": "2025-09-30T08:46:01.504660Z",
     "iopub.status.idle": "2025-09-30T08:46:01.513963Z",
     "shell.execute_reply": "2025-09-30T08:46:01.513383Z",
     "shell.execute_reply.started": "2025-09-30T08:46:01.504874Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# save the current parameters (instead of config)\n",
    "params_dict = {\n",
    "    'dataset_name': dataset_name,\n",
    "    'train_split': train_split,\n",
    "    'val_split': val_split,\n",
    "    'model_name': model_name,\n",
    "    'backbone_type': backbone_type,\n",
    "    'fpn_type': fpn_type,\n",
    "    'learning_rate': learning_rate,\n",
    "    'batch_size': batch_size,\n",
    "    'epochs': epochs,\n",
    "    # ... add other important parameters as needed\n",
    "}\n",
    "\n",
    "# save the current config\n",
    "with open(os.path.join(ckpt_folder, 'config.txt'), 'w') as fid:\n",
    "    pprint(params_dict, stream=fid)\n",
    "    fid.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a28cf3a4-3732-4f14-9d9e-e96d75e2e9a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:46:01.517449Z",
     "iopub.status.busy": "2025-09-30T08:46:01.517276Z",
     "iopub.status.idle": "2025-09-30T08:46:01.526842Z",
     "shell.execute_reply": "2025-09-30T08:46:01.526145Z",
     "shell.execute_reply.started": "2025-09-30T08:46:01.517437Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def truncate_feats(\n",
    "    data_dict,\n",
    "    max_seq_len,\n",
    "    trunc_thresh,\n",
    "    offset,\n",
    "    crop_ratio=None,\n",
    "    max_num_trials=200,\n",
    "    has_action=True,\n",
    "    no_trunc=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Truncate feats and time stamps in a dict item\n",
    "\n",
    "    data_dict = {'video_id'        : str\n",
    "                 'feats'           : Tensor C x T\n",
    "                 'segments'        : Tensor N x 2 (in feature grid)\n",
    "                 'labels'          : Tensor N\n",
    "                 'fps'             : float\n",
    "                 'feat_stride'     : int\n",
    "                 'feat_num_frames' : in\n",
    "\n",
    "    \"\"\"\n",
    "    # get the meta info\n",
    "    feat_len = data_dict['feats'].shape[1]\n",
    "    num_segs = data_dict['segments'].shape[0]\n",
    "\n",
    "    # seq_len < max_seq_len\n",
    "    if feat_len <= max_seq_len:\n",
    "        # do nothing\n",
    "        if crop_ratio == None:\n",
    "            return data_dict\n",
    "        # randomly crop the seq by setting max_seq_len to a value in [l, r]\n",
    "        else:\n",
    "            max_seq_len = random.randint(\n",
    "                max(round(crop_ratio[0] * feat_len), 1),\n",
    "                min(round(crop_ratio[1] * feat_len), feat_len),\n",
    "            )\n",
    "            # # corner case\n",
    "            if feat_len == max_seq_len:\n",
    "                return data_dict\n",
    "\n",
    "    # otherwise, deep copy the dict\n",
    "    data_dict = copy.deepcopy(data_dict)\n",
    "\n",
    "    # try a few times till a valid truncation with at least one action\n",
    "    for _ in range(max_num_trials):\n",
    "\n",
    "        # sample a random truncation of the video feats\n",
    "        st = random.randint(0, feat_len - max_seq_len)\n",
    "        ed = st + max_seq_len\n",
    "        window = torch.as_tensor([st, ed], dtype=torch.float32)\n",
    "\n",
    "        # compute the intersection between the sampled window and all segments\n",
    "        window = window[None].repeat(num_segs, 1)\n",
    "        left = torch.maximum(window[:, 0] - offset, data_dict['segments'][:, 0])\n",
    "        right = torch.minimum(window[:, 1] + offset, data_dict['segments'][:, 1])\n",
    "        inter = (right - left).clamp(min=0)\n",
    "        area_segs = torch.abs(\n",
    "            data_dict['segments'][:, 1] - data_dict['segments'][:, 0])\n",
    "        inter_ratio = inter / area_segs\n",
    "\n",
    "        # only select those segments over the thresh\n",
    "        seg_idx = (inter_ratio >= trunc_thresh)\n",
    "\n",
    "        if no_trunc:\n",
    "            # with at least one action and not truncating any actions\n",
    "            seg_trunc_idx = torch.logical_and(\n",
    "                (inter_ratio > 0.0), (inter_ratio < 1.0)\n",
    "            )\n",
    "            if (seg_idx.sum().item() > 0) and (seg_trunc_idx.sum().item() == 0):\n",
    "                break\n",
    "        elif has_action:\n",
    "            # with at least one action\n",
    "            if seg_idx.sum().item() > 0:\n",
    "                break\n",
    "        else:\n",
    "            # without any constraints\n",
    "            break\n",
    "\n",
    "    # feats: C x T\n",
    "    data_dict['feats'] = data_dict['feats'][:, st:ed].clone()\n",
    "    # segments: N x 2 in feature grids\n",
    "    data_dict['segments'] = torch.stack((left[seg_idx], right[seg_idx]), dim=1)\n",
    "    # shift the time stamps due to truncation\n",
    "    data_dict['segments'] = data_dict['segments'] - st\n",
    "    # labels: N\n",
    "    data_dict['labels'] = data_dict['labels'][seg_idx].clone()\n",
    "\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "faddc9a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:46:01.527705Z",
     "iopub.status.busy": "2025-09-30T08:46:01.527485Z",
     "iopub.status.idle": "2025-09-30T08:46:01.544021Z",
     "shell.execute_reply": "2025-09-30T08:46:01.543244Z",
     "shell.execute_reply.started": "2025-09-30T08:46:01.527683Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    train_loader,\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    curr_epoch,\n",
    "    model_ema = None,\n",
    "    clip_grad_l2norm = -1,\n",
    "    tb_writer = None,\n",
    "    print_freq = 20\n",
    "):\n",
    "    \"\"\"Training the model for one epoch\"\"\"\n",
    "    # set up meters\n",
    "    batch_time = AverageMeter()\n",
    "    losses_tracker = {}\n",
    "    # number of iterations per epoch\n",
    "    num_iters = len(train_loader)\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    # main training loop\n",
    "    print(\"\\n[Train]: Epoch {:d} started\".format(curr_epoch))\n",
    "    start = time.time()\n",
    "    for iter_idx, video_list in enumerate(train_loader, 0):\n",
    "        # zero out optim\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # forward / backward the model\n",
    "        losses = model(video_list)\n",
    "        losses['final_loss'].backward()\n",
    "        # gradient cliping (to stabilize training if necessary)\n",
    "        if clip_grad_l2norm > 0.0:\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                clip_grad_l2norm\n",
    "            )\n",
    "        # step optimizer / scheduler\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "\n",
    "        # printing (only check the stats when necessary to avoid extra cost)\n",
    "        if (iter_idx != 0) and (iter_idx % print_freq) == 0:\n",
    "            # measure elapsed time (sync all kernels)\n",
    "            torch.cuda.synchronize()\n",
    "            batch_time.update((time.time() - start) / print_freq)\n",
    "            start = time.time()\n",
    "\n",
    "            # track all losses\n",
    "            for key, value in losses.items():\n",
    "                # init meter if necessary\n",
    "                if key not in losses_tracker:\n",
    "                    losses_tracker[key] = AverageMeter()\n",
    "                # update\n",
    "                losses_tracker[key].update(value.item())\n",
    "\n",
    "            # log to tensor board\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            global_step = curr_epoch * num_iters + iter_idx\n",
    "            if tb_writer is not None:\n",
    "                # learning rate (after stepping)\n",
    "                tb_writer.add_scalar(\n",
    "                    'train/learning_rate',\n",
    "                    lr,\n",
    "                    global_step\n",
    "                )\n",
    "                # all losses\n",
    "                tag_dict = {}\n",
    "                for key, value in losses_tracker.items():\n",
    "                    if key != \"final_loss\":\n",
    "                        tag_dict[key] = value.val\n",
    "                tb_writer.add_scalars(\n",
    "                    'train/all_losses',\n",
    "                    tag_dict,\n",
    "                    global_step\n",
    "                )\n",
    "                # final loss\n",
    "                tb_writer.add_scalar(\n",
    "                    'train/final_loss',\n",
    "                    losses_tracker['final_loss'].val,\n",
    "                    global_step\n",
    "                )\n",
    "\n",
    "            # print to terminal\n",
    "            block1 = 'Epoch: [{:03d}][{:05d}/{:05d}]'.format(\n",
    "                curr_epoch, iter_idx, num_iters\n",
    "            )\n",
    "            block2 = 'Time {:.2f} ({:.2f})'.format(\n",
    "                batch_time.val, batch_time.avg\n",
    "            )\n",
    "            block3 = 'Loss {:.2f} ({:.2f})\\n'.format(\n",
    "                losses_tracker['final_loss'].val,\n",
    "                losses_tracker['final_loss'].avg\n",
    "            )\n",
    "            block4 = ''\n",
    "            for key, value in losses_tracker.items():\n",
    "                if key != \"final_loss\":\n",
    "                    block4  += '\\t{:s} {:.2f} ({:.2f})'.format(\n",
    "                        key, value.val, value.avg\n",
    "                    )\n",
    "\n",
    "            print('\\t'.join([block1, block2, block3, block4]))\n",
    "\n",
    "    # finish up and print\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    print(\"[Train]: Epoch {:d} finished with lr={:.8f}\\n\".format(curr_epoch, lr))\n",
    "    return\n",
    "\n",
    "\n",
    "def valid_one_epoch(\n",
    "    val_loader,\n",
    "    model,\n",
    "    curr_epoch,\n",
    "    ext_score_file = None,\n",
    "    evaluator = None,\n",
    "    output_file = None,\n",
    "    tb_writer = None,\n",
    "    print_freq = 20\n",
    "):\n",
    "    \"\"\"Test the model on the validation set\"\"\"\n",
    "    # either evaluate the results or save the results\n",
    "    assert (evaluator is not None) or (output_file is not None)\n",
    "\n",
    "    # set up meters\n",
    "    batch_time = AverageMeter()\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    # dict for results (for our evaluation code)\n",
    "    results = {\n",
    "        'video-id': [],\n",
    "        't-start' : [],\n",
    "        't-end': [],\n",
    "        'label': [],\n",
    "        'score': []\n",
    "    }\n",
    "\n",
    "    # loop over validation set\n",
    "    start = time.time()\n",
    "    for iter_idx, video_list in enumerate(val_loader, 0):\n",
    "        # forward the model (wo. grad)\n",
    "        with torch.no_grad():\n",
    "            output = model(video_list)\n",
    "\n",
    "            # unpack the results into ANet format\n",
    "            num_vids = len(output)\n",
    "            for vid_idx in range(num_vids):\n",
    "                if output[vid_idx]['segments'].shape[0] > 0:\n",
    "                    results['video-id'].extend(\n",
    "                        [output[vid_idx]['video_id']] *\n",
    "                        output[vid_idx]['segments'].shape[0]\n",
    "                    )\n",
    "                    results['t-start'].append(output[vid_idx]['segments'][:, 0])\n",
    "                    results['t-end'].append(output[vid_idx]['segments'][:, 1])\n",
    "                    results['label'].append(output[vid_idx]['labels'])\n",
    "                    results['score'].append(output[vid_idx]['scores'])\n",
    "\n",
    "        # printing\n",
    "        if (iter_idx != 0) and iter_idx % (print_freq) == 0:\n",
    "            # measure elapsed time (sync all kernels)\n",
    "            torch.cuda.synchronize()\n",
    "            batch_time.update((time.time() - start) / print_freq)\n",
    "            start = time.time()\n",
    "\n",
    "            # print timing\n",
    "            print('Test: [{0:05d}/{1:05d}]\\t'\n",
    "                  'Time {batch_time.val:.2f} ({batch_time.avg:.2f})'.format(\n",
    "                  iter_idx, len(val_loader), batch_time=batch_time))\n",
    "\n",
    "    # gather all stats and evaluate\n",
    "    results['t-start'] = torch.cat(results['t-start']).numpy()\n",
    "    results['t-end'] = torch.cat(results['t-end']).numpy()\n",
    "    results['label'] = torch.cat(results['label']).numpy()\n",
    "    results['score'] = torch.cat(results['score']).numpy()\n",
    "\n",
    "    if evaluator is not None:\n",
    "        if ext_score_file is not None and isinstance(ext_score_file, str):\n",
    "            results = postprocess_results(results, ext_score_file)\n",
    "        # call the evaluator\n",
    "        _, mAP, _ = evaluator.evaluate(results, verbose=True)\n",
    "    else:\n",
    "        # dump to a pickle file that can be directly used for evaluation\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pickle.dump(results, f)\n",
    "        mAP = 0.0\n",
    "\n",
    "    # log mAP to tb_writer\n",
    "    if tb_writer is not None:\n",
    "        tb_writer.add_scalar('validation/mAP', mAP, curr_epoch)\n",
    "\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "178e7669",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:46:01.545094Z",
     "iopub.status.busy": "2025-09-30T08:46:01.544842Z",
     "iopub.status.idle": "2025-09-30T08:46:01.557979Z",
     "shell.execute_reply": "2025-09-30T08:46:01.557428Z",
     "shell.execute_reply.started": "2025-09-30T08:46:01.545073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, file_folder,\n",
    "                    file_name='checkpoint.pth.tar'):\n",
    "    \"\"\"save checkpoint to file\"\"\"\n",
    "    if not os.path.exists(file_folder):\n",
    "        os.mkdir(file_folder)\n",
    "    torch.save(state, os.path.join(file_folder, file_name))\n",
    "    if is_best:\n",
    "        # skip the optimization / scheduler state\n",
    "        state.pop('optimizer', None)\n",
    "        state.pop('scheduler', None)\n",
    "        torch.save(state, os.path.join(file_folder, 'model_best.pth.tar'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1c4662e7-56f6-4265-94f2-1a028eb0eb70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:46:01.558933Z",
     "iopub.status.busy": "2025-09-30T08:46:01.558696Z",
     "iopub.status.idle": "2025-09-30T08:46:01.572048Z",
     "shell.execute_reply": "2025-09-30T08:46:01.571238Z",
     "shell.execute_reply.started": "2025-09-30T08:46:01.558913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value.\n",
    "    Used to compute dataset stats from mini-batches\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.initialized = False\n",
    "        self.val = None\n",
    "        self.avg = None\n",
    "        self.sum = None\n",
    "        self.count = 0.0\n",
    "\n",
    "    def initialize(self, val, n):\n",
    "        self.val = val\n",
    "        self.avg = val\n",
    "        self.sum = val * n\n",
    "        self.count = n\n",
    "        self.initialized = True\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if not self.initialized:\n",
    "            self.initialize(val, n)\n",
    "        else:\n",
    "            self.add(val, n)\n",
    "\n",
    "    def add(self, val, n):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "466e259f-67c4-465d-a034-dd4cce19e804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:46:01.573085Z",
     "iopub.status.busy": "2025-09-30T08:46:01.572860Z",
     "iopub.status.idle": "2025-09-30T08:46:01.641284Z",
     "shell.execute_reply": "2025-09-30T08:46:01.640831Z",
     "shell.execute_reply.started": "2025-09-30T08:46:01.573062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def sigmoid_focal_loss(\n",
    "    inputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    alpha: float = 0.25,\n",
    "    gamma: float = 2.0,\n",
    "    reduction: str = \"none\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
    "    Taken from\n",
    "    https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py\n",
    "    # Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n",
    "\n",
    "    Args:\n",
    "        inputs: A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets: A float tensor with the same shape as inputs. Stores the binary\n",
    "                 classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "        alpha: (optional) Weighting factor in range (0,1) to balance\n",
    "                positive vs negative examples. Default = 0.25.\n",
    "        gamma: Exponent of the modulating factor (1 - p_t) to\n",
    "               balance easy vs hard examples.\n",
    "        reduction: 'none' | 'mean' | 'sum'\n",
    "                 'none': No reduction will be applied to the output.\n",
    "                 'mean': The output will be averaged.\n",
    "                 'sum': The output will be summed.\n",
    "    Returns:\n",
    "        Loss tensor with the reduction option applied.\n",
    "    \"\"\"\n",
    "    inputs = inputs.float()\n",
    "    targets = targets.float()\n",
    "    p = torch.sigmoid(inputs)\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "    p_t = p * targets + (1 - p) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        loss = loss.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        loss = loss.sum()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def ctr_giou_loss_1d(\n",
    "    input_offsets: torch.Tensor,\n",
    "    target_offsets: torch.Tensor,\n",
    "    reduction: str = 'none',\n",
    "    eps: float = 1e-8,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generalized Intersection over Union Loss (Hamid Rezatofighi et. al)\n",
    "    https://arxiv.org/abs/1902.09630\n",
    "\n",
    "    This is an implementation that assumes a 1D event is represented using\n",
    "    the same center point with different offsets, e.g.,\n",
    "    (t1, t2) = (c - o_1, c + o_2) with o_i >= 0\n",
    "\n",
    "    Reference code from\n",
    "    https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/giou_loss.py\n",
    "\n",
    "    Args:\n",
    "        input/target_offsets (Tensor): 1D offsets of size (N, 2)\n",
    "        reduction: 'none' | 'mean' | 'sum'\n",
    "                 'none': No reduction will be applied to the output.\n",
    "                 'mean': The output will be averaged.\n",
    "                 'sum': The output will be summed.\n",
    "        eps (float): small number to prevent division by zero\n",
    "    \"\"\"\n",
    "    input_offsets = input_offsets.float()\n",
    "    target_offsets = target_offsets.float()\n",
    "    # check all 1D events are valid\n",
    "    assert (input_offsets >= 0.0).all(), \"predicted offsets must be non-negative\"\n",
    "    assert (target_offsets >= 0.0).all(), \"GT offsets must be non-negative\"\n",
    "\n",
    "    lp, rp = input_offsets[:, 0], input_offsets[:, 1]\n",
    "    lg, rg = target_offsets[:, 0], target_offsets[:, 1]\n",
    "\n",
    "    # intersection key points\n",
    "    lkis = torch.min(lp, lg)\n",
    "    rkis = torch.min(rp, rg)\n",
    "\n",
    "    # iou\n",
    "    intsctk = rkis + lkis\n",
    "    unionk = (lp + rp) + (lg + rg) - intsctk\n",
    "    iouk = intsctk / unionk.clamp(min=eps)\n",
    "\n",
    "    # giou is reduced to iou in our setting, skip unnecessary steps\n",
    "    loss = 1.0 - iouk\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        loss = loss.mean() if loss.numel() > 0 else 0.0 * loss.sum()\n",
    "    elif reduction == \"sum\":\n",
    "        loss = loss.sum()\n",
    "\n",
    "    return loss\n",
    "\n",
    "@torch.jit.script\n",
    "def ctr_diou_loss_1d(\n",
    "    input_offsets: torch.Tensor,\n",
    "    target_offsets: torch.Tensor,\n",
    "    reduction: str = 'none',\n",
    "    eps: float = 1e-8,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Distance-IoU Loss (Zheng et. al)\n",
    "    https://arxiv.org/abs/1911.08287\n",
    "\n",
    "    This is an implementation that assumes a 1D event is represented using\n",
    "    the same center point with different offsets, e.g.,\n",
    "    (t1, t2) = (c - o_1, c + o_2) with o_i >= 0\n",
    "\n",
    "    Reference code from\n",
    "    https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/giou_loss.py\n",
    "\n",
    "    Args:\n",
    "        input/target_offsets (Tensor): 1D offsets of size (N, 2)\n",
    "        reduction: 'none' | 'mean' | 'sum'\n",
    "                 'none': No reduction will be applied to the output.\n",
    "                 'mean': The output will be averaged.\n",
    "                 'sum': The output will be summed.\n",
    "        eps (float): small number to prevent division by zero\n",
    "    \"\"\"\n",
    "    input_offsets = input_offsets.float()\n",
    "    target_offsets = target_offsets.float()\n",
    "    # check all 1D events are valid\n",
    "    assert (input_offsets >= 0.0).all(), \"predicted offsets must be non-negative\"\n",
    "    assert (target_offsets >= 0.0).all(), \"GT offsets must be non-negative\"\n",
    "\n",
    "    lp, rp = input_offsets[:, 0], input_offsets[:, 1]\n",
    "    lg, rg = target_offsets[:, 0], target_offsets[:, 1]\n",
    "\n",
    "    # intersection key points\n",
    "    lkis = torch.min(lp, lg)\n",
    "    rkis = torch.min(rp, rg)\n",
    "\n",
    "    # iou\n",
    "    intsctk = rkis + lkis\n",
    "    unionk = (lp + rp) + (lg + rg) - intsctk\n",
    "    iouk = intsctk / unionk.clamp(min=eps)\n",
    "\n",
    "    # smallest enclosing box\n",
    "    lc = torch.max(lp, lg)\n",
    "    rc = torch.max(rp, rg)\n",
    "    len_c = lc + rc\n",
    "\n",
    "    # offset between centers\n",
    "    rho = 0.5 * (rp - lp - rg + lg)\n",
    "\n",
    "    # diou\n",
    "    loss = 1.0 - iouk + torch.square(rho / len_c.clamp(min=eps))\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        loss = loss.mean() if loss.numel() > 0 else 0.0 * loss.sum()\n",
    "    elif reduction == \"sum\":\n",
    "        loss = loss.sum()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8e7b3f59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T08:46:01.642124Z",
     "iopub.status.busy": "2025-09-30T08:46:01.641941Z",
     "iopub.status.idle": "2025-09-30T09:12:53.866938Z",
     "shell.execute_reply": "2025-09-30T09:12:53.866147Z",
     "shell.execute_reply.started": "2025-09-30T08:46:01.642109Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start training model LocPointTransformer ...\n",
      "\n",
      "[Train]: Epoch 0 started\n",
      "Epoch: [000][00010/00100]\tTime 0.94 (0.94)\tLoss 0.75 (0.75)\n",
      "\t\tcls_loss 0.40 (0.40)\treg_loss 0.35 (0.35)\n",
      "Epoch: [000][00020/00100]\tTime 0.43 (0.68)\tLoss 1.65 (1.20)\n",
      "\t\tcls_loss 0.99 (0.70)\treg_loss 0.65 (0.50)\n",
      "Epoch: [000][00030/00100]\tTime 0.43 (0.60)\tLoss 6.26 (2.89)\n",
      "\t\tcls_loss 3.81 (1.73)\treg_loss 2.45 (1.15)\n",
      "Epoch: [000][00040/00100]\tTime 0.44 (0.56)\tLoss 1.65 (2.58)\n",
      "\t\tcls_loss 1.35 (1.64)\treg_loss 0.30 (0.94)\n",
      "Epoch: [000][00050/00100]\tTime 0.44 (0.53)\tLoss 2.29 (2.52)\n",
      "\t\tcls_loss 1.73 (1.66)\treg_loss 0.55 (0.86)\n",
      "Epoch: [000][00060/00100]\tTime 0.44 (0.52)\tLoss 1.06 (2.28)\n",
      "\t\tcls_loss 0.79 (1.51)\treg_loss 0.27 (0.76)\n",
      "Epoch: [000][00070/00100]\tTime 0.45 (0.51)\tLoss 0.13 (1.97)\n",
      "\t\tcls_loss 0.10 (1.31)\treg_loss 0.03 (0.66)\n",
      "Epoch: [000][00080/00100]\tTime 0.44 (0.50)\tLoss 0.50 (1.79)\n",
      "\t\tcls_loss 0.35 (1.19)\treg_loss 0.15 (0.60)\n",
      "Epoch: [000][00090/00100]\tTime 0.45 (0.50)\tLoss 1.05 (1.70)\n",
      "\t\tcls_loss 0.75 (1.14)\treg_loss 0.30 (0.56)\n",
      "[Train]: Epoch 0 finished with lr=0.00002004\n",
      "\n",
      "\n",
      "[Train]: Epoch 1 started\n",
      "Epoch: [001][00010/00100]\tTime 0.54 (0.54)\tLoss 0.48 (0.48)\n",
      "\t\tcls_loss 0.35 (0.35)\treg_loss 0.13 (0.13)\n",
      "Epoch: [001][00020/00100]\tTime 0.47 (0.50)\tLoss 0.80 (0.64)\n",
      "\t\tcls_loss 0.56 (0.45)\treg_loss 0.25 (0.19)\n",
      "Epoch: [001][00030/00100]\tTime 0.46 (0.49)\tLoss 0.32 (0.54)\n",
      "\t\tcls_loss 0.21 (0.37)\treg_loss 0.11 (0.16)\n",
      "Epoch: [001][00040/00100]\tTime 0.46 (0.48)\tLoss 0.65 (0.56)\n",
      "\t\tcls_loss 0.47 (0.40)\treg_loss 0.18 (0.17)\n",
      "Epoch: [001][00050/00100]\tTime 0.47 (0.48)\tLoss 0.73 (0.60)\n",
      "\t\tcls_loss 0.51 (0.42)\treg_loss 0.22 (0.18)\n",
      "Epoch: [001][00060/00100]\tTime 0.47 (0.48)\tLoss 0.65 (0.61)\n",
      "\t\tcls_loss 0.43 (0.42)\treg_loss 0.22 (0.18)\n",
      "Epoch: [001][00070/00100]\tTime 0.47 (0.48)\tLoss 0.33 (0.57)\n",
      "\t\tcls_loss 0.24 (0.40)\treg_loss 0.09 (0.17)\n",
      "Epoch: [001][00080/00100]\tTime 0.47 (0.48)\tLoss 1.80 (0.72)\n",
      "\t\tcls_loss 1.22 (0.50)\treg_loss 0.58 (0.22)\n",
      "Epoch: [001][00090/00100]\tTime 0.46 (0.47)\tLoss 0.09 (0.65)\n",
      "\t\tcls_loss 0.07 (0.45)\treg_loss 0.02 (0.20)\n",
      "[Train]: Epoch 1 finished with lr=0.00004008\n",
      "\n",
      "\n",
      "[Train]: Epoch 2 started\n",
      "Epoch: [002][00010/00100]\tTime 0.51 (0.51)\tLoss 1.44 (1.44)\n",
      "\t\tcls_loss 0.89 (0.89)\treg_loss 0.55 (0.55)\n",
      "Epoch: [002][00020/00100]\tTime 0.45 (0.48)\tLoss 0.51 (0.97)\n",
      "\t\tcls_loss 0.35 (0.62)\treg_loss 0.15 (0.35)\n",
      "Epoch: [002][00030/00100]\tTime 0.45 (0.47)\tLoss 0.84 (0.93)\n",
      "\t\tcls_loss 0.56 (0.60)\treg_loss 0.29 (0.33)\n",
      "Epoch: [002][00040/00100]\tTime 0.46 (0.47)\tLoss 0.29 (0.77)\n",
      "\t\tcls_loss 0.20 (0.50)\treg_loss 0.09 (0.27)\n",
      "Epoch: [002][00050/00100]\tTime 0.45 (0.46)\tLoss 0.36 (0.69)\n",
      "\t\tcls_loss 0.23 (0.45)\treg_loss 0.13 (0.24)\n",
      "Epoch: [002][00060/00100]\tTime 0.46 (0.46)\tLoss 1.89 (0.89)\n",
      "\t\tcls_loss 1.17 (0.57)\treg_loss 0.73 (0.32)\n",
      "Epoch: [002][00070/00100]\tTime 0.45 (0.46)\tLoss 0.49 (0.83)\n",
      "\t\tcls_loss 0.36 (0.54)\treg_loss 0.13 (0.30)\n",
      "Epoch: [002][00080/00100]\tTime 0.45 (0.46)\tLoss 1.74 (0.95)\n",
      "\t\tcls_loss 0.77 (0.57)\treg_loss 0.97 (0.38)\n",
      "Epoch: [002][00090/00100]\tTime 0.46 (0.46)\tLoss 1.87 (1.05)\n",
      "\t\tcls_loss 0.97 (0.61)\treg_loss 0.90 (0.44)\n",
      "[Train]: Epoch 2 finished with lr=0.00006012\n",
      "\n",
      "\n",
      "[Train]: Epoch 3 started\n",
      "Epoch: [003][00010/00100]\tTime 0.52 (0.52)\tLoss 1.28 (1.28)\n",
      "\t\tcls_loss 0.85 (0.85)\treg_loss 0.43 (0.43)\n",
      "Epoch: [003][00020/00100]\tTime 0.45 (0.49)\tLoss 0.22 (0.75)\n",
      "\t\tcls_loss 0.17 (0.51)\treg_loss 0.06 (0.24)\n",
      "Epoch: [003][00030/00100]\tTime 0.45 (0.48)\tLoss 0.35 (0.62)\n",
      "\t\tcls_loss 0.24 (0.42)\treg_loss 0.11 (0.20)\n",
      "Epoch: [003][00040/00100]\tTime 0.45 (0.47)\tLoss 1.19 (0.76)\n",
      "\t\tcls_loss 0.75 (0.50)\treg_loss 0.44 (0.26)\n",
      "Epoch: [003][00050/00100]\tTime 0.47 (0.47)\tLoss 0.59 (0.73)\n",
      "\t\tcls_loss 0.46 (0.49)\treg_loss 0.14 (0.23)\n",
      "Epoch: [003][00060/00100]\tTime 0.46 (0.47)\tLoss 0.30 (0.66)\n",
      "\t\tcls_loss 0.22 (0.45)\treg_loss 0.08 (0.21)\n",
      "Epoch: [003][00070/00100]\tTime 0.46 (0.47)\tLoss 1.87 (0.83)\n",
      "\t\tcls_loss 1.17 (0.55)\treg_loss 0.71 (0.28)\n",
      "Epoch: [003][00080/00100]\tTime 0.46 (0.47)\tLoss 0.21 (0.75)\n",
      "\t\tcls_loss 0.13 (0.50)\treg_loss 0.08 (0.25)\n",
      "Epoch: [003][00090/00100]\tTime 0.46 (0.47)\tLoss 0.63 (0.74)\n",
      "\t\tcls_loss 0.44 (0.49)\treg_loss 0.20 (0.25)\n",
      "[Train]: Epoch 3 finished with lr=0.00008016\n",
      "\n",
      "\n",
      "[Train]: Epoch 4 started\n",
      "Epoch: [004][00010/00100]\tTime 0.53 (0.53)\tLoss 0.51 (0.51)\n",
      "\t\tcls_loss 0.32 (0.32)\treg_loss 0.18 (0.18)\n",
      "Epoch: [004][00020/00100]\tTime 0.46 (0.49)\tLoss 0.33 (0.42)\n",
      "\t\tcls_loss 0.25 (0.28)\treg_loss 0.08 (0.13)\n",
      "Epoch: [004][00030/00100]\tTime 0.46 (0.48)\tLoss 0.78 (0.54)\n",
      "\t\tcls_loss 0.48 (0.35)\treg_loss 0.30 (0.19)\n",
      "Epoch: [004][00040/00100]\tTime 0.46 (0.47)\tLoss 0.36 (0.49)\n",
      "\t\tcls_loss 0.23 (0.32)\treg_loss 0.13 (0.17)\n",
      "Epoch: [004][00050/00100]\tTime 0.46 (0.47)\tLoss 0.51 (0.50)\n",
      "\t\tcls_loss 0.32 (0.32)\treg_loss 0.19 (0.18)\n",
      "Epoch: [004][00060/00100]\tTime 0.45 (0.47)\tLoss 2.65 (0.86)\n",
      "\t\tcls_loss 1.92 (0.59)\treg_loss 0.74 (0.27)\n",
      "Epoch: [004][00070/00100]\tTime 0.45 (0.47)\tLoss 0.23 (0.77)\n",
      "\t\tcls_loss 0.14 (0.52)\treg_loss 0.08 (0.24)\n",
      "Epoch: [004][00080/00100]\tTime 0.45 (0.46)\tLoss 0.46 (0.73)\n",
      "\t\tcls_loss 0.36 (0.50)\treg_loss 0.10 (0.23)\n",
      "Epoch: [004][00090/00100]\tTime 0.46 (0.46)\tLoss 0.94 (0.75)\n",
      "\t\tcls_loss 0.64 (0.52)\treg_loss 0.31 (0.23)\n",
      "[Train]: Epoch 4 finished with lr=0.00010000\n",
      "\n",
      "\n",
      "[Train]: Epoch 5 started\n",
      "Epoch: [005][00010/00100]\tTime 0.52 (0.52)\tLoss 0.58 (0.58)\n",
      "\t\tcls_loss 0.36 (0.36)\treg_loss 0.22 (0.22)\n",
      "Epoch: [005][00020/00100]\tTime 0.46 (0.49)\tLoss 1.23 (0.90)\n",
      "\t\tcls_loss 0.84 (0.60)\treg_loss 0.39 (0.30)\n",
      "Epoch: [005][00030/00100]\tTime 0.45 (0.48)\tLoss 3.06 (1.62)\n",
      "\t\tcls_loss 2.23 (1.14)\treg_loss 0.83 (0.48)\n",
      "Epoch: [005][00040/00100]\tTime 0.45 (0.47)\tLoss 0.78 (1.41)\n",
      "\t\tcls_loss 0.48 (0.98)\treg_loss 0.29 (0.43)\n",
      "Epoch: [005][00050/00100]\tTime 0.46 (0.47)\tLoss 0.53 (1.23)\n",
      "\t\tcls_loss 0.30 (0.84)\treg_loss 0.23 (0.39)\n",
      "Epoch: [005][00060/00100]\tTime 0.46 (0.47)\tLoss 2.29 (1.41)\n",
      "\t\tcls_loss 1.57 (0.96)\treg_loss 0.71 (0.45)\n",
      "Epoch: [005][00070/00100]\tTime 0.45 (0.47)\tLoss 0.68 (1.31)\n",
      "\t\tcls_loss 0.39 (0.88)\treg_loss 0.29 (0.42)\n",
      "Epoch: [005][00080/00100]\tTime 0.45 (0.46)\tLoss 0.16 (1.16)\n",
      "\t\tcls_loss 0.10 (0.78)\treg_loss 0.06 (0.38)\n",
      "Epoch: [005][00090/00100]\tTime 0.45 (0.46)\tLoss 0.91 (1.13)\n",
      "\t\tcls_loss 0.59 (0.76)\treg_loss 0.32 (0.37)\n",
      "[Train]: Epoch 5 finished with lr=0.00009973\n",
      "\n",
      "\n",
      "[Train]: Epoch 6 started\n",
      "Epoch: [006][00010/00100]\tTime 0.52 (0.52)\tLoss 1.93 (1.93)\n",
      "\t\tcls_loss 1.33 (1.33)\treg_loss 0.61 (0.61)\n",
      "Epoch: [006][00020/00100]\tTime 0.46 (0.49)\tLoss 1.23 (1.58)\n",
      "\t\tcls_loss 0.74 (1.03)\treg_loss 0.49 (0.55)\n",
      "Epoch: [006][00030/00100]\tTime 0.45 (0.48)\tLoss 0.43 (1.20)\n",
      "\t\tcls_loss 0.27 (0.78)\treg_loss 0.16 (0.42)\n",
      "Epoch: [006][00040/00100]\tTime 0.45 (0.47)\tLoss 0.79 (1.09)\n",
      "\t\tcls_loss 0.51 (0.71)\treg_loss 0.28 (0.38)\n",
      "Epoch: [006][00050/00100]\tTime 0.46 (0.47)\tLoss 0.28 (0.93)\n",
      "\t\tcls_loss 0.19 (0.61)\treg_loss 0.10 (0.33)\n",
      "Epoch: [006][00060/00100]\tTime 0.46 (0.47)\tLoss 0.75 (0.90)\n",
      "\t\tcls_loss 0.43 (0.58)\treg_loss 0.32 (0.32)\n",
      "Epoch: [006][00070/00100]\tTime 0.46 (0.46)\tLoss 0.73 (0.88)\n",
      "\t\tcls_loss 0.48 (0.56)\treg_loss 0.25 (0.31)\n",
      "Epoch: [006][00080/00100]\tTime 0.46 (0.46)\tLoss 0.16 (0.79)\n",
      "\t\tcls_loss 0.10 (0.50)\treg_loss 0.06 (0.28)\n",
      "Epoch: [006][00090/00100]\tTime 0.46 (0.46)\tLoss 1.64 (0.88)\n",
      "\t\tcls_loss 1.05 (0.57)\treg_loss 0.59 (0.32)\n",
      "[Train]: Epoch 6 finished with lr=0.00009891\n",
      "\n",
      "\n",
      "[Train]: Epoch 7 started\n",
      "Epoch: [007][00010/00100]\tTime 0.51 (0.51)\tLoss 0.19 (0.19)\n",
      "\t\tcls_loss 0.12 (0.12)\treg_loss 0.06 (0.06)\n",
      "Epoch: [007][00020/00100]\tTime 0.46 (0.48)\tLoss 0.18 (0.18)\n",
      "\t\tcls_loss 0.11 (0.12)\treg_loss 0.07 (0.06)\n",
      "Epoch: [007][00030/00100]\tTime 0.46 (0.48)\tLoss 0.67 (0.35)\n",
      "\t\tcls_loss 0.40 (0.21)\treg_loss 0.27 (0.13)\n",
      "Epoch: [007][00040/00100]\tTime 0.46 (0.47)\tLoss 0.33 (0.34)\n",
      "\t\tcls_loss 0.18 (0.20)\treg_loss 0.15 (0.14)\n",
      "Epoch: [007][00050/00100]\tTime 0.46 (0.47)\tLoss 0.21 (0.32)\n",
      "\t\tcls_loss 0.16 (0.20)\treg_loss 0.05 (0.12)\n",
      "Epoch: [007][00060/00100]\tTime 0.46 (0.47)\tLoss 0.16 (0.29)\n",
      "\t\tcls_loss 0.10 (0.18)\treg_loss 0.06 (0.11)\n",
      "Epoch: [007][00070/00100]\tTime 0.45 (0.46)\tLoss 0.09 (0.26)\n",
      "\t\tcls_loss 0.07 (0.16)\treg_loss 0.03 (0.10)\n",
      "Epoch: [007][00080/00100]\tTime 0.45 (0.46)\tLoss 0.94 (0.35)\n",
      "\t\tcls_loss 0.54 (0.21)\treg_loss 0.40 (0.14)\n",
      "Epoch: [007][00090/00100]\tTime 0.45 (0.46)\tLoss 0.31 (0.34)\n",
      "\t\tcls_loss 0.17 (0.21)\treg_loss 0.14 (0.14)\n",
      "[Train]: Epoch 7 finished with lr=0.00009755\n",
      "\n",
      "\n",
      "[Train]: Epoch 8 started\n",
      "Epoch: [008][00010/00100]\tTime 0.53 (0.53)\tLoss 0.58 (0.58)\n",
      "\t\tcls_loss 0.35 (0.35)\treg_loss 0.24 (0.24)\n",
      "Epoch: [008][00020/00100]\tTime 0.46 (0.49)\tLoss 0.28 (0.43)\n",
      "\t\tcls_loss 0.20 (0.27)\treg_loss 0.08 (0.16)\n",
      "Epoch: [008][00030/00100]\tTime 0.46 (0.48)\tLoss 0.67 (0.51)\n",
      "\t\tcls_loss 0.38 (0.31)\treg_loss 0.29 (0.20)\n",
      "Epoch: [008][00040/00100]\tTime 0.46 (0.48)\tLoss 1.16 (0.68)\n",
      "\t\tcls_loss 0.73 (0.41)\treg_loss 0.43 (0.26)\n",
      "Epoch: [008][00050/00100]\tTime 0.45 (0.47)\tLoss 0.38 (0.62)\n",
      "\t\tcls_loss 0.25 (0.38)\treg_loss 0.13 (0.24)\n",
      "Epoch: [008][00060/00100]\tTime 0.45 (0.47)\tLoss 0.84 (0.65)\n",
      "\t\tcls_loss 0.57 (0.41)\treg_loss 0.26 (0.24)\n",
      "Epoch: [008][00070/00100]\tTime 0.45 (0.47)\tLoss 0.20 (0.59)\n",
      "\t\tcls_loss 0.13 (0.37)\treg_loss 0.07 (0.22)\n",
      "Epoch: [008][00080/00100]\tTime 0.46 (0.46)\tLoss 0.32 (0.56)\n",
      "\t\tcls_loss 0.21 (0.35)\treg_loss 0.11 (0.20)\n",
      "Epoch: [008][00090/00100]\tTime 0.46 (0.46)\tLoss 1.48 (0.66)\n",
      "\t\tcls_loss 0.94 (0.42)\treg_loss 0.54 (0.24)\n",
      "[Train]: Epoch 8 finished with lr=0.00009568\n",
      "\n",
      "\n",
      "[Train]: Epoch 9 started\n",
      "Epoch: [009][00010/00100]\tTime 0.52 (0.52)\tLoss 0.56 (0.56)\n",
      "\t\tcls_loss 0.31 (0.31)\treg_loss 0.26 (0.26)\n",
      "Epoch: [009][00020/00100]\tTime 0.45 (0.49)\tLoss 0.28 (0.42)\n",
      "\t\tcls_loss 0.18 (0.24)\treg_loss 0.10 (0.18)\n",
      "Epoch: [009][00030/00100]\tTime 0.45 (0.47)\tLoss 0.67 (0.50)\n",
      "\t\tcls_loss 0.34 (0.28)\treg_loss 0.33 (0.23)\n",
      "Epoch: [009][00040/00100]\tTime 0.46 (0.47)\tLoss 1.49 (0.75)\n",
      "\t\tcls_loss 0.83 (0.41)\treg_loss 0.66 (0.34)\n",
      "Epoch: [009][00050/00100]\tTime 0.46 (0.47)\tLoss 0.23 (0.65)\n",
      "\t\tcls_loss 0.13 (0.36)\treg_loss 0.10 (0.29)\n",
      "Epoch: [009][00060/00100]\tTime 0.46 (0.47)\tLoss 0.21 (0.57)\n",
      "\t\tcls_loss 0.12 (0.32)\treg_loss 0.09 (0.26)\n",
      "Epoch: [009][00070/00100]\tTime 0.46 (0.46)\tLoss 0.11 (0.51)\n",
      "\t\tcls_loss 0.07 (0.28)\treg_loss 0.04 (0.23)\n",
      "Epoch: [009][00080/00100]\tTime 0.45 (0.46)\tLoss 0.73 (0.54)\n",
      "\t\tcls_loss 0.42 (0.30)\treg_loss 0.31 (0.24)\n",
      "Epoch: [009][00090/00100]\tTime 0.46 (0.46)\tLoss 1.33 (0.62)\n",
      "\t\tcls_loss 0.66 (0.34)\treg_loss 0.67 (0.28)\n",
      "[Train]: Epoch 9 finished with lr=0.00009330\n",
      "\n",
      "\n",
      "[Train]: Epoch 10 started\n",
      "Epoch: [010][00010/00100]\tTime 0.53 (0.53)\tLoss 0.23 (0.23)\n",
      "\t\tcls_loss 0.14 (0.14)\treg_loss 0.10 (0.10)\n",
      "Epoch: [010][00020/00100]\tTime 0.46 (0.49)\tLoss 0.41 (0.32)\n",
      "\t\tcls_loss 0.22 (0.18)\treg_loss 0.19 (0.14)\n",
      "Epoch: [010][00030/00100]\tTime 0.46 (0.48)\tLoss 1.31 (0.65)\n",
      "\t\tcls_loss 0.67 (0.34)\treg_loss 0.64 (0.31)\n",
      "Epoch: [010][00040/00100]\tTime 0.46 (0.47)\tLoss 0.32 (0.57)\n",
      "\t\tcls_loss 0.19 (0.31)\treg_loss 0.12 (0.26)\n",
      "Epoch: [010][00050/00100]\tTime 0.45 (0.47)\tLoss 0.38 (0.53)\n",
      "\t\tcls_loss 0.24 (0.29)\treg_loss 0.14 (0.24)\n",
      "Epoch: [010][00060/00100]\tTime 0.46 (0.47)\tLoss 0.58 (0.54)\n",
      "\t\tcls_loss 0.33 (0.30)\treg_loss 0.26 (0.24)\n",
      "Epoch: [010][00070/00100]\tTime 0.45 (0.47)\tLoss 0.20 (0.49)\n",
      "\t\tcls_loss 0.14 (0.28)\treg_loss 0.06 (0.22)\n",
      "Epoch: [010][00080/00100]\tTime 0.46 (0.46)\tLoss 0.32 (0.47)\n",
      "\t\tcls_loss 0.17 (0.26)\treg_loss 0.15 (0.21)\n",
      "Epoch: [010][00090/00100]\tTime 0.46 (0.46)\tLoss 0.28 (0.45)\n",
      "\t\tcls_loss 0.15 (0.25)\treg_loss 0.13 (0.20)\n",
      "[Train]: Epoch 10 finished with lr=0.00009045\n",
      "\n",
      "\n",
      "[Train]: Epoch 11 started\n",
      "Epoch: [011][00010/00100]\tTime 0.52 (0.52)\tLoss 0.65 (0.65)\n",
      "\t\tcls_loss 0.33 (0.33)\treg_loss 0.32 (0.32)\n",
      "Epoch: [011][00020/00100]\tTime 0.45 (0.48)\tLoss 1.25 (0.95)\n",
      "\t\tcls_loss 0.68 (0.51)\treg_loss 0.57 (0.44)\n",
      "Epoch: [011][00030/00100]\tTime 0.45 (0.47)\tLoss 1.46 (1.12)\n",
      "\t\tcls_loss 0.69 (0.57)\treg_loss 0.77 (0.55)\n",
      "Epoch: [011][00040/00100]\tTime 0.45 (0.47)\tLoss 0.34 (0.92)\n",
      "\t\tcls_loss 0.21 (0.48)\treg_loss 0.13 (0.45)\n",
      "Epoch: [011][00050/00100]\tTime 0.46 (0.47)\tLoss 1.37 (1.01)\n",
      "\t\tcls_loss 0.67 (0.52)\treg_loss 0.69 (0.50)\n",
      "Epoch: [011][00060/00100]\tTime 0.46 (0.47)\tLoss 0.24 (0.88)\n",
      "\t\tcls_loss 0.17 (0.46)\treg_loss 0.08 (0.43)\n",
      "Epoch: [011][00070/00100]\tTime 0.46 (0.46)\tLoss 0.36 (0.81)\n",
      "\t\tcls_loss 0.21 (0.42)\treg_loss 0.15 (0.39)\n",
      "Epoch: [011][00080/00100]\tTime 0.46 (0.46)\tLoss 0.31 (0.75)\n",
      "\t\tcls_loss 0.19 (0.39)\treg_loss 0.11 (0.35)\n",
      "Epoch: [011][00090/00100]\tTime 0.45 (0.46)\tLoss 0.79 (0.75)\n",
      "\t\tcls_loss 0.45 (0.40)\treg_loss 0.34 (0.35)\n",
      "[Train]: Epoch 11 finished with lr=0.00008716\n",
      "\n",
      "\n",
      "[Train]: Epoch 12 started\n",
      "Epoch: [012][00010/00100]\tTime 0.52 (0.52)\tLoss 0.38 (0.38)\n",
      "\t\tcls_loss 0.24 (0.24)\treg_loss 0.14 (0.14)\n",
      "Epoch: [012][00020/00100]\tTime 0.46 (0.49)\tLoss 0.19 (0.29)\n",
      "\t\tcls_loss 0.12 (0.18)\treg_loss 0.08 (0.11)\n",
      "Epoch: [012][00030/00100]\tTime 0.46 (0.48)\tLoss 0.35 (0.31)\n",
      "\t\tcls_loss 0.20 (0.18)\treg_loss 0.15 (0.12)\n",
      "Epoch: [012][00040/00100]\tTime 0.45 (0.47)\tLoss 0.12 (0.26)\n",
      "\t\tcls_loss 0.07 (0.15)\treg_loss 0.05 (0.11)\n",
      "Epoch: [012][00050/00100]\tTime 0.45 (0.47)\tLoss 0.24 (0.26)\n",
      "\t\tcls_loss 0.12 (0.15)\treg_loss 0.12 (0.11)\n",
      "Epoch: [012][00060/00100]\tTime 0.45 (0.46)\tLoss 0.88 (0.36)\n",
      "\t\tcls_loss 0.52 (0.21)\treg_loss 0.36 (0.15)\n",
      "Epoch: [012][00070/00100]\tTime 0.46 (0.46)\tLoss 0.22 (0.34)\n",
      "\t\tcls_loss 0.14 (0.20)\treg_loss 0.07 (0.14)\n",
      "Epoch: [012][00080/00100]\tTime 0.46 (0.46)\tLoss 0.10 (0.31)\n",
      "\t\tcls_loss 0.06 (0.18)\treg_loss 0.04 (0.13)\n",
      "Epoch: [012][00090/00100]\tTime 0.46 (0.46)\tLoss 1.05 (0.39)\n",
      "\t\tcls_loss 0.50 (0.22)\treg_loss 0.55 (0.17)\n",
      "[Train]: Epoch 12 finished with lr=0.00008346\n",
      "\n",
      "\n",
      "[Train]: Epoch 13 started\n",
      "Epoch: [013][00010/00100]\tTime 0.52 (0.52)\tLoss 0.18 (0.18)\n",
      "\t\tcls_loss 0.10 (0.10)\treg_loss 0.08 (0.08)\n",
      "Epoch: [013][00020/00100]\tTime 0.46 (0.49)\tLoss 0.67 (0.43)\n",
      "\t\tcls_loss 0.37 (0.24)\treg_loss 0.30 (0.19)\n",
      "Epoch: [013][00030/00100]\tTime 0.46 (0.48)\tLoss 0.05 (0.30)\n",
      "\t\tcls_loss 0.02 (0.17)\treg_loss 0.02 (0.13)\n",
      "Epoch: [013][00040/00100]\tTime 0.45 (0.47)\tLoss 0.27 (0.29)\n",
      "\t\tcls_loss 0.20 (0.17)\treg_loss 0.07 (0.12)\n",
      "Epoch: [013][00050/00100]\tTime 0.46 (0.47)\tLoss 0.52 (0.34)\n",
      "\t\tcls_loss 0.26 (0.19)\treg_loss 0.26 (0.15)\n",
      "Epoch: [013][00060/00100]\tTime 0.46 (0.47)\tLoss 2.15 (0.64)\n",
      "\t\tcls_loss 1.57 (0.42)\treg_loss 0.58 (0.22)\n",
      "Epoch: [013][00070/00100]\tTime 0.45 (0.47)\tLoss 0.42 (0.61)\n",
      "\t\tcls_loss 0.21 (0.39)\treg_loss 0.21 (0.22)\n",
      "Epoch: [013][00080/00100]\tTime 0.45 (0.46)\tLoss 0.09 (0.54)\n",
      "\t\tcls_loss 0.06 (0.35)\treg_loss 0.04 (0.20)\n",
      "Epoch: [013][00090/00100]\tTime 0.45 (0.46)\tLoss 1.00 (0.59)\n",
      "\t\tcls_loss 0.54 (0.37)\treg_loss 0.46 (0.23)\n",
      "[Train]: Epoch 13 finished with lr=0.00007939\n",
      "\n",
      "\n",
      "[Train]: Epoch 14 started\n",
      "Epoch: [014][00010/00100]\tTime 0.51 (0.51)\tLoss 0.66 (0.66)\n",
      "\t\tcls_loss 0.37 (0.37)\treg_loss 0.28 (0.28)\n",
      "Epoch: [014][00020/00100]\tTime 0.46 (0.49)\tLoss 0.32 (0.49)\n",
      "\t\tcls_loss 0.17 (0.27)\treg_loss 0.15 (0.22)\n",
      "Epoch: [014][00030/00100]\tTime 0.46 (0.48)\tLoss 0.92 (0.63)\n",
      "\t\tcls_loss 0.51 (0.35)\treg_loss 0.40 (0.28)\n",
      "Epoch: [014][00040/00100]\tTime 0.46 (0.47)\tLoss 0.14 (0.51)\n",
      "\t\tcls_loss 0.07 (0.28)\treg_loss 0.07 (0.23)\n",
      "Epoch: [014][00050/00100]\tTime 0.45 (0.47)\tLoss 0.29 (0.46)\n",
      "\t\tcls_loss 0.17 (0.26)\treg_loss 0.11 (0.21)\n",
      "Epoch: [014][00060/00100]\tTime 0.46 (0.47)\tLoss 0.98 (0.55)\n",
      "\t\tcls_loss 0.47 (0.29)\treg_loss 0.52 (0.26)\n",
      "Epoch: [014][00070/00100]\tTime 0.45 (0.46)\tLoss 0.58 (0.55)\n",
      "\t\tcls_loss 0.32 (0.30)\treg_loss 0.26 (0.26)\n",
      "Epoch: [014][00080/00100]\tTime 0.45 (0.46)\tLoss 0.23 (0.51)\n",
      "\t\tcls_loss 0.13 (0.28)\treg_loss 0.10 (0.24)\n",
      "Epoch: [014][00090/00100]\tTime 0.46 (0.46)\tLoss 0.27 (0.49)\n",
      "\t\tcls_loss 0.16 (0.26)\treg_loss 0.11 (0.22)\n",
      "[Train]: Epoch 14 finished with lr=0.00007500\n",
      "\n",
      "\n",
      "[Train]: Epoch 15 started\n",
      "Epoch: [015][00010/00100]\tTime 0.54 (0.54)\tLoss 0.68 (0.68)\n",
      "\t\tcls_loss 0.37 (0.37)\treg_loss 0.31 (0.31)\n",
      "Epoch: [015][00020/00100]\tTime 0.46 (0.50)\tLoss 0.15 (0.42)\n",
      "\t\tcls_loss 0.09 (0.23)\treg_loss 0.06 (0.18)\n",
      "Epoch: [015][00030/00100]\tTime 0.46 (0.48)\tLoss 0.30 (0.38)\n",
      "\t\tcls_loss 0.19 (0.22)\treg_loss 0.10 (0.16)\n",
      "Epoch: [015][00040/00100]\tTime 0.45 (0.48)\tLoss 0.27 (0.35)\n",
      "\t\tcls_loss 0.17 (0.21)\treg_loss 0.11 (0.15)\n",
      "Epoch: [015][00050/00100]\tTime 0.46 (0.47)\tLoss 1.22 (0.52)\n",
      "\t\tcls_loss 0.63 (0.29)\treg_loss 0.59 (0.23)\n",
      "Epoch: [015][00060/00100]\tTime 0.46 (0.47)\tLoss 0.35 (0.50)\n",
      "\t\tcls_loss 0.20 (0.27)\treg_loss 0.16 (0.22)\n",
      "Epoch: [015][00070/00100]\tTime 0.45 (0.47)\tLoss 0.63 (0.51)\n",
      "\t\tcls_loss 0.40 (0.29)\treg_loss 0.22 (0.22)\n",
      "Epoch: [015][00080/00100]\tTime 0.45 (0.47)\tLoss 0.16 (0.47)\n",
      "\t\tcls_loss 0.12 (0.27)\treg_loss 0.04 (0.20)\n",
      "Epoch: [015][00090/00100]\tTime 0.45 (0.46)\tLoss 0.41 (0.46)\n",
      "\t\tcls_loss 0.24 (0.27)\treg_loss 0.17 (0.20)\n",
      "[Train]: Epoch 15 finished with lr=0.00007034\n",
      "\n",
      "\n",
      "[Train]: Epoch 16 started\n",
      "Epoch: [016][00010/00100]\tTime 0.52 (0.52)\tLoss 0.51 (0.51)\n",
      "\t\tcls_loss 0.29 (0.29)\treg_loss 0.22 (0.22)\n",
      "Epoch: [016][00020/00100]\tTime 0.45 (0.49)\tLoss 1.17 (0.84)\n",
      "\t\tcls_loss 0.59 (0.44)\treg_loss 0.59 (0.40)\n",
      "Epoch: [016][00030/00100]\tTime 0.46 (0.48)\tLoss 0.16 (0.62)\n",
      "\t\tcls_loss 0.08 (0.32)\treg_loss 0.08 (0.29)\n",
      "Epoch: [016][00040/00100]\tTime 0.46 (0.47)\tLoss 0.40 (0.56)\n",
      "\t\tcls_loss 0.20 (0.29)\treg_loss 0.20 (0.27)\n",
      "Epoch: [016][00050/00100]\tTime 0.46 (0.47)\tLoss 0.10 (0.47)\n",
      "\t\tcls_loss 0.07 (0.25)\treg_loss 0.04 (0.22)\n",
      "Epoch: [016][00060/00100]\tTime 0.45 (0.47)\tLoss 0.35 (0.45)\n",
      "\t\tcls_loss 0.20 (0.24)\treg_loss 0.14 (0.21)\n",
      "Epoch: [016][00070/00100]\tTime 0.46 (0.47)\tLoss 0.25 (0.42)\n",
      "\t\tcls_loss 0.13 (0.22)\treg_loss 0.12 (0.20)\n",
      "Epoch: [016][00080/00100]\tTime 0.46 (0.46)\tLoss 0.61 (0.44)\n",
      "\t\tcls_loss 0.36 (0.24)\treg_loss 0.25 (0.20)\n",
      "Epoch: [016][00090/00100]\tTime 0.46 (0.46)\tLoss 0.32 (0.43)\n",
      "\t\tcls_loss 0.21 (0.24)\treg_loss 0.11 (0.19)\n",
      "[Train]: Epoch 16 finished with lr=0.00006545\n",
      "\n",
      "\n",
      "[Train]: Epoch 17 started\n",
      "Epoch: [017][00010/00100]\tTime 0.53 (0.53)\tLoss 0.90 (0.90)\n",
      "\t\tcls_loss 0.49 (0.49)\treg_loss 0.41 (0.41)\n",
      "Epoch: [017][00020/00100]\tTime 0.46 (0.49)\tLoss 0.11 (0.50)\n",
      "\t\tcls_loss 0.08 (0.28)\treg_loss 0.03 (0.22)\n",
      "Epoch: [017][00030/00100]\tTime 0.45 (0.48)\tLoss 0.21 (0.41)\n",
      "\t\tcls_loss 0.14 (0.24)\treg_loss 0.07 (0.17)\n",
      "Epoch: [017][00040/00100]\tTime 0.45 (0.47)\tLoss 0.04 (0.31)\n",
      "\t\tcls_loss 0.02 (0.18)\treg_loss 0.02 (0.13)\n",
      "Epoch: [017][00050/00100]\tTime 0.46 (0.47)\tLoss 0.14 (0.28)\n",
      "\t\tcls_loss 0.08 (0.16)\treg_loss 0.06 (0.12)\n",
      "Epoch: [017][00060/00100]\tTime 0.46 (0.47)\tLoss 0.91 (0.38)\n",
      "\t\tcls_loss 0.47 (0.21)\treg_loss 0.44 (0.17)\n",
      "Epoch: [017][00070/00100]\tTime 0.46 (0.47)\tLoss 0.47 (0.40)\n",
      "\t\tcls_loss 0.31 (0.22)\treg_loss 0.16 (0.17)\n",
      "Epoch: [017][00080/00100]\tTime 0.45 (0.46)\tLoss 0.07 (0.35)\n",
      "\t\tcls_loss 0.04 (0.20)\treg_loss 0.03 (0.15)\n",
      "Epoch: [017][00090/00100]\tTime 0.46 (0.46)\tLoss 0.43 (0.36)\n",
      "\t\tcls_loss 0.24 (0.21)\treg_loss 0.19 (0.16)\n",
      "[Train]: Epoch 17 finished with lr=0.00006040\n",
      "\n",
      "\n",
      "[Train]: Epoch 18 started\n",
      "Epoch: [018][00010/00100]\tTime 0.52 (0.52)\tLoss 0.10 (0.10)\n",
      "\t\tcls_loss 0.05 (0.05)\treg_loss 0.05 (0.05)\n",
      "Epoch: [018][00020/00100]\tTime 0.45 (0.49)\tLoss 1.30 (0.70)\n",
      "\t\tcls_loss 0.59 (0.32)\treg_loss 0.71 (0.38)\n",
      "Epoch: [018][00030/00100]\tTime 0.46 (0.48)\tLoss 0.23 (0.54)\n",
      "\t\tcls_loss 0.12 (0.26)\treg_loss 0.11 (0.29)\n",
      "Epoch: [018][00040/00100]\tTime 0.45 (0.47)\tLoss 0.08 (0.43)\n",
      "\t\tcls_loss 0.04 (0.20)\treg_loss 0.04 (0.22)\n",
      "Epoch: [018][00050/00100]\tTime 0.45 (0.47)\tLoss 0.75 (0.49)\n",
      "\t\tcls_loss 0.41 (0.24)\treg_loss 0.34 (0.25)\n",
      "Epoch: [018][00060/00100]\tTime 0.45 (0.47)\tLoss 0.47 (0.49)\n",
      "\t\tcls_loss 0.25 (0.24)\treg_loss 0.22 (0.24)\n",
      "Epoch: [018][00070/00100]\tTime 0.46 (0.46)\tLoss 0.35 (0.47)\n",
      "\t\tcls_loss 0.23 (0.24)\treg_loss 0.12 (0.22)\n",
      "Epoch: [018][00080/00100]\tTime 0.46 (0.46)\tLoss 0.13 (0.42)\n",
      "\t\tcls_loss 0.07 (0.22)\treg_loss 0.06 (0.20)\n",
      "Epoch: [018][00090/00100]\tTime 0.45 (0.46)\tLoss 0.74 (0.46)\n",
      "\t\tcls_loss 0.40 (0.24)\treg_loss 0.34 (0.22)\n",
      "[Train]: Epoch 18 finished with lr=0.00005523\n",
      "\n",
      "\n",
      "[Train]: Epoch 19 started\n",
      "Epoch: [019][00010/00100]\tTime 0.51 (0.51)\tLoss 0.10 (0.10)\n",
      "\t\tcls_loss 0.05 (0.05)\treg_loss 0.05 (0.05)\n",
      "Epoch: [019][00020/00100]\tTime 0.46 (0.48)\tLoss 0.63 (0.37)\n",
      "\t\tcls_loss 0.38 (0.21)\treg_loss 0.25 (0.15)\n",
      "Epoch: [019][00030/00100]\tTime 0.46 (0.47)\tLoss 0.29 (0.34)\n",
      "\t\tcls_loss 0.16 (0.20)\treg_loss 0.13 (0.14)\n",
      "Epoch: [019][00040/00100]\tTime 0.46 (0.47)\tLoss 0.45 (0.37)\n",
      "\t\tcls_loss 0.23 (0.21)\treg_loss 0.22 (0.16)\n",
      "Epoch: [019][00050/00100]\tTime 0.45 (0.47)\tLoss 0.29 (0.35)\n",
      "\t\tcls_loss 0.15 (0.19)\treg_loss 0.14 (0.16)\n",
      "Epoch: [019][00060/00100]\tTime 0.45 (0.46)\tLoss 0.32 (0.35)\n",
      "\t\tcls_loss 0.15 (0.19)\treg_loss 0.16 (0.16)\n",
      "Epoch: [019][00070/00100]\tTime 0.46 (0.46)\tLoss 0.27 (0.34)\n",
      "\t\tcls_loss 0.14 (0.18)\treg_loss 0.13 (0.16)\n",
      "Epoch: [019][00080/00100]\tTime 0.46 (0.46)\tLoss 0.19 (0.32)\n",
      "\t\tcls_loss 0.11 (0.17)\treg_loss 0.08 (0.15)\n",
      "Epoch: [019][00090/00100]\tTime 0.46 (0.46)\tLoss 0.17 (0.30)\n",
      "\t\tcls_loss 0.09 (0.16)\treg_loss 0.08 (0.14)\n",
      "[Train]: Epoch 19 finished with lr=0.00005000\n",
      "\n",
      "\n",
      "[Train]: Epoch 20 started\n",
      "Epoch: [020][00010/00100]\tTime 0.51 (0.51)\tLoss 0.38 (0.38)\n",
      "\t\tcls_loss 0.19 (0.19)\treg_loss 0.19 (0.19)\n",
      "Epoch: [020][00020/00100]\tTime 0.45 (0.48)\tLoss 0.29 (0.34)\n",
      "\t\tcls_loss 0.15 (0.17)\treg_loss 0.14 (0.17)\n",
      "Epoch: [020][00030/00100]\tTime 0.46 (0.47)\tLoss 0.36 (0.34)\n",
      "\t\tcls_loss 0.22 (0.18)\treg_loss 0.14 (0.16)\n",
      "Epoch: [020][00040/00100]\tTime 0.46 (0.47)\tLoss 0.16 (0.30)\n",
      "\t\tcls_loss 0.08 (0.16)\treg_loss 0.08 (0.14)\n",
      "Epoch: [020][00050/00100]\tTime 0.46 (0.47)\tLoss 0.07 (0.25)\n",
      "\t\tcls_loss 0.04 (0.14)\treg_loss 0.03 (0.12)\n",
      "Epoch: [020][00060/00100]\tTime 0.46 (0.47)\tLoss 1.79 (0.51)\n",
      "\t\tcls_loss 0.95 (0.27)\treg_loss 0.84 (0.24)\n",
      "Epoch: [020][00070/00100]\tTime 0.46 (0.47)\tLoss 0.40 (0.49)\n",
      "\t\tcls_loss 0.21 (0.26)\treg_loss 0.19 (0.23)\n",
      "Epoch: [020][00080/00100]\tTime 0.46 (0.47)\tLoss 0.58 (0.50)\n",
      "\t\tcls_loss 0.31 (0.27)\treg_loss 0.27 (0.24)\n",
      "Epoch: [020][00090/00100]\tTime 0.46 (0.46)\tLoss 0.06 (0.45)\n",
      "\t\tcls_loss 0.03 (0.24)\treg_loss 0.03 (0.21)\n",
      "[Train]: Epoch 20 finished with lr=0.00004478\n",
      "\n",
      "\n",
      "[Train]: Epoch 21 started\n",
      "Epoch: [021][00010/00100]\tTime 0.52 (0.52)\tLoss 0.44 (0.44)\n",
      "\t\tcls_loss 0.23 (0.23)\treg_loss 0.21 (0.21)\n",
      "Epoch: [021][00020/00100]\tTime 0.46 (0.49)\tLoss 0.34 (0.39)\n",
      "\t\tcls_loss 0.17 (0.20)\treg_loss 0.17 (0.19)\n",
      "Epoch: [021][00030/00100]\tTime 0.46 (0.48)\tLoss 0.19 (0.32)\n",
      "\t\tcls_loss 0.11 (0.17)\treg_loss 0.08 (0.15)\n",
      "Epoch: [021][00040/00100]\tTime 0.46 (0.47)\tLoss 0.28 (0.31)\n",
      "\t\tcls_loss 0.15 (0.17)\treg_loss 0.13 (0.15)\n",
      "Epoch: [021][00050/00100]\tTime 0.45 (0.47)\tLoss 1.46 (0.54)\n",
      "\t\tcls_loss 1.02 (0.34)\treg_loss 0.44 (0.21)\n",
      "Epoch: [021][00060/00100]\tTime 0.46 (0.47)\tLoss 0.31 (0.51)\n",
      "\t\tcls_loss 0.17 (0.31)\treg_loss 0.14 (0.20)\n",
      "Epoch: [021][00070/00100]\tTime 0.45 (0.47)\tLoss 0.12 (0.45)\n",
      "\t\tcls_loss 0.07 (0.27)\treg_loss 0.05 (0.18)\n",
      "Epoch: [021][00080/00100]\tTime 0.46 (0.46)\tLoss 0.52 (0.46)\n",
      "\t\tcls_loss 0.27 (0.27)\treg_loss 0.25 (0.19)\n",
      "Epoch: [021][00090/00100]\tTime 0.45 (0.46)\tLoss 0.77 (0.49)\n",
      "\t\tcls_loss 0.37 (0.28)\treg_loss 0.40 (0.21)\n",
      "[Train]: Epoch 21 finished with lr=0.00003961\n",
      "\n",
      "\n",
      "[Train]: Epoch 22 started\n",
      "Epoch: [022][00010/00100]\tTime 0.51 (0.51)\tLoss 0.30 (0.30)\n",
      "\t\tcls_loss 0.18 (0.18)\treg_loss 0.13 (0.13)\n",
      "Epoch: [022][00020/00100]\tTime 0.45 (0.48)\tLoss 1.64 (0.97)\n",
      "\t\tcls_loss 1.05 (0.61)\treg_loss 0.59 (0.36)\n",
      "Epoch: [022][00030/00100]\tTime 0.46 (0.48)\tLoss 0.46 (0.80)\n",
      "\t\tcls_loss 0.21 (0.48)\treg_loss 0.25 (0.32)\n",
      "Epoch: [022][00040/00100]\tTime 0.45 (0.47)\tLoss 0.14 (0.64)\n",
      "\t\tcls_loss 0.08 (0.38)\treg_loss 0.06 (0.26)\n",
      "Epoch: [022][00050/00100]\tTime 0.45 (0.47)\tLoss 0.16 (0.54)\n",
      "\t\tcls_loss 0.09 (0.32)\treg_loss 0.07 (0.22)\n",
      "Epoch: [022][00060/00100]\tTime 0.46 (0.47)\tLoss 0.42 (0.52)\n",
      "\t\tcls_loss 0.22 (0.30)\treg_loss 0.20 (0.22)\n",
      "Epoch: [022][00070/00100]\tTime 0.45 (0.46)\tLoss 0.08 (0.46)\n",
      "\t\tcls_loss 0.04 (0.27)\treg_loss 0.04 (0.19)\n",
      "Epoch: [022][00080/00100]\tTime 0.46 (0.46)\tLoss 0.05 (0.41)\n",
      "\t\tcls_loss 0.03 (0.24)\treg_loss 0.02 (0.17)\n",
      "Epoch: [022][00090/00100]\tTime 0.46 (0.46)\tLoss 0.23 (0.39)\n",
      "\t\tcls_loss 0.12 (0.22)\treg_loss 0.11 (0.16)\n",
      "[Train]: Epoch 22 finished with lr=0.00003456\n",
      "\n",
      "\n",
      "[Train]: Epoch 23 started\n",
      "Epoch: [023][00010/00100]\tTime 0.53 (0.53)\tLoss 1.22 (1.22)\n",
      "\t\tcls_loss 0.55 (0.55)\treg_loss 0.67 (0.67)\n",
      "Epoch: [023][00020/00100]\tTime 0.46 (0.49)\tLoss 0.10 (0.66)\n",
      "\t\tcls_loss 0.05 (0.30)\treg_loss 0.05 (0.36)\n",
      "Epoch: [023][00030/00100]\tTime 0.46 (0.48)\tLoss 1.10 (0.80)\n",
      "\t\tcls_loss 0.56 (0.39)\treg_loss 0.54 (0.42)\n",
      "Epoch: [023][00040/00100]\tTime 0.46 (0.48)\tLoss 0.31 (0.68)\n",
      "\t\tcls_loss 0.17 (0.33)\treg_loss 0.15 (0.35)\n",
      "Epoch: [023][00050/00100]\tTime 0.46 (0.47)\tLoss 0.06 (0.56)\n",
      "\t\tcls_loss 0.03 (0.27)\treg_loss 0.03 (0.28)\n",
      "Epoch: [023][00060/00100]\tTime 0.46 (0.47)\tLoss 0.24 (0.50)\n",
      "\t\tcls_loss 0.12 (0.25)\treg_loss 0.11 (0.26)\n",
      "Epoch: [023][00070/00100]\tTime 0.46 (0.47)\tLoss 0.05 (0.44)\n",
      "\t\tcls_loss 0.03 (0.22)\treg_loss 0.02 (0.22)\n",
      "Epoch: [023][00080/00100]\tTime 0.46 (0.47)\tLoss 0.30 (0.42)\n",
      "\t\tcls_loss 0.15 (0.21)\treg_loss 0.15 (0.21)\n",
      "Epoch: [023][00090/00100]\tTime 0.46 (0.47)\tLoss 0.56 (0.44)\n",
      "\t\tcls_loss 0.29 (0.22)\treg_loss 0.27 (0.22)\n",
      "[Train]: Epoch 23 finished with lr=0.00002967\n",
      "\n",
      "\n",
      "[Train]: Epoch 24 started\n",
      "Epoch: [024][00010/00100]\tTime 0.52 (0.52)\tLoss 0.16 (0.16)\n",
      "\t\tcls_loss 0.10 (0.10)\treg_loss 0.06 (0.06)\n",
      "Epoch: [024][00020/00100]\tTime 0.45 (0.49)\tLoss 0.19 (0.18)\n",
      "\t\tcls_loss 0.11 (0.10)\treg_loss 0.08 (0.07)\n",
      "Epoch: [024][00030/00100]\tTime 0.45 (0.48)\tLoss 1.04 (0.47)\n",
      "\t\tcls_loss 0.52 (0.24)\treg_loss 0.52 (0.22)\n",
      "Epoch: [024][00040/00100]\tTime 0.45 (0.47)\tLoss 0.13 (0.38)\n",
      "\t\tcls_loss 0.08 (0.20)\treg_loss 0.06 (0.18)\n",
      "Epoch: [024][00050/00100]\tTime 0.46 (0.47)\tLoss 0.21 (0.35)\n",
      "\t\tcls_loss 0.11 (0.18)\treg_loss 0.10 (0.16)\n",
      "Epoch: [024][00060/00100]\tTime 0.46 (0.47)\tLoss 0.13 (0.31)\n",
      "\t\tcls_loss 0.07 (0.16)\treg_loss 0.07 (0.15)\n",
      "Epoch: [024][00070/00100]\tTime 0.46 (0.46)\tLoss 0.55 (0.35)\n",
      "\t\tcls_loss 0.25 (0.18)\treg_loss 0.30 (0.17)\n",
      "Epoch: [024][00080/00100]\tTime 0.46 (0.46)\tLoss 0.19 (0.33)\n",
      "\t\tcls_loss 0.10 (0.17)\treg_loss 0.09 (0.16)\n",
      "Epoch: [024][00090/00100]\tTime 0.46 (0.46)\tLoss 0.68 (0.37)\n",
      "\t\tcls_loss 0.37 (0.19)\treg_loss 0.31 (0.18)\n",
      "[Train]: Epoch 24 finished with lr=0.00002501\n",
      "\n",
      "\n",
      "[Train]: Epoch 25 started\n",
      "Epoch: [025][00010/00100]\tTime 0.52 (0.52)\tLoss 0.30 (0.30)\n",
      "\t\tcls_loss 0.16 (0.16)\treg_loss 0.14 (0.14)\n",
      "Epoch: [025][00020/00100]\tTime 0.46 (0.49)\tLoss 0.01 (0.16)\n",
      "\t\tcls_loss 0.01 (0.08)\treg_loss 0.01 (0.07)\n",
      "Epoch: [025][00030/00100]\tTime 0.46 (0.48)\tLoss 0.06 (0.12)\n",
      "\t\tcls_loss 0.04 (0.07)\treg_loss 0.03 (0.06)\n",
      "Epoch: [025][00040/00100]\tTime 0.46 (0.47)\tLoss 0.19 (0.14)\n",
      "\t\tcls_loss 0.11 (0.08)\treg_loss 0.08 (0.06)\n",
      "Epoch: [025][00050/00100]\tTime 0.45 (0.47)\tLoss 0.15 (0.14)\n",
      "\t\tcls_loss 0.08 (0.08)\treg_loss 0.07 (0.06)\n",
      "Epoch: [025][00060/00100]\tTime 0.45 (0.47)\tLoss 0.67 (0.23)\n",
      "\t\tcls_loss 0.37 (0.13)\treg_loss 0.31 (0.11)\n",
      "Epoch: [025][00070/00100]\tTime 0.46 (0.47)\tLoss 0.52 (0.27)\n",
      "\t\tcls_loss 0.25 (0.14)\treg_loss 0.27 (0.13)\n",
      "Epoch: [025][00080/00100]\tTime 0.46 (0.47)\tLoss 0.48 (0.30)\n",
      "\t\tcls_loss 0.25 (0.16)\treg_loss 0.23 (0.14)\n",
      "Epoch: [025][00090/00100]\tTime 0.45 (0.46)\tLoss 0.10 (0.28)\n",
      "\t\tcls_loss 0.06 (0.15)\treg_loss 0.04 (0.13)\n",
      "[Train]: Epoch 25 finished with lr=0.00002062\n",
      "\n",
      "\n",
      "[Train]: Epoch 26 started\n",
      "Epoch: [026][00010/00100]\tTime 0.52 (0.52)\tLoss 0.19 (0.19)\n",
      "\t\tcls_loss 0.09 (0.09)\treg_loss 0.09 (0.09)\n",
      "Epoch: [026][00020/00100]\tTime 0.46 (0.49)\tLoss 0.22 (0.20)\n",
      "\t\tcls_loss 0.11 (0.10)\treg_loss 0.10 (0.10)\n",
      "Epoch: [026][00030/00100]\tTime 0.46 (0.48)\tLoss 0.15 (0.18)\n",
      "\t\tcls_loss 0.11 (0.11)\treg_loss 0.04 (0.08)\n",
      "Epoch: [026][00040/00100]\tTime 0.46 (0.47)\tLoss 0.57 (0.28)\n",
      "\t\tcls_loss 0.33 (0.16)\treg_loss 0.24 (0.12)\n",
      "Epoch: [026][00050/00100]\tTime 0.45 (0.47)\tLoss 1.20 (0.46)\n",
      "\t\tcls_loss 0.53 (0.23)\treg_loss 0.67 (0.23)\n",
      "Epoch: [026][00060/00100]\tTime 0.45 (0.47)\tLoss 0.18 (0.42)\n",
      "\t\tcls_loss 0.09 (0.21)\treg_loss 0.09 (0.20)\n",
      "Epoch: [026][00070/00100]\tTime 0.46 (0.47)\tLoss 0.07 (0.37)\n",
      "\t\tcls_loss 0.03 (0.18)\treg_loss 0.04 (0.18)\n",
      "Epoch: [026][00080/00100]\tTime 0.46 (0.46)\tLoss 0.20 (0.35)\n",
      "\t\tcls_loss 0.11 (0.18)\treg_loss 0.09 (0.17)\n",
      "Epoch: [026][00090/00100]\tTime 0.46 (0.46)\tLoss 0.11 (0.32)\n",
      "\t\tcls_loss 0.06 (0.16)\treg_loss 0.05 (0.16)\n",
      "[Train]: Epoch 26 finished with lr=0.00001655\n",
      "\n",
      "\n",
      "[Train]: Epoch 27 started\n",
      "Epoch: [027][00010/00100]\tTime 0.54 (0.54)\tLoss 0.05 (0.05)\n",
      "\t\tcls_loss 0.03 (0.03)\treg_loss 0.02 (0.02)\n",
      "Epoch: [027][00020/00100]\tTime 0.46 (0.50)\tLoss 0.28 (0.17)\n",
      "\t\tcls_loss 0.16 (0.10)\treg_loss 0.12 (0.07)\n",
      "Epoch: [027][00030/00100]\tTime 0.45 (0.48)\tLoss 0.26 (0.20)\n",
      "\t\tcls_loss 0.14 (0.11)\treg_loss 0.12 (0.09)\n",
      "Epoch: [027][00040/00100]\tTime 0.46 (0.48)\tLoss 0.30 (0.22)\n",
      "\t\tcls_loss 0.17 (0.13)\treg_loss 0.13 (0.10)\n",
      "Epoch: [027][00050/00100]\tTime 0.45 (0.47)\tLoss 0.12 (0.20)\n",
      "\t\tcls_loss 0.06 (0.11)\treg_loss 0.06 (0.09)\n",
      "Epoch: [027][00060/00100]\tTime 0.45 (0.47)\tLoss 0.22 (0.21)\n",
      "\t\tcls_loss 0.12 (0.11)\treg_loss 0.10 (0.09)\n",
      "Epoch: [027][00070/00100]\tTime 0.45 (0.47)\tLoss 0.94 (0.31)\n",
      "\t\tcls_loss 0.45 (0.16)\treg_loss 0.50 (0.15)\n",
      "Epoch: [027][00080/00100]\tTime 0.46 (0.47)\tLoss 0.26 (0.30)\n",
      "\t\tcls_loss 0.12 (0.16)\treg_loss 0.14 (0.15)\n",
      "Epoch: [027][00090/00100]\tTime 0.46 (0.47)\tLoss 0.32 (0.31)\n",
      "\t\tcls_loss 0.17 (0.16)\treg_loss 0.15 (0.15)\n",
      "[Train]: Epoch 27 finished with lr=0.00001285\n",
      "\n",
      "\n",
      "[Train]: Epoch 28 started\n",
      "Epoch: [028][00010/00100]\tTime 0.52 (0.52)\tLoss 0.05 (0.05)\n",
      "\t\tcls_loss 0.02 (0.02)\treg_loss 0.03 (0.03)\n",
      "Epoch: [028][00020/00100]\tTime 0.46 (0.49)\tLoss 0.08 (0.07)\n",
      "\t\tcls_loss 0.04 (0.03)\treg_loss 0.04 (0.03)\n",
      "Epoch: [028][00030/00100]\tTime 0.46 (0.48)\tLoss 0.03 (0.05)\n",
      "\t\tcls_loss 0.01 (0.02)\treg_loss 0.01 (0.03)\n",
      "Epoch: [028][00040/00100]\tTime 0.46 (0.47)\tLoss 0.24 (0.10)\n",
      "\t\tcls_loss 0.11 (0.05)\treg_loss 0.13 (0.05)\n",
      "Epoch: [028][00050/00100]\tTime 0.45 (0.47)\tLoss 0.44 (0.17)\n",
      "\t\tcls_loss 0.22 (0.08)\treg_loss 0.22 (0.09)\n",
      "Epoch: [028][00060/00100]\tTime 0.46 (0.47)\tLoss 0.36 (0.20)\n",
      "\t\tcls_loss 0.18 (0.10)\treg_loss 0.18 (0.10)\n",
      "Epoch: [028][00070/00100]\tTime 0.46 (0.47)\tLoss 0.35 (0.22)\n",
      "\t\tcls_loss 0.16 (0.11)\treg_loss 0.18 (0.11)\n",
      "Epoch: [028][00080/00100]\tTime 0.45 (0.47)\tLoss 0.17 (0.21)\n",
      "\t\tcls_loss 0.10 (0.11)\treg_loss 0.06 (0.11)\n",
      "Epoch: [028][00090/00100]\tTime 0.46 (0.46)\tLoss 0.05 (0.20)\n",
      "\t\tcls_loss 0.03 (0.10)\treg_loss 0.02 (0.10)\n",
      "[Train]: Epoch 28 finished with lr=0.00000956\n",
      "\n",
      "\n",
      "[Train]: Epoch 29 started\n",
      "Epoch: [029][00010/00100]\tTime 0.51 (0.51)\tLoss 0.84 (0.84)\n",
      "\t\tcls_loss 0.57 (0.57)\treg_loss 0.27 (0.27)\n",
      "Epoch: [029][00020/00100]\tTime 0.46 (0.49)\tLoss 0.12 (0.48)\n",
      "\t\tcls_loss 0.07 (0.32)\treg_loss 0.04 (0.16)\n",
      "Epoch: [029][00030/00100]\tTime 0.46 (0.48)\tLoss 0.12 (0.36)\n",
      "\t\tcls_loss 0.06 (0.23)\treg_loss 0.07 (0.13)\n",
      "Epoch: [029][00040/00100]\tTime 0.46 (0.47)\tLoss 0.28 (0.34)\n",
      "\t\tcls_loss 0.15 (0.21)\treg_loss 0.13 (0.13)\n",
      "Epoch: [029][00050/00100]\tTime 0.46 (0.47)\tLoss 0.08 (0.29)\n",
      "\t\tcls_loss 0.05 (0.18)\treg_loss 0.04 (0.11)\n",
      "Epoch: [029][00060/00100]\tTime 0.46 (0.47)\tLoss 0.66 (0.35)\n",
      "\t\tcls_loss 0.33 (0.20)\treg_loss 0.34 (0.15)\n",
      "Epoch: [029][00070/00100]\tTime 0.46 (0.47)\tLoss 0.10 (0.32)\n",
      "\t\tcls_loss 0.05 (0.18)\treg_loss 0.05 (0.13)\n",
      "Epoch: [029][00080/00100]\tTime 0.45 (0.46)\tLoss 0.89 (0.39)\n",
      "\t\tcls_loss 0.44 (0.21)\treg_loss 0.46 (0.17)\n",
      "Epoch: [029][00090/00100]\tTime 0.46 (0.46)\tLoss 0.05 (0.35)\n",
      "\t\tcls_loss 0.03 (0.19)\treg_loss 0.02 (0.16)\n",
      "[Train]: Epoch 29 finished with lr=0.00000671\n",
      "\n",
      "\n",
      "[Train]: Epoch 30 started\n",
      "Epoch: [030][00010/00100]\tTime 0.54 (0.54)\tLoss 0.04 (0.04)\n",
      "\t\tcls_loss 0.03 (0.03)\treg_loss 0.01 (0.01)\n",
      "Epoch: [030][00020/00100]\tTime 0.46 (0.50)\tLoss 0.10 (0.07)\n",
      "\t\tcls_loss 0.06 (0.04)\treg_loss 0.04 (0.03)\n",
      "Epoch: [030][00030/00100]\tTime 0.46 (0.49)\tLoss 0.18 (0.11)\n",
      "\t\tcls_loss 0.09 (0.06)\treg_loss 0.09 (0.05)\n",
      "Epoch: [030][00040/00100]\tTime 0.46 (0.48)\tLoss 0.06 (0.09)\n",
      "\t\tcls_loss 0.03 (0.05)\treg_loss 0.03 (0.04)\n",
      "Epoch: [030][00050/00100]\tTime 0.46 (0.48)\tLoss 0.17 (0.11)\n",
      "\t\tcls_loss 0.09 (0.06)\treg_loss 0.09 (0.05)\n",
      "Epoch: [030][00060/00100]\tTime 0.46 (0.47)\tLoss 0.11 (0.11)\n",
      "\t\tcls_loss 0.05 (0.06)\treg_loss 0.06 (0.05)\n",
      "Epoch: [030][00070/00100]\tTime 0.46 (0.47)\tLoss 0.23 (0.13)\n",
      "\t\tcls_loss 0.12 (0.07)\treg_loss 0.11 (0.06)\n",
      "Epoch: [030][00080/00100]\tTime 0.45 (0.47)\tLoss 0.16 (0.13)\n",
      "\t\tcls_loss 0.08 (0.07)\treg_loss 0.08 (0.06)\n",
      "Epoch: [030][00090/00100]\tTime 0.45 (0.47)\tLoss 0.14 (0.13)\n",
      "\t\tcls_loss 0.07 (0.07)\treg_loss 0.07 (0.06)\n",
      "[Train]: Epoch 30 finished with lr=0.00000433\n",
      "\n",
      "\n",
      "[Train]: Epoch 31 started\n",
      "Epoch: [031][00010/00100]\tTime 0.53 (0.53)\tLoss 0.22 (0.22)\n",
      "\t\tcls_loss 0.12 (0.12)\treg_loss 0.11 (0.11)\n",
      "Epoch: [031][00020/00100]\tTime 0.46 (0.49)\tLoss 0.34 (0.28)\n",
      "\t\tcls_loss 0.18 (0.15)\treg_loss 0.16 (0.13)\n",
      "Epoch: [031][00030/00100]\tTime 0.46 (0.48)\tLoss 0.09 (0.22)\n",
      "\t\tcls_loss 0.05 (0.12)\treg_loss 0.04 (0.10)\n",
      "Epoch: [031][00040/00100]\tTime 0.46 (0.47)\tLoss 0.08 (0.18)\n",
      "\t\tcls_loss 0.03 (0.09)\treg_loss 0.05 (0.09)\n",
      "Epoch: [031][00050/00100]\tTime 0.46 (0.47)\tLoss 0.28 (0.20)\n",
      "\t\tcls_loss 0.17 (0.11)\treg_loss 0.11 (0.09)\n",
      "Epoch: [031][00060/00100]\tTime 0.45 (0.47)\tLoss 0.05 (0.18)\n",
      "\t\tcls_loss 0.03 (0.10)\treg_loss 0.02 (0.08)\n",
      "Epoch: [031][00070/00100]\tTime 0.45 (0.47)\tLoss 0.25 (0.19)\n",
      "\t\tcls_loss 0.15 (0.10)\treg_loss 0.11 (0.09)\n",
      "Epoch: [031][00080/00100]\tTime 0.46 (0.46)\tLoss 0.30 (0.20)\n",
      "\t\tcls_loss 0.15 (0.11)\treg_loss 0.15 (0.09)\n",
      "Epoch: [031][00090/00100]\tTime 0.46 (0.46)\tLoss 0.07 (0.19)\n",
      "\t\tcls_loss 0.04 (0.10)\treg_loss 0.04 (0.09)\n",
      "[Train]: Epoch 31 finished with lr=0.00000246\n",
      "\n",
      "\n",
      "[Train]: Epoch 32 started\n",
      "Epoch: [032][00010/00100]\tTime 0.52 (0.52)\tLoss 0.12 (0.12)\n",
      "\t\tcls_loss 0.07 (0.07)\treg_loss 0.05 (0.05)\n",
      "Epoch: [032][00020/00100]\tTime 0.45 (0.49)\tLoss 0.24 (0.18)\n",
      "\t\tcls_loss 0.13 (0.10)\treg_loss 0.11 (0.08)\n",
      "Epoch: [032][00030/00100]\tTime 0.45 (0.48)\tLoss 0.10 (0.15)\n",
      "\t\tcls_loss 0.07 (0.09)\treg_loss 0.03 (0.06)\n",
      "Epoch: [032][00040/00100]\tTime 0.45 (0.47)\tLoss 1.40 (0.46)\n",
      "\t\tcls_loss 0.64 (0.23)\treg_loss 0.75 (0.24)\n",
      "Epoch: [032][00050/00100]\tTime 0.45 (0.47)\tLoss 0.15 (0.40)\n",
      "\t\tcls_loss 0.09 (0.20)\treg_loss 0.06 (0.20)\n",
      "Epoch: [032][00060/00100]\tTime 0.46 (0.47)\tLoss 0.13 (0.36)\n",
      "\t\tcls_loss 0.07 (0.18)\treg_loss 0.06 (0.18)\n",
      "Epoch: [032][00070/00100]\tTime 0.46 (0.47)\tLoss 0.08 (0.32)\n",
      "\t\tcls_loss 0.05 (0.16)\treg_loss 0.03 (0.16)\n",
      "Epoch: [032][00080/00100]\tTime 0.46 (0.46)\tLoss 0.14 (0.29)\n",
      "\t\tcls_loss 0.07 (0.15)\treg_loss 0.07 (0.15)\n",
      "Epoch: [032][00090/00100]\tTime 0.46 (0.46)\tLoss 0.27 (0.29)\n",
      "\t\tcls_loss 0.14 (0.15)\treg_loss 0.13 (0.14)\n",
      "[Train]: Epoch 32 finished with lr=0.00000110\n",
      "\n",
      "\n",
      "[Train]: Epoch 33 started\n",
      "Epoch: [033][00010/00100]\tTime 0.53 (0.53)\tLoss 0.09 (0.09)\n",
      "\t\tcls_loss 0.05 (0.05)\treg_loss 0.04 (0.04)\n",
      "Epoch: [033][00020/00100]\tTime 0.46 (0.50)\tLoss 0.13 (0.11)\n",
      "\t\tcls_loss 0.08 (0.06)\treg_loss 0.05 (0.05)\n",
      "Epoch: [033][00030/00100]\tTime 0.45 (0.48)\tLoss 0.40 (0.21)\n",
      "\t\tcls_loss 0.19 (0.10)\treg_loss 0.21 (0.10)\n",
      "Epoch: [033][00040/00100]\tTime 0.45 (0.47)\tLoss 0.04 (0.16)\n",
      "\t\tcls_loss 0.02 (0.08)\treg_loss 0.02 (0.08)\n",
      "Epoch: [033][00050/00100]\tTime 0.45 (0.47)\tLoss 0.08 (0.15)\n",
      "\t\tcls_loss 0.04 (0.08)\treg_loss 0.04 (0.07)\n",
      "Epoch: [033][00060/00100]\tTime 0.46 (0.47)\tLoss 0.26 (0.17)\n",
      "\t\tcls_loss 0.13 (0.09)\treg_loss 0.12 (0.08)\n",
      "Epoch: [033][00070/00100]\tTime 0.45 (0.47)\tLoss 0.09 (0.15)\n",
      "\t\tcls_loss 0.06 (0.08)\treg_loss 0.03 (0.07)\n",
      "Epoch: [033][00080/00100]\tTime 0.46 (0.47)\tLoss 0.05 (0.14)\n",
      "\t\tcls_loss 0.03 (0.07)\treg_loss 0.02 (0.07)\n",
      "Epoch: [033][00090/00100]\tTime 0.46 (0.46)\tLoss 0.83 (0.22)\n",
      "\t\tcls_loss 0.37 (0.11)\treg_loss 0.46 (0.11)\n",
      "[Train]: Epoch 33 finished with lr=0.00000028\n",
      "\n",
      "\n",
      "[Train]: Epoch 34 started\n",
      "Epoch: [034][00010/00100]\tTime 0.53 (0.53)\tLoss 0.46 (0.46)\n",
      "\t\tcls_loss 0.25 (0.25)\treg_loss 0.21 (0.21)\n",
      "Epoch: [034][00020/00100]\tTime 0.45 (0.49)\tLoss 0.12 (0.29)\n",
      "\t\tcls_loss 0.07 (0.16)\treg_loss 0.05 (0.13)\n",
      "Epoch: [034][00030/00100]\tTime 0.46 (0.48)\tLoss 0.31 (0.29)\n",
      "\t\tcls_loss 0.18 (0.16)\treg_loss 0.13 (0.13)\n",
      "Epoch: [034][00040/00100]\tTime 0.46 (0.47)\tLoss 0.53 (0.35)\n",
      "\t\tcls_loss 0.24 (0.18)\treg_loss 0.28 (0.17)\n",
      "Epoch: [034][00050/00100]\tTime 0.46 (0.47)\tLoss 0.33 (0.35)\n",
      "\t\tcls_loss 0.19 (0.19)\treg_loss 0.14 (0.16)\n",
      "Epoch: [034][00060/00100]\tTime 0.45 (0.47)\tLoss 0.13 (0.31)\n",
      "\t\tcls_loss 0.08 (0.17)\treg_loss 0.05 (0.14)\n",
      "Epoch: [034][00070/00100]\tTime 0.46 (0.47)\tLoss 0.05 (0.27)\n",
      "\t\tcls_loss 0.02 (0.15)\treg_loss 0.02 (0.13)\n",
      "Epoch: [034][00080/00100]\tTime 0.46 (0.47)\tLoss 0.70 (0.33)\n",
      "\t\tcls_loss 0.33 (0.17)\treg_loss 0.37 (0.16)\n",
      "Epoch: [034][00090/00100]\tTime 0.46 (0.47)\tLoss 0.26 (0.32)\n",
      "\t\tcls_loss 0.12 (0.16)\treg_loss 0.13 (0.16)\n",
      "[Train]: Epoch 34 finished with lr=0.00000001\n",
      "\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"5. training loop\"\"\"\n",
    "print(f\"\\nStart training model {model_name} ...\")\n",
    "\n",
    "# start training\n",
    "max_epochs = epochs + warmup_epochs\n",
    "for epoch in range(start_epoch, max_epochs):\n",
    "    # train for one epoch\n",
    "    train_one_epoch(\n",
    "        train_loader,\n",
    "        model,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        epoch,\n",
    "        model_ema=model_ema,\n",
    "        clip_grad_l2norm=clip_grad_l2norm,\n",
    "        tb_writer=tb_writer,\n",
    "        print_freq=print_freq\n",
    "    )\n",
    "\n",
    "    # save ckpt once in a while\n",
    "    if (\n",
    "        ((epoch + 1) == max_epochs) or\n",
    "        ((ckpt_freq > 0) and ((epoch + 1) % ckpt_freq == 0))\n",
    "    ):\n",
    "        save_states = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "        save_states['state_dict_ema'] = model_ema.module.state_dict()\n",
    "        save_checkpoint(\n",
    "            save_states,\n",
    "            False,\n",
    "            file_folder=ckpt_folder,\n",
    "            file_name='epoch_{:03d}.pth.tar'.format(epoch + 1)\n",
    "        )\n",
    "\n",
    "# wrap up\n",
    "tb_writer.close()\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc8b72-6f33-4ced-861e-2fef015351af",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8350933,
     "sourceId": 13178212,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8350977,
     "sourceId": 13178272,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8351098,
     "sourceId": 13178446,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8351253,
     "sourceId": 13178652,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8351296,
     "sourceId": 13178721,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8351781,
     "sourceId": 13179398,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8351861,
     "sourceId": 13179501,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8351869,
     "sourceId": 13179510,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8358483,
     "sourceId": 13189597,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8368234,
     "sourceId": 13203934,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8375566,
     "sourceId": 13214413,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
