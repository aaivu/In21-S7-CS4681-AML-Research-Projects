{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"11c6111fcf2b45f8bb395f48e0d4df97":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d2b6e62f59e14cddb11b53dbca5e79fb","IPY_MODEL_16db708797d641808e84f3a4181e1f18","IPY_MODEL_41fea03b4ee44d60baa52ae39984588a"],"layout":"IPY_MODEL_661bcd5fb40a492686e639f2f0990963"}},"d2b6e62f59e14cddb11b53dbca5e79fb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43e8f3eaddc346ebb4e9bb1d71382f3f","placeholder":"​","style":"IPY_MODEL_15bce210c3034e839a405d5d508702ea","value":"README.md: "}},"16db708797d641808e84f3a4181e1f18":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc7b7d763c284ee589ea8a8c589e2d1e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_961349215cea4a32976aeba9d2572951","value":1}},"41fea03b4ee44d60baa52ae39984588a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_176d56dad3c449daa7a4f75439bd48eb","placeholder":"​","style":"IPY_MODEL_268cbb0ab0e8497c82e30cfc72976610","value":" 4.95k/? [00:00&lt;00:00, 354kB/s]"}},"661bcd5fb40a492686e639f2f0990963":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43e8f3eaddc346ebb4e9bb1d71382f3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15bce210c3034e839a405d5d508702ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc7b7d763c284ee589ea8a8c589e2d1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"961349215cea4a32976aeba9d2572951":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"176d56dad3c449daa7a4f75439bd48eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"268cbb0ab0e8497c82e30cfc72976610":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a69ea694b6e42c7b38b060db5c1acd3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c50f3b1b368842cc8f1e64c3d1937984","IPY_MODEL_18231ddca15a4c6bb1cc88bd96f12b5b","IPY_MODEL_68b87a380d504eac9a6352b6efcd4f75"],"layout":"IPY_MODEL_72334716bac345d290e00dbf7539f084"}},"c50f3b1b368842cc8f1e64c3d1937984":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_738fbc8bb30a49ce9413a26283ca88a1","placeholder":"​","style":"IPY_MODEL_b9e969b464964b8fb7bd5d6d8b24b00c","value":"train-00000-of-00013.parquet: 100%"}},"18231ddca15a4c6bb1cc88bd96f12b5b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8442288c92eb404c85028ba148eabb99","max":485203140,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b9416396c22d4653bc1f60a1ef1e2a63","value":485203140}},"68b87a380d504eac9a6352b6efcd4f75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee7c89ac2be24cd3855ae02ddf3851b2","placeholder":"​","style":"IPY_MODEL_30467cf596674375a68cd1099c020585","value":" 485M/485M [00:01&lt;00:00, 313MB/s]"}},"72334716bac345d290e00dbf7539f084":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"738fbc8bb30a49ce9413a26283ca88a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9e969b464964b8fb7bd5d6d8b24b00c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8442288c92eb404c85028ba148eabb99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9416396c22d4653bc1f60a1ef1e2a63":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee7c89ac2be24cd3855ae02ddf3851b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30467cf596674375a68cd1099c020585":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f9d0f5b2c1e466d80fd18989e5498ea":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0811db3cd18a4a28b57dbab437bcf319","IPY_MODEL_cee0cf138a7648018a84eea0222bc16e","IPY_MODEL_0e8d61964b074e3791800c623a3828e8"],"layout":"IPY_MODEL_92da5dc27e8942de9b0203eabc281b34"}},"0811db3cd18a4a28b57dbab437bcf319":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4e95b8bad4e430687da9669628cb569","placeholder":"​","style":"IPY_MODEL_dc138693cece42f0ac27c7a4f3b096bb","value":"train-00001-of-00013.parquet: 100%"}},"cee0cf138a7648018a84eea0222bc16e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a35f6199075346ba8cc54686ad773803","max":360931566,"min":0,"orientation":"horizontal","style":"IPY_MODEL_27887835cf5344be8488aecd9075f1b8","value":360931566}},"0e8d61964b074e3791800c623a3828e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b2212657ac24b0db37bd81d6a66fc07","placeholder":"​","style":"IPY_MODEL_918b7b2d1c574356a1c137ab6b6e751e","value":" 361M/361M [00:01&lt;00:00, 276MB/s]"}},"92da5dc27e8942de9b0203eabc281b34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4e95b8bad4e430687da9669628cb569":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc138693cece42f0ac27c7a4f3b096bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a35f6199075346ba8cc54686ad773803":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27887835cf5344be8488aecd9075f1b8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b2212657ac24b0db37bd81d6a66fc07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"918b7b2d1c574356a1c137ab6b6e751e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"15b3af8496ed48e6bb6390adcf54af70":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_78d89612f3bd4aac9381cba65fa1bd4b","IPY_MODEL_0ba7eb06e8684ed08be25bc4a1240da4","IPY_MODEL_cfbf82f8611f4d3d855f4260e712ee79"],"layout":"IPY_MODEL_5c87129e94de475a8d668044f9591286"}},"78d89612f3bd4aac9381cba65fa1bd4b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d4626034e5c4634892a96170314138d","placeholder":"​","style":"IPY_MODEL_82e6876cd571435e9280664215698985","value":"train-00002-of-00013.parquet: 100%"}},"0ba7eb06e8684ed08be25bc4a1240da4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3254c209656440db92a90bf7ac68d8bf","max":410359730,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e25850d3873242beb30b01ff26e6c0ca","value":410359730}},"cfbf82f8611f4d3d855f4260e712ee79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51891ff8f9904fdbbb2c825075c7a7cf","placeholder":"​","style":"IPY_MODEL_823463db5422415a946834988f568962","value":" 410M/410M [00:01&lt;00:00, 366MB/s]"}},"5c87129e94de475a8d668044f9591286":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d4626034e5c4634892a96170314138d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82e6876cd571435e9280664215698985":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3254c209656440db92a90bf7ac68d8bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e25850d3873242beb30b01ff26e6c0ca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"51891ff8f9904fdbbb2c825075c7a7cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"823463db5422415a946834988f568962":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a35188d5e16473391ba877ece2fa949":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_da15550a1c3648179607ef339b19e85b","IPY_MODEL_8f08fc056bdf46e78759fccd02b6c510","IPY_MODEL_5b979b2264f94d819c6d6fcac071a409"],"layout":"IPY_MODEL_33fb8e9471be4c74af6b9ba9cc659bfe"}},"da15550a1c3648179607ef339b19e85b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_253946cd096447ecbc519773aff60487","placeholder":"​","style":"IPY_MODEL_34db46630ad24515be8e744e9d53a77f","value":"train-00003-of-00013.parquet: 100%"}},"8f08fc056bdf46e78759fccd02b6c510":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4091d246bc224a8ba8c16b7d86b36650","max":452139950,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bdad2ae19cbd496497ee260c732bf07e","value":452139950}},"5b979b2264f94d819c6d6fcac071a409":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f977412e27f4451aa56e6b33a5458503","placeholder":"​","style":"IPY_MODEL_4042e23570994eba846dcfdf214f3cfe","value":" 452M/452M [00:03&lt;00:00, 110MB/s]"}},"33fb8e9471be4c74af6b9ba9cc659bfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"253946cd096447ecbc519773aff60487":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34db46630ad24515be8e744e9d53a77f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4091d246bc224a8ba8c16b7d86b36650":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdad2ae19cbd496497ee260c732bf07e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f977412e27f4451aa56e6b33a5458503":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4042e23570994eba846dcfdf214f3cfe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b6f649cc1fe4229b2ee2a6e0c46b57c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2001bc5d1a404bfd9c5892c8f83358fa","IPY_MODEL_f52335d01d1249ada76d1c4fcb2b27ec","IPY_MODEL_c7cef884eb9b430b83aa7cc12520259e"],"layout":"IPY_MODEL_8a7a50041fa046cd8d1f41334e6fd924"}},"2001bc5d1a404bfd9c5892c8f83358fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7b77d0cdfb64dbd90a3caf5a9413d75","placeholder":"​","style":"IPY_MODEL_3156f99f3a7c40c6b9e397b0669a3890","value":"train-00004-of-00013.parquet: 100%"}},"f52335d01d1249ada76d1c4fcb2b27ec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e487a8985c84d09bf348a9633b47f13","max":394658878,"min":0,"orientation":"horizontal","style":"IPY_MODEL_19fa1a7829d44499985f13f797a9c396","value":394658878}},"c7cef884eb9b430b83aa7cc12520259e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69ab4f7794be477293128008f5fad075","placeholder":"​","style":"IPY_MODEL_442388c3a51944358ef89251b624b48a","value":" 395M/395M [00:01&lt;00:00, 371MB/s]"}},"8a7a50041fa046cd8d1f41334e6fd924":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7b77d0cdfb64dbd90a3caf5a9413d75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3156f99f3a7c40c6b9e397b0669a3890":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e487a8985c84d09bf348a9633b47f13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19fa1a7829d44499985f13f797a9c396":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"69ab4f7794be477293128008f5fad075":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"442388c3a51944358ef89251b624b48a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44c40fbb2f31400ab0a271ded03e184d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_98cead2efcec45e39a443e88f7879c07","IPY_MODEL_cccd8728e1524250b38e07ffe79d0d15","IPY_MODEL_7f6778b7e0c04aeeb7a8fb1daf990ffa"],"layout":"IPY_MODEL_a7a4bf1b7b6f40ae8882234a4e806fca"}},"98cead2efcec45e39a443e88f7879c07":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e89adc99b14b4c18a58fa88b3f13f245","placeholder":"​","style":"IPY_MODEL_e262fff9580941c4a35cfc8221d0391f","value":"train-00005-of-00013.parquet: 100%"}},"cccd8728e1524250b38e07ffe79d0d15":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e10ef53fced44cb90684a38463fd729","max":480834565,"min":0,"orientation":"horizontal","style":"IPY_MODEL_32e6961728e94f8c8335305ab6cfb300","value":480834565}},"7f6778b7e0c04aeeb7a8fb1daf990ffa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2569dbae188942e2ac75cfea3bb7ea92","placeholder":"​","style":"IPY_MODEL_7eb654530e4c422aba550305d7906765","value":" 481M/481M [00:02&lt;00:00, 233MB/s]"}},"a7a4bf1b7b6f40ae8882234a4e806fca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e89adc99b14b4c18a58fa88b3f13f245":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e262fff9580941c4a35cfc8221d0391f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e10ef53fced44cb90684a38463fd729":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32e6961728e94f8c8335305ab6cfb300":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2569dbae188942e2ac75cfea3bb7ea92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7eb654530e4c422aba550305d7906765":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8549f1a6482f457e96efcc006cf90ab5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_97753cb715e64c398022427bfa39af9d","IPY_MODEL_7e88b0546c894300aed06329f9018e5a","IPY_MODEL_c457a62899824a788206e807064210a2"],"layout":"IPY_MODEL_bd6399f390ef4becb048807ffe8368b2"}},"97753cb715e64c398022427bfa39af9d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f670cb6b2844d338eff38b9cdf5f7c2","placeholder":"​","style":"IPY_MODEL_621455c4707d49cda5c35d243bcb9d38","value":"train-00006-of-00013.parquet: 100%"}},"7e88b0546c894300aed06329f9018e5a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_03084adcabec4ddfa744504609c2b131","max":412038758,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0443371f06714490becee2d83bba69c2","value":412038758}},"c457a62899824a788206e807064210a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_abaca1f94d0a47fd8f5d232ad6f4a05c","placeholder":"​","style":"IPY_MODEL_6897069c50a045e0a4f1db4209e2fc9e","value":" 412M/412M [00:04&lt;00:00, 42.0MB/s]"}},"bd6399f390ef4becb048807ffe8368b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f670cb6b2844d338eff38b9cdf5f7c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"621455c4707d49cda5c35d243bcb9d38":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"03084adcabec4ddfa744504609c2b131":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0443371f06714490becee2d83bba69c2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"abaca1f94d0a47fd8f5d232ad6f4a05c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6897069c50a045e0a4f1db4209e2fc9e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e3da694e8254b31b6c8c68339106b72":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d4c85b46ca4e40cca214e73cafe3ebc9","IPY_MODEL_d7925ec4ab0b4a5c971233575dcaa553","IPY_MODEL_cccd828b974340c982fffa7d00a3288b"],"layout":"IPY_MODEL_78211839c80c46b1970727522bce859d"}},"d4c85b46ca4e40cca214e73cafe3ebc9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecc01ef3a7fa401eb14d4dc97a3b70a1","placeholder":"​","style":"IPY_MODEL_07383da613f045b9b132b54f00ef0ca5","value":"train-00007-of-00013.parquet: 100%"}},"d7925ec4ab0b4a5c971233575dcaa553":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6846fc547a79479d91e0766c06642bb1","max":368097670,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2a915011d7e3468395200a9172283633","value":368097670}},"cccd828b974340c982fffa7d00a3288b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32bd39d4d81d44ca8e04805820920d96","placeholder":"​","style":"IPY_MODEL_faae0326138a43f89503be3ef2c6f169","value":" 368M/368M [00:01&lt;00:00, 373MB/s]"}},"78211839c80c46b1970727522bce859d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecc01ef3a7fa401eb14d4dc97a3b70a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07383da613f045b9b132b54f00ef0ca5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6846fc547a79479d91e0766c06642bb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a915011d7e3468395200a9172283633":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"32bd39d4d81d44ca8e04805820920d96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"faae0326138a43f89503be3ef2c6f169":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d35b2c731e7041c5ade37e65daecc41c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4403462eb3fe4d5da1009f45dde08a82","IPY_MODEL_6a244b63395548d8923bce5771fc7d07","IPY_MODEL_fdbde500ae814064b355463cc39a8323"],"layout":"IPY_MODEL_ba0c95023f48495f8b082675b4281432"}},"4403462eb3fe4d5da1009f45dde08a82":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52f5d0ecd6b24fe18f48346eb871c444","placeholder":"​","style":"IPY_MODEL_c522b40c8ed146bcaab8ea0fcc9d82d5","value":"train-00008-of-00013.parquet: 100%"}},"6a244b63395548d8923bce5771fc7d07":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9772c8cc1b86444398650a29d4990355","max":396281173,"min":0,"orientation":"horizontal","style":"IPY_MODEL_655ffa2fcd8d4d279a40e364f038b630","value":396281173}},"fdbde500ae814064b355463cc39a8323":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c51862df4ceb4808bb294a296302c45e","placeholder":"​","style":"IPY_MODEL_489a22fdf8d24b90a67ced8fa3ac7012","value":" 396M/396M [00:01&lt;00:00, 277MB/s]"}},"ba0c95023f48495f8b082675b4281432":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52f5d0ecd6b24fe18f48346eb871c444":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c522b40c8ed146bcaab8ea0fcc9d82d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9772c8cc1b86444398650a29d4990355":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"655ffa2fcd8d4d279a40e364f038b630":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c51862df4ceb4808bb294a296302c45e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"489a22fdf8d24b90a67ced8fa3ac7012":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb06195a6e40427184a254664c2aae6c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ee49f5b3451e453ab07fd1e52e573f78","IPY_MODEL_a62123f22f554bf9acdee1990d30e0ee","IPY_MODEL_6cbd257e1e374f5d9793623cc8e7037e"],"layout":"IPY_MODEL_78fa31023bb3487baad3cf7e898bf22b"}},"ee49f5b3451e453ab07fd1e52e573f78":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0ed7e398ef54a60adad7d2b633be3a8","placeholder":"​","style":"IPY_MODEL_e48a8aac8e924a1b8d109121e319ebda","value":"train-00009-of-00013.parquet: 100%"}},"a62123f22f554bf9acdee1990d30e0ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6dac8b9cf76f4dbf8d25a770cae10e53","max":447238443,"min":0,"orientation":"horizontal","style":"IPY_MODEL_083341a34dc84f72be20eee6318c1c8e","value":447238443}},"6cbd257e1e374f5d9793623cc8e7037e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82c80922eaa647ecb560ea8d6f8d9e8b","placeholder":"​","style":"IPY_MODEL_4a45cf787d27422ba0d32a48bd318908","value":" 447M/447M [00:03&lt;00:00, 168MB/s]"}},"78fa31023bb3487baad3cf7e898bf22b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0ed7e398ef54a60adad7d2b633be3a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e48a8aac8e924a1b8d109121e319ebda":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6dac8b9cf76f4dbf8d25a770cae10e53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"083341a34dc84f72be20eee6318c1c8e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"82c80922eaa647ecb560ea8d6f8d9e8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a45cf787d27422ba0d32a48bd318908":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0255b0ebf7d84227b838a79b43db94dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_69330777b9964a3caf3661ab6969eff3","IPY_MODEL_11d1d70010b24efda7145e7ceea6eb3c","IPY_MODEL_0c06a7dc9b4c48528dbb23c234395366"],"layout":"IPY_MODEL_036fb111cf9f4675a59a9208d3048e7b"}},"69330777b9964a3caf3661ab6969eff3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd0a0a2dbc6241e0beb581ac717306a7","placeholder":"​","style":"IPY_MODEL_7cac29bf12ac4cc8ab0d78e2b2f13112","value":"train-00010-of-00013.parquet: 100%"}},"11d1d70010b24efda7145e7ceea6eb3c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9c90c13559c44f5a1107cfbceb008b4","max":495322584,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac45040c721941658b25852b0da9000e","value":495322584}},"0c06a7dc9b4c48528dbb23c234395366":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bec720cb0da400b967eef45eca9d19e","placeholder":"​","style":"IPY_MODEL_7d3389f0ecf7443d9d8e552565b55dbc","value":" 495M/495M [00:03&lt;00:00, 126MB/s]"}},"036fb111cf9f4675a59a9208d3048e7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd0a0a2dbc6241e0beb581ac717306a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cac29bf12ac4cc8ab0d78e2b2f13112":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b9c90c13559c44f5a1107cfbceb008b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac45040c721941658b25852b0da9000e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9bec720cb0da400b967eef45eca9d19e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d3389f0ecf7443d9d8e552565b55dbc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ff27042e3284c92827cadaf2ed358f8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0cfe7a1c7b574a549dbcbf8140716afa","IPY_MODEL_efb3ee2cf496400fa32e3ee8999c9da2","IPY_MODEL_10034d003c3449bfbf28258cc522df9a"],"layout":"IPY_MODEL_975545bd0d614772a73cbdac6139d8ac"}},"0cfe7a1c7b574a549dbcbf8140716afa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d5cafb013eb45bf92a03364fa9dc2d5","placeholder":"​","style":"IPY_MODEL_c663dca41a2642acb70e4cf2d3d6a2ae","value":"train-00011-of-00013.parquet: 100%"}},"efb3ee2cf496400fa32e3ee8999c9da2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a484115306940d0a1566e97821095f2","max":438364171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5a8ef11aaa334e7fb11698101ffd42e6","value":438364171}},"10034d003c3449bfbf28258cc522df9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_beb2c71ed5e647a1984679508865da1f","placeholder":"​","style":"IPY_MODEL_f0dcea3c583c403f9b00343ee2bf9e6f","value":" 438M/438M [00:19&lt;00:00, 21.5MB/s]"}},"975545bd0d614772a73cbdac6139d8ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d5cafb013eb45bf92a03364fa9dc2d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c663dca41a2642acb70e4cf2d3d6a2ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a484115306940d0a1566e97821095f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a8ef11aaa334e7fb11698101ffd42e6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"beb2c71ed5e647a1984679508865da1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0dcea3c583c403f9b00343ee2bf9e6f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ee1f892afdb4aa08e213edceb6294a3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6187c91ef7b416ba32970f38183d106","IPY_MODEL_10028f22e5c94c5dbf59d057bd4922c6","IPY_MODEL_c4242ee13e244d05bb46f489c5467aa4"],"layout":"IPY_MODEL_3546d8df869e4d878cbd371fb4cafd78"}},"e6187c91ef7b416ba32970f38183d106":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc8d9d8a49c84ffe9e3a3608e97aab4d","placeholder":"​","style":"IPY_MODEL_908b91efdbfe4cddbf9d9aca27553c40","value":"train-00012-of-00013.parquet: 100%"}},"10028f22e5c94c5dbf59d057bd4922c6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_68ccbe0a58894442b5f3c1512b78c3fe","max":505012119,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5a74e3eaabf74fe7867b88a6ce74e631","value":505012119}},"c4242ee13e244d05bb46f489c5467aa4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da67f91be25c425db4ced63d5f942430","placeholder":"​","style":"IPY_MODEL_a2369dff8eef46d1b3ddb194e607db10","value":" 505M/505M [00:02&lt;00:00, 249MB/s]"}},"3546d8df869e4d878cbd371fb4cafd78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc8d9d8a49c84ffe9e3a3608e97aab4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"908b91efdbfe4cddbf9d9aca27553c40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68ccbe0a58894442b5f3c1512b78c3fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a74e3eaabf74fe7867b88a6ce74e631":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"da67f91be25c425db4ced63d5f942430":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2369dff8eef46d1b3ddb194e607db10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86c4be34357242969f841197d28e7f30":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4db1ca4ea170457fa33d07e8c827c6db","IPY_MODEL_b794d4905da143e1a6310baec3b2456a","IPY_MODEL_c80804ece8314b9d83b4208446b64434"],"layout":"IPY_MODEL_7b0d9a289873486583d1aa6d9d250aab"}},"4db1ca4ea170457fa33d07e8c827c6db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_642eb957a9d0487cb6ad28bac4c04206","placeholder":"​","style":"IPY_MODEL_531b3ef7131f4c9794a2bf01fb323889","value":"validation-00000-of-00003.parquet: 100%"}},"b794d4905da143e1a6310baec3b2456a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_47149e7044cc40fe9d48d273d6c66bb8","max":376029315,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6757053001224ee89220c1288e3fe0b0","value":376029315}},"c80804ece8314b9d83b4208446b64434":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b9bd9a7f66d4c9c8f0a75418cd9b407","placeholder":"​","style":"IPY_MODEL_4dd41bdb85da4b30b2eb61715002192a","value":" 376M/376M [00:01&lt;00:00, 253MB/s]"}},"7b0d9a289873486583d1aa6d9d250aab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"642eb957a9d0487cb6ad28bac4c04206":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"531b3ef7131f4c9794a2bf01fb323889":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47149e7044cc40fe9d48d273d6c66bb8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6757053001224ee89220c1288e3fe0b0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0b9bd9a7f66d4c9c8f0a75418cd9b407":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dd41bdb85da4b30b2eb61715002192a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e643dbd4cf8747f8a139eab6ee0dde27":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9cc670c1bd9a4291bff45714365a6c69","IPY_MODEL_8ec3798ff0b644a0b627198da47fae39","IPY_MODEL_753bb37bb4a94c95b3d1a5048aeb1fba"],"layout":"IPY_MODEL_7bb212a7d7b844c19416f2140cac9447"}},"9cc670c1bd9a4291bff45714365a6c69":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bb3b897dcba4a36be0670de448f4e20","placeholder":"​","style":"IPY_MODEL_0fe030ddbc5a41d4808d3fe0835f2a23","value":"validation-00001-of-00003.parquet: 100%"}},"8ec3798ff0b644a0b627198da47fae39":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5842206cff8e4a4b9c31bc208ab5d085","max":343931206,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9579373442d140d690279f925cae9953","value":343931206}},"753bb37bb4a94c95b3d1a5048aeb1fba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9456bdf0b8c34603b15e99144f643a45","placeholder":"​","style":"IPY_MODEL_acce6e4345d841beb6249289fbe1ca33","value":" 344M/344M [00:01&lt;00:00, 300MB/s]"}},"7bb212a7d7b844c19416f2140cac9447":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bb3b897dcba4a36be0670de448f4e20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0fe030ddbc5a41d4808d3fe0835f2a23":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5842206cff8e4a4b9c31bc208ab5d085":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9579373442d140d690279f925cae9953":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9456bdf0b8c34603b15e99144f643a45":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acce6e4345d841beb6249289fbe1ca33":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff9ec5fe77554bb084a582b75a4fc0af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7867a424442b4ec6a2c93b4ff6141b56","IPY_MODEL_f501a91df28642cdadc094e40d9b9c92","IPY_MODEL_cc1fdf5da17447a1a75f219f8dac4261"],"layout":"IPY_MODEL_5cf9c436a5e945669222132329ad6909"}},"7867a424442b4ec6a2c93b4ff6141b56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_049190e4a30745b4960edd8c76459d59","placeholder":"​","style":"IPY_MODEL_7a20f4d95d594624af87c69690f65c49","value":"validation-00002-of-00003.parquet: 100%"}},"f501a91df28642cdadc094e40d9b9c92":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a851e67d82fb41b78551146ff79ed8e2","max":404819164,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d13213844fa64ae7b0fb602c1d9ed816","value":404819164}},"cc1fdf5da17447a1a75f219f8dac4261":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f62db02c1a704b6ead8342b5215bd014","placeholder":"​","style":"IPY_MODEL_aa177f9d083d44169b6bf5ddecad0fa9","value":" 405M/405M [00:01&lt;00:00, 301MB/s]"}},"5cf9c436a5e945669222132329ad6909":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"049190e4a30745b4960edd8c76459d59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a20f4d95d594624af87c69690f65c49":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a851e67d82fb41b78551146ff79ed8e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d13213844fa64ae7b0fb602c1d9ed816":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f62db02c1a704b6ead8342b5215bd014":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa177f9d083d44169b6bf5ddecad0fa9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf4fa11b513e4dd0993e34f797808c90":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c24fd3de70f64eb0bafc7bf2d14f4ee0","IPY_MODEL_42d07ef01b944054953279e64e9fc625","IPY_MODEL_6cd11638086a4847982f5c2a3d0d91a1"],"layout":"IPY_MODEL_a6ddda03095140ffa08e26cfe10e40c9"}},"c24fd3de70f64eb0bafc7bf2d14f4ee0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_261de8abfbd54fd9b1f41f0797380776","placeholder":"​","style":"IPY_MODEL_d154a0771ae64c33ba5df3e6ec7f1a01","value":"test-00000-of-00002.parquet: 100%"}},"42d07ef01b944054953279e64e9fc625":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_33a2b90a51554a3f9dc38fc830a2e816","max":325755980,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0caa602b48b14a80b781c398787407cd","value":325755980}},"6cd11638086a4847982f5c2a3d0d91a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f3fb0b8f14440d195ec1653c7cb8d88","placeholder":"​","style":"IPY_MODEL_7205a90a25f846c794fa188f805264b9","value":" 326M/326M [00:01&lt;00:00, 315MB/s]"}},"a6ddda03095140ffa08e26cfe10e40c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"261de8abfbd54fd9b1f41f0797380776":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d154a0771ae64c33ba5df3e6ec7f1a01":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33a2b90a51554a3f9dc38fc830a2e816":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0caa602b48b14a80b781c398787407cd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5f3fb0b8f14440d195ec1653c7cb8d88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7205a90a25f846c794fa188f805264b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f77517bb4e244b85ae2d1cc462fc1d44":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1af15184efdb43f2b13000e59c53d4cb","IPY_MODEL_cb6039e0005f4c0e85f755aae0fe835a","IPY_MODEL_83d967d44cb3450e8536b1f30aea4ce4"],"layout":"IPY_MODEL_64f52fa58408485cbe9c1c8502e4b7cb"}},"1af15184efdb43f2b13000e59c53d4cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30abe146ffce4679b2f656c3812b6e19","placeholder":"​","style":"IPY_MODEL_b4b43adcf3704cbf9ea0355c6a820350","value":"test-00001-of-00002.parquet: 100%"}},"cb6039e0005f4c0e85f755aae0fe835a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_76cae621f55047e79c742a729289b825","max":336443271,"min":0,"orientation":"horizontal","style":"IPY_MODEL_69b692c9ac3f413f868eacad0f50c486","value":336443271}},"83d967d44cb3450e8536b1f30aea4ce4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb00e607da5b4e4cae96e1063fb45093","placeholder":"​","style":"IPY_MODEL_842568ab017e4e6b9ff6c136bb8b1fa1","value":" 336M/336M [00:14&lt;00:00, 23.5MB/s]"}},"64f52fa58408485cbe9c1c8502e4b7cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30abe146ffce4679b2f656c3812b6e19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4b43adcf3704cbf9ea0355c6a820350":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76cae621f55047e79c742a729289b825":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69b692c9ac3f413f868eacad0f50c486":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb00e607da5b4e4cae96e1063fb45093":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"842568ab017e4e6b9ff6c136bb8b1fa1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5d46a0599e14d4eb102fc11bd9c55cb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_12ca999fa77143f1b50848448824f10f","IPY_MODEL_c669f89df11c48b69d81e91b0e632c71","IPY_MODEL_db2b74b81dcf4250a698dbf5caae9b07"],"layout":"IPY_MODEL_42bfcd31ffe8427d92cbb69defdf5046"}},"12ca999fa77143f1b50848448824f10f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ce1efbb709546e08d3769bed1a589a1","placeholder":"​","style":"IPY_MODEL_4b6130d4646d4a2bb2e7828c7bebaae9","value":"Generating train split: 100%"}},"c669f89df11c48b69d81e91b0e632c71":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d6aae2f375848edb4f012f526ecf3bc","max":50000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f32a1513912148d6906df2f3918afe2d","value":50000}},"db2b74b81dcf4250a698dbf5caae9b07":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1725eba5a61044ba8d222bc35840588e","placeholder":"​","style":"IPY_MODEL_5f738c5f852a42d4b0f86ee5c58f2ab5","value":" 50000/50000 [00:43&lt;00:00, 275.57 examples/s]"}},"42bfcd31ffe8427d92cbb69defdf5046":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ce1efbb709546e08d3769bed1a589a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b6130d4646d4a2bb2e7828c7bebaae9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d6aae2f375848edb4f012f526ecf3bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f32a1513912148d6906df2f3918afe2d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1725eba5a61044ba8d222bc35840588e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f738c5f852a42d4b0f86ee5c58f2ab5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3cbf4ae83c74faaab69a1fedc9e41d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_974124e77fe543baae0c27386895d69f","IPY_MODEL_ebf4b90793144db4bb32bc2b8ee69385","IPY_MODEL_196e890b1d294749907ab07a50846b8b"],"layout":"IPY_MODEL_db412116979c4f6b8b537ca28b2dda77"}},"974124e77fe543baae0c27386895d69f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_36f517586b8b49b08aea2aca0d7c9784","placeholder":"​","style":"IPY_MODEL_57f1b9b98ac041cab1aa4731516773b4","value":"Generating validation split: 100%"}},"ebf4b90793144db4bb32bc2b8ee69385":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b37a2cf93f73442592e300b098a2aa44","max":10000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_973420f23387491c8d6ff253de8a4e41","value":10000}},"196e890b1d294749907ab07a50846b8b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f05d29c54544edc8a0f505eee543f1e","placeholder":"​","style":"IPY_MODEL_08cdda81d3724a0582b570c657a21fcd","value":" 10000/10000 [00:11&lt;00:00, 1246.17 examples/s]"}},"db412116979c4f6b8b537ca28b2dda77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36f517586b8b49b08aea2aca0d7c9784":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57f1b9b98ac041cab1aa4731516773b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b37a2cf93f73442592e300b098a2aa44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"973420f23387491c8d6ff253de8a4e41":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5f05d29c54544edc8a0f505eee543f1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08cdda81d3724a0582b570c657a21fcd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"99c23c3a3b3349ae8e62c2efb28f4aa3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7444c44c705e459c9f250c6b8cb228bb","IPY_MODEL_2d1f79834c5e4df491f02725a47db609","IPY_MODEL_31b91e514b604b62b72bce9a1f48a30f"],"layout":"IPY_MODEL_ad9e2a78b50d46fd83095fe1f7f1c621"}},"7444c44c705e459c9f250c6b8cb228bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_044e9dbcf6534aa9b32983dec5907af0","placeholder":"​","style":"IPY_MODEL_148754d940054fbf91720d13cd561bf5","value":"Generating test split: 100%"}},"2d1f79834c5e4df491f02725a47db609":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_347648688adb476db758300c7cc1379d","max":5000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_692f5980a46e416fb3e8f68c838f5eef","value":5000}},"31b91e514b604b62b72bce9a1f48a30f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_157a313946d744d38580bc880064ab2a","placeholder":"​","style":"IPY_MODEL_88dc9b37c05e40b880dbfe97c1ba88c5","value":" 5000/5000 [00:03&lt;00:00, 1222.79 examples/s]"}},"ad9e2a78b50d46fd83095fe1f7f1c621":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"044e9dbcf6534aa9b32983dec5907af0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"148754d940054fbf91720d13cd561bf5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"347648688adb476db758300c7cc1379d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"692f5980a46e416fb3e8f68c838f5eef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"157a313946d744d38580bc880064ab2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88dc9b37c05e40b880dbfe97c1ba88c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13226953,"sourceType":"datasetVersion","datasetId":8378637}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install open_clip_torch transformers","metadata":{"id":"JvaEkx8Cyvhg"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import open_clip\nimport torch\n\nmodel, _, transform = open_clip.create_model_and_transforms(\n  model_name=\"coca_ViT-L-14\",\n  pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\"\n)","metadata":{"id":"vE4lFFkKyotX"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget https://i.imgur.com/8H7XCH0.jpg -O cat.jpg","metadata":{"id":"oOaE1AmDyth_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Image\nImage('cat.jpg')","metadata":{"id":"Y9Q6bhVA2L01","outputId":"1b920080-e8cd-4d2f-fb23-30e6f1b612f9"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nim = Image.open(\"cat.jpg\").convert(\"RGB\")\nim = transform(im).unsqueeze(0)\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n  generated = model.generate(im)\n\nprint(open_clip.decode(generated[0]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\"))","metadata":{"id":"byZKXMGzyr5Y","outputId":"122eb099-6704-4e3c-fa7c-a05dd87ce64f"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# mini image net sym links\n","metadata":{"id":"MtL4rA9ZGbzL"}},{"cell_type":"code","source":"import os\nfrom collections import defaultdict\nfrom datasets import load_dataset, get_dataset_config_info\nfrom PIL import Image\n\ndef create_fewshot_splits(dataset_name=\"timm/mini-imagenet\", output_dir=\"mini_imagenet_fewshot\", num_images_per_class=20):\n    \"\"\"\n    Loads the mini-imagenet dataset and creates train, validation, and test splits\n    with a specified number of images per class, saved into folders.\n\n    This version is optimized for low RAM usage by streaming the dataset.\n\n    Args:\n        dataset_name (str): The name of the dataset on the Hugging Face Hub.\n        output_dir (str): The root directory to save the new dataset splits.\n        num_images_per_class (int): The number of images to save for each class in each split.\n    \"\"\"\n    print(f\"Loading '{dataset_name}' dataset info...\")\n    # The original dataset only has 'train' and 'validation' splits.\n    # We will create our train set from the original 'train' split,\n    # and our validation and test sets from the original 'validation' split.\n    \n    # For streaming, we can't access .features directly on the dataset object.\n    # We get the dataset info first to extract class names.\n    try:\n        info = get_dataset_config_info(dataset_name, trust_remote_code=True)\n        class_names = info.features['label'].names\n    except Exception as e:\n        print(f\"Could not automatically get class names via info: {e}\")\n        print(\"Attempting to load a small part of the dataset to infer class names (this may use more RAM).\")\n        # Fallback for datasets where info might not be straightforward\n        temp_ds = load_dataset(dataset_name, split='train', trust_remote_code=True)\n        class_names = temp_ds.features['label'].names\n        del temp_ds # free up memory\n\n    print(f\"Found {len(class_names)} classes.\")\n    print(\"Starting to stream and process datasets to save RAM...\")\n\n    # Use streaming=True to avoid loading the entire dataset into memory.\n    original_train_ds = load_dataset(dataset_name, split='train', trust_remote_code=True, streaming=True)\n    original_val_ds = load_dataset(dataset_name, split='validation', trust_remote_code=True, streaming=True)\n    \n    # --- 1. Process the Training Split ---\n    print(\"\\nProcessing 'train' split...\")\n    process_split(\n        ds=original_train_ds,\n        split_name='train',\n        output_dir=output_dir,\n        class_names=class_names,\n        num_images=num_images_per_class\n    )\n\n    # --- 2. Process the Validation and Test Splits from the original validation set ---\n    print(\"\\nProcessing 'validation' and 'test' splits...\")\n    # This part is rewritten to handle streaming data and avoid loading everything into memory.\n    val_counts = defaultdict(int)\n    test_counts = defaultdict(int)\n    total_classes = len(class_names)\n    # Use a set to track completed classes for faster lookups\n    completed_classes = set()\n\n    for item in original_val_ds:\n        label_idx = item['label']\n        \n        # If we have already processed this class, skip to the next item\n        if label_idx in completed_classes:\n            continue\n\n        class_name = class_names[label_idx]\n\n        # Try to add to validation set if not full\n        if val_counts[label_idx] < num_images_per_class:\n            val_path = os.path.join(output_dir, 'validation', class_name)\n            os.makedirs(val_path, exist_ok=True)\n            img_path = os.path.join(val_path, f\"{val_counts[label_idx]}.jpg\")\n            item['image'].convert(\"RGB\").save(img_path)\n            val_counts[label_idx] += 1\n        # If validation is full, try to add to test set\n        elif test_counts[label_idx] < num_images_per_class:\n            test_path = os.path.join(output_dir, 'test', class_name)\n            os.makedirs(test_path, exist_ok=True)\n            img_path = os.path.join(test_path, f\"{test_counts[label_idx]}.jpg\")\n            item['image'].convert(\"RGB\").save(img_path)\n            test_counts[label_idx] += 1\n        \n        # Check if the class is now complete for both splits after the potential save\n        if val_counts[label_idx] >= num_images_per_class and test_counts[label_idx] >= num_images_per_class:\n             if label_idx not in completed_classes:\n                completed_classes.add(label_idx)\n                print(f\"  Finished val/test for class: {class_name} ({len(completed_classes)}/{total_classes})\")\n        \n        # Early exit if all classes are done\n        if len(completed_classes) == total_classes:\n            print(\"Successfully created 'validation' and 'test' splits.\")\n            break\n            \n    print(\"\\nDataset creation complete.\")\n    print(f\"Your few-shot dataset is ready in the '{output_dir}' directory.\")\n\n\ndef process_split(ds, split_name, output_dir, class_names, num_images):\n    \"\"\"\n    Helper function to iterate through a dataset split and save a few images per class.\n    \"\"\"\n    class_counts = defaultdict(int)\n    total_classes = len(class_names)\n    classes_done = 0\n\n    for item in ds:\n        label_idx = item['label']\n\n        if class_counts[label_idx] < num_images:\n            class_name = class_names[label_idx]\n            \n            # Create the directory path for the class\n            class_dir = os.path.join(output_dir, split_name, class_name)\n            os.makedirs(class_dir, exist_ok=True)\n            \n            # Save the image\n            image = item['image']\n            image_path = os.path.join(class_dir, f\"{class_counts[label_idx]}.jpg\")\n            # Convert to RGB to handle potential RGBA or grayscale images\n            image.convert(\"RGB\").save(image_path)\n            \n            # Increment the counter for this class\n            class_counts[label_idx] += 1\n\n            # Check if this class is now complete\n            if class_counts[label_idx] == num_images:\n                classes_done += 1\n                print(f\"  Finished class: {class_name} ({classes_done}/{total_classes})\")\n        \n        # Early exit if we have collected enough images for all classes\n        if classes_done == total_classes:\n            print(f\"Successfully created '{split_name}' split.\")\n            break\n\nif __name__ == '__main__':\n    # You might need to install these libraries if you haven't already:\n    # pip install datasets Pillow\n    create_fewshot_splits()\n\n","metadata":{"id":"LRRZpR41GlYh","outputId":"346d9373-265d-4f46-e258-116a3ff3f97f","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T09:22:19.983267Z","iopub.execute_input":"2025-10-04T09:22:19.983529Z","iopub.status.idle":"2025-10-04T09:22:33.085613Z","shell.execute_reply.started":"2025-10-04T09:22:19.983509Z","shell.execute_reply":"2025-10-04T09:22:33.084685Z"}},"outputs":[{"name":"stdout","text":"Loading 'timm/mini-imagenet' dataset info...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd10b5b402764556b52100212b206494"}},"metadata":{}},{"name":"stdout","text":"Found 100 classes.\nStarting to stream and process datasets to save RAM...\n\nProcessing 'train' split...\n  Finished class: n01532829 (1/100)\n  Finished class: n01558993 (2/100)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1829083050.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;31m# You might need to install these libraries if you haven't already:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# pip install datasets Pillow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mcreate_fewshot_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/1829083050.py\u001b[0m in \u001b[0;36mcreate_fewshot_splits\u001b[0;34m(dataset_name, output_dir, num_images_per_class)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# --- 1. Process the Training Split ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nProcessing 'train' split...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     process_split(\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moriginal_train_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0msplit_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/1829083050.py\u001b[0m in \u001b[0;36mprocess_split\u001b[0;34m(ds, split_name, output_dir, class_names, num_images)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mclasses_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mlabel_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/iterable_dataset.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2268\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2270\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex_iterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2271\u001b[0m             \u001b[0;31m# no need to format thanks to FormattedExamplesIterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2272\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/iterable_dataset.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1855\u001b[0m             \u001b[0;31m# feature casting (inc column addition) handled within self._iter_arrow()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_arrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1857\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1858\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_batch_to_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1859\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mLazyBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mdecode_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36mdecode_batch\u001b[0;34m(self, batch, token_per_repo_id)\u001b[0m\n\u001b[1;32m   2141\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcolumn_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2142\u001b[0m             decoded_batch[column_name] = (\n\u001b[0;32m-> 2143\u001b[0;31m                 [\n\u001b[0m\u001b[1;32m   2144\u001b[0m                     \u001b[0mdecode_nested_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_per_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_per_repo_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2142\u001b[0m             decoded_batch[column_name] = (\n\u001b[1;32m   2143\u001b[0m                 [\n\u001b[0;32m-> 2144\u001b[0;31m                     \u001b[0mdecode_nested_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_per_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_per_repo_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2145\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2146\u001b[0m                     \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36mdecode_nested_example\u001b[0;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[1;32m   1412\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"decode_example\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"decode\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0;31m# we pass the token to read and decode files from private repositories in streaming mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_per_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_per_repo_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/features/image.py\u001b[0m in \u001b[0;36mdecode_example\u001b[0;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# to avoid \"Too many open files\" errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetexif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExifTags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrientation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageOps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexif_transpose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport zipfile\nimport sys\n\ndef zip_folder(folder_path, output_path=None):\n    \"\"\"Create a zip file from a folder.\"\"\"\n    if not os.path.isdir(folder_path):\n        raise ValueError(f\"{folder_path} is not a valid directory\")\n\n    if output_path is None:\n        output_path = folder_path.rstrip(os.sep) + \".zip\"\n\n    with zipfile.ZipFile(output_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n        for root, _, files in os.walk(folder_path):\n            for file in files:\n                abs_path = os.path.join(root, file)\n                rel_path = os.path.relpath(abs_path, os.path.dirname(folder_path))\n                zipf.write(abs_path, rel_path)\n\n    print(f\"Created zip: {output_path}\")\n\n\nzip_folder(\"/kaggle/working/mini_imagenet_fewshot\", \"/kaggle/working/asd2.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T12:14:39.507032Z","iopub.execute_input":"2025-10-01T12:14:39.507850Z","iopub.status.idle":"2025-10-01T12:14:46.799963Z","shell.execute_reply.started":"2025-10-01T12:14:39.507827Z","shell.execute_reply":"2025-10-01T12:14:46.799330Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom datasets import get_dataset_infos\n\ndef view_and_save_dataset_readme(dataset_name=\"timm/mini-imagenet\", output_dir=\"mini_imagenet_fewshot\"):\n    \"\"\"\n    Loads the README (dataset card) for a specified Hugging Face dataset,\n    prints it to the console, and saves it to a file.\n    This is memory-efficient as it only fetches metadata, not the image data.\n\n    Args:\n        dataset_name (str): The name of the dataset on the Hugging Face Hub.\n        output_dir (str): The directory where the README.md file will be saved.\n    \"\"\"\n    print(f\"Fetching README for '{dataset_name}'...\")\n    try:\n        # get_dataset_infos returns a dictionary of configurations for the dataset.\n        # We'll use the information from the primary (first) configuration.\n        all_infos = get_dataset_infos(dataset_name, trust_remote_code=True)\n        \n        if not all_infos:\n            print(f\"Could not find any configuration info for '{dataset_name}'.\")\n            return\n\n        # Get the info object from the first available configuration\n        primary_config_key = next(iter(all_infos))\n        info = all_infos[primary_config_key]\n        \n        readme_content = info.description\n        if readme_content:\n            # --- Print to Console ---\n            print(\"\\n\" + \"=\"*25 + \" README / Dataset Card \" + \"=\"*25 + \"\\n\")\n            print(readme_content)\n            print(\"\\n\" + \"=\"*25 + \"  End of README  \" + \"=\"*25)\n\n            # --- Save to File ---\n            # Ensure the output directory exists\n            os.makedirs(output_dir, exist_ok=True)\n            output_path = os.path.join(output_dir, \"README.md\")\n            \n            try:\n                with open(output_path, \"w\", encoding=\"utf-8\") as f:\n                    f.write(readme_content)\n                print(f\"\\nSuccessfully saved README to: '{output_path}'\")\n            except IOError as e:\n                print(f\"\\nError: Could not write README to file '{output_path}'. Reason: {e}\")\n\n        else:\n            print(\"No README or description was found for this dataset.\")\n\n    except Exception as e:\n        print(f\"\\nAn error occurred while trying to fetch the dataset info: {e}\")\n        print(\"Please check if the dataset name is correct and you have an internet connection.\")\n\nif __name__ == '__main__':\n    # You might need to install this library if you haven't already:\n    # pip install datasets\n    view_and_save_dataset_readme()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-07T19:59:01.315357Z","iopub.execute_input":"2025-09-07T19:59:01.315766Z","iopub.status.idle":"2025-09-07T19:59:05.901919Z","shell.execute_reply.started":"2025-09-07T19:59:01.315740Z","shell.execute_reply":"2025-09-07T19:59:05.900890Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nroot = \"/kaggle/input/mini-image-net-fewshot-dataset/mini_imagenet_fewshot/train\"\nfolders = os.listdir(root)\nprint(folders)  # copy these and fill them in the dict above\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:31:03.972305Z","iopub.execute_input":"2025-09-30T15:31:03.972885Z","iopub.status.idle":"2025-09-30T15:31:03.993908Z","shell.execute_reply.started":"2025-09-30T15:31:03.972858Z","shell.execute_reply":"2025-09-30T15:31:03.993166Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# start here","metadata":{}},{"cell_type":"code","source":"synset_to_caption = {\n    \"n02457408\": \"three-toed sloth\",\n    \"n02101006\": \"Gordon setter dog\",\n    \"n02950826\": \"cannon\",\n    \"n03854065\": \"organ keyboard instrument\",\n    \"n02219486\": \"ant\",\n    \"n03888605\": \"parallel bars\",\n    \"n03017168\": \"chime or bell\",\n    \"n01558993\": \"robin bird\",\n    \"n02108551\": \"Tibetan mastiff dog\",\n    \"n03676483\": \"lipstick\",\n    \"n03400231\": \"frying pan\",\n    \"n03838899\": \"oboe\",\n    \"n03347037\": \"fire screen\",\n    \"n04509417\": \"unicycle\",\n    \"n03062245\": \"cocktail shaker\",\n    \"n04149813\": \"scoreboard\",\n    \"n03337140\": \"file cabinet\",\n    \"n04522168\": \"vase\",\n    \"n02966193\": \"car radiator grille\",\n    \"n13133613\": \"ear of corn\",\n    \"n03207743\": \"dishwasher\",\n    \"n02091831\": \"Saluki dog\",\n    \"n04612504\": \"yawl sailboat\",\n    \"n03770439\": \"minivan\",\n    \"n02091244\": \"Ibizan hound\",\n    \"n03924679\": \"photocopier\",\n    \"n02111277\": \"Newfoundland dog\",\n    \"n02981792\": \"chain saw\",\n    \"n07747607\": \"orange fruit\",\n    \"n04418357\": \"theater curtain\",\n    \"n01981276\": \"king crab\",\n    \"n02113712\": \"miniature poodle\",\n    \"n01855672\": \"goose\",\n    \"n04604644\": \"worm fence\",\n    \"n04251144\": \"snorkel\",\n    \"n02108915\": \"French bulldog\",\n    \"n04275548\": \"spatula\",\n    \"n03272010\": \"electric guitar\",\n    \"n02795169\": \"barber chair\",\n    \"n13054560\": \"bolete mushroom\",\n    \"n02823428\": \"beer bottle\",\n    \"n04596742\": \"wok\",\n    \"n03775546\": \"mixing bowl\",\n    \"n09256479\": \"coral reef\",\n    \"n03476684\": \"hair slide\",\n    \"n02099601\": \"golden retriever\",\n    \"n02110063\": \"malamute dog\",\n    \"n04146614\": \"school bus\",\n    \"n01930112\": \"nematode worm\",\n    \"n01532829\": \"house sparrow\",\n    \"n02165456\": \"ladybug\",\n    \"n07697537\": \"hotdog sandwich\",\n    \"n04515003\": \"upright piano\",\n    \"n03544143\": \"hourglass\",\n    \"n03527444\": \"holster\",\n    \"n06794110\": \"street sign\",\n    \"n02129165\": \"lion\",\n    \"n04067472\": \"reflex camera\",\n    \"n01704323\": \"triceratops\",\n    \"n03047690\": \"clog shoe\",\n    \"n03417042\": \"garbage truck\",\n    \"n02089867\": \"Walker hound\",\n    \"n02074367\": \"dugong\",\n    \"n02120079\": \"Arctic fox\",\n    \"n03220513\": \"dome\",\n    \"n02114548\": \"white wolf\",\n    \"n03075370\": \"combination lock\",\n    \"n02443484\": \"black-footed ferret\",\n    \"n04243546\": \"slot machine\",\n    \"n01749939\": \"green mamba\",\n    \"n01770081\": \"harvestman spider\",\n    \"n03584254\": \"iPod\",\n    \"n02687172\": \"aircraft carrier\",\n    \"n02971356\": \"cardigan sweater\",\n    \"n02606052\": \"rock beauty fish\",\n    \"n02174001\": \"rhinoceros beetle\",\n    \"n01910747\": \"jellyfish\",\n    \"n09246464\": \"cliff\",\n    \"n03127925\": \"crash helmet\",\n    \"n07613480\": \"trifle dessert\",\n    \"n02110341\": \"dalmatian dog\",\n    \"n04296562\": \"stage\",\n    \"n04443257\": \"tobacco shop\",\n    \"n02105505\": \"komondor dog\",\n    \"n03535780\": \"horizontal bar\",\n    \"n04389033\": \"tank\",\n    \"n02116738\": \"African hunting dog\",\n    \"n02747177\": \"ashcan trash bin\",\n    \"n03773504\": \"missile\",\n    \"n02108089\": \"boxer dog\",\n    \"n03146219\": \"cuirass armor\",\n    \"n03980874\": \"poncho\",\n    \"n03908618\": \"pencil box\",\n    \"n01843383\": \"toucan\",\n    \"n04258138\": \"solar dish\",\n    \"n03998194\": \"prayer rug\",\n    \"n07584110\": \"consomme soup\",\n    \"n04435653\": \"tile roof\",\n    \"n02138441\": \"meerkat\",\n    \"n02871525\": \"book jacket\"\n}\nimport os\n\nsrc_root = \"/kaggle/input/mini-image-net-fewshot-dataset/asd2/mini_imagenet_fewshot\"\ndst_root = \"/kaggle/working/mini_imagenet_fewshot_renamed\"\n\nsplits = [\"train\", \"validation\", \"test\"]\n\n# create output dirs\nfor split in splits:\n    os.makedirs(os.path.join(dst_root, split), exist_ok=True)\n\n# symlink each class folder with a new readable name\nfor split in splits:\n    split_src = os.path.join(src_root, split)\n    split_dst = os.path.join(dst_root, split)\n\n    for synset in os.listdir(split_src):\n        readable = synset_to_caption.get(synset, synset).replace(\" \", \"_\")\n        src_path = os.path.join(split_src, synset)\n        dst_path = os.path.join(split_dst, readable)\n\n        # create symlink\n        if not os.path.exists(dst_path):\n            os.symlink(src_path, dst_path)\n            print(f\"🔗 Linked {synset} → {readable}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T09:27:56.825274Z","iopub.execute_input":"2025-10-04T09:27:56.826049Z","iopub.status.idle":"2025-10-04T09:27:57.134786Z","shell.execute_reply.started":"2025-10-04T09:27:56.826021Z","shell.execute_reply":"2025-10-04T09:27:57.133940Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# libraries install","metadata":{}},{"cell_type":"code","source":"!pip install open_clip_torch transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T09:28:06.617639Z","iopub.execute_input":"2025-10-04T09:28:06.617955Z","iopub.status.idle":"2025-10-04T09:28:09.881400Z","shell.execute_reply.started":"2025-10-04T09:28:06.617932Z","shell.execute_reply":"2025-10-04T09:28:09.880432Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: open_clip_torch in /usr/local/lib/python3.11/dist-packages (3.2.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (0.21.0+cu124)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (2024.11.6)\nRequirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (6.3.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (4.67.1)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (0.33.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (0.5.3)\nRequirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.11/dist-packages (from open_clip_torch) (1.0.20)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->open_clip_torch) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->open_clip_torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->open_clip_torch) (1.3.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->open_clip_torch) (0.2.13)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->open_clip_torch) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->open_clip_torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T09:28:12.427259Z","iopub.execute_input":"2025-10-04T09:28:12.428081Z","iopub.status.idle":"2025-10-04T09:28:15.669059Z","shell.execute_reply.started":"2025-10-04T09:28:12.428043Z","shell.execute_reply":"2025-10-04T09:28:15.668258Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.52.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.8.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\nRequirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.33.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.5.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.6.15)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# prototype baseline","metadata":{}},{"cell_type":"code","source":"import open_clip\nimport torch\nimport torch.nn.functional as F\nfrom PIL import Image\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple\nimport numpy as np\nfrom tqdm import tqdm\nimport json\n\nclass CoCaFewShotBenchmark:\n    def __init__(\n        self,\n        model_name: str = \"coca_ViT-L-14\",\n        pretrained: str = \"mscoco_finetuned_laion2B-s13B-b90k\",\n        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    ):\n        \"\"\"\n        Initialize CoCa model for few-shot learning.\n        \n        Args:\n            model_name: CoCa model architecture\n            pretrained: Pretrained weights to use\n            device: Device to run inference on\n        \"\"\"\n        self.device = device\n        print(f\"Loading CoCa model on {device}...\")\n        \n        self.model, _, self.transform = open_clip.create_model_and_transforms(\n            model_name=model_name,\n            pretrained=pretrained\n        )\n        self.model = self.model.to(device)\n        self.model.eval()\n        \n        self.tokenizer = open_clip.get_tokenizer(model_name)\n        \n    def create_text_prompts(self, class_name: str) -> List[str]:\n        \"\"\"\n        Create multiple prompt templates for a class name.\n        \n        Args:\n            class_name: Name of the class\n            \n        Returns:\n            List of text prompts\n        \"\"\"\n        # Clean up class name (replace underscores with spaces)\n        clean_name = class_name.replace(\"_\", \" \")\n        \n        templates = [\n            f\"a photo of a {clean_name}\",\n            f\"an image of a {clean_name}\",\n            f\"a picture of a {clean_name}\",\n            f\"a {clean_name}\",\n            f\"the {clean_name}\",\n        ]\n        return templates\n    \n    def encode_text_prompts(self, prompts: List[str]) -> torch.Tensor:\n        \"\"\"\n        Encode text prompts and average them.\n        \n        Args:\n            prompts: List of text prompts\n            \n        Returns:\n            Averaged text embedding\n        \"\"\"\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            text_tokens = self.tokenizer(prompts).to(self.device)\n            text_features = self.model.encode_text(text_tokens)\n            text_features = F.normalize(text_features, dim=-1)\n            # Average across prompts\n            text_prototype = text_features.mean(dim=0)\n            text_prototype = F.normalize(text_prototype, dim=-1)\n        return text_prototype\n    \n    def encode_image(self, image_path: Path) -> torch.Tensor:\n        \"\"\"\n        Encode a single image.\n        \n        Args:\n            image_path: Path to image file\n            \n        Returns:\n            Image embedding\n        \"\"\"\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            image = Image.open(image_path).convert(\"RGB\")\n            image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n            image_features = self.model.encode_image(image_tensor)\n            image_features = F.normalize(image_features, dim=-1)\n        return image_features.squeeze(0)\n    \n    def build_class_prototypes(\n        self,\n        train_dir: Path,\n        n_shot: int = 5,\n        text_weight: float = 0.5\n    ) -> Tuple[Dict[str, torch.Tensor], List[str]]:\n        \"\"\"\n        Build class prototypes from few-shot training data.\n        \n        Args:\n            train_dir: Path to training directory\n            n_shot: Number of examples per class\n            text_weight: Weight for text embeddings (1 - text_weight for images)\n            \n        Returns:\n            Dictionary mapping class names to prototypes, and list of class names\n        \"\"\"\n        class_prototypes = {}\n        class_names = []\n        \n        # Get all class directories\n        class_dirs = sorted([d for d in train_dir.iterdir() if d.is_dir()])\n        \n        print(f\"\\nBuilding {n_shot}-shot prototypes for {len(class_dirs)} classes...\")\n        \n        for class_dir in tqdm(class_dirs, desc=\"Processing classes\"):\n            class_name = class_dir.name\n            class_names.append(class_name)\n            \n            # Get image paths (limit to n_shot)\n            image_paths = sorted(class_dir.glob(\"*.jpg\"))[:n_shot]\n            \n            if len(image_paths) < n_shot:\n                print(f\"Warning: {class_name} has only {len(image_paths)} images (expected {n_shot})\")\n            \n            # Encode images\n            image_embeddings = []\n            for img_path in image_paths:\n                img_emb = self.encode_image(img_path)\n                image_embeddings.append(img_emb)\n            \n            # Average image embeddings\n            image_prototype = torch.stack(image_embeddings).mean(dim=0)\n            image_prototype = F.normalize(image_prototype, dim=-1)\n            \n            # Create and encode text prompts\n            text_prompts = self.create_text_prompts(class_name)\n            text_prototype = self.encode_text_prompts(text_prompts)\n            \n            # Combine text and image prototypes\n            class_prototype = (\n                text_weight * text_prototype + \n                (1 - text_weight) * image_prototype\n            )\n            class_prototype = F.normalize(class_prototype, dim=-1)\n            \n            class_prototypes[class_name] = class_prototype\n        \n        return class_prototypes, class_names\n    \n    def evaluate(\n        self,\n        test_dir: Path,\n        class_prototypes: Dict[str, torch.Tensor],\n        class_names: List[str]\n    ) -> Dict[str, float]:\n        \"\"\"\n        Evaluate on test set.\n        \n        Args:\n            test_dir: Path to test directory\n            class_prototypes: Dictionary of class prototypes\n            class_names: List of class names (in order)\n            \n        Returns:\n            Dictionary of evaluation metrics\n        \"\"\"\n        correct = 0\n        total = 0\n        per_class_correct = {name: 0 for name in class_names}\n        per_class_total = {name: 0 for name in class_names}\n        \n        # Stack all prototypes for efficient similarity computation\n        prototype_matrix = torch.stack([class_prototypes[name] for name in class_names])\n        \n        print(f\"\\nEvaluating on test set...\")\n        \n        # Get all test class directories\n        test_class_dirs = sorted([d for d in test_dir.iterdir() if d.is_dir()])\n        \n        for test_class_dir in tqdm(test_class_dirs, desc=\"Testing classes\"):\n            true_class = test_class_dir.name\n            \n            # Get all test images\n            test_images = sorted(test_class_dir.glob(\"*.jpg\"))\n            \n            for img_path in test_images:\n                # Encode test image\n                img_embedding = self.encode_image(img_path)\n                \n                # Compute similarities with all class prototypes\n                similarities = img_embedding @ prototype_matrix.T\n                \n                # Get predicted class\n                pred_idx = similarities.argmax().item()\n                pred_class = class_names[pred_idx]\n                \n                # Update metrics\n                total += 1\n                per_class_total[true_class] += 1\n                \n                if pred_class == true_class:\n                    correct += 1\n                    per_class_correct[true_class] += 1\n        \n        # Calculate metrics\n        overall_accuracy = (correct / total) * 100 if total > 0 else 0\n        \n        per_class_accuracy = {\n            name: (per_class_correct[name] / per_class_total[name] * 100) \n            if per_class_total[name] > 0 else 0\n            for name in class_names\n        }\n        \n        mean_per_class_accuracy = np.mean(list(per_class_accuracy.values()))\n        \n        results = {\n            \"overall_accuracy\": overall_accuracy,\n            \"mean_per_class_accuracy\": mean_per_class_accuracy,\n            \"total_samples\": total,\n            \"correct_predictions\": correct,\n            \"per_class_accuracy\": per_class_accuracy\n        }\n        \n        return results\n    \n    def run_benchmark(\n        self,\n        dataset_dir: Path,\n        n_shot: int = 5,\n        text_weight: float = 0.5,\n        save_results: bool = True\n    ) -> Dict:\n        \"\"\"\n        Run complete few-shot benchmark.\n        \n        Args:\n            dataset_dir: Path to dataset root\n            n_shot: Number of examples per class\n            text_weight: Weight for text vs image prototypes\n            save_results: Whether to save results to JSON\n            \n        Returns:\n            Evaluation results dictionary\n        \"\"\"\n        train_dir = dataset_dir / \"train\"\n        test_dir = dataset_dir / \"test\"\n        \n        # Build prototypes\n        class_prototypes, class_names = self.build_class_prototypes(\n            train_dir, n_shot, text_weight\n        )\n        \n        # Evaluate\n        results = self.evaluate(test_dir, class_prototypes, class_names)\n        \n        # Add experiment metadata\n        results[\"experiment_config\"] = {\n            \"n_shot\": n_shot,\n            \"text_weight\": text_weight,\n            \"num_classes\": len(class_names),\n            \"model\": \"coca_ViT-L-14\"\n        }\n        \n        # Print results\n        print(\"\\n\" + \"=\"*60)\n        print(f\"Few-Shot Learning Results ({n_shot}-shot)\")\n        print(\"=\"*60)\n        print(f\"Overall Accuracy: {results['overall_accuracy']:.2f}%\")\n        print(f\"Mean Per-Class Accuracy: {results['mean_per_class_accuracy']:.2f}%\")\n        print(f\"Total Samples: {results['total_samples']}\")\n        print(f\"Correct Predictions: {results['correct_predictions']}\")\n        print(\"=\"*60)\n        \n        # Save results\n        if save_results:\n            output_file = f\"coca_fewshot_results_{n_shot}shot.json\"\n            with open(output_file, 'w') as f:\n                # Convert per-class accuracies to serializable format\n                serializable_results = results.copy()\n                serializable_results[\"per_class_accuracy\"] = {\n                    k: float(v) for k, v in results[\"per_class_accuracy\"].items()\n                }\n                json.dump(serializable_results, f, indent=2)\n            print(f\"Results saved to {output_file}\")\n        \n        return results\n\n\ndef main():\n    \"\"\"Example usage of the benchmark.\"\"\"\n    \n    # Configuration\n    DATASET_DIR = Path(\"/kaggle/working/mini_imagenet_fewshot_renamed\")\n    N_SHOTS = [10,20]  # Different shot configurations to test\n    TEXT_WEIGHT = 0.7  # Balance between text and image prototypes\n    \n    # Initialize benchmark\n    benchmark = CoCaFewShotBenchmark()\n    \n    # Run experiments for different shot settings\n    all_results = {}\n    \n    for n_shot in N_SHOTS:\n        print(f\"\\n{'='*60}\")\n        print(f\"Running {n_shot}-shot experiment\")\n        print(f\"{'='*60}\")\n        \n        results = benchmark.run_benchmark(\n            dataset_dir=DATASET_DIR,\n            n_shot=n_shot,\n            text_weight=TEXT_WEIGHT,\n            save_results=True\n        )\n        \n        all_results[f\"{n_shot}_shot\"] = results\n    \n    # Print summary comparison\n    print(\"\\n\" + \"=\"*60)\n    print(\"SUMMARY COMPARISON\")\n    print(\"=\"*60)\n    print(f\"{'N-Shot':<10} {'Overall Acc':<15} {'Mean Per-Class Acc':<20}\")\n    print(\"-\"*60)\n    for n_shot in N_SHOTS:\n        res = all_results[f\"{n_shot}_shot\"]\n        print(f\"{n_shot:<10} {res['overall_accuracy']:<15.2f} {res['mean_per_class_accuracy']:<20.2f}\")\n    print(\"=\"*60)\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-03T18:02:19.304Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# finetuning 1 classification head","metadata":{}},{"cell_type":"code","source":"import open_clip\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple, Optional\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nfrom torchvision import transforms\nimport random\n\n\nclass FewShotDataset(Dataset):\n    \"\"\"Dataset for few-shot learning.\"\"\"\n    \n    def __init__(self, root_dir: Path, transform=None, augment: bool = False):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.augment = augment\n        \n        # Collect all images and labels\n        self.samples = []\n        self.class_to_idx = {}\n        \n        class_dirs = sorted([d for d in root_dir.iterdir() if d.is_dir()])\n        for idx, class_dir in enumerate(class_dirs):\n            self.class_to_idx[class_dir.name] = idx\n            for img_path in sorted(class_dir.glob(\"*.jpg\")):\n                self.samples.append((img_path, idx))\n        \n        # Augmentation pipeline\n        if self.augment:\n            self.augment_transform = transforms.Compose([\n                transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n                transforms.RandomHorizontalFlip(),\n                transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n            ])\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        if self.augment:\n            image = self.augment_transform(image)\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n\nclass LinearClassifier(nn.Module):\n    \"\"\"Linear classification head.\"\"\"\n    \n    def __init__(self, input_dim: int, num_classes: int):\n        super().__init__()\n        self.classifier = nn.Linear(input_dim, num_classes)\n        \n        # Initialize with small values\n        nn.init.normal_(self.classifier.weight, std=0.01)\n        nn.init.zeros_(self.classifier.bias)\n    \n    def forward(self, x):\n        return self.classifier(x)\n\n\nclass CoCaFewShotFinetune:\n    def __init__(\n        self,\n        model_name: str = \"coca_ViT-L-14\",\n        pretrained: str = \"mscoco_finetuned_laion2B-s13B-b90k\",\n        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    ):\n        \"\"\"\n        Initialize CoCa model for few-shot fine-tuning.\n        \n        Args:\n            model_name: CoCa model architecture\n            pretrained: Pretrained weights to use\n            device: Device to run on\n        \"\"\"\n        self.device = device\n        print(f\"Loading CoCa model on {device}...\")\n        \n        self.model, _, self.transform = open_clip.create_model_and_transforms(\n            model_name=model_name,\n            pretrained=pretrained\n        )\n        self.model = self.model.to(device)\n        self.model.eval()  # Keep in eval mode (frozen)\n        \n        # Freeze all parameters\n        for param in self.model.parameters():\n            param.requires_grad = False\n        \n        self.classifier = None\n        self.class_names = None\n    \n    @torch.no_grad()\n    def extract_features(self, dataloader: DataLoader) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Extract image features from frozen encoder.\n        \n        Args:\n            dataloader: DataLoader for images\n            \n        Returns:\n            Features and labels tensors\n        \"\"\"\n        all_features = []\n        all_labels = []\n        \n        for images, labels in tqdm(dataloader, desc=\"Extracting features\"):\n            images = images.to(self.device)\n            \n            with torch.cuda.amp.autocast():\n                features = self.model.encode_image(images)\n                features = F.normalize(features, dim=-1)\n            \n            all_features.append(features.cpu())\n            all_labels.append(labels)\n        \n        return torch.cat(all_features), torch.cat(all_labels)\n    \n    def train_classifier(\n        self,\n        train_dir: Path,\n        n_shot: int = 5,\n        learning_rate: float = 1e-4,\n        weight_decay: float = 0.01,\n        num_epochs: int = 100,\n        batch_size: int = 32,\n        label_smoothing: float = 0.1,\n        use_augmentation: bool = True,\n        warmup_epochs: int = 10,\n        patience: int = 20\n    ) -> Dict:\n        \"\"\"\n        Train linear classifier on few-shot data.\n        \n        Args:\n            train_dir: Path to training directory\n            n_shot: Number of examples per class\n            learning_rate: Learning rate for optimizer\n            weight_decay: Weight decay coefficient\n            num_epochs: Maximum number of epochs\n            batch_size: Batch size for training\n            label_smoothing: Label smoothing factor\n            use_augmentation: Whether to use data augmentation\n            warmup_epochs: Number of warmup epochs\n            patience: Early stopping patience\n            \n        Returns:\n            Training history dictionary\n        \"\"\"\n        # Create limited dataset (n-shot per class)\n        train_samples = []\n        class_dirs = sorted([d for d in train_dir.iterdir() if d.is_dir()])\n        self.class_names = [d.name for d in class_dirs]\n        num_classes = len(self.class_names)\n        \n        print(f\"\\nPreparing {n_shot}-shot training data...\")\n        class_to_idx = {name: idx for idx, name in enumerate(self.class_names)}\n        \n        for class_dir in class_dirs:\n            class_name = class_dir.name\n            class_idx = class_to_idx[class_name]\n            image_paths = sorted(class_dir.glob(\"*.jpg\"))[:n_shot]\n            \n            for img_path in image_paths:\n                train_samples.append((img_path, class_idx))\n        \n        print(f\"Total training samples: {len(train_samples)}\")\n        \n        # Create dataset with or without augmentation\n        class TempDataset(Dataset):\n            def __init__(self, samples, transform, augment):\n                self.samples = samples\n                self.transform = transform\n                self.augment = augment\n                \n                if self.augment:\n                    self.augment_transform = transforms.Compose([\n                        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n                        transforms.RandomHorizontalFlip(),                   \n                        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), \n                    ])\n            \n            def __len__(self):\n                return len(self.samples)\n            \n            def __getitem__(self, idx):\n                img_path, label = self.samples[idx]\n                image = Image.open(img_path).convert(\"RGB\")\n                \n                if self.augment:\n                    image = self.augment_transform(image)\n                \n                if self.transform:\n                    image = self.transform(image)\n                \n                return image, label\n        \n        train_dataset = TempDataset(train_samples, self.transform, use_augmentation)\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=batch_size,\n            shuffle=True,\n            num_workers=4,\n            pin_memory=True\n        )\n        \n        # Extract features once (for validation without augmentation)\n        print(\"\\nExtracting validation features (no augmentation)...\")\n        val_dataset = TempDataset(train_samples, self.transform, augment=False)\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n        val_features, val_labels = self.extract_features(val_loader)\n        val_features = val_features.to(self.device)\n        val_labels = val_labels.to(self.device)\n        \n        # Get feature dimension\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            sample_img = train_dataset[0][0].unsqueeze(0).to(self.device)\n            sample_feat = self.model.encode_image(sample_img)\n            feature_dim = sample_feat.shape[1]\n        \n        # Initialize classifier\n        self.classifier = LinearClassifier(feature_dim, num_classes).to(self.device)\n        \n        # Setup optimizer and loss\n        optimizer = torch.optim.AdamW(\n            self.classifier.parameters(),\n            lr=learning_rate,\n            weight_decay=weight_decay\n        )\n        \n        criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n        \n        # Learning rate scheduler with warmup\n        def lr_lambda(epoch):\n            if epoch < warmup_epochs:\n                return (epoch + 1) / warmup_epochs\n            else:\n                # Cosine decay after warmup\n                progress = (epoch - warmup_epochs) / (num_epochs - warmup_epochs)\n                return 0.5 * (1.0 + np.cos(np.pi * progress))\n        \n        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n        \n        # Training loop\n        history = {\n            \"train_loss\": [],\n            \"val_accuracy\": [],\n            \"learning_rates\": []\n        }\n        \n        best_val_acc = 0.0\n        patience_counter = 0\n        \n        print(f\"\\nTraining linear classifier...\")\n        print(f\"Config: LR={learning_rate}, WD={weight_decay}, Label Smoothing={label_smoothing}\")\n        print(f\"Augmentation: {use_augmentation}, Warmup Epochs: {warmup_epochs}\\n\")\n        \n        for epoch in range(num_epochs):\n            # Training phase\n            self.classifier.train()\n            train_loss = 0.0\n            \n            for images, labels in train_loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n                \n                # Extract features (frozen encoder)\n                with torch.no_grad(), torch.cuda.amp.autocast():\n                    features = self.model.encode_image(images)\n                    features = F.normalize(features, dim=-1)\n                \n                # Forward pass through classifier\n                logits = self.classifier(features)\n                loss = criterion(logits, labels)\n                \n                # Backward pass\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                \n                train_loss += loss.item()\n            \n            train_loss /= len(train_loader)\n            \n            # Validation phase (on training data without augmentation)\n            self.classifier.eval()\n            with torch.no_grad():\n                val_logits = self.classifier(val_features)\n                val_preds = val_logits.argmax(dim=1)\n                val_accuracy = (val_preds == val_labels).float().mean().item() * 100\n            \n            # Update scheduler\n            current_lr = optimizer.param_groups[0]['lr']\n            scheduler.step()\n            \n            # Record history\n            history[\"train_loss\"].append(train_loss)\n            history[\"val_accuracy\"].append(val_accuracy)\n            history[\"learning_rates\"].append(current_lr)\n            \n            # Early stopping check\n            if val_accuracy > best_val_acc:\n                best_val_acc = val_accuracy\n                patience_counter = 0\n                # Save best model\n                best_state = self.classifier.state_dict()\n            else:\n                patience_counter += 1\n            \n            # Print progress\n            if (epoch + 1) % 10 == 0 or epoch == 0:\n                print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n                      f\"Loss: {train_loss:.4f} | \"\n                      f\"Val Acc: {val_accuracy:.2f}% | \"\n                      f\"LR: {current_lr:.6f}\")\n            \n            # Early stopping\n            if patience_counter >= patience:\n                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n                break\n        \n        # Load best model\n        self.classifier.load_state_dict(best_state)\n        print(f\"\\nTraining complete! Best validation accuracy: {best_val_acc:.2f}%\")\n        \n        return history\n    \n    def evaluate(self, test_dir: Path, batch_size: int = 32) -> Dict[str, float]:\n        \"\"\"\n        Evaluate classifier on test set.\n        \n        Args:\n            test_dir: Path to test directory\n            batch_size: Batch size for evaluation\n            \n        Returns:\n            Dictionary of evaluation metrics\n        \"\"\"\n        if self.classifier is None:\n            raise ValueError(\"Classifier not trained. Call train_classifier first.\")\n        \n        self.classifier.eval()\n        \n        # Create test dataset\n        test_dataset = FewShotDataset(test_dir, transform=self.transform, augment=False)\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=4\n        )\n        \n        # Extract features\n        print(\"\\nExtracting test features...\")\n        test_features, test_labels = self.extract_features(test_loader)\n        test_features = test_features.to(self.device)\n        test_labels = test_labels.to(self.device)\n        \n        # Predict\n        with torch.no_grad():\n            logits = self.classifier(test_features)\n            preds = logits.argmax(dim=1)\n        \n        # Calculate metrics\n        correct = (preds == test_labels).sum().item()\n        total = len(test_labels)\n        overall_accuracy = (correct / total) * 100\n        \n        # Per-class accuracy\n        per_class_correct = {}\n        per_class_total = {}\n        \n        for pred, label in zip(preds.cpu().numpy(), test_labels.cpu().numpy()):\n            class_name = self.class_names[label]\n            per_class_total[class_name] = per_class_total.get(class_name, 0) + 1\n            if pred == label:\n                per_class_correct[class_name] = per_class_correct.get(class_name, 0) + 1\n        \n        per_class_accuracy = {\n            name: (per_class_correct.get(name, 0) / per_class_total[name] * 100)\n            for name in per_class_total\n        }\n        \n        mean_per_class_accuracy = np.mean(list(per_class_accuracy.values()))\n        \n        results = {\n            \"overall_accuracy\": overall_accuracy,\n            \"mean_per_class_accuracy\": mean_per_class_accuracy,\n            \"total_samples\": total,\n            \"correct_predictions\": correct,\n            \"per_class_accuracy\": per_class_accuracy\n        }\n        \n        return results\n    \n    def run_finetuning_experiment(\n        self,\n        dataset_dir: Path,\n        n_shot: int = 5,\n        learning_rate: float = 1e-4,\n        weight_decay: float = 0.01,\n        num_epochs: int = 100,\n        label_smoothing: float = 0.1,\n        use_augmentation: bool = True,\n        save_results: bool = True\n    ) -> Dict:\n        \"\"\"\n        Run complete fine-tuning experiment.\n        \n        Args:\n            dataset_dir: Path to dataset root\n            n_shot: Number of examples per class\n            learning_rate: Learning rate\n            weight_decay: Weight decay\n            num_epochs: Number of training epochs\n            label_smoothing: Label smoothing factor\n            use_augmentation: Whether to use data augmentation\n            save_results: Whether to save results\n            \n        Returns:\n            Complete results dictionary\n        \"\"\"\n        train_dir = dataset_dir / \"train\"\n        test_dir = dataset_dir / \"test\"\n        \n        # Train classifier\n        history = self.train_classifier(\n            train_dir=train_dir,\n            n_shot=n_shot,\n            learning_rate=learning_rate,\n            weight_decay=weight_decay,\n            num_epochs=num_epochs,\n            label_smoothing=label_smoothing,\n            use_augmentation=use_augmentation\n        )\n        \n        # Evaluate on test set\n        results = self.evaluate(test_dir)\n        \n        # Combine results\n        results[\"training_history\"] = history\n        results[\"experiment_config\"] = {\n            \"n_shot\": n_shot,\n            \"learning_rate\": learning_rate,\n            \"weight_decay\": weight_decay,\n            \"num_epochs\": num_epochs,\n            \"label_smoothing\": label_smoothing,\n            \"use_augmentation\": use_augmentation,\n            \"model\": \"coca_ViT-L-14\"\n        }\n        \n        # Print results\n        print(\"\\n\" + \"=\"*60)\n        print(f\"Fine-tuning Results ({n_shot}-shot)\")\n        print(\"=\"*60)\n        print(f\"Overall Accuracy: {results['overall_accuracy']:.2f}%\")\n        print(f\"Mean Per-Class Accuracy: {results['mean_per_class_accuracy']:.2f}%\")\n        print(f\"Total Samples: {results['total_samples']}\")\n        print(f\"Correct Predictions: {results['correct_predictions']}\")\n        print(\"=\"*60)\n        \n        # Save results\n        if save_results:\n            output_file = f\"coca_finetune_{n_shot}shot_aug{use_augmentation}.json\"\n            with open(output_file, 'w') as f:\n                serializable_results = results.copy()\n                serializable_results[\"per_class_accuracy\"] = {\n                    k: float(v) for k, v in results[\"per_class_accuracy\"].items()\n                }\n                json.dump(serializable_results, f, indent=2)\n            print(f\"Results saved to {output_file}\")\n        \n        return results\n\n\ndef main():\n    \"\"\"Example usage demonstrating different configurations.\"\"\"\n    \n    DATASET_DIR = Path(\"/kaggle/working/mini_imagenet_fewshot_renamed\")\n    \n    # Configuration options\n    configs = [\n        # Compare different shot numbers with augmentation\n        {\"n_shot\": 1, \"use_augmentation\": False, \"num_epochs\": 200},\n        {\"n_shot\": 3, \"use_augmentation\": False, \"num_epochs\": 150},\n        {\"n_shot\": 1, \"use_augmentation\": True, \"num_epochs\": 200},\n        {\"n_shot\": 3, \"use_augmentation\": True, \"num_epochs\": 150},\n        #{\"n_shot\": 5, \"use_augmentation\": True, \"num_epochs\": 200},\n        \n        # Compare with/without augmentation for 5-shot\n        #{\"n_shot\": 10, \"use_augmentation\": True, \"num_epochs\": 200},\n        # {\"n_shot\": 10, \"use_augmentation\": False, \"num_epochs\": 200},\n        #{\"n_shot\": 20, \"use_augmentation\": True, \"num_epochs\": 200},\n        # {\"n_shot\": 20, \"use_augmentation\": False, \"num_epochs\": 200}\n    ]\n    \n    all_results = {}\n    \n    for config in configs:\n        n_shot = config[\"n_shot\"]\n        use_aug = config[\"use_augmentation\"]\n        \n        print(f\"\\n{'='*70}\")\n        print(f\"Running {n_shot}-shot experiment (Augmentation: {use_aug})\")\n        print(f\"{'='*70}\")\n        \n        # Initialize new instance for each experiment\n        finetuner = CoCaFewShotFinetune()\n        \n        results = finetuner.run_finetuning_experiment(\n            dataset_dir=DATASET_DIR,\n            n_shot=n_shot,\n            learning_rate=5e-5,\n            weight_decay=0.05,\n            num_epochs=config[\"num_epochs\"],\n            label_smoothing=0.1,\n            use_augmentation=use_aug,\n            save_results=True\n        )\n        \n        key = f\"{n_shot}_shot_aug{use_aug}\"\n        all_results[key] = results\n    \n    # Print summary\n    print(\"\\n\" + \"=\"*80)\n    print(\"SUMMARY COMPARISON\")\n    print(\"=\"*80)\n    print(f\"{'Config':<25} {'Overall Acc':<15} {'Mean Per-Class Acc':<20}\")\n    print(\"-\"*80)\n    for key, res in all_results.items():\n        print(f\"{key:<25} {res['overall_accuracy']:<15.2f} {res['mean_per_class_accuracy']:<20.2f}\")\n    print(\"=\"*80)\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T09:29:09.867830Z","iopub.execute_input":"2025-10-04T09:29:09.868493Z","iopub.status.idle":"2025-10-04T10:12:04.953358Z","shell.execute_reply.started":"2025-10-04T09:29:09.868463Z","shell.execute_reply":"2025-10-04T10:12:04.952443Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nRunning 1-shot experiment (Augmentation: False)\n======================================================================\nLoading CoCa model on cuda...\n\nPreparing 1-shot training data...\nTotal training samples: 100\n\nExtracting validation features (no augmentation)...\n","output_type":"stream"},{"name":"stderr","text":"Extracting features:   0%|          | 0/4 [00:00<?, ?it/s]/tmp/ipykernel_197/2214031627.py:122: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nExtracting features: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]","output_type":"stream"},{"name":"stdout","text":"\nTraining linear classifier...\nConfig: LR=5e-05, WD=0.05, Label Smoothing=0.1\nAugmentation: False, Warmup Epochs: 10\n\n","output_type":"stream"},{"name":"stderr","text":"\n/tmp/ipykernel_197/2214031627.py:228: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.no_grad(), torch.cuda.amp.autocast():\n/tmp/ipykernel_197/2214031627.py:279: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.no_grad(), torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/200] Loss: 4.6048 | Val Acc: 2.00% | LR: 0.000005\nEpoch [10/200] Loss: 4.5987 | Val Acc: 13.00% | LR: 0.000050\nEpoch [20/200] Loss: 4.5835 | Val Acc: 42.00% | LR: 0.000050\nEpoch [30/200] Loss: 4.5714 | Val Acc: 71.00% | LR: 0.000049\nEpoch [40/200] Loss: 4.5646 | Val Acc: 81.00% | LR: 0.000047\nEpoch [50/200] Loss: 4.5526 | Val Acc: 89.00% | LR: 0.000045\nEpoch [60/200] Loss: 4.5413 | Val Acc: 93.00% | LR: 0.000042\nEpoch [70/200] Loss: 4.5303 | Val Acc: 94.00% | LR: 0.000039\nEpoch [80/200] Loss: 4.5235 | Val Acc: 95.00% | LR: 0.000035\nEpoch [90/200] Loss: 4.5167 | Val Acc: 95.00% | LR: 0.000032\n\nEarly stopping at epoch 93\n\nTraining complete! Best validation accuracy: 95.00%\n\nExtracting test features...\n","output_type":"stream"},{"name":"stderr","text":"Extracting features: 100%|██████████| 63/63 [01:00<00:00,  1.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nFine-tuning Results (1-shot)\n============================================================\nOverall Accuracy: 55.05%\nMean Per-Class Accuracy: 55.05%\nTotal Samples: 2000\nCorrect Predictions: 1101\n============================================================\nResults saved to coca_finetune_1shot_augFalse.json\n\n======================================================================\nRunning 3-shot experiment (Augmentation: False)\n======================================================================\nLoading CoCa model on cuda...\n\nPreparing 3-shot training data...\nTotal training samples: 300\n\nExtracting validation features (no augmentation)...\n","output_type":"stream"},{"name":"stderr","text":"Extracting features: 100%|██████████| 10/10 [00:11<00:00,  1.17s/it]","output_type":"stream"},{"name":"stdout","text":"\nTraining linear classifier...\nConfig: LR=5e-05, WD=0.05, Label Smoothing=0.1\nAugmentation: False, Warmup Epochs: 10\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/150] Loss: 4.6049 | Val Acc: 0.67% | LR: 0.000005\nEpoch [10/150] Loss: 4.5928 | Val Acc: 17.67% | LR: 0.000050\nEpoch [20/150] Loss: 4.5675 | Val Acc: 78.00% | LR: 0.000049\nEpoch [30/150] Loss: 4.5430 | Val Acc: 93.33% | LR: 0.000048\nEpoch [40/150] Loss: 4.5197 | Val Acc: 97.67% | LR: 0.000045\nEpoch [50/150] Loss: 4.4974 | Val Acc: 98.67% | LR: 0.000041\nEpoch [60/150] Loss: 4.4786 | Val Acc: 99.67% | LR: 0.000036\nEpoch [70/150] Loss: 4.4609 | Val Acc: 99.67% | LR: 0.000031\n\nEarly stopping at epoch 79\n\nTraining complete! Best validation accuracy: 99.67%\n\nExtracting test features...\n","output_type":"stream"},{"name":"stderr","text":"Extracting features: 100%|██████████| 63/63 [01:00<00:00,  1.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nFine-tuning Results (3-shot)\n============================================================\nOverall Accuracy: 82.55%\nMean Per-Class Accuracy: 82.55%\nTotal Samples: 2000\nCorrect Predictions: 1651\n============================================================\nResults saved to coca_finetune_3shot_augFalse.json\n\n======================================================================\nRunning 1-shot experiment (Augmentation: True)\n======================================================================\nLoading CoCa model on cuda...\n\nPreparing 1-shot training data...\nTotal training samples: 100\n\nExtracting validation features (no augmentation)...\n","output_type":"stream"},{"name":"stderr","text":"Extracting features: 100%|██████████| 4/4 [00:03<00:00,  1.07it/s]","output_type":"stream"},{"name":"stdout","text":"\nTraining linear classifier...\nConfig: LR=5e-05, WD=0.05, Label Smoothing=0.1\nAugmentation: True, Warmup Epochs: 10\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/200] Loss: 4.6051 | Val Acc: 0.00% | LR: 0.000005\nEpoch [10/200] Loss: 4.5994 | Val Acc: 3.00% | LR: 0.000050\nEpoch [20/200] Loss: 4.5873 | Val Acc: 38.00% | LR: 0.000050\nEpoch [30/200] Loss: 4.5747 | Val Acc: 67.00% | LR: 0.000049\nEpoch [40/200] Loss: 4.5659 | Val Acc: 82.00% | LR: 0.000047\nEpoch [50/200] Loss: 4.5550 | Val Acc: 90.00% | LR: 0.000045\nEpoch [60/200] Loss: 4.5480 | Val Acc: 96.00% | LR: 0.000042\nEpoch [70/200] Loss: 4.5355 | Val Acc: 99.00% | LR: 0.000039\nEpoch [80/200] Loss: 4.5319 | Val Acc: 99.00% | LR: 0.000035\nEpoch [90/200] Loss: 4.5207 | Val Acc: 99.00% | LR: 0.000032\n\nEarly stopping at epoch 94\n\nTraining complete! Best validation accuracy: 100.00%\n\nExtracting test features...\n","output_type":"stream"},{"name":"stderr","text":"Extracting features: 100%|██████████| 63/63 [01:00<00:00,  1.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nFine-tuning Results (1-shot)\n============================================================\nOverall Accuracy: 58.30%\nMean Per-Class Accuracy: 58.30%\nTotal Samples: 2000\nCorrect Predictions: 1166\n============================================================\nResults saved to coca_finetune_1shot_augTrue.json\n\n======================================================================\nRunning 3-shot experiment (Augmentation: True)\n======================================================================\nLoading CoCa model on cuda...\n\nPreparing 3-shot training data...\nTotal training samples: 300\n\nExtracting validation features (no augmentation)...\n","output_type":"stream"},{"name":"stderr","text":"Extracting features: 100%|██████████| 10/10 [00:11<00:00,  1.11s/it]","output_type":"stream"},{"name":"stdout","text":"\nTraining linear classifier...\nConfig: LR=5e-05, WD=0.05, Label Smoothing=0.1\nAugmentation: True, Warmup Epochs: 10\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/150] Loss: 4.6050 | Val Acc: 0.67% | LR: 0.000005\nEpoch [10/150] Loss: 4.5932 | Val Acc: 17.33% | LR: 0.000050\nEpoch [20/150] Loss: 4.5683 | Val Acc: 79.67% | LR: 0.000049\nEpoch [30/150] Loss: 4.5443 | Val Acc: 90.67% | LR: 0.000048\nEpoch [40/150] Loss: 4.5210 | Val Acc: 95.67% | LR: 0.000045\nEpoch [50/150] Loss: 4.5002 | Val Acc: 97.00% | LR: 0.000041\nEpoch [60/150] Loss: 4.4804 | Val Acc: 98.67% | LR: 0.000036\nEpoch [70/150] Loss: 4.4646 | Val Acc: 99.00% | LR: 0.000031\nEpoch [80/150] Loss: 4.4498 | Val Acc: 99.00% | LR: 0.000026\n\nEarly stopping at epoch 82\n\nTraining complete! Best validation accuracy: 99.00%\n\nExtracting test features...\n","output_type":"stream"},{"name":"stderr","text":"Extracting features: 100%|██████████| 63/63 [01:00<00:00,  1.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nFine-tuning Results (3-shot)\n============================================================\nOverall Accuracy: 82.10%\nMean Per-Class Accuracy: 82.10%\nTotal Samples: 2000\nCorrect Predictions: 1642\n============================================================\nResults saved to coca_finetune_3shot_augTrue.json\n\n================================================================================\nSUMMARY COMPARISON\n================================================================================\nConfig                    Overall Acc     Mean Per-Class Acc  \n--------------------------------------------------------------------------------\n1_shot_augFalse           55.05           55.05               \n3_shot_augFalse           82.55           82.55               \n1_shot_augTrue            58.30           58.30               \n3_shot_augTrue            82.10           82.10               \n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# ----------------------------------------LoRa-----------------------------------------------------","metadata":{}},{"cell_type":"code","source":"import open_clip\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple, Optional, Union\nimport numpy as np\nfrom tqdm import tqdm\nimport json\nfrom torchvision import transforms\nimport random\nfrom peft import LoraConfig, get_peft_model\n\n\nclass LinearClassifier(nn.Module):\n    \"\"\"Simple linear classifier head.\"\"\"\n    def __init__(self, input_dim: int, num_classes: int):\n        super().__init__()\n        self.fc = nn.Linear(input_dim, num_classes)\n        \n    def forward(self, x):\n        return self.fc(x)\n\n\nclass FewShotDataset(Dataset):\n    \"\"\"Dataset for few-shot learning.\"\"\"\n    def __init__(self, data_dir: Path, transform=None, augment=False):\n        self.samples = []\n        self.transform = transform\n        self.class_to_idx = {}\n        \n        class_dirs = sorted([d for d in data_dir.iterdir() if d.is_dir()])\n        \n        for idx, class_dir in enumerate(class_dirs):\n            self.class_to_idx[class_dir.name] = idx\n            image_paths = list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\")) + list(class_dir.glob(\"*.JPEG\"))\n            for img_path in image_paths:\n                self.samples.append((img_path, idx))\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, label\n\n\nclass LoRACoCaFinetune:\n    def __init__(\n        self,\n        model_name: str = \"coca_ViT-L-14\",\n        pretrained: str = \"mscoco_finetuned_laion2B-s13B-b90k\",\n        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    ):\n        \"\"\"\n        Initialize CoCa model for LoRA fine-tuning with configurable loss functions.\n        \"\"\"\n        self.device = device\n        print(f\"Loading CoCa model on {device}...\")\n        \n        self.model, _, self.transform = open_clip.create_model_and_transforms(\n            model_name=model_name,\n            pretrained=pretrained\n        )\n        self.model = self.model.to(device)\n        \n        # Store original model for reference\n        self.original_model = self.model\n        self.lora_model = None\n        self.classifier = None\n        self.class_names = None\n\n    def run_lora_experiment(\n        self,\n        dataset_dir: Path,\n        n_shot: int = 5,\n        learning_rate: float = 1e-4,\n        weight_decay: float = 0.01,\n        num_epochs: int = 100,\n        label_smoothing: float = 0.1,\n        loss_type: str = \"cross_entropy\",\n        use_augmentation: bool = True,\n        temperature: float = 0.1,\n        save_results: bool = True\n    ) -> Dict:\n        \"\"\"Run complete LoRA fine-tuning experiment.\"\"\"\n        train_dir = dataset_dir / \"train\"\n        test_dir = dataset_dir / \"test\"\n        \n        # Auto-select loss type\n        if loss_type == \"auto\":\n            if n_shot <= 2:\n                loss_type = \"prototypical\"\n            elif n_shot <= 5:\n                loss_type = \"contrastive\"\n            else:\n                loss_type = \"cross_entropy\"\n        \n        print(f\"Using {loss_type} loss for {n_shot}-shot learning\")\n        \n        # Train\n        history = self.train_with_lora(\n            train_dir=train_dir,\n            n_shot=n_shot,\n            learning_rate=learning_rate,\n            weight_decay=weight_decay,\n            num_epochs=num_epochs,\n            label_smoothing=label_smoothing,\n            loss_type=loss_type,\n            use_augmentation=use_augmentation,\n            temperature=temperature\n        )\n        \n        # Evaluate\n        results = self.evaluate(test_dir)\n        \n        # Combine results\n        results[\"training_history\"] = history\n        results[\"experiment_config\"] = {\n            \"n_shot\": n_shot,\n            \"learning_rate\": learning_rate,\n            \"weight_decay\": weight_decay,\n            \"num_epochs\": num_epochs,\n            \"label_smoothing\": label_smoothing,\n            \"loss_type\": loss_type,\n            \"use_augmentation\": use_augmentation,\n            \"temperature\": temperature,\n            \"model\": \"coca_ViT-L-14 + LoRA\",\n        }\n        \n        # Print results\n        print(\"\\n\" + \"=\"*60)\n        print(f\"LoRA Fine-tuning Results ({n_shot}-shot, {loss_type} loss)\")\n        print(\"=\"*60)\n        print(f\"Overall Accuracy: {results['overall_accuracy']:.2f}%\")\n        print(f\"Mean Per-Class Accuracy: {results['mean_per_class_accuracy']:.2f}%\")\n        print(f\"Total Samples: {results['total_samples']}\")\n        print(f\"Correct Predictions: {results['correct_predictions']}\")\n        print(\"=\"*60)\n        \n        # Save results\n        if save_results:\n            output_file = f\"lora_coca_{n_shot}shot_{loss_type}_aug{use_augmentation}.json\"\n            with open(output_file, 'w') as f:\n                serializable_results = results.copy()\n                serializable_results[\"per_class_accuracy\"] = {\n                    k: float(v) for k, v in results[\"per_class_accuracy\"].items()\n                }\n                json.dump(serializable_results, f, indent=2)\n            print(f\"Results saved to {output_file}\")\n        \n        return results\n        \n    \n        \n    def setup_lora_for_fewshot(self, n_shot: int) -> LoraConfig:\n        \"\"\"\n        Configure LoRA based on number of shots.\n        Target the out_proj and MLP layers since MultiheadAttention doesn't expose q/k/v directly.\n        \n        Args:\n            n_shot: Number of examples per class\n            \n        Returns:\n            LoRA configuration\n        \"\"\"\n        if n_shot <= 2:\n            # 1-2 shot: out_proj only, small rank\n            config = LoraConfig(\n                r=4,\n                lora_alpha=16,\n                target_modules=[\"attn.out_proj\"],  # Attention output projection\n                # REMOVED: layers_to_transform=list(range(20, 24)),\n                lora_dropout=0.15,\n                bias=\"none\",\n            )\n            print(f\"LoRA Config: attn.out_proj all layers, r={config.r}\")\n            \n        elif n_shot <= 10:\n            # 5-10 shot: out_proj in all layers, medium rank\n            config = LoraConfig(\n                r=4,\n                lora_alpha=16,\n                target_modules=[\"attn.out_proj\"],  # Attention output in all layers\n                lora_dropout=0.15,\n                bias=\"none\",\n            )\n            print(f\"LoRA Config: attn.out_proj all layers, r={config.r}, droupout={config.lora_dropout}\")\n            \n        else:  # 10-20+ shot\n            # out_proj + MLP layers, larger rank\n            config = LoraConfig(\n                r=8,\n                lora_alpha=32,\n                target_modules=[\"attn.out_proj\", \"mlp.c_fc\", \"mlp.c_proj\"],\n                lora_dropout=0.15,\n                bias=\"none\",\n            )\n            print(f\"LoRA Config: attn.out_proj all layers, r={config.r}\")\n            \n        return config\n\n    def apply_lora(self, n_shot: int):\n        \"\"\"Apply LoRA to the model based on few-shot configuration.\"\"\"\n        lora_config = self.setup_lora_for_fewshot(n_shot)\n        \n        # Apply LoRA to the visual encoder only\n        self.lora_model = get_peft_model(self.model.visual, lora_config)\n        self.lora_model.print_trainable_parameters()\n    \n    def prototypical_loss(self, features: torch.Tensor, labels: torch.Tensor, n_support: int = 2) -> torch.Tensor:\n        \"\"\"\n        Prototypical loss for few-shot learning.\n        \"\"\"\n        unique_labels = torch.unique(labels)\n        n_classes = len(unique_labels)\n        \n        # Create label to index mapping\n        label_to_idx = {label.item(): idx for idx, label in enumerate(unique_labels)}\n        \n        # Create prototypes for each class\n        prototypes = []\n        query_features_list = []\n        query_labels_list = []\n        \n        for label in unique_labels:\n            mask = labels == label\n            class_features = features[mask]\n            \n            if len(class_features) <= n_support:\n                # Not enough samples for this class, use all as support\n                prototype = class_features.mean(dim=0)\n                prototypes.append(prototype)\n            else:\n                # Split into support and query\n                support_features = class_features[:n_support]\n                query_features = class_features[n_support:]\n                \n                prototype = support_features.mean(dim=0)\n                prototypes.append(prototype)\n                \n                query_features_list.append(query_features)\n                query_labels_list.extend([label_to_idx[label.item()]] * len(query_features))\n        \n        if len(query_features_list) == 0:\n            # No query samples, fallback to cross-entropy with prototypes\n            distances = torch.cdist(features, torch.stack(prototypes), p=2)\n            logits = -distances\n            mapped_labels = torch.tensor([label_to_idx[l.item()] for l in labels], device=features.device)\n            return F.cross_entropy(logits, mapped_labels)\n        \n        prototypes = torch.stack(prototypes)\n        query_features = torch.cat(query_features_list)\n        query_labels = torch.tensor(query_labels_list, device=features.device)\n        \n        # Calculate distances to prototypes\n        distances = torch.cdist(query_features, prototypes, p=2)\n        logits = -distances\n        \n        return F.cross_entropy(logits, query_labels)\n    \n    def contrastive_loss(self, features: torch.Tensor, labels: torch.Tensor, temperature: float = 0.1) -> torch.Tensor:\n        \"\"\"\n        Supervised contrastive loss (SupCon).\n        \"\"\"\n        batch_size = features.size(0)\n        \n        if batch_size < 2:\n            return torch.tensor(0.0, device=features.device)\n        \n        # Normalize features\n        features = F.normalize(features, dim=-1)\n        \n        # Compute similarity matrix\n        similarity_matrix = torch.matmul(features, features.T) / temperature\n        \n        # Create mask for positive pairs (same class)\n        labels = labels.contiguous().view(-1, 1)\n        mask = torch.eq(labels, labels.T).float().to(features.device)\n        \n        # Remove diagonal\n        logits_mask = torch.scatter(\n            torch.ones_like(mask),\n            1,\n            torch.arange(batch_size).view(-1, 1).to(features.device),\n            0\n        )\n        mask = mask * logits_mask\n        \n        # Compute log_prob\n        exp_logits = torch.exp(similarity_matrix) * logits_mask\n        log_prob = similarity_matrix - torch.log(exp_logits.sum(1, keepdim=True))\n        \n        # Compute mean of log-likelihood over positive\n        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-6)\n        \n        loss = -mean_log_prob_pos.mean()\n        return loss\n    \n    def train_with_lora(\n        self,\n        train_dir: Path,\n        n_shot: int = 5,\n        learning_rate: float = 1e-4,\n        weight_decay: float = 0.01,\n        num_epochs: int = 100,\n        batch_size: int = 32,\n        label_smoothing: float = 0.1,\n        loss_type: str = \"cross_entropy\",\n        use_augmentation: bool = True,\n        temperature: float = 0.1,\n        patience: int = 20\n    ) -> Dict:\n        \"\"\"\n        Train with LoRA fine-tuning and configurable loss functions.\n        \"\"\"\n        # Apply LoRA configuration based on few-shot setting\n        self.apply_lora(n_shot)\n        \n        # Create dataset\n        train_samples = []\n        class_dirs = sorted([d for d in train_dir.iterdir() if d.is_dir()])\n        self.class_names = [d.name for d in class_dirs]\n        num_classes = len(self.class_names)\n        class_to_idx = {name: idx for idx, name in enumerate(self.class_names)}\n        \n        print(f\"\\nPreparing {n_shot}-shot training data...\")\n        for class_dir in class_dirs:\n            class_name = class_dir.name\n            class_idx = class_to_idx[class_name]\n            image_files = list(class_dir.glob(\"*.jpg\")) + list(class_dir.glob(\"*.png\")) + list(class_dir.glob(\"*.JPEG\"))\n            image_paths = sorted(image_files)[:n_shot]\n            \n            for img_path in image_paths:\n                train_samples.append((img_path, class_idx))\n        \n        print(f\"Total training samples: {len(train_samples)}\")\n        \n        # Create dataset with augmentation\n        class TempDataset(Dataset):\n            def __init__(self, samples, transform, augment):\n                self.samples = samples\n                self.transform = transform\n                self.augment = augment\n                \n                if self.augment:\n                    self.augment_transform = transforms.Compose([\n                        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n                        transforms.RandomHorizontalFlip(),\n                        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n                        transforms.RandomGrayscale(p=0.1),\n                        transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n                    ])\n            \n            def __len__(self):\n                return len(self.samples)\n            \n            def __getitem__(self, idx):\n                img_path, label = self.samples[idx]\n                image = Image.open(img_path).convert(\"RGB\")\n                \n                if self.augment:\n                    image = self.augment_transform(image)\n                \n                if self.transform:\n                    image = self.transform(image)\n                \n                return image, label\n        \n        train_dataset = TempDataset(train_samples, self.transform, use_augmentation)\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=min(batch_size, len(train_dataset)),\n            shuffle=True,\n            num_workers=4,\n            pin_memory=True\n        )\n        \n        # Get feature dimension and setup classifier\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            sample_img = train_dataset[0][0].unsqueeze(0).to(self.device)\n            sample_feat = self.lora_model(sample_img)\n            feature_tensor = sample_feat[0]\n            feature_dim = feature_tensor.shape[1]\n            \n        \n        self.classifier = LinearClassifier(feature_dim, num_classes).to(self.device)\n        \n        # Setup optimizer\n        lora_params = [p for p in self.lora_model.parameters() if p.requires_grad]\n        classifier_params = [p for p in self.classifier.parameters() if p.requires_grad]\n        \n        optimizer = torch.optim.AdamW(\n            [\n                {'params': lora_params, 'lr': learning_rate},\n                {'params': classifier_params, 'lr': learning_rate * 10}\n            ],\n            weight_decay=weight_decay\n        )\n        \n        # Loss functions\n        if loss_type == \"cross_entropy\":\n            criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n        elif loss_type == \"prototypical\":\n            criterion = lambda features, labels: self.prototypical_loss(features, labels, n_support=max(1, n_shot//2))\n        elif loss_type == \"contrastive\":\n            criterion = lambda features, labels: self.contrastive_loss(features, labels, temperature)\n        else:\n            raise ValueError(f\"Unknown loss type: {loss_type}\")\n        \n        # Learning rate scheduler\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n        \n        # Training history\n        history = {\n            \"train_loss\": [],\n            \"val_accuracy\": [],\n            \"learning_rates\": []\n        }\n        \n        best_val_acc = 0.0\n        patience_counter = 0\n        \n        print(f\"\\nTraining with LoRA and {loss_type} loss...\")\n        print(f\"Config: LR={learning_rate}, WD={weight_decay}, Label Smoothing={label_smoothing}\")\n        print(f\"Augmentation: {use_augmentation}, Temperature: {temperature}\")\n        \n        for epoch in range(num_epochs):\n            # Training phase\n            self.lora_model.train()\n            self.classifier.train()\n            train_loss = 0.0\n            \n            for images, labels in train_loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n                \n                # Forward pass\n                with torch.cuda.amp.autocast(): # Consider updating to torch.amp.autocast('cuda')\n                    features = self.lora_model(images)[0]\n                    # It's better to normalize inside loss functions if needed, \n                    # but keeping it here is fine.\n                    # features = F.normalize(features, dim=-1)\n                    \n                    if loss_type == \"cross_entropy\":\n                        logits = self.classifier(features)\n                        loss = criterion(logits, labels)\n                    else:\n                        # For metric learning losses, use a hybrid approach\n                        \n                        # 1. Metric Loss on features to structure the embedding space\n                        metric_loss = criterion(features, labels)\n                        \n                        # 2. CrossEntropy Loss on classifier to train the head\n                        # THIS MUST BE OUTSIDE a no_grad() block\n                        logits = self.classifier(features)\n                        ce_loss = F.cross_entropy(logits, labels, label_smoothing=label_smoothing)\n                        \n                        # 3. Combine the losses\n                        loss = metric_loss + ce_loss\n                        \n                # Backward pass\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                \n                # Update the train_loss tracking\n                train_loss += loss.item()\n            \n            train_loss /= len(train_loader)\n            \n            # Validation phase\n            self.lora_model.eval()\n            self.classifier.eval()\n            val_accuracy = self._validate_on_train(train_samples, batch_size)\n            \n            # Update scheduler\n            current_lr = optimizer.param_groups[0]['lr']\n            scheduler.step()\n            \n            # Record history\n            history[\"train_loss\"].append(train_loss)\n            history[\"val_accuracy\"].append(val_accuracy)\n            history[\"learning_rates\"].append(current_lr)\n            \n            # Early stopping check\n            if val_accuracy > best_val_acc:\n                best_val_acc = val_accuracy\n                patience_counter = 0\n                best_lora_state = self.lora_model.state_dict()\n                best_classifier_state = self.classifier.state_dict()\n            else:\n                patience_counter += 1\n            \n            # Print progress\n            if (epoch + 1) % 10 == 0 or epoch == 0:\n                print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n                      f\"Loss: {train_loss:.4f} | \"\n                      f\"Val Acc: {val_accuracy:.2f}% | \"\n                      f\"LR: {current_lr:.6f}\")\n            \n            # Early stopping\n            if patience_counter >= patience:\n                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n                break\n        \n        # Load best model\n        self.lora_model.load_state_dict(best_lora_state)\n        self.classifier.load_state_dict(best_classifier_state)\n        print(f\"\\nTraining complete! Best validation accuracy: {best_val_acc:.2f}%\")\n        \n        return history\n    \n    def _validate_on_train(self, train_samples: List, batch_size: int) -> float:\n        \"\"\"Validate on training data (without augmentation).\"\"\"\n        class TempValDataset(Dataset):\n            def __init__(self, samples, transform):\n                self.samples = samples\n                self.transform = transform\n            \n            def __len__(self):\n                return len(self.samples)\n            \n            def __getitem__(self, idx):\n                img_path, label = self.samples[idx]\n                image = Image.open(img_path).convert(\"RGB\")\n                if self.transform:\n                    image = self.transform(image)\n                return image, label\n        \n        val_dataset = TempValDataset(train_samples, self.transform)\n        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n        \n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(self.device), labels.to(self.device)\n                \n                with torch.cuda.amp.autocast():\n                    features = self.lora_model(images)[0]\n                    logits = self.classifier(features)\n                    preds = logits.argmax(dim=1)\n                \n                correct += (preds == labels).sum().item()\n                total += len(labels)\n        \n        return (correct / total) * 100\n    \n    def evaluate(self, test_dir: Path, batch_size: int = 32) -> Dict[str, float]:\n        \"\"\"Evaluate the fine-tuned model on test set.\"\"\"\n        if self.lora_model is None or self.classifier is None:\n            raise ValueError(\"Model not trained. Call train_with_lora first.\")\n        \n        self.lora_model.eval()\n        self.classifier.eval()\n        \n        # Create test dataset\n        test_dataset = FewShotDataset(test_dir, transform=self.transform, augment=False)\n        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n        \n        all_preds = []\n        all_labels = []\n        \n        print(\"\\nEvaluating on test set...\")\n        with torch.no_grad():\n            for images, labels in tqdm(test_loader, desc=\"Testing\"):\n                images = images.to(self.device)\n                \n                with torch.cuda.amp.autocast():\n                    features = self.lora_model(images)[0]\n                    logits = self.classifier(features)\n                    preds = logits.argmax(dim=1)\n                \n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.numpy())\n        \n        all_preds = np.array(all_preds)\n        all_labels = np.array(all_labels)\n        \n        # Calculate metrics\n        overall_accuracy = (all_preds == all_labels).mean() * 100\n        \n        # Per-class accuracy\n        per_class_accuracy = {}\n        for class_idx, class_name in enumerate(self.class_names):\n            class_mask = all_labels == class_idx\n            if class_mask.sum() > 0:\n                class_acc = (all_preds[class_mask] == class_idx).mean() * 100\n                per_class_accuracy[class_name] = class_acc\n        \n        mean_per_class_accuracy = np.mean(list(per_class_accuracy.values()))\n        \n        results = {\n            # Convert NumPy floats to Python floats\n            \"overall_accuracy\": float(overall_accuracy),\n            \"mean_per_class_accuracy\": float(mean_per_class_accuracy),\n            \n            \"total_samples\": len(all_labels), # This is already a Python int\n            \n            # Convert the numpy.int64 to a Python int\n            \"correct_predictions\": int((all_preds == all_labels).sum()),\n            \n            \"per_class_accuracy\": per_class_accuracy\n        }\n        \n        return results\n\n\ndef compare_strategies():\n    \"\"\"Compare different fine-tuning strategies.\"\"\"\n    DATASET_DIR = Path(\"/kaggle/working/mini_imagenet_fewshot_renamed\")\n    \n    strategies = [\n        # Few-shot scenarios with different loss functions\n        # {\"n_shot\": 1, \"loss_type\": \"prototypical\", \"use_augmentation\": False},\n        # {\"n_shot\": 1, \"loss_type\": \"contrastive\", \"use_augmentation\": False},\n        # {\"n_shot\": 1, \"loss_type\": \"cross_entropy\", \"use_augmentation\": False},\n        \n        # {\"n_shot\": 5, \"loss_type\": \"contrastive\", \"use_augmentation\": True},\n        # {\"n_shot\": 5, \"loss_type\": \"cross_entropy\", \"use_augmentation\": True},\n        \n        {\"n_shot\": 10, \"loss_type\": \"contrastive\", \"use_augmentation\": False},\n        # {\"n_shot\": 20, \"loss_type\": \"cross_entropy\", \"use_augmentation\": True},\n    ]\n    \n    all_results = {}\n    \n    for strategy in strategies:\n        n_shot = strategy[\"n_shot\"]\n        loss_type = strategy[\"loss_type\"]\n        use_aug = strategy[\"use_augmentation\"]\n        \n        print(f\"\\n{'='*70}\")\n        print(f\"Running {n_shot}-shot with {loss_type} loss (Augmentation: {use_aug})\")\n        print(f\"{'='*70}\")\n        \n        finetuner = LoRACoCaFinetune()\n        \n        results = finetuner.run_lora_experiment(\n            dataset_dir=DATASET_DIR,\n            n_shot=n_shot,\n            learning_rate=5e-5,\n            weight_decay=0.01,\n            num_epochs=100,\n            label_smoothing=0.1,\n            loss_type=loss_type,\n            use_augmentation=use_aug,\n            temperature=0.1,\n            save_results=True\n        )\n        \n        key = f\"{n_shot}shot_{loss_type}_aug{use_aug}\"\n        all_results[key] = results\n    \n    # Print summary\n    print(\"\\n\" + \"=\"*80)\n    print(\"STRATEGY COMPARISON SUMMARY\")\n    print(\"=\"*80)\n    print(f\"{'Config':<35} {'Overall Acc':<12} {'Mean Per-Class':<15}\")\n    print(\"-\"*80)\n    for key, res in all_results.items():\n        print(f\"{key:<35} {res['overall_accuracy']:<12.2f} {res['mean_per_class_accuracy']:<15.2f}\")\n    print(\"=\"*80)\n    \n    return all_results\n\nresults = compare_strategies()\n\n\n# def run_single_experiment():\n#     \"\"\"Run a single experiment with specific configuration.\"\"\"\n#     DATASET_DIR = Path(\"/kaggle/working/mini_imagenet_fewshot_renamed\")\n    \n#     # Configuration\n#     config = {\n#         \"n_shot\": 5,\n#         \"learning_rate\": 1e-4,\n#         \"weight_decay\": 0.01,\n#         \"num_epochs\": 100,\n#         \"batch_size\": 32,\n#         \"label_smoothing\": 0.1,\n#         \"loss_type\": \"cross_entropy\",  # Options: \"cross_entropy\", \"prototypical\", \"contrastive\", \"auto\"\n#         \"use_augmentation\": True,\n#         \"temperature\": 0.1,  # For contrastive loss\n#         \"save_results\": True\n#     }\n    \n#     print(\"=\"*70)\n#     print(\"Running LoRA Fine-tuning Experiment\")\n#     print(\"=\"*70)\n#     print(f\"Configuration:\")\n#     for key, value in config.items():\n#         print(f\"  {key}: {value}\")\n#     print(\"=\"*70)\n    \n#     # Initialize model\n#     finetuner = LoRACoCaFinetune(\n#         model_name=\"coca_ViT-L-14\",\n#         pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\",\n#     )\n    \n#     # Run experiment\n#     results = finetuner.run_lora_experiment(\n#         dataset_dir=DATASET_DIR,\n#         **config\n#     )\n    \n#     return results\n\n\n# def main():\n#     \"\"\"Main function with options.\"\"\"\n#     import argparse\n    \n#     parser = argparse.ArgumentParser(description='LoRA Fine-tuning for CoCa')\n#     parser.add_argument('--mode', type=str, default='single', \n#                         choices=['single', 'compare'],\n#                         help='Run single experiment or compare strategies')\n#     parser.add_argument('--dataset_dir', type=str, \n#                         default='/kaggle/working/mini_imagenet_fewshot_renamed',\n#                         help='Path to dataset directory')\n#     parser.add_argument('--n_shot', type=int, default=5,\n#                         help='Number of shots per class')\n#     parser.add_argument('--loss_type', type=str, default='cross_entropy',\n#                         choices=['cross_entropy', 'prototypical', 'contrastive', 'auto'],\n#                         help='Loss function to use')\n#     parser.add_argument('--lr', type=float, default=1e-4,\n#                         help='Learning rate')\n#     parser.add_argument('--epochs', type=int, default=100,\n#                         help='Number of training epochs')\n#     parser.add_argument('--batch_size', type=int, default=32,\n#                         help='Batch size')\n#     parser.add_argument('--no_augmentation', action='store_true',\n#                         help='Disable data augmentation')\n#     parser.add_argument('--temperature', type=float, default=0.1,\n#                         help='Temperature for contrastive loss')\n    \n#     args = parser.parse_args()\n    \n#     if args.mode == 'compare':\n#         # Run comparison of multiple strategies\n#         results = compare_strategies()\n#     else:\n#         # Run single experiment\n#         DATASET_DIR = Path(args.dataset_dir)\n        \n#         config = {\n#             \"n_shot\": args.n_shot,\n#             \"learning_rate\": args.lr,\n#             \"weight_decay\": 0.01,\n#             \"num_epochs\": args.epochs,\n#             \"batch_size\": args.batch_size,\n#             \"label_smoothing\": 0.1,\n#             \"loss_type\": args.loss_type,\n#             \"use_augmentation\": not args.no_augmentation,\n#             \"temperature\": args.temperature,\n#             \"save_results\": True\n#         }\n        \n#         print(\"=\"*70)\n#         print(\"Running LoRA Fine-tuning Experiment\")\n#         print(\"=\"*70)\n#         print(f\"Configuration:\")\n#         for key, value in config.items():\n#             print(f\"  {key}: {value}\")\n#         print(\"=\"*70)\n        \n#         finetuner = LoRACoCaFinetune()\n#         results = finetuner.run_lora_experiment(dataset_dir=DATASET_DIR, **config)\n    \n#     print(\"\\n\" + \"=\"*70)\n#     print(\"Experiment Complete!\")\n#     print(\"=\"*70)\n\n\n# if __name__ == \"__main__\":\n#     # For Kaggle/Jupyter notebook, use these simple functions:\n    \n#     # Option 1: Run a single experiment with default settings\n#     # results = run_single_experiment()\n    \n#     # Option 2: Run comparison of multiple strategies\n#     results = compare_strategies()\n    \n#     # Option 3: For command line usage\n#     # main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:03:26.091537Z","iopub.execute_input":"2025-10-03T09:03:26.092914Z","iopub.status.idle":"2025-10-03T09:53:13.531015Z","shell.execute_reply.started":"2025-10-03T09:03:26.092876Z","shell.execute_reply":"2025-10-03T09:53:13.530042Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nRunning 10-shot with contrastive loss (Augmentation: False)\n======================================================================\nLoading CoCa model on cuda...\nUsing contrastive loss for 10-shot learning\nLoRA Config: attn.out_proj all layers, r=4, droupout=0.15\ntrainable params: 202,752 || all params: 306,927,616 || trainable%: 0.0661\n\nPreparing 10-shot training data...\nTotal training samples: 1000\n\nTraining with LoRA and contrastive loss...\nConfig: LR=5e-05, WD=0.01, Label Smoothing=0.1\nAugmentation: False, Temperature: 0.1\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/705350942.py:387: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.no_grad(), torch.cuda.amp.autocast():\n/tmp/ipykernel_36/705350942.py:445: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(): # Consider updating to torch.amp.autocast('cuda')\n/tmp/ipykernel_36/705350942.py:547: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/100] Loss: 4.6193 | Val Acc: 61.30% | LR: 0.000050\nEpoch [10/100] Loss: 1.3777 | Val Acc: 99.40% | LR: 0.000049\nEpoch [20/100] Loss: 1.2056 | Val Acc: 99.90% | LR: 0.000046\nEpoch [30/100] Loss: 1.1277 | Val Acc: 100.00% | LR: 0.000040\nEpoch [40/100] Loss: 1.1269 | Val Acc: 100.00% | LR: 0.000033\n\nEarly stopping at epoch 44\n\nTraining complete! Best validation accuracy: 100.00%\n\nEvaluating on test set...\n","output_type":"stream"},{"name":"stderr","text":"Testing:   0%|          | 0/63 [00:00<?, ?it/s]/tmp/ipykernel_36/705350942.py:577: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nTesting: 100%|██████████| 63/63 [01:00<00:00,  1.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nLoRA Fine-tuning Results (10-shot, contrastive loss)\n============================================================\nOverall Accuracy: 91.95%\nMean Per-Class Accuracy: 91.95%\nTotal Samples: 2000\nCorrect Predictions: 1839\n============================================================\nResults saved to lora_coca_10shot_contrastive_augFalse.json\n\n================================================================================\nSTRATEGY COMPARISON SUMMARY\n================================================================================\nConfig                              Overall Acc  Mean Per-Class \n--------------------------------------------------------------------------------\n10shot_contrastive_augFalse         91.95        91.95          \n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7}]}