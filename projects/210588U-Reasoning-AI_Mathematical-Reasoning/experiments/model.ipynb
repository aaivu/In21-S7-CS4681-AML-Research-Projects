{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9483b11edf344114bb03a8f8ddb1015c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9ea21081f914f52b883a979f17197f6",
              "IPY_MODEL_e6723202618b4544b45a4ee7f5598911",
              "IPY_MODEL_9eb2930cae8d416e9db38dd63ec4948a"
            ],
            "layout": "IPY_MODEL_a727032928b54d5b8289746dc8e05a54"
          }
        },
        "e9ea21081f914f52b883a979f17197f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f85d52de7e947b69b3a7e060727c953",
            "placeholder": "​",
            "style": "IPY_MODEL_413ebbda14b64e6a9c2f4699793b7487",
            "value": "Map: 100%"
          }
        },
        "e6723202618b4544b45a4ee7f5598911": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25880b56c8474c719b81a58e27788428",
            "max": 1152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c07aadf23294e9990cc33e6e846ac52",
            "value": 1152
          }
        },
        "9eb2930cae8d416e9db38dd63ec4948a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0448a7f75b194d6781963d954b28a865",
            "placeholder": "​",
            "style": "IPY_MODEL_a758592911ad4f90acdbd721b09ff548",
            "value": " 1152/1152 [00:03&lt;00:00, 296.46 examples/s]"
          }
        },
        "a727032928b54d5b8289746dc8e05a54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f85d52de7e947b69b3a7e060727c953": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "413ebbda14b64e6a9c2f4699793b7487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25880b56c8474c719b81a58e27788428": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c07aadf23294e9990cc33e6e846ac52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0448a7f75b194d6781963d954b28a865": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a758592911ad4f90acdbd721b09ff548": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      },
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fvH2nT_R2HCF"
      },
      "outputs": [],
      "source": [
        "# @title 1. Install Dependencies\n",
        "!pip install -q datasets transformers accelerate evaluate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ],
      "metadata": {
        "id": "-8HWbyy6OOUN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 2. Imports and Device Check\n",
        "import torch, os, re\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, T5ForConditionalGeneration,\n",
        "    DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        ")\n",
        "\n",
        "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "o74lkHBE2TLC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b629ad66-c490-4f8e-edd4-029a699d9691"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Configuration\n",
        "dataset_name = \"lighteval/MATH-Hard\"     # or \"hendrycks/competition_math\"\n",
        "model_name = \"google/byt5-base\"\n",
        "output_dir = \"./byt5_math_proof_output\"\n",
        "\n",
        "num_train_epochs = 10\n",
        "per_device_train_batch_size = 4\n",
        "per_device_eval_batch_size = 4\n",
        "num_think_tokens = 1      # adds <think> tokens for reasoning\n",
        "max_input_length = 512\n",
        "max_target_length = 512\n",
        "self_consistency_k = 0    # >0 enables majority-vote inference\n"
      ],
      "metadata": {
        "id": "HR0QrZlf2Vsr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Load Dataset\n",
        "def load_math_dataset(name, split=\"train\", max_examples=None):\n",
        "    ds = load_dataset(name, split=split)\n",
        "    if max_examples:\n",
        "        ds = ds.select(range(max_examples))\n",
        "    print(ds)\n",
        "    return ds\n",
        "\n",
        "train_dataset = load_math_dataset(dataset_name, split=\"train[:50%]\")\n",
        "test_dataset  = load_math_dataset(dataset_name, split=\"test[:1%]\")\n"
      ],
      "metadata": {
        "id": "S1kZH1xR2cbU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e3d7784-2d8d-4b7f-a162-b694fcf7c079"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['problem', 'level', 'type', 'solution'],\n",
            "    num_rows: 1152\n",
            "})\n",
            "Dataset({\n",
            "    features: ['problem', 'level', 'type', 'solution'],\n",
            "    num_rows: 13\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Preprocessing Functions\n",
        "def build_input(problem):\n",
        "    return (\n",
        "        \"Solve this problem step by step. \"\n",
        "        \"Show reasoning clearly and put the final answer inside \\\\boxed{...}.\\n\\n\"\n",
        "        f\"Problem: {problem}\\n\\nSolution:\"\n",
        "    )\n",
        "\n",
        "\n",
        "def preprocess_examples(examples, tokenizer, max_input_length, max_target_length, num_think_tokens=0):\n",
        "    inputs  = [build_input(p) for p in examples[\"problem\"]]\n",
        "\n",
        "    examples[\"solution\"] = [s.replace(\"\\\\\\\\boxed\", \"\\\\boxed\") for s in examples[\"solution\"]]\n",
        "    targets = examples[\"solution\"]  # full LaTeX solution with \\boxed{...}\n",
        "\n",
        "\n",
        "    if num_think_tokens > 0:\n",
        "        prefix = \"<think>\" * num_think_tokens\n",
        "        targets = [prefix + t for t in targets]\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs, max_length=max_input_length, truncation=True, padding=\"max_length\"\n",
        "    )\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            targets, max_length=max_target_length, truncation=True, padding=\"max_length\"\n",
        "        )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n"
      ],
      "metadata": {
        "id": "idWo8SHa2e4p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6. Load Tokenizer & Model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "if num_think_tokens > 0 and \"<think>\" not in tokenizer.get_vocab():\n",
        "    tokenizer.add_tokens([\"<think>\"])\n",
        "    model.resize_token_embeddings(len(tokenizer))\n"
      ],
      "metadata": {
        "id": "nZXGyAXB2g7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c6dacde-f64f-45a4-d6f4-e954320498e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 7. Tokenize Datasets\n",
        "def preprocess_fn(batch):\n",
        "    return preprocess_examples(batch, tokenizer, max_input_length, max_target_length, num_think_tokens)\n",
        "\n",
        "tokenized_train = train_dataset.map(preprocess_fn, batched=True, remove_columns=train_dataset.column_names)\n",
        "tokenized_test  = test_dataset.map(preprocess_fn,  batched=True, remove_columns=test_dataset.column_names)\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n"
      ],
      "metadata": {
        "id": "hY5wOHHo2jP1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "9483b11edf344114bb03a8f8ddb1015c",
            "e9ea21081f914f52b883a979f17197f6",
            "e6723202618b4544b45a4ee7f5598911",
            "9eb2930cae8d416e9db38dd63ec4948a",
            "a727032928b54d5b8289746dc8e05a54",
            "3f85d52de7e947b69b3a7e060727c953",
            "413ebbda14b64e6a9c2f4699793b7487",
            "25880b56c8474c719b81a58e27788428",
            "5c07aadf23294e9990cc33e6e846ac52",
            "0448a7f75b194d6781963d954b28a865",
            "a758592911ad4f90acdbd721b09ff548"
          ]
        },
        "outputId": "8b6b18f8-d4ef-496f-ad27-4edb648dd51c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1152 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9483b11edf344114bb03a8f8ddb1015c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 8. Evaluation Metrics\n",
        "def extract_boxed_answer(text: str):\n",
        "    # matches \\boxed{ ... } (raw string in HF JSON often has '\\\\boxed{')\n",
        "    m = re.findall(r\"\\\\boxed\\{([^}]*)\\}\", text)\n",
        "    return m[-1].strip() if m else None\n",
        "\n",
        "def final_answer_accuracy(preds, refs):\n",
        "    total, hits = 0, 0\n",
        "    for p, r in zip(preds, refs):\n",
        "        p_box, r_box = extract_boxed_answer(p), extract_boxed_answer(r)\n",
        "        # Count only if reference actually has a boxed answer\n",
        "        # (MATH should, but this guards weird cases)\n",
        "        if r_box is None:\n",
        "            continue\n",
        "        total += 1\n",
        "        if p_box is not None and p_box == r_box:\n",
        "            hits += 1\n",
        "    return {\"final_answer_acc\": (hits / total) if total else 0.0}\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    if isinstance(preds, tuple): preds = preds[0]\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    acc = final_answer_accuracy(decoded_preds, decoded_labels)\n",
        "    return acc\n"
      ],
      "metadata": {
        "id": "g1VXnyEb2nSW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 9. Training Arguments & Trainer\n",
        "args_1 = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "    predict_with_generate=True,\n",
        "    logging_dir=Path(output_dir) / \"logs\",\n",
        "    logging_steps=100,\n",
        "    # evaluation_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    # evaluation_strategy=  # ← removed for compatibility\n",
        "    # If you want periodic saving/logging by steps instead of epochs, you can add:\n",
        "    # save_steps=500,\n",
        "    # logging_strategy=\"steps\",\n",
        ")\n",
        "\n",
        "# ================================================================================\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    learning_rate=6e-4,           # T5/ByT5 like a higher LR than BERT\n",
        "    warmup_ratio=0.05,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16, # effective batch 32\n",
        "    dataloader_num_workers=2,\n",
        "\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=512,\n",
        "\n",
        "    logging_dir=Path(output_dir) / \"logs\",\n",
        "    logging_steps=100,\n",
        "    # evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_total_limit=2,\n",
        "    bf16=torch.cuda.is_available(),   # or fp16=True if no bf16\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ],
      "metadata": {
        "id": "hnJ6nLFi2wL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b389d37-4de6-451c-ddc4-61b48437780f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/tmp/ipython-input-3891800150.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 10. Train & Evaluate\n",
        "train = True\n",
        "evaluate = True\n",
        "\n",
        "if train:\n",
        "    print(\"🚀 Starting fine-tuning...\")\n",
        "    trainer.train()\n",
        "    trainer.save_model()\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "if evaluate:\n",
        "    print(\"📈 Evaluating...\")\n",
        "    results = trainer.evaluate()\n",
        "    print(\"Results:\", results)\n"
      ],
      "metadata": {
        "id": "CK2j0HC820qj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "34b9bf6b-cbfd-4271-8c75-7820f04a5997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting fine-tuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='678' max='720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [678/720 2:52:45 < 10:44, 0.07 it/s, Epoch 9.40/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.211400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.803700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.690000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.612400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.559600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.526700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 11. Self-Consistency Evaluation\n",
        "if self_consistency_k > 0:\n",
        "    from collections import Counter\n",
        "    print(f\"Running self-consistency with k={self_consistency_k}\")\n",
        "    model.eval()\n",
        "    preds, refs = [], []\n",
        "    for ex in test_dataset:\n",
        "        q, ref = ex[\"problem\"], ex[\"solution\"]\n",
        "        input_ids = tokenizer(q, return_tensors=\"pt\").input_ids\n",
        "        candidates = []\n",
        "        for _ in range(self_consistency_k):\n",
        "            out = model.generate(input_ids, do_sample=True, top_p=0.9, max_length=max_target_length)\n",
        "            text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "            candidates.append(text)\n",
        "        preds.append(Counter(candidates).most_common(1)[0][0])\n",
        "        refs.append(ref)\n",
        "    acc = final_answer_accuracy(preds, refs)\n",
        "    print(\"Self-consistency final-answer Acc:\", acc)\n"
      ],
      "metadata": {
        "id": "eS8RfKS_22_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 12. Quick Inference\n",
        "# Inference (device-safe)\n",
        "# --- generation config ---\n",
        "gen_kwargs = dict(\n",
        "    max_new_tokens=512,\n",
        "    num_beams=4,                # or do_sample=True, top_p=0.9 for self-consistency\n",
        "    length_penalty=0.8,         # discourage overly long rambles\n",
        "    no_repeat_ngram_size=3,\n",
        ")\n",
        "\n",
        "# --- inference ---\n",
        "question = \"Compute the derivative of x^3 + 2x^2 + 5x + 7.\"\n",
        "inp = build_input(question)  # <-- use your build_input() if you added it\n",
        "inputs = tokenizer(inp, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(**inputs, **gen_kwargs)\n",
        "\n",
        "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "owQZxE_m26sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0].keys())\n",
        "print(train_dataset[0][\"problem\"])\n",
        "print(train_dataset[0][\"solution\"])\n"
      ],
      "metadata": {
        "id": "0Ucn8kxR7_E6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RTbEbY22B5f8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}