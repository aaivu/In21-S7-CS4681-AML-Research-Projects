{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c45878",
   "metadata": {},
   "source": [
    "# Hybrid Dense-Sparse Retrieval Experiments\n",
    "\n",
    "This notebook implements the experimental setup for evaluating our hybrid dense-sparse retrieval approach on the HotpotQA dataset. We'll evaluate the system's performance with different configurations and analyze the results.\n",
    "\n",
    "## Table of Contents:\n",
    "1. Project Setup and Dependencies\n",
    "2. Configuration and Parameters\n",
    "3. Data Loading and Preprocessing\n",
    "4. Model Implementation\n",
    "5. Experimental Evaluation\n",
    "6. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dac70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import project modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from multihop_dense_retrieval.dense.model import DenseRetriever\n",
    "from multihop_dense_retrieval.sparse.retriever import SparseRetriever\n",
    "from multihop_dense_retrieval.hybrid.retriever import HybridRetriever\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbbbf20",
   "metadata": {},
   "source": [
    "## Project Configuration\n",
    "\n",
    "First, we'll set up the project configuration including paths and experiment parameters. We'll create a configuration class to manage all experiment settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffd77fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentConfig:\n",
    "    def __init__(self):\n",
    "        # Project paths\n",
    "        self.base_path = Path('../')\n",
    "        self.data_path = self.base_path / 'data/hotpotqa'\n",
    "        self.results_path = self.base_path / 'experiments/results'\n",
    "        self.models_path = self.base_path / 'models'\n",
    "        \n",
    "        # Create directories if they don't exist\n",
    "        self.data_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.results_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.models_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Model configuration\n",
    "        self.model_config = {\n",
    "            'dense': {\n",
    "                'model_name': 'roberta-base',\n",
    "                'max_length': 512,\n",
    "                'batch_size': 32,\n",
    "                'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            },\n",
    "            'sparse': {\n",
    "                'k1': 1.5,\n",
    "                'b': 0.75\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Experiment settings\n",
    "        self.experiment_config = {\n",
    "            'alpha_values': [0.0, 0.3, 0.5, 0.7, 0.8, 0.9, 1.0],\n",
    "            'top_k_values': [1, 5, 10, 50],\n",
    "            'batch_size': 32,\n",
    "            'num_samples': None  # None for full dataset\n",
    "        }\n",
    "        \n",
    "    def save_config(self, filename: str):\n",
    "        \"\"\"Save configuration to JSON file.\"\"\"\n",
    "        config = {\n",
    "            'model_config': self.model_config,\n",
    "            'experiment_config': self.experiment_config\n",
    "        }\n",
    "        with open(self.results_path / filename, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "            \n",
    "    @staticmethod\n",
    "    def load_config(filepath: str):\n",
    "        \"\"\"Load configuration from JSON file.\"\"\"\n",
    "        with open(filepath) as f:\n",
    "            config = json.load(f)\n",
    "        return config\n",
    "\n",
    "# Initialize configuration\n",
    "config = ExperimentConfig()\n",
    "print(\"Configuration initialized with device:\", config.model_config['dense']['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd865e",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "Next, we'll implement functions to load and preprocess the HotpotQA dataset. This includes loading the dev set and the Wikipedia corpus, as well as implementing evaluation utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f14606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def load_hotpotqa_dev(self) -> List[Dict]:\n",
    "        \"\"\"Load HotpotQA dev set.\"\"\"\n",
    "        filepath = self.config.data_path / 'dev.json'\n",
    "        with open(filepath) as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if self.config.experiment_config['num_samples']:\n",
    "            data = data[:self.config.experiment_config['num_samples']]\n",
    "            \n",
    "        print(f\"Loaded {len(data)} examples from dev set\")\n",
    "        return data\n",
    "    \n",
    "    def load_wiki_corpus(self) -> Dict[str, str]:\n",
    "        \"\"\"Load Wikipedia corpus.\"\"\"\n",
    "        filepath = self.config.data_path / 'wiki.json'\n",
    "        with open(filepath) as f:\n",
    "            wiki_data = json.load(f)\n",
    "            \n",
    "        # Concatenate title and text for each document\n",
    "        corpus = {\n",
    "            doc_id: f\"{content['title']} {content['text']}\"\n",
    "            for doc_id, content in wiki_data.items()\n",
    "        }\n",
    "        \n",
    "        print(f\"Loaded {len(corpus)} documents from Wikipedia\")\n",
    "        return corpus\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_gold_docs(example: Dict) -> List[str]:\n",
    "        \"\"\"Extract gold supporting document IDs from example.\"\"\"\n",
    "        return [doc[0] for doc in example['supporting_facts']]\n",
    "    \n",
    "# Initialize data loader\n",
    "data_loader = DataLoader(config)\n",
    "\n",
    "# Load development set and Wikipedia corpus\n",
    "try:\n",
    "    dev_data = data_loader.load_hotpotqa_dev()\n",
    "    wiki_corpus = data_loader.load_wiki_corpus()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Dataset files not found. Please run the download script first:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f429afc",
   "metadata": {},
   "source": [
    "## Model Setup and Evaluation\n",
    "\n",
    "Now we'll set up the evaluation framework including metrics computation and the experiment runner class that will handle the model evaluation across different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1320b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, config: ExperimentConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def compute_metrics(\n",
    "        self,\n",
    "        predictions: List[Tuple[List[str], List[float]]],\n",
    "        gold_docs: List[List[str]]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Compute retrieval metrics.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Compute top-k accuracy\n",
    "        for k in self.config.experiment_config['top_k_values']:\n",
    "            correct = 0\n",
    "            for (pred_docs, _), gold in zip(predictions, gold_docs):\n",
    "                top_k_docs = pred_docs[:k]\n",
    "                if any(doc in gold for doc in top_k_docs):\n",
    "                    correct += 1\n",
    "            metrics[f'top_{k}_accuracy'] = correct / len(predictions)\n",
    "        \n",
    "        # Compute MRR\n",
    "        mrr_sum = 0\n",
    "        for (pred_docs, _), gold in zip(predictions, gold_docs):\n",
    "            for rank, doc in enumerate(pred_docs, 1):\n",
    "                if doc in gold:\n",
    "                    mrr_sum += 1 / rank\n",
    "                    break\n",
    "        metrics['mrr'] = mrr_sum / len(predictions)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, config: ExperimentConfig, data_loader: DataLoader):\n",
    "        self.config = config\n",
    "        self.data_loader = data_loader\n",
    "        self.evaluator = Evaluator(config)\n",
    "        \n",
    "        # Initialize models\n",
    "        self.dense_retriever = DenseRetriever(\n",
    "            model_name=config.model_config['dense']['model_name'],\n",
    "            device=config.model_config['dense']['device']\n",
    "        )\n",
    "        self.sparse_retriever = SparseRetriever(\n",
    "            k1=config.model_config['sparse']['k1'],\n",
    "            b=config.model_config['sparse']['b']\n",
    "        )\n",
    "        \n",
    "    def run_experiment(self) -> pd.DataFrame:\n",
    "        \"\"\"Run experiments for different alpha values.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for alpha in tqdm(self.config.experiment_config['alpha_values']):\n",
    "            # Initialize hybrid retriever\n",
    "            hybrid_retriever = HybridRetriever(\n",
    "                dense_retriever=self.dense_retriever,\n",
    "                sparse_retriever=self.sparse_retriever,\n",
    "                alpha=alpha\n",
    "            )\n",
    "            \n",
    "            # Get predictions for dev set\n",
    "            predictions = []\n",
    "            for example in tqdm(self.data_loader.dev_data):\n",
    "                pred = hybrid_retriever.retrieve(\n",
    "                    example['question'],\n",
    "                    top_k=max(self.config.experiment_config['top_k_values'])\n",
    "                )\n",
    "                predictions.append(pred)\n",
    "                \n",
    "            # Compute metrics\n",
    "            gold_docs = [self.data_loader.get_gold_docs(ex) for ex in self.data_loader.dev_data]\n",
    "            metrics = self.evaluator.compute_metrics(predictions, gold_docs)\n",
    "            \n",
    "            # Record results\n",
    "            results.append({\n",
    "                'alpha': alpha,\n",
    "                **metrics\n",
    "            })\n",
    "            \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Initialize experiment runner\n",
    "runner = ExperimentRunner(config, data_loader)\n",
    "\n",
    "# Run experiments\n",
    "print(\"Starting experiments...\")\n",
    "results_df = runner.run_experiment()\n",
    "print(\"\\nExperiment Results:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab455e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_metrics_vs_alpha(results_df: pd.DataFrame):\n",
    "    \"\"\"Plot metrics vs alpha values.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    metric_cols = [col for col in results_df.columns if col != 'alpha']\n",
    "    \n",
    "    for metric in metric_cols:\n",
    "        plt.plot(results_df['alpha'], results_df[metric], \n",
    "                marker='o', label=metric)\n",
    "    \n",
    "    plt.xlabel('Alpha (Dense Retriever Weight)')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Retrieval Metrics vs Alpha')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_metric_heatmap(results_df: pd.DataFrame):\n",
    "    \"\"\"Plot correlation heatmap between metrics.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    corr = results_df.corr()\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Metric Correlations')\n",
    "    plt.show()\n",
    "\n",
    "# Plot results\n",
    "print(\"Plotting results...\")\n",
    "plot_metrics_vs_alpha(results_df)\n",
    "plot_metric_heatmap(results_df)\n",
    "\n",
    "# Find best configurations\n",
    "print(\"\\nBest configurations for each metric:\")\n",
    "metric_cols = [col for col in results_df.columns if col != 'alpha']\n",
    "for metric in metric_cols:\n",
    "    best_idx = results_df[metric].idxmax()\n",
    "    print(f\"\\nBest {metric}:\")\n",
    "    print(f\"Alpha: {results_df.loc[best_idx, 'alpha']:.3f}\")\n",
    "    print(f\"Score: {results_df.loc[best_idx, metric]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
