# Research Proposal: AI Evaluation: Reasoning Evaluation

**Student:** 210574A  
**Research Area:** AI Evaluation: Reasoning Evaluation  
**Date:** 2025-09-01

## Abstract

The research aims to explore innovative methods to enhance financial sentiment analysis by leveraging Large Language Models (LLMs) such as GPT-5 for data augmentation in the context of FinBERT. This proposal addresses challenges in handling class imbalances and domain-specific language in financial sentiment datasets. By generating synthetic paraphrased samples, this study aims to improve model robustness, accuracy, and generalization on unseen financial texts. The objective is to develop a methodology that integrates synthetic data with the original dataset to enhance the performance of financial sentiment analysis tools, using the Financial PhraseBank dataset for experimentation. The expected outcome is a measurable improvement in sentiment classification accuracy and a reduction in misclassification errors, especially for underrepresented sentiment categories.

## 1. Introduction

Financial sentiment analysis has become a critical tool for predicting market movements and guiding investment decisions. Traditional sentiment analysis models often struggle with the specialized vocabulary and class imbalances inherent in financial datasets. FinBERT, a domain-specific model based on BERT, has shown promising results but faces challenges due to limited labeled data and overfitting issues. This research explores the use of LLMs for data augmentation to enhance the training process, creating a more balanced and robust dataset that addresses these challenges. This work is significant because it provides a scalable solution for improving the performance of domain-specific NLP models, especially in financial applications where data scarcity and imbalance are prevalent.

## 2. Problem Statement

The research problem revolves around enhancing the performance of FinBERT in financial sentiment analysis tasks by addressing issues such as class imbalance, overfitting, and domain-specific language complexity. The goal is to augment FinBERT’s training dataset using LLM-generated synthetic data to improve model robustness and generalization to new, unseen financial texts.

## 3. Literature Review Summary

Various studies have shown the effectiveness of domain-specific models like FinBERT for financial sentiment analysis, but the limited availability of labeled data often hampers their performance. Conventional data augmentation methods such as back-translation and paraphrasing have been used, but they often fail to produce sufficiently diverse or contextually relevant samples, especially in niche areas like finance. Recent advancements in LLMs, such as GPT-5, offer new opportunities for generating high-quality, domain-specific synthetic data. This study builds upon existing literature by combining LLM-based data augmentation with FinBERT to improve sentiment classification performance, particularly in the face of data imbalance and class-specific challenges.

## 4. Research Objectives

### Primary Objective

- To enhance the performance of FinBERT in financial sentiment analysis through the integration of synthetic data generated by LLMs for data augmentation.

### Secondary Objectives

- To evaluate the impact of LLM-generated data on model accuracy and F1-score across different sentiment classes.
- To explore the effectiveness of augmenting only the minority classes (positive and negative sentiments) to balance class distributions.
- To develop a scalable framework for integrating LLM-generated synthetic data into financial sentiment analysis models.

## 5. Methodology

The methodology involves using GPT-5 to generate paraphrased examples of financial sentiment data to address the imbalance in sentiment classes. The generated paraphrases will be semantically similar to the original texts but will exhibit diverse linguistic structures. This synthetic data will be integrated with the original dataset, and the augmented dataset will be used to fine-tune FinBERT. The performance of the baseline model (trained only on the original dataset) will be compared with the augmented model (trained on the combined dataset) to evaluate improvements in classification accuracy and F1-score.

## 6. Expected Outcomes

The expected outcomes of this research include:

- A significant improvement in the accuracy and F1-score of the FinBERT model, particularly for the underrepresented sentiment classes (positive and negative).
- A reduction in misclassification errors, especially neutral-negative and positive-neutral misclassifications, across various levels of annotator agreement in financial sentiment data.
- A scalable and modular data augmentation framework for financial sentiment analysis models.

## 7. Timeline

| Week  | Task                                 |
| ----- | ------------------------------------ |
| 1-2   | Literature Review                    |
| 3-4   | Methodology Development              |
| 5-8   | Data Augmentation and Model Training |
| 9-12  | Experimentation                      |
| 13-15 | Data Analysis and Writing            |
| 16    | Final Submission                     |

## 8. Resources Required

- Financial PhraseBank dataset (for training and testing)
- GPT-5 API for data augmentation
- FinBERT model for fine-tuning
- Computing resources for model training and data processing
- Python libraries (e.g., PyTorch, scikit-learn, transformers)

## References

[1] D. Araci, FinBERT: Financial sentiment analysis with pre-trained language models, Aug. 27, 2019. DOI: 10.48550/arXiv.1908.10063.  
[2] B. Li, Y. Hou, and W. Che, "Data augmentation approaches in natural language processing: A survey," AI Open, vol. 3, pp. 71–90, 2022, ISSN: 26666510. DOI: 10.1016/j.aiopen.2022.03.001.  
[3] G. Hinton, O. Vinyals, and J. Dean, Distilling the knowledge in a neural network, Mar. 9, 2015. DOI: 10.48550/arXiv.1503.02531.  
[4] X. Jiao et al., TinyBERT: Distilling BERT for natural language understanding, Oct. 16, 2020. DOI: 10.48550/arXiv.1909.10351.  
[5] V. Mathur, Vrunm/text-classification-financial-phrase-bank, original-date: 2022-11-21T03:23:50Z, Sep. 22, 2025.  
[6] "Introducing GPT-5," Accessed: Oct. 4, 2025. [Online]. Available: https://openai.com/index/introducing-gpt-5/.  
[7] "Introducing meta llama 3: The most capable openly available LLM to date," Accessed: Oct. 4, 2025. [Online]. Available: https://ai.meta.com/blog/meta-llama-3/.
