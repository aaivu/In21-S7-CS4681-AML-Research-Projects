{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi2F6Eo6Hp8z"
      },
      "source": [
        "# **PER (Prioritized Experience Replay) QR-DQN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prioritized Experience Replay (PER) improves the sample efficiency of reinforcement learning by prioritizing experiences that are expected to provide more informative gradients. In standard experience replay, all transitions are stored and sampled uniformly, but with PER, we prioritize transitions based on their TD error or temporal difference (TD) residual. The higher the TD error, the more important that experience is for training, since it indicates that the model's prediction for that state-action pair was significantly off. By sampling transitions with a higher probability, PER ensures that the agent learns more effectively from the most surprising or informative experiences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpgzJ-cTJOFQ",
        "outputId": "0f608709-3f1e-4529-d898-01a797ebfba1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# 1) Install compatible versions (clean), then *force restart* the Python process.\n",
        "#    - NumPy 2.0.2 plays nicely with Gymnasium 0.29.1 and MinAtar 1.0.15.\n",
        "#    - We *don't* touch TensorFlow/numba; we just won't import them.\n",
        "%pip -q install --upgrade --force-reinstall \"numpy==2.0.2\" gymnasium==0.29.1 minatar==1.0.15\n",
        "\n",
        "import os, sys\n",
        "print(\"Restarting runtime now to finalize NumPy ABI ...\")\n",
        "os.kill(os.getpid(), 9)  # <-- forces a clean interpreter (same as Runtime->Restart)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wq3sCUBLcZi",
        "outputId": "f0cca115-51dd-467e-bf03-6876f11dcc55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NumPy: 2.0.2 | Gymnasium: 0.29.1\n",
            "MinAtar Breakout state shape (H,W,C): (10, 10, 4)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np, gymnasium as gym\n",
        "from minatar import Environment as MinAtarBaseEnv\n",
        "print(\"NumPy:\", np.__version__, \"| Gymnasium:\", gym.__version__)\n",
        "\n",
        "# quick smoke test (no training yet)\n",
        "env = MinAtarBaseEnv(\"breakout\", sticky_action_prob=0.1)\n",
        "env.reset(); env.seed(123)\n",
        "s = env.state()   # (H,W,C)\n",
        "print(\"MinAtar Breakout state shape (H,W,C):\", s.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_xM3Qo5Ijxo"
      },
      "source": [
        "Imports, config, and helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X408oqF9IlSx"
      },
      "outputs": [],
      "source": [
        "import os, math, copy, random, collections\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --------------------------\n",
        "# Repro & device\n",
        "# --------------------------\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --------------------------\n",
        "# Epsilon schedule\n",
        "# --------------------------\n",
        "def epsilon_by_step(step, cfg):\n",
        "    # simple linear decay\n",
        "    t = min(1.0, step / max(1, cfg.eps_decay))\n",
        "    return cfg.eps_start + t * (cfg.eps_final - cfg.eps_start)\n",
        "\n",
        "# --------------------------\n",
        "# Config\n",
        "# --------------------------\n",
        "@dataclass\n",
        "class Config:\n",
        "    # RL\n",
        "    gamma: float = 0.99\n",
        "    n_quantiles: int = 51\n",
        "    hidden: int = 128\n",
        "    lr: float = 1e-3\n",
        "    adam_eps: float = 1e-8\n",
        "    grad_clip: float = 10.0\n",
        "\n",
        "    # training\n",
        "    total_steps: int = 200_000\n",
        "    buffer_size: int = 100_000\n",
        "    batch_size: int = 64\n",
        "    learn_start: int = 1_000\n",
        "    target_tau: int = 1_000\n",
        "\n",
        "    # exploration\n",
        "    eps_start: float = 1.0\n",
        "    eps_final: float = 0.01\n",
        "    eps_decay: int = 100_000\n",
        "\n",
        "    # seeds\n",
        "    base_seed: int = 0\n",
        "\n",
        "    # env selection\n",
        "    env_kind: str = \"cartpole\"   # \"cartpole\" or \"minatar\"\n",
        "    game: str = \"breakout\"       # MinAtar game name\n",
        "    sticky: float = 0.1          # MinAtar sticky-action prob\n",
        "\n",
        "# Handy printer\n",
        "def summarize(name, scores: np.ndarray):\n",
        "    s = np.array(scores, dtype=np.float32)\n",
        "    print(f\"{name}: mean={s.mean():.2f}  median={np.median(s):.2f}  std={s.std(ddof=1):.2f}  n={len(s)}  scores={s.round(1)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9NEyGieIosf"
      },
      "source": [
        "Environments (Gymnasium CartPole + MinAtar wrapper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inqVnikfIquM"
      },
      "outputs": [],
      "source": [
        "# import gymnasium as gym\n",
        "# from minatar import Environment as MinAtarBaseEnv\n",
        "\n",
        "# # ------------- Gym CartPole -------------\n",
        "# def make_env_cartpole(seed: int):\n",
        "#     env = gym.make(\"CartPole-v1\")\n",
        "#     env.reset(seed=seed)\n",
        "#     return env\n",
        "\n",
        "# # ------------- MinAtar (Gym-like) -------------\n",
        "# class ActionSpace:\n",
        "#     def __init__(self, n): self.n = n\n",
        "\n",
        "# class MinAtarGymLike:\n",
        "#     \"\"\"\n",
        "#     Wrap MinAtar to feel like Gymnasium:\n",
        "#       - reset(seed) -> (obs, info)\n",
        "#       - step(a)     -> (obs, reward, terminated, truncated, info)\n",
        "#       - action_space.n\n",
        "#       - observation: float32 (C,H,W) in {0,1}\n",
        "#     \"\"\"\n",
        "#     def __init__(self, game=\"breakout\", sticky=0.1, seed=0):\n",
        "#         self.env = MinAtarBaseEnv(game, sticky_action_prob=sticky)\n",
        "#         self._seed = seed\n",
        "#         self.env.reset()      # MinAtar does not take seed in ctor\n",
        "#         self.env.seed(seed)   # separate seeding call\n",
        "#         self.action_space = ActionSpace(self.env.num_actions())\n",
        "#         s = self.env.state()  # (H,W,C)\n",
        "#         H, W, C = s.shape\n",
        "#         self.shape = (C, H, W)\n",
        "\n",
        "#     def reset(self, seed=None):\n",
        "#         if seed is not None:\n",
        "#             self._seed = seed\n",
        "#             self.env.seed(seed)\n",
        "#         self.env.reset()\n",
        "#         s = self.env.state()            # (H,W,C)\n",
        "#         s = np.transpose(s, (2,0,1)).astype(np.float32)\n",
        "#         return s, {}\n",
        "\n",
        "#     def step(self, a):\n",
        "#         r, done = self.env.act(a)       # MinAtar returns (reward, terminal)\n",
        "#         s = self.env.state()\n",
        "#         s = np.transpose(s, (2,0,1)).astype(np.float32)\n",
        "#         return s, float(r), bool(done), False, {}\n",
        "\n",
        "#     def close(self): pass\n",
        "\n",
        "# def make_env_minatar(seed: int, game=\"breakout\", sticky=0.1):\n",
        "#     env = MinAtarGymLike(game=game, sticky=sticky, seed=seed)\n",
        "#     return env\n",
        "\n",
        "# # ------------- Unified factory -------------\n",
        "# def make_env(cfg: Config, seed: int):\n",
        "#     if cfg.env_kind == \"cartpole\":\n",
        "#         return make_env_cartpole(seed)\n",
        "#     elif cfg.env_kind == \"minatar\":\n",
        "#         return make_env_minatar(seed, game=cfg.game, sticky=cfg.sticky)\n",
        "#     else:\n",
        "#         raise ValueError(f\"Unknown env_kind: {cfg.env_kind}\")\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from minatar import Environment as MinAtarBaseEnv\n",
        "\n",
        "def make_env_cartpole(seed: int):\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    env.reset(seed=seed)\n",
        "    return env\n",
        "\n",
        "class ActionSpace:\n",
        "    def __init__(self, n): self.n = n\n",
        "\n",
        "class MinAtarGymLike:\n",
        "    \"\"\"\n",
        "    Wrap MinAtar to feel like Gymnasium:\n",
        "      - reset(seed) -> (obs, info)\n",
        "      - step(a)     -> (obs, reward, terminated, truncated, info)\n",
        "      - action_space.n\n",
        "      - observation: float32 (C,H,W) in {0,1}\n",
        "    \"\"\"\n",
        "    def __init__(self, game=\"breakout\", sticky=0.1, seed=0):\n",
        "        self.env = MinAtarBaseEnv(game, sticky_action_prob=sticky)\n",
        "        self._seed = seed\n",
        "        self.env.reset()\n",
        "        self.env.seed(seed)                    # MinAtar uses a separate seed() call\n",
        "        self.action_space = ActionSpace(self.env.num_actions())\n",
        "        s = self.env.state()                   # (H,W,C)\n",
        "        H, W, C = s.shape\n",
        "        self.shape = (C, H, W)\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        if seed is not None:\n",
        "            self._seed = seed\n",
        "            self.env.seed(seed)\n",
        "        self.env.reset()\n",
        "        s = self.env.state()                   # (H,W,C)\n",
        "        s = np.transpose(s, (2,0,1)).astype(np.float32)   # -> (C,H,W)\n",
        "        return s, {}\n",
        "\n",
        "    def step(self, a):\n",
        "        r, done = self.env.act(a)              # (reward, terminal)\n",
        "        s = self.env.state()\n",
        "        s = np.transpose(s, (2,0,1)).astype(np.float32)\n",
        "        return s, float(r), bool(done), False, {}\n",
        "\n",
        "    def close(self): pass\n",
        "\n",
        "def make_env_minatar(seed: int, game=\"breakout\", sticky=0.1):\n",
        "    return MinAtarGymLike(game=game, sticky=sticky, seed=seed)\n",
        "\n",
        "class Config:\n",
        "    def __init__(self, env_kind=\"minatar\", game=\"breakout\", sticky=0.1):\n",
        "        self.env_kind = env_kind\n",
        "        self.game = game\n",
        "        self.sticky = sticky\n",
        "\n",
        "def make_env(cfg: Config, seed: int):\n",
        "    if cfg.env_kind == \"cartpole\":\n",
        "        return make_env_cartpole(seed)\n",
        "    elif cfg.env_kind == \"minatar\":\n",
        "        return make_env_minatar(seed, game=cfg.game, sticky=cfg.sticky)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown env_kind: {cfg.env_kind}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAd_226EMAvn"
      },
      "source": [
        "QR-DQN networks (MLP for vector obs, Conv for MinAtar), action helper, quantile grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHW86gduMCUp"
      },
      "outputs": [],
      "source": [
        "# --------- Quantile support points (fixed midpoints) ----------\n",
        "def quantile_midpoints(N: int, device=None):\n",
        "    # taus_n = (2i + 1) / (2N), i=0..N-1\n",
        "    i = torch.arange(N, dtype=torch.float32, device=device)\n",
        "    return (2.0 * i + 1.0) / (2.0 * N)\n",
        "\n",
        "# --------- Models ----------\n",
        "class QRDQN_MLP(nn.Module):\n",
        "    def __init__(self, obs_dim, n_actions, n_quantiles=51, hidden=128):\n",
        "        super().__init__()\n",
        "        self.n_actions = n_actions\n",
        "        self.n_quantiles = n_quantiles\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden), nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),  nn.ReLU(),\n",
        "            nn.Linear(hidden, n_actions * n_quantiles)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 1: x = x.unsqueeze(0)\n",
        "        y = self.net(x)\n",
        "        B = y.shape[0]\n",
        "        return y.view(B, self.n_actions, self.n_quantiles)\n",
        "\n",
        "class QRDQN_Conv_MinAtar(nn.Module):\n",
        "    def __init__(self, C, n_actions, n_quantiles=51, hidden=128):\n",
        "        super().__init__()\n",
        "        self.n_actions = n_actions\n",
        "        self.n_quantiles = n_quantiles\n",
        "        self.torso = nn.Sequential(\n",
        "            nn.Conv2d(C, 16, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(32*10*10, hidden), nn.ReLU(),\n",
        "            nn.Linear(hidden, n_actions * n_quantiles)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 3: x = x.unsqueeze(0)\n",
        "        z = self.torso(x)\n",
        "        z = torch.flatten(z, 1)\n",
        "        y = self.head(z)\n",
        "        B = y.shape[0]\n",
        "        return y.view(B, self.n_actions, self.n_quantiles)\n",
        "\n",
        "def build_model_for_env(env, cfg: Config):\n",
        "    if hasattr(env.observation_space, \"shape\") and env.observation_space.shape is not None:\n",
        "        shape = env.observation_space.shape\n",
        "    else:\n",
        "        s0,_ = env.reset()\n",
        "        shape = np.array(s0).shape\n",
        "\n",
        "    if len(shape) == 1:\n",
        "        obs_dim = int(shape[0])\n",
        "        nA = env.action_space.n\n",
        "        net = QRDQN_MLP(obs_dim, nA, cfg.n_quantiles, cfg.hidden)\n",
        "    elif len(shape) == 3:\n",
        "        C,H,W = shape\n",
        "        nA = env.action_space.n\n",
        "        net = QRDQN_Conv_MinAtar(C, nA, cfg.n_quantiles, cfg.hidden)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported observation shape: {shape}\")\n",
        "    return net.to(device)\n",
        "\n",
        "# --------- Action helper (QR mean-greedy with ε) ----------\n",
        "@torch.no_grad()\n",
        "def act_epsilon_greedy(state, online, epsilon: float):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(0, online.n_actions)\n",
        "    s = torch.tensor(state, dtype=torch.float32, device=device)\n",
        "    if s.dim() == 1: s = s.unsqueeze(0)\n",
        "    a = online(s).mean(2).argmax(1).item()\n",
        "    return int(a)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVcESDKCMYkG"
      },
      "source": [
        "Losses (pinball & Huber-quantile) and per-sample loss for PER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXka0qOQMY4K"
      },
      "outputs": [],
      "source": [
        "def pinball_loss(q_pred, q_tgt, taus):\n",
        "    u = q_tgt - q_pred\n",
        "    tau = taus.view(1, -1)\n",
        "    ind = (u.detach() < 0).float()\n",
        "    return (torch.abs(tau - ind) * torch.abs(u)).mean()\n",
        "\n",
        "def huber_quantile_loss(q_pred, q_tgt, taus, kappa=1.0):\n",
        "    u = q_tgt - q_pred\n",
        "    tau = taus.view(1, -1)\n",
        "    ind = (u.detach() < 0).float()\n",
        "    abs_u = u.abs()\n",
        "    hub = torch.where(abs_u <= kappa, 0.5 * u**2, kappa * (abs_u - 0.5 * kappa))\n",
        "    return (torch.abs(tau - ind) * hub / kappa).mean()\n",
        "\n",
        "# Per-sample version for PER weighting\n",
        "def qr_loss_per_sample(q_pred, q_tgt, taus, kappa=1.0, use_huber=True):\n",
        "    u = q_tgt - q_pred\n",
        "    tau = taus.view(1, -1)\n",
        "    ind = (u.detach() < 0).float()\n",
        "    if use_huber and kappa > 0.0:\n",
        "        abs_u = u.abs()\n",
        "        hub = torch.where(abs_u <= kappa, 0.5 * u**2, kappa * (abs_u - 0.5 * kappa))\n",
        "        base = hub / kappa\n",
        "    else:\n",
        "        base = u.abs()\n",
        "    loss_q = torch.abs(tau - ind) * base\n",
        "    return loss_q.mean(dim=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a62yvuoUMaU3"
      },
      "source": [
        "Replay buffers (Uniform + PER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YknlFjWCMb1v"
      },
      "outputs": [],
      "source": [
        "# --------- Uniform replay (for A/B, optional) ----------\n",
        "class ReplayUniform:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.data = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, s,a,r,ns,d):\n",
        "        self.data.append((s.copy(), int(a), float(r), ns.copy(), float(d)))\n",
        "\n",
        "    def sample(self, B: int):\n",
        "        idx = np.random.randint(0, len(self.data), size=B)\n",
        "        batch = [self.data[i] for i in idx]\n",
        "        return idx, batch, np.ones(B, dtype=np.float32)\n",
        "\n",
        "    def update_priorities(self, idxs, new_p): pass\n",
        "    def __len__(self): return len(self.data)\n",
        "\n",
        "# --------- PER (SumTree) ----------\n",
        "class SumTree:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.n = 1\n",
        "        while self.n < capacity: self.n <<= 1\n",
        "        self.size = 0\n",
        "        self.write = 0\n",
        "        self.data = [None] * self.n\n",
        "        self.tree = np.zeros(2*self.n, dtype=np.float32)\n",
        "\n",
        "    def total(self) -> float:\n",
        "        return float(self.tree[1])\n",
        "\n",
        "    def _update(self, idx: int, p: float):\n",
        "        i = idx + self.n\n",
        "        self.tree[i] = p\n",
        "        i //= 2\n",
        "        while i >= 1:\n",
        "            self.tree[i] = self.tree[2*i] + self.tree[2*i+1]\n",
        "            i //= 2\n",
        "\n",
        "    def add(self, p: float, data):\n",
        "        self.data[self.write] = data\n",
        "        self._update(self.write, p)\n",
        "        self.write = (self.write + 1) % self.n\n",
        "        self.size = min(self.size + 1, self.n)\n",
        "\n",
        "    def sample(self, mass: float):\n",
        "        i = 1\n",
        "        while i < self.n:\n",
        "            left = 2*i\n",
        "            if self.tree[left] >= mass:\n",
        "                i = left\n",
        "            else:\n",
        "                mass -= self.tree[left]\n",
        "                i = left+1\n",
        "        idx = i - self.n\n",
        "        return idx, self.data[idx], float(self.tree[i])\n",
        "\n",
        "    def update(self, idx: int, p: float):\n",
        "        self._update(idx, p)\n",
        "\n",
        "class PERReplay:\n",
        "    def __init__(self, capacity, alpha=0.6, beta0=0.4, beta1=1.0, steps=1_000_000, eps=1e-6):\n",
        "        self.tree = SumTree(capacity)\n",
        "        self.alpha = float(alpha)\n",
        "        self.beta0, self.beta1 = float(beta0), float(beta1)\n",
        "        self.steps = int(steps)\n",
        "        self.step = 0\n",
        "        self.eps = float(eps)\n",
        "        self.max_p = 1.0\n",
        "\n",
        "    def push(self, s,a,r,ns,d):\n",
        "        self.tree.add(self.max_p, (s.copy(), int(a), float(r), ns.copy(), float(d)))\n",
        "\n",
        "    def sample(self, B):\n",
        "        self.step += 1\n",
        "        beta = self.beta0 + (self.beta1 - self.beta0) * min(1.0, self.step / self.steps)\n",
        "        total = self.tree.total() + 1e-8\n",
        "        seg = total / B\n",
        "        idxs, batch, probs = [], [], []\n",
        "        for i in range(B):\n",
        "            a, b = seg*i, seg*(i+1)\n",
        "            mass = np.random.uniform(a, b)\n",
        "            idx, data, p = self.tree.sample(mass)\n",
        "            idxs.append(idx); batch.append(data); probs.append(p)\n",
        "        probs = np.asarray(probs, dtype=np.float32) / total\n",
        "        N = max(1, self.tree.size)\n",
        "        w = (N * probs) ** (-beta)\n",
        "        w /= (w.max() + 1e-8)\n",
        "        return idxs, batch, w.astype(np.float32)\n",
        "\n",
        "    def update_priorities(self, idxs, new_p):\n",
        "        for i, p in zip(idxs, new_p):\n",
        "            p_alpha = (abs(float(p)) + self.eps) ** self.alpha\n",
        "            self.tree.update(i, p_alpha)\n",
        "            if p_alpha > self.max_p: self.max_p = p_alpha\n",
        "\n",
        "    def __len__(self): return self.tree.size\n",
        "\n",
        "class PERBuffer:\n",
        "    def __init__(self, capacity, obs_shape, alpha=0.6, eps_prio=1e-6, p_min=1e-3, p_max=10.0):\n",
        "        self.capacity  = capacity\n",
        "        self.alpha     = alpha\n",
        "        self.eps_prio  = eps_prio\n",
        "        self.p_min     = p_min\n",
        "        self.p_max     = p_max\n",
        "        self.ptr       = 0\n",
        "        self.full      = False\n",
        "\n",
        "        self.S  = np.zeros((capacity,)+obs_shape, dtype=np.float32)\n",
        "        self.A  = np.zeros((capacity,), dtype=np.int64)\n",
        "        self.R  = np.zeros((capacity,), dtype=np.float32)\n",
        "        self.NS = np.zeros((capacity,)+obs_shape, dtype=np.float32)\n",
        "        self.D  = np.zeros((capacity,), dtype=np.float32)\n",
        "        self.P  = np.ones((capacity,), dtype=np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.capacity if self.full else self.ptr\n",
        "\n",
        "    def push(self, s, a, r, ns, d, prio=None):\n",
        "        i = self.ptr\n",
        "        self.S[i]  = s\n",
        "        self.A[i]  = a\n",
        "        self.R[i]  = r\n",
        "        self.NS[i] = ns\n",
        "        self.D[i]  = d\n",
        "        if prio is None:\n",
        "            prio = float(self.P[:len(self)].max() if len(self) > 0 else 1.0)\n",
        "        prio = np.clip(prio, self.p_min, self.p_max)\n",
        "        self.P[i] = prio\n",
        "\n",
        "        self.ptr += 1\n",
        "        if self.ptr >= self.capacity:\n",
        "            self.ptr  = 0\n",
        "            self.full = True\n",
        "\n",
        "    def sample(self, B, beta=0.4):\n",
        "        n = len(self)\n",
        "        scaled = self.P[:n] ** self.alpha\n",
        "        probs  = scaled / scaled.sum()\n",
        "        idx    = np.random.choice(n, size=B, p=probs, replace=False)\n",
        "\n",
        "        w = (n * probs[idx]) ** (-beta)\n",
        "        w = w / w.max()\n",
        "        w = w.astype(np.float32)\n",
        "\n",
        "        s  = torch.tensor(self.S[idx],  dtype=torch.float32, device=device)\n",
        "        ns = torch.tensor(self.NS[idx], dtype=torch.float32, device=device)\n",
        "        a  = torch.tensor(self.A[idx],  dtype=torch.int64,   device=device)\n",
        "        r  = torch.tensor(self.R[idx],  dtype=torch.float32, device=device)\n",
        "        d  = torch.tensor(self.D[idx],  dtype=torch.float32, device=device)\n",
        "        w_t= torch.tensor(w,            dtype=torch.float32, device=device)\n",
        "        return (idx, s, a, r, ns, d, w_t)\n",
        "\n",
        "    def update_priorities(self, idx, new_prio):\n",
        "        new_prio = np.asarray(new_prio, dtype=np.float32)\n",
        "        new_prio = np.abs(new_prio) + self.eps_prio\n",
        "        new_prio = np.clip(new_prio, self.p_min, self.p_max)\n",
        "        self.P[idx] = new_prio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZyxyJYwMevL"
      },
      "source": [
        "Training (n=1), evaluation, and multi-seed runner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCP2PGanMfGV"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def greedy_eval(env_maker, cfg: Config, online_net, episodes=10, seed_offset=2025):\n",
        "    def run_one(ep):\n",
        "        env = env_maker(cfg, cfg.base_seed + seed_offset + ep)\n",
        "        s,_ = env.reset()\n",
        "        total = 0.0\n",
        "        for _ in range(5000):\n",
        "            a = act_epsilon_greedy(s, online_net, 0.0)\n",
        "            s, r, term, trunc, _ = env.step(a)\n",
        "            total += r\n",
        "            if term or trunc: break\n",
        "        try: env.close()\n",
        "        except: pass\n",
        "        return total\n",
        "    scores = [run_one(i) for i in range(episodes)]\n",
        "    arr = np.array(scores, dtype=np.float32)\n",
        "    return float(arr.mean()), float(arr.std(ddof=1)), scores\n",
        "\n",
        "def train_qrdqn(run_name: str, cfg: Config, use_per=True, loss_mode=\"pinball\", seed=None):\n",
        "    seed = cfg.base_seed if seed is None else seed\n",
        "    set_seed(seed)\n",
        "    env = make_env(cfg, seed)\n",
        "    online = build_model_for_env(env, cfg)\n",
        "    target = build_model_for_env(env, cfg)\n",
        "    target.load_state_dict(online.state_dict())\n",
        "    opt = torch.optim.Adam(online.parameters(), lr=cfg.lr, eps=cfg.adam_eps)\n",
        "    taus = quantile_midpoints(cfg.n_quantiles, device=device)\n",
        "\n",
        "    # choose replay\n",
        "    if use_per:\n",
        "        rb = PERReplay(cfg.buffer_size, alpha=0.6, beta0=0.4, beta1=1.0, steps=cfg.total_steps, eps=1e-6)\n",
        "    else:\n",
        "        rb = ReplayUniform(cfg.buffer_size)\n",
        "\n",
        "    # prime\n",
        "    s,_ = env.reset(seed=seed)\n",
        "    ep_return = 0.0\n",
        "    recent_returns = []\n",
        "\n",
        "    print(f\"=== Training {run_name} | {'PER' if use_per else 'Uniform'} | loss={loss_mode} | n=1 ===\")\n",
        "    for step in range(1, cfg.total_steps+1):\n",
        "        eps = epsilon_by_step(step, cfg)\n",
        "        a = act_epsilon_greedy(s, online, eps)\n",
        "        ns, r, term, trunc, _ = env.step(a)\n",
        "        d = float(term or trunc)\n",
        "        rb.push(s,a,r,ns,d)\n",
        "        s = ns\n",
        "        ep_return += r\n",
        "\n",
        "        if len(rb) >= cfg.learn_start:\n",
        "            idxs, batch, w = rb.sample(cfg.batch_size)\n",
        "            bs, ba, br, bns, bd = zip(*batch)\n",
        "            s_b  = torch.tensor(np.stack(bs),  dtype=torch.float32, device=device)\n",
        "            ns_b = torch.tensor(np.stack(bns), dtype=torch.float32, device=device)\n",
        "            a_b  = torch.tensor(ba, dtype=torch.int64, device=device)\n",
        "            r_b  = torch.tensor(br, dtype=torch.float32, device=device)\n",
        "            d_b  = torch.tensor(bd, dtype=torch.float32, device=device)\n",
        "            w_b  = torch.tensor(w,  dtype=torch.float32, device=device)\n",
        "\n",
        "            # forward\n",
        "            q_all = online(s_b)\n",
        "            B,A,N = q_all.shape\n",
        "            q_sel = q_all.gather(1, a_b.view(B,1,1).expand(B,1,N)).squeeze(1)\n",
        "            with torch.no_grad():\n",
        "                next_q_online = online(ns_b)\n",
        "                next_a = next_q_online.mean(2).argmax(1, keepdim=True)\n",
        "                next_q_target = target(ns_b).gather(1, next_a.unsqueeze(-1).expand(-1,-1,N)).squeeze(1)\n",
        "                tgt = r_b.unsqueeze(1) + (1.0 - d_b.unsqueeze(1)) * cfg.gamma * next_q_target\n",
        "\n",
        "            if use_per:\n",
        "                per_elem = qr_loss_per_sample(q_sel, tgt, taus, kappa=1.0, use_huber=(loss_mode==\"huber1\"))\n",
        "                loss = (w_b * per_elem).mean()\n",
        "            else:\n",
        "                if loss_mode == \"pinball\":\n",
        "                    loss = pinball_loss(q_sel, tgt, taus)\n",
        "                else:\n",
        "                    loss = huber_quantile_loss(q_sel, tgt, taus, kappa=1.0)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(online.parameters(), cfg.grad_clip)\n",
        "            opt.step()\n",
        "\n",
        "            if use_per:\n",
        "                with torch.no_grad():\n",
        "                    rb.update_priorities(idxs, per_elem.detach().cpu().numpy())\n",
        "\n",
        "        if step % cfg.target_tau == 0:\n",
        "            target.load_state_dict(online.state_dict())\n",
        "\n",
        "        if term or trunc:\n",
        "            recent_returns.append(ep_return)\n",
        "            s,_ = env.reset()\n",
        "            ep_return = 0.0\n",
        "\n",
        "        if step % 5000 == 0:\n",
        "            last10 = np.mean(recent_returns[-10:]) if recent_returns else 0.0\n",
        "            print(f\"[{run_name}] step {step:7d} | buffer={len(rb):6d} | eps={eps:.3f} | recent(10)={last10:.1f}\")\n",
        "\n",
        "    # save & quick eval (10 eps)\n",
        "    ckpt = f\"/content/{run_name}.pt\"\n",
        "    torch.save({\"model\": online.state_dict()}, ckpt)\n",
        "    mean_eval, std_eval, _ = greedy_eval(make_env, cfg, online, episodes=10, seed_offset=2025)\n",
        "    print(f\"Saved: {ckpt} | Mean greedy eval (10 eps): {mean_eval:.2f}\")\n",
        "\n",
        "    try: env.close()\n",
        "    except: pass\n",
        "    return {\"name\": run_name, \"ckpt\": ckpt, \"mean_eval\": mean_eval}\n",
        "\n",
        "def run_multi_seed(tag, cfg: Config, seeds, use_per=True, loss_mode=\"pinball\"):\n",
        "    scores, ckpts = [], []\n",
        "    for s in seeds:\n",
        "        out = train_qrdqn(f\"{tag}_seed{s}\", cfg, use_per=use_per, loss_mode=loss_mode, seed=s)\n",
        "        scores.append(out[\"mean_eval\"]); ckpts.append(out[\"ckpt\"])\n",
        "    return np.array(scores, dtype=np.float32), ckpts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yGMEw9-Mi_T"
      },
      "source": [
        "CartPole with PER vs Uniform (quick sanity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si66dRYzMjTG",
        "outputId": "52462fc9-951f-4a50-dfbf-694bf6914325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CartPole-v1 — Multi-seed (PER n=1 vs Uniform n=1) ===\n",
            "=== Training cp_per_pinball_seed7 | PER | loss=pinball | n=1 ===\n",
            "[cp_per_pinball_seed7] step    5000 | buffer=  5000 | eps=0.802 | recent(10)=25.2\n",
            "[cp_per_pinball_seed7] step   10000 | buffer= 10000 | eps=0.604 | recent(10)=36.4\n",
            "[cp_per_pinball_seed7] step   15000 | buffer= 15000 | eps=0.406 | recent(10)=121.5\n",
            "[cp_per_pinball_seed7] step   20000 | buffer= 20000 | eps=0.208 | recent(10)=200.5\n",
            "[cp_per_pinball_seed7] step   25000 | buffer= 25000 | eps=0.010 | recent(10)=438.0\n",
            "[cp_per_pinball_seed7] step   30000 | buffer= 30000 | eps=0.010 | recent(10)=457.7\n",
            "[cp_per_pinball_seed7] step   35000 | buffer= 32768 | eps=0.010 | recent(10)=200.2\n",
            "[cp_per_pinball_seed7] step   40000 | buffer= 32768 | eps=0.010 | recent(10)=181.7\n",
            "[cp_per_pinball_seed7] step   45000 | buffer= 32768 | eps=0.010 | recent(10)=18.1\n",
            "[cp_per_pinball_seed7] step   50000 | buffer= 32768 | eps=0.010 | recent(10)=9.6\n",
            "Saved: /content/cp_per_pinball_seed7.pt | Mean greedy eval (10 eps): 9.20\n",
            "=== Training cp_per_pinball_seed8 | PER | loss=pinball | n=1 ===\n",
            "[cp_per_pinball_seed8] step    5000 | buffer=  5000 | eps=0.802 | recent(10)=30.0\n",
            "[cp_per_pinball_seed8] step   10000 | buffer= 10000 | eps=0.604 | recent(10)=30.8\n",
            "[cp_per_pinball_seed8] step   15000 | buffer= 15000 | eps=0.406 | recent(10)=74.2\n",
            "[cp_per_pinball_seed8] step   20000 | buffer= 20000 | eps=0.208 | recent(10)=139.9\n",
            "[cp_per_pinball_seed8] step   25000 | buffer= 25000 | eps=0.010 | recent(10)=13.4\n",
            "[cp_per_pinball_seed8] step   30000 | buffer= 30000 | eps=0.010 | recent(10)=9.7\n",
            "[cp_per_pinball_seed8] step   35000 | buffer= 32768 | eps=0.010 | recent(10)=9.6\n",
            "[cp_per_pinball_seed8] step   40000 | buffer= 32768 | eps=0.010 | recent(10)=9.3\n",
            "[cp_per_pinball_seed8] step   45000 | buffer= 32768 | eps=0.010 | recent(10)=9.2\n",
            "[cp_per_pinball_seed8] step   50000 | buffer= 32768 | eps=0.010 | recent(10)=9.5\n",
            "Saved: /content/cp_per_pinball_seed8.pt | Mean greedy eval (10 eps): 9.50\n",
            "=== Training cp_per_pinball_seed9 | PER | loss=pinball | n=1 ===\n",
            "[cp_per_pinball_seed9] step    5000 | buffer=  5000 | eps=0.802 | recent(10)=32.3\n",
            "[cp_per_pinball_seed9] step   10000 | buffer= 10000 | eps=0.604 | recent(10)=36.2\n",
            "[cp_per_pinball_seed9] step   15000 | buffer= 15000 | eps=0.406 | recent(10)=108.5\n",
            "[cp_per_pinball_seed9] step   20000 | buffer= 20000 | eps=0.208 | recent(10)=237.2\n",
            "[cp_per_pinball_seed9] step   25000 | buffer= 25000 | eps=0.010 | recent(10)=398.5\n",
            "[cp_per_pinball_seed9] step   30000 | buffer= 30000 | eps=0.010 | recent(10)=200.6\n",
            "[cp_per_pinball_seed9] step   35000 | buffer= 32768 | eps=0.010 | recent(10)=61.4\n",
            "[cp_per_pinball_seed9] step   40000 | buffer= 32768 | eps=0.010 | recent(10)=11.1\n",
            "[cp_per_pinball_seed9] step   45000 | buffer= 32768 | eps=0.010 | recent(10)=9.8\n",
            "[cp_per_pinball_seed9] step   50000 | buffer= 32768 | eps=0.010 | recent(10)=9.4\n",
            "Saved: /content/cp_per_pinball_seed9.pt | Mean greedy eval (10 eps): 9.20\n",
            "=== Training cp_uni_pinball_seed7 | Uniform | loss=pinball | n=1 ===\n",
            "[cp_uni_pinball_seed7] step    5000 | buffer=  5000 | eps=0.802 | recent(10)=27.5\n",
            "[cp_uni_pinball_seed7] step   10000 | buffer= 10000 | eps=0.604 | recent(10)=60.7\n",
            "[cp_uni_pinball_seed7] step   15000 | buffer= 15000 | eps=0.406 | recent(10)=88.5\n",
            "[cp_uni_pinball_seed7] step   20000 | buffer= 20000 | eps=0.208 | recent(10)=75.5\n",
            "[cp_uni_pinball_seed7] step   25000 | buffer= 20000 | eps=0.010 | recent(10)=11.4\n",
            "[cp_uni_pinball_seed7] step   30000 | buffer= 20000 | eps=0.010 | recent(10)=8.8\n",
            "[cp_uni_pinball_seed7] step   35000 | buffer= 20000 | eps=0.010 | recent(10)=9.9\n",
            "[cp_uni_pinball_seed7] step   40000 | buffer= 20000 | eps=0.010 | recent(10)=8.8\n",
            "[cp_uni_pinball_seed7] step   45000 | buffer= 20000 | eps=0.010 | recent(10)=9.5\n",
            "[cp_uni_pinball_seed7] step   50000 | buffer= 20000 | eps=0.010 | recent(10)=9.4\n",
            "Saved: /content/cp_uni_pinball_seed7.pt | Mean greedy eval (10 eps): 9.50\n",
            "=== Training cp_uni_pinball_seed8 | Uniform | loss=pinball | n=1 ===\n",
            "[cp_uni_pinball_seed8] step    5000 | buffer=  5000 | eps=0.802 | recent(10)=22.6\n",
            "[cp_uni_pinball_seed8] step   10000 | buffer= 10000 | eps=0.604 | recent(10)=42.6\n",
            "[cp_uni_pinball_seed8] step   15000 | buffer= 15000 | eps=0.406 | recent(10)=91.0\n",
            "[cp_uni_pinball_seed8] step   20000 | buffer= 20000 | eps=0.208 | recent(10)=142.3\n",
            "[cp_uni_pinball_seed8] step   25000 | buffer= 20000 | eps=0.010 | recent(10)=172.1\n",
            "[cp_uni_pinball_seed8] step   30000 | buffer= 20000 | eps=0.010 | recent(10)=163.6\n",
            "[cp_uni_pinball_seed8] step   35000 | buffer= 20000 | eps=0.010 | recent(10)=150.0\n",
            "[cp_uni_pinball_seed8] step   40000 | buffer= 20000 | eps=0.010 | recent(10)=156.3\n",
            "[cp_uni_pinball_seed8] step   45000 | buffer= 20000 | eps=0.010 | recent(10)=115.7\n",
            "[cp_uni_pinball_seed8] step   50000 | buffer= 20000 | eps=0.010 | recent(10)=143.2\n",
            "Saved: /content/cp_uni_pinball_seed8.pt | Mean greedy eval (10 eps): 133.20\n",
            "=== Training cp_uni_pinball_seed9 | Uniform | loss=pinball | n=1 ===\n",
            "[cp_uni_pinball_seed9] step    5000 | buffer=  5000 | eps=0.802 | recent(10)=30.9\n",
            "[cp_uni_pinball_seed9] step   10000 | buffer= 10000 | eps=0.604 | recent(10)=29.7\n",
            "[cp_uni_pinball_seed9] step   15000 | buffer= 15000 | eps=0.406 | recent(10)=145.7\n",
            "[cp_uni_pinball_seed9] step   20000 | buffer= 20000 | eps=0.208 | recent(10)=206.7\n",
            "[cp_uni_pinball_seed9] step   25000 | buffer= 20000 | eps=0.010 | recent(10)=280.8\n",
            "[cp_uni_pinball_seed9] step   30000 | buffer= 20000 | eps=0.010 | recent(10)=174.2\n",
            "[cp_uni_pinball_seed9] step   35000 | buffer= 20000 | eps=0.010 | recent(10)=171.2\n",
            "[cp_uni_pinball_seed9] step   40000 | buffer= 20000 | eps=0.010 | recent(10)=143.3\n",
            "[cp_uni_pinball_seed9] step   45000 | buffer= 20000 | eps=0.010 | recent(10)=125.4\n",
            "[cp_uni_pinball_seed9] step   50000 | buffer= 20000 | eps=0.010 | recent(10)=114.3\n",
            "Saved: /content/cp_uni_pinball_seed9.pt | Mean greedy eval (10 eps): 153.60\n",
            "PER (pinball): mean=9.30  median=9.20  std=0.17  n=3  scores=[9.2 9.5 9.2]\n",
            "Uniform (pinball): mean=98.77  median=133.20  std=77.98  n=3  scores=[  9.5 133.2 153.6]\n"
          ]
        }
      ],
      "source": [
        "# CartPole sanity check\n",
        "cfg_cp = Config(\n",
        "    env_kind=\"cartpole\",\n",
        "    total_steps=50_000,\n",
        "    buffer_size=20_000,\n",
        "    batch_size=64,\n",
        "    learn_start=1_000,\n",
        "    target_tau=1_000,\n",
        "    eps_start=1.0, eps_final=0.01, eps_decay=25_000,\n",
        "    n_quantiles=51, hidden=128, lr=1e-3, gamma=0.99,\n",
        "    base_seed=0\n",
        ")\n",
        "\n",
        "seeds = [7,8,9]\n",
        "\n",
        "print(\"=== CartPole-v1 — Multi-seed (PER n=1 vs Uniform n=1) ===\")\n",
        "per_scores, per_ckpts = run_multi_seed(\"cp_per_pinball\", cfg_cp, seeds, use_per=True,  loss_mode=\"pinball\")\n",
        "uni_scores, uni_ckpts = run_multi_seed(\"cp_uni_pinball\", cfg_cp, seeds, use_per=False, loss_mode=\"pinball\")\n",
        "\n",
        "summarize(\"PER (pinball)\", per_scores)\n",
        "summarize(\"Uniform (pinball)\", uni_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8al4z7qXM8j1"
      },
      "source": [
        "MinAtar Breakout with PER (you can add Uniform for A/B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "IDiveveNM87R",
        "outputId": "4b24bcf3-735e-41ee-840f-098b8d76c5a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== MinAtar Breakout — Multi-seed (PER n=1) ===\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MinAtarGymLike' object has no attribute 'observation_space'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2916343369.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=== MinAtar Breakout — Multi-seed (PER n=1) ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mb_per_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_per_ckpts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_multi_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"minatar_breakout_per\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pinball\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Breakout PER (pinball)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_per_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3949737760.py\u001b[0m in \u001b[0;36mrun_multi_seed\u001b[0;34m(tag, cfg, seeds, use_per, loss_mode)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_qrdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{tag}_seed{s}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mean_eval\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mckpts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ckpt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3949737760.py\u001b[0m in \u001b[0;36mtrain_qrdqn\u001b[0;34m(run_name, cfg, use_per, loss_mode, seed)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0monline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model_for_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model_for_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2691430732.py\u001b[0m in \u001b[0;36mbuild_model_for_env\u001b[0;34m(env, cfg)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model_for_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# detect obs shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'MinAtarGymLike' object has no attribute 'observation_space'"
          ]
        }
      ],
      "source": [
        "# MinAtar Breakout\n",
        "cfg_b = Config(\n",
        "    env_kind=\"minatar\",\n",
        "    game=\"breakout\",\n",
        "    sticky=0.1,\n",
        "    total_steps=200_000,\n",
        "    buffer_size=100_000,\n",
        "    batch_size=64,\n",
        "    learn_start=5_000,\n",
        "    target_tau=2_000,\n",
        "    eps_start=1.0, eps_final=0.01, eps_decay=100_000,\n",
        "    n_quantiles=51, hidden=128, lr=1e-3, gamma=0.99,\n",
        "    base_seed=0\n",
        ")\n",
        "\n",
        "seeds = [7,8,9]\n",
        "\n",
        "print(\"=== MinAtar Breakout — Multi-seed (PER n=1) ===\")\n",
        "b_per_scores, b_per_ckpts = run_multi_seed(\"minatar_breakout_per\", cfg_b, seeds, use_per=True, loss_mode=\"pinball\")\n",
        "summarize(\"Breakout PER (pinball)\", b_per_scores)\n",
        "\n",
        "# Optional uniform A/B:\n",
        "print(\"=== MinAtar Breakout — Multi-seed (Uniform n=1) ===\")\n",
        "b_uni_scores, b_uni_ckpts = run_multi_seed(\"minatar_breakout_uni\", cfg_b, seeds, use_per=False, loss_mode=\"pinball\")\n",
        "summarize(\"Breakout Uniform (pinball)\", b_uni_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CslWfL7iNC5A"
      },
      "source": [
        "MinAtar Asterix with PER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQT4WqZ8NDSB"
      },
      "outputs": [],
      "source": [
        "cfg_a = copy.deepcopy(cfg_b)\n",
        "cfg_a.game = \"asterix\"\n",
        "\n",
        "print(\"=== MinAtar Asterix — Multi-seed (PER n=1) ===\")\n",
        "a_per_scores, a_per_ckpts = run_multi_seed(\"minatar_asterix_per\", cfg_a, seeds, use_per=True, loss_mode=\"pinball\")\n",
        "summarize(\"Asterix PER (pinball)\", a_per_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hteYja6JNGKr"
      },
      "source": [
        "Reload & evaluate any checkpoint (more episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxv_3X3sNKGf"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def load_and_eval(ckpt_path: str, cfg: Config, episodes=100):\n",
        "    env = make_env(cfg, cfg.base_seed + 13579)\n",
        "    net = build_model_for_env(env, cfg)\n",
        "    payload = torch.load(ckpt_path, map_location=device)\n",
        "    net.load_state_dict(payload[\"model\"])\n",
        "\n",
        "    m, s, scores = greedy_eval(make_env, cfg, net, episodes=episodes, seed_offset=24680)\n",
        "    print(f\"[Reload check] {os.path.basename(ckpt_path)}: mean={m:.2f} ± {s:.2f} over {episodes} eps\")\n",
        "    try: env.close()\n",
        "    except: pass\n",
        "    return m, s, scores\n",
        "\n",
        "# Example:\n",
        "load_and_eval(b_per_ckpts[0], cfg_b, episodes=300)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_dV8jvCU34f"
      },
      "source": [
        "# **full implementation of QR-DQN (n=1) with fixed PER (priorities from scalar TD-error on mean-Q, IS weights applied per-sample before reduction, clipped priorities), plus the uniform baseline.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw1Sox3kU5NG",
        "outputId": "d99515f5-cb32-44e8-cef6-0d796a8d1cb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "import math, os, random, time\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # env\n",
        "    env_kind: str = \"cartpole\"   # \"cartpole\" | \"minatar\"\n",
        "    game: str    = \"breakout\"    # for MinAtar\n",
        "    sticky: float = 0.1          # for MinAtar\n",
        "    base_seed: int = 7\n",
        "\n",
        "    # algo\n",
        "    total_steps: int = 50_000\n",
        "    buffer_size: int = 32_768\n",
        "    batch_size: int  = 128\n",
        "    gamma: float     = 0.99\n",
        "    n_quantiles: int = 51\n",
        "    hidden: int      = 128\n",
        "    lr: float        = 3e-4\n",
        "    adam_eps: float  = 1e-8\n",
        "    target_tau: int  = 2_000\n",
        "    grad_clip: float = 10.0\n",
        "\n",
        "    # exploration\n",
        "    eps_start: float = 1.0\n",
        "    eps_final: float = 0.01\n",
        "    eps_decay: int   = 25_000\n",
        "\n",
        "    # learning start\n",
        "    learn_start: int = 2_000\n",
        "\n",
        "def set_global_seeds(seed: int):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIwGcQU2U-Ou"
      },
      "source": [
        "Quantile utils + loss (per-sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7PkZ5fJU7tA"
      },
      "outputs": [],
      "source": [
        "def quantile_midpoints(n: int, device=None):\n",
        "    i = torch.arange(n, dtype=torch.float32, device=device)\n",
        "    return (i + 0.5) / n\n",
        "\n",
        "def epsilon_by_step(step: int, cfg: Config):\n",
        "    t = min(1.0, step / max(1, cfg.eps_decay))\n",
        "    return cfg.eps_start + t * (cfg.eps_final - cfg.eps_start)\n",
        "\n",
        "def pinball_loss(q_pred: torch.Tensor,\n",
        "                 q_targ: torch.Tensor,\n",
        "                 taus: torch.Tensor,\n",
        "                 reduce: str = \"mean\") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Standard quantile (pinball) loss.\n",
        "    q_pred, q_targ: [B, N]\n",
        "    taus: [N]\n",
        "    If reduce='none' -> returns per-sample loss [B].\n",
        "    \"\"\"\n",
        "    B, N = q_pred.shape\n",
        "    diff = q_targ.unsqueeze(2) - q_pred.unsqueeze(1)\n",
        "    # Huber-0: plain pinball via absolute value\n",
        "    abs_u = diff.abs()\n",
        "    tau = taus.view(1, 1, N)\n",
        "    weight = torch.where(diff < 0.0, 1.0 - tau, tau)\n",
        "    loss_all = weight * abs_u  # [B, N, N]\n",
        "    # Average over target-quantiles and predicted-quantiles\n",
        "    per_sample = loss_all.mean(dim=(1, 2))  # [B]\n",
        "    if reduce == \"none\":\n",
        "        return per_sample\n",
        "    elif reduce == \"mean\":\n",
        "        return per_sample.mean()\n",
        "    else:\n",
        "        raise ValueError(\"reduce must be 'none' or 'mean'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5GYfyslVB0K"
      },
      "source": [
        "Models (MLP for CartPole, Conv for MinAtar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrnXL7YTVCNJ"
      },
      "outputs": [],
      "source": [
        "class QRDQN(nn.Module):\n",
        "    \"\"\"MLP head (CartPole). Outputs [B, A, N] quantiles.\"\"\"\n",
        "    def __init__(self, obs_dim: int, n_actions: int, n_quantiles: int, hidden: int):\n",
        "        super().__init__()\n",
        "        self.n_actions  = n_actions\n",
        "        self.n_quantiles= n_quantiles\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, hidden), nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden),  nn.ReLU(),\n",
        "            nn.Linear(hidden, n_actions * n_quantiles)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        z = self.net(x)\n",
        "        return z.view(-1, self.n_actions, self.n_quantiles)\n",
        "\n",
        "class QRDQNConv(nn.Module):\n",
        "    \"\"\"Simple Conv torso for MinAtar (C,H,W in {0,1}). Outputs [B, A, N].\"\"\"\n",
        "    def __init__(self, C: int, n_actions: int, n_quantiles: int, hidden: int=128):\n",
        "        super().__init__()\n",
        "        self.n_actions   = n_actions\n",
        "        self.n_quantiles = n_quantiles\n",
        "        self.torso = nn.Sequential(\n",
        "            nn.Conv2d(C, 32, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1), nn.ReLU()\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(32*10*10, hidden), nn.ReLU(),\n",
        "            nn.Linear(hidden, n_actions * n_quantiles)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        h = self.torso(x)\n",
        "        h = h.view(h.size(0), -1)\n",
        "        z = self.head(h)\n",
        "        return z.view(-1, self.n_actions, self.n_quantiles)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYsFBGOXVE_S"
      },
      "source": [
        "Replay buffers (Uniform + PER with fixes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJWI2JaXVFVr"
      },
      "outputs": [],
      "source": [
        "# --- Uniform replay ---\n",
        "\n",
        "class Replay:\n",
        "    def __init__(self, capacity: int, obs_shape: Tuple[int, ...]):\n",
        "        self.capacity = capacity\n",
        "        self.ptr = 0\n",
        "        self.full = False\n",
        "        self.S  = np.zeros((capacity,)+obs_shape, dtype=np.float32)\n",
        "        self.A  = np.zeros(capacity, dtype=np.int64)\n",
        "        self.R  = np.zeros(capacity, dtype=np.float32)\n",
        "        self.NS = np.zeros((capacity,)+obs_shape, dtype=np.float32)\n",
        "        self.D  = np.zeros(capacity, dtype=np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.capacity if self.full else self.ptr\n",
        "\n",
        "    def push(self, s,a,r,ns,d):\n",
        "        i = self.ptr\n",
        "        self.S[i], self.A[i], self.R[i], self.NS[i], self.D[i] = s, a, r, ns, d\n",
        "        self.ptr += 1\n",
        "        if self.ptr >= self.capacity:\n",
        "            self.ptr = 0\n",
        "            self.full = True\n",
        "\n",
        "    def sample(self, B: int):\n",
        "        n = len(self)\n",
        "        idx = np.random.randint(0, n, size=B)\n",
        "        s  = torch.tensor(self.S[idx],  dtype=torch.float32, device=device)\n",
        "        ns = torch.tensor(self.NS[idx], dtype=torch.float32, device=device)\n",
        "        a  = torch.tensor(self.A[idx],  dtype=torch.int64,   device=device)\n",
        "        r  = torch.tensor(self.R[idx],  dtype=torch.float32, device=device)\n",
        "        d  = torch.tensor(self.D[idx],  dtype=torch.float32, device=device)\n",
        "        return idx, s, a, r, ns, d\n",
        "\n",
        "# --- Prioritized replay (fixed) ---\n",
        "\n",
        "class PERBuffer:\n",
        "    \"\"\"\n",
        "    Priorities from scalar TD-error on mean-Q.\n",
        "    IS weights applied per-sample before reduction.\n",
        "    Priorities clipped to [p_min, p_max].\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity, obs_shape, alpha=0.6, eps_prio=1e-6, p_min=1e-3, p_max=10.0):\n",
        "        self.capacity = capacity\n",
        "        self.alpha    = alpha\n",
        "        self.eps_prio = eps_prio\n",
        "        self.p_min    = p_min\n",
        "        self.p_max    = p_max\n",
        "        self.ptr = 0\n",
        "        self.full = False\n",
        "\n",
        "        self.S  = np.zeros((capacity,)+obs_shape, dtype=np.float32)\n",
        "        self.A  = np.zeros(capacity, dtype=np.int64)\n",
        "        self.R  = np.zeros(capacity, dtype=np.float32)\n",
        "        self.NS = np.zeros((capacity,)+obs_shape, dtype=np.float32)\n",
        "        self.D  = np.zeros(capacity, dtype=np.float32)\n",
        "        self.P  = np.ones(capacity, dtype=np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.capacity if self.full else self.ptr\n",
        "\n",
        "    def push(self, s,a,r,ns,d, prio=None):\n",
        "        i = self.ptr\n",
        "        self.S[i], self.A[i], self.R[i], self.NS[i], self.D[i] = s, a, r, ns, d\n",
        "        if prio is None:\n",
        "            prio = float(self.P[:len(self)].max() if len(self) > 0 else 1.0)\n",
        "        self.P[i] = np.clip(prio, self.p_min, self.p_max)\n",
        "        self.ptr += 1\n",
        "        if self.ptr >= self.capacity:\n",
        "            self.ptr = 0\n",
        "            self.full = True\n",
        "\n",
        "    def sample(self, B, beta=0.4):\n",
        "        n = len(self)\n",
        "        scaled = self.P[:n] ** self.alpha\n",
        "        probs  = scaled / scaled.sum()\n",
        "        idx    = np.random.choice(n, size=B, p=probs, replace=False)\n",
        "        w = (n * probs[idx]) ** (-beta)\n",
        "        w = w / w.max()\n",
        "        s  = torch.tensor(self.S[idx],  dtype=torch.float32, device=device)\n",
        "        ns = torch.tensor(self.NS[idx], dtype=torch.float32, device=device)\n",
        "        a  = torch.tensor(self.A[idx],  dtype=torch.int64,   device=device)\n",
        "        r  = torch.tensor(self.R[idx],  dtype=torch.float32, device=device)\n",
        "        d  = torch.tensor(self.D[idx],  dtype=torch.float32, device=device)\n",
        "        w_t= torch.tensor(w,            dtype=torch.float32, device=device)\n",
        "        return idx, s, a, r, ns, d, w_t\n",
        "\n",
        "    def update_priorities(self, idx, new_prio):\n",
        "        new_prio = np.asarray(new_prio, dtype=np.float32)\n",
        "        new_prio = np.abs(new_prio) + self.eps_prio\n",
        "        new_prio = np.clip(new_prio, self.p_min, self.p_max)\n",
        "        self.P[idx] = new_prio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gYN_AbcVIdL"
      },
      "source": [
        "Environments (CartPole + MinAtar wrapper) + factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd860DnPVIwu"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from minatar import Environment as MinAtarEnv\n",
        "\n",
        "def make_env_cartpole(seed: int):\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    env.reset(seed=seed)\n",
        "    return env\n",
        "\n",
        "class ActionSpace:\n",
        "    def __init__(self, n): self.n = n\n",
        "\n",
        "class MinAtarGymLike:\n",
        "    \"\"\"\n",
        "    Wrap MinAtar to emulate Gymnasium API:\n",
        "    - reset(seed)->(obs, info)\n",
        "    - step(a)->(obs, reward, terminated, truncated, info)\n",
        "    - action_space.n\n",
        "    - obs is float32 (C,H,W) in {0,1}\n",
        "    \"\"\"\n",
        "    def __init__(self, game=\"breakout\", sticky=0.1, seed=0):\n",
        "        self.env = MinAtarEnv(game, sticky_action_prob=sticky)\n",
        "        self.env.reset()\n",
        "        self.env.seed(seed)\n",
        "        self.action_space = ActionSpace(self.env.num_actions())\n",
        "        s = self.env.state()\n",
        "        H, W, C = s.shape\n",
        "        self.shape = (C, H, W)\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        if seed is not None:\n",
        "            self.env.seed(seed)\n",
        "        self.env.reset()\n",
        "        s = self.env.state()\n",
        "        s = np.transpose(s, (2,0,1)).astype(np.float32)\n",
        "        return s, {}\n",
        "\n",
        "    def step(self, a):\n",
        "        r, done = self.env.act(a)\n",
        "        s = self.env.state()\n",
        "        s = np.transpose(s, (2,0,1)).astype(np.float32)\n",
        "        return s, float(r), bool(done), False, {}\n",
        "\n",
        "    def close(self): pass\n",
        "\n",
        "def make_env(cfg: Config, seed: int):\n",
        "    if cfg.env_kind == \"cartpole\":\n",
        "        return make_env_cartpole(seed)\n",
        "    elif cfg.env_kind == \"minatar\":\n",
        "        return MinAtarGymLike(cfg.game, cfg.sticky, seed)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown env_kind: {cfg.env_kind}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRfdvl6FVL4-"
      },
      "source": [
        "Policy, action, and eval helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez1onEuqVMQI"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def act_epsilon_greedy(state, net, epsilon: float):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(0, net.n_actions)\n",
        "    if state.ndim == 1:\n",
        "        s = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    else:\n",
        "        s = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    q = net(s)\n",
        "    a = q.mean(2).argmax(1)    # mean-over-quantiles\n",
        "    return int(a.item())\n",
        "\n",
        "@torch.no_grad()\n",
        "def greedy_rollout(env, net, max_steps=5000):\n",
        "    s,_ = env.reset()\n",
        "    total = 0.0\n",
        "    for _ in range(max_steps):\n",
        "        a = act_epsilon_greedy(s, net, 0.0)\n",
        "        s, r, term, trunc, _ = env.step(a)\n",
        "        total += r\n",
        "        if term or trunc: break\n",
        "    return float(total)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model_greedy(net, cfg: Config, episodes=10, seed_base=12_345, max_steps=5000):\n",
        "    env = make_env(cfg, seed_base)\n",
        "    scores = []\n",
        "    for i in range(episodes):\n",
        "        try:\n",
        "            s,_ = env.reset(seed=seed_base + i)\n",
        "        except TypeError:\n",
        "            s,_ = env.reset()\n",
        "        total = 0.0\n",
        "        for _ in range(max_steps):\n",
        "            a = act_epsilon_greedy(s, net, 0.0)\n",
        "            s, r, term, trunc, _ = env.step(a)\n",
        "            total += r\n",
        "            if term or trunc: break\n",
        "        scores.append(float(total))\n",
        "    env.close()\n",
        "    arr = np.array(scores, dtype=np.float32)\n",
        "    return float(arr.mean()), float(arr.std(ddof=1)), scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM12kLjKVPLH"
      },
      "source": [
        "Training — Uniform vs PER (fixed) (n=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0RtT9KVVPko"
      },
      "outputs": [],
      "source": [
        "def build_net_for_env(cfg: Config, env):\n",
        "    if cfg.env_kind == \"cartpole\":\n",
        "        obs_dim = env.observation_space.shape[0]\n",
        "        nA = env.action_space.n\n",
        "        net = QRDQN(obs_dim, nA, cfg.n_quantiles, cfg.hidden).to(device)\n",
        "        obs_shape = (obs_dim,)\n",
        "    else:\n",
        "        s0,_ = env.reset()\n",
        "        C,H,W = s0.shape\n",
        "        nA = env.action_space.n\n",
        "        net = QRDQNConv(C, nA, cfg.n_quantiles, cfg.hidden).to(device)\n",
        "        obs_shape = (C,H,W)\n",
        "    return net, obs_shape, nA\n",
        "\n",
        "def train_uniform(run_name: str, cfg: Config, seed=0, log_eval_every=5_000):\n",
        "    set_global_seeds(cfg.base_seed + seed)\n",
        "    env = make_env(cfg, cfg.base_seed + seed)\n",
        "    online, obs_shape, nA = build_net_for_env(cfg, env)\n",
        "    target = build_net_for_env(cfg, env)[0]\n",
        "    target.load_state_dict(online.state_dict())\n",
        "\n",
        "    opt  = torch.optim.Adam(online.parameters(), lr=cfg.lr, eps=cfg.adam_eps)\n",
        "    taus = quantile_midpoints(cfg.n_quantiles, device=device)\n",
        "\n",
        "    rb = Replay(cfg.buffer_size, obs_shape)\n",
        "\n",
        "    s,_ = env.reset()\n",
        "    for step in range(1, cfg.total_steps+1):\n",
        "        eps = epsilon_by_step(step, cfg)\n",
        "        a = act_epsilon_greedy(s, online, eps)\n",
        "        ns, r, term, trunc, _ = env.step(a)\n",
        "        d = float(term or trunc)\n",
        "        rb.push(s,a,r,ns,d)\n",
        "        s = ns\n",
        "\n",
        "        if len(rb) >= cfg.learn_start:\n",
        "            _, bs, ba, br, bns, bd = rb.sample(cfg.batch_size)\n",
        "            with torch.no_grad():\n",
        "                next_q_online = online(bns)\n",
        "                next_a = next_q_online.mean(2).argmax(1, keepdim=True)\n",
        "                next_q_target = target(bns).gather(\n",
        "                    1, next_a.unsqueeze(-1).expand(-1,-1,cfg.n_quantiles)\n",
        "                ).squeeze(1)\n",
        "                target_q = br.unsqueeze(1) + (1.0 - bd.unsqueeze(1)) * cfg.gamma * next_q_target\n",
        "\n",
        "            q_all = online(bs)\n",
        "            q_chosen = q_all.gather(1, ba.view(-1,1,1).expand(-1,1,cfg.n_quantiles)).squeeze(1)\n",
        "\n",
        "            loss = pinball_loss(q_chosen, target_q, taus, reduce=\"mean\")\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(online.parameters(), cfg.grad_clip)\n",
        "            opt.step()\n",
        "\n",
        "        if step % cfg.target_tau == 0:\n",
        "            target.load_state_dict(online.state_dict())\n",
        "\n",
        "        if term or trunc:\n",
        "            s,_ = env.reset()\n",
        "\n",
        "        if step % log_eval_every == 0:\n",
        "            print(f\"[{run_name}] step {step:7d} | buffer={len(rb):6d} | eps={eps:.3f}\")\n",
        "\n",
        "    mean10, _, _ = eval_model_greedy(online, cfg, episodes=10)\n",
        "    path = f\"/content/{run_name}.pt\"\n",
        "    torch.save(online.state_dict(), path)\n",
        "    env.close()\n",
        "    print(f\"Saved: {path} | Mean greedy eval (10 eps): {mean10:.2f}\")\n",
        "    return {\"ckpt\": path, \"mean10\": mean10}\n",
        "\n",
        "def train_per_fixed(run_name: str, cfg: Config, seed=0,\n",
        "                    alpha=0.6, beta_start=0.4, beta_end=1.0,\n",
        "                    log_eval_every=5_000):\n",
        "    set_global_seeds(cfg.base_seed + seed)\n",
        "    env = make_env(cfg, cfg.base_seed + seed)\n",
        "    online, obs_shape, nA = build_net_for_env(cfg, env)\n",
        "    target = build_net_for_env(cfg, env)[0]\n",
        "    target.load_state_dict(online.state_dict())\n",
        "\n",
        "    opt  = torch.optim.Adam(online.parameters(), lr=cfg.lr, eps=cfg.adam_eps)\n",
        "    taus = quantile_midpoints(cfg.n_quantiles, device=device)\n",
        "\n",
        "    rb = PERBuffer(cfg.buffer_size, obs_shape, alpha=alpha, eps_prio=1e-6, p_min=1e-3, p_max=10.0)\n",
        "\n",
        "    s,_ = env.reset()\n",
        "\n",
        "    def beta_at(step):\n",
        "        t = min(1.0, step / max(1, cfg.total_steps))\n",
        "        return beta_start + t * (beta_end - beta_start)\n",
        "\n",
        "    for step in range(1, cfg.total_steps+1):\n",
        "        eps = epsilon_by_step(step, cfg)\n",
        "        a = act_epsilon_greedy(s, online, eps)\n",
        "        ns, r, term, trunc, _ = env.step(a)\n",
        "        d = float(term or trunc)\n",
        "        rb.push(s,a,r,ns,d)\n",
        "        s = ns\n",
        "\n",
        "        if len(rb) >= cfg.learn_start:\n",
        "            beta = beta_at(step)\n",
        "            idx, bs, ba, br, bns, bd, is_w = rb.sample(cfg.batch_size, beta=beta)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                next_q_online = online(bns)\n",
        "                next_a = next_q_online.mean(2).argmax(1, keepdim=True)\n",
        "                next_q_target = target(bns).gather(\n",
        "                    1, next_a.unsqueeze(-1).expand(-1,-1,cfg.n_quantiles)\n",
        "                ).squeeze(1)\n",
        "                target_q = br.unsqueeze(1) + (1.0 - bd.unsqueeze(1)) * cfg.gamma * next_q_target\n",
        "\n",
        "            q_all = online(bs)\n",
        "            q_chosen = q_all.gather(1, ba.view(-1,1,1).expand(-1,1,cfg.n_quantiles)).squeeze(1)\n",
        "\n",
        "            # Priorities from scalar mean-Q TD-error\n",
        "            with torch.no_grad():\n",
        "                q_mean      = q_chosen.mean(dim=1)\n",
        "                target_mean = target_q.mean(dim=1)\n",
        "                td_err_mean = (target_mean - q_mean).abs().detach().cpu().numpy()\n",
        "\n",
        "            # Per-sample quantile loss, IS-weighted mean\n",
        "            per_sample = pinball_loss(q_chosen, target_q, taus, reduce='none')\n",
        "            loss = (is_w * per_sample).mean()\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(online.parameters(), cfg.grad_clip)\n",
        "            opt.step()\n",
        "\n",
        "            if step % cfg.target_tau == 0:\n",
        "                target.load_state_dict(online.state_dict())\n",
        "\n",
        "            rb.update_priorities(idx, td_err_mean)\n",
        "\n",
        "        if term or trunc:\n",
        "            s,_ = env.reset()\n",
        "\n",
        "        if step % log_eval_every == 0:\n",
        "            print(f\"[{run_name}] step {step:7d} | buffer={len(rb):6d} | eps={eps:.3f}\")\n",
        "\n",
        "    mean10, _, _ = eval_model_greedy(online, cfg, episodes=10)\n",
        "    path = f\"/content/{run_name}.pt\"\n",
        "    torch.save(online.state_dict(), path)\n",
        "    env.close()\n",
        "    print(f\"Saved: {path} | Mean greedy eval (10 eps): {mean10:.2f}\")\n",
        "    return {\"ckpt\": path, \"mean10\": mean10}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTlFF1dIVS8R"
      },
      "source": [
        "Simple runner (CartPole + MinAtar examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL3tOZUXVTQD",
        "outputId": "84d9961d-4cd6-46e9-9915-2eff4ce73720"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CartPole-v1 — PER (fixed) vs Uniform (n=1) ===\n",
            "[cp_per_fixed_pinball_seed7] step    5000 | buffer=  5000 | eps=0.802\n",
            "[cp_per_fixed_pinball_seed7] step   10000 | buffer= 10000 | eps=0.604\n",
            "[cp_per_fixed_pinball_seed7] step   15000 | buffer= 15000 | eps=0.406\n",
            "[cp_per_fixed_pinball_seed7] step   20000 | buffer= 20000 | eps=0.208\n",
            "[cp_per_fixed_pinball_seed7] step   25000 | buffer= 25000 | eps=0.010\n",
            "[cp_per_fixed_pinball_seed7] step   30000 | buffer= 30000 | eps=0.010\n",
            "[cp_per_fixed_pinball_seed7] step   35000 | buffer= 32768 | eps=0.010\n",
            "[cp_per_fixed_pinball_seed7] step   40000 | buffer= 32768 | eps=0.010\n",
            "[cp_per_fixed_pinball_seed7] step   45000 | buffer= 32768 | eps=0.010\n",
            "[cp_per_fixed_pinball_seed7] step   50000 | buffer= 32768 | eps=0.010\n",
            "Saved: /content/cp_per_fixed_pinball_seed7.pt | Mean greedy eval (10 eps): 294.30\n",
            "[cp_uniform_pinball_seed7] step    5000 | buffer=  5000 | eps=0.802\n",
            "[cp_uniform_pinball_seed7] step   10000 | buffer= 10000 | eps=0.604\n",
            "[cp_uniform_pinball_seed7] step   15000 | buffer= 15000 | eps=0.406\n",
            "[cp_uniform_pinball_seed7] step   20000 | buffer= 20000 | eps=0.208\n",
            "[cp_uniform_pinball_seed7] step   25000 | buffer= 25000 | eps=0.010\n",
            "[cp_uniform_pinball_seed7] step   30000 | buffer= 30000 | eps=0.010\n",
            "[cp_uniform_pinball_seed7] step   35000 | buffer= 32768 | eps=0.010\n",
            "[cp_uniform_pinball_seed7] step   40000 | buffer= 32768 | eps=0.010\n",
            "[cp_uniform_pinball_seed7] step   45000 | buffer= 32768 | eps=0.010\n",
            "[cp_uniform_pinball_seed7] step   50000 | buffer= 32768 | eps=0.010\n",
            "Saved: /content/cp_uniform_pinball_seed7.pt | Mean greedy eval (10 eps): 164.40\n",
            "\n",
            "=== MinAtar Breakout — PER (fixed) vs Uniform (n=1) ===\n",
            "[minatar_breakout_per_seed7] step    5000 | buffer=  5000 | eps=0.951\n",
            "[minatar_breakout_per_seed7] step   10000 | buffer= 10000 | eps=0.901\n",
            "[minatar_breakout_per_seed7] step   15000 | buffer= 15000 | eps=0.852\n",
            "[minatar_breakout_per_seed7] step   20000 | buffer= 20000 | eps=0.802\n",
            "[minatar_breakout_per_seed7] step   25000 | buffer= 25000 | eps=0.752\n",
            "[minatar_breakout_per_seed7] step   30000 | buffer= 30000 | eps=0.703\n",
            "[minatar_breakout_per_seed7] step   35000 | buffer= 35000 | eps=0.653\n",
            "[minatar_breakout_per_seed7] step   40000 | buffer= 40000 | eps=0.604\n",
            "[minatar_breakout_per_seed7] step   45000 | buffer= 45000 | eps=0.554\n",
            "[minatar_breakout_per_seed7] step   50000 | buffer= 50000 | eps=0.505\n",
            "[minatar_breakout_per_seed7] step   55000 | buffer= 55000 | eps=0.456\n",
            "[minatar_breakout_per_seed7] step   60000 | buffer= 60000 | eps=0.406\n",
            "[minatar_breakout_per_seed7] step   65000 | buffer= 65000 | eps=0.357\n",
            "[minatar_breakout_per_seed7] step   70000 | buffer= 70000 | eps=0.307\n",
            "[minatar_breakout_per_seed7] step   75000 | buffer= 75000 | eps=0.258\n",
            "[minatar_breakout_per_seed7] step   80000 | buffer= 80000 | eps=0.208\n",
            "[minatar_breakout_per_seed7] step   85000 | buffer= 85000 | eps=0.158\n",
            "[minatar_breakout_per_seed7] step   90000 | buffer= 90000 | eps=0.109\n",
            "[minatar_breakout_per_seed7] step   95000 | buffer= 95000 | eps=0.059\n",
            "[minatar_breakout_per_seed7] step  100000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  105000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  110000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  115000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  120000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  125000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  130000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  135000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  140000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  145000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  150000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  155000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  160000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  165000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  170000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  175000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  180000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  185000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  190000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  195000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_per_seed7] step  200000 | buffer=100000 | eps=0.010\n",
            "Saved: /content/minatar_breakout_per_seed7.pt | Mean greedy eval (10 eps): 9.90\n",
            "[minatar_breakout_uni_seed7] step    5000 | buffer=  5000 | eps=0.951\n",
            "[minatar_breakout_uni_seed7] step   10000 | buffer= 10000 | eps=0.901\n",
            "[minatar_breakout_uni_seed7] step   15000 | buffer= 15000 | eps=0.852\n",
            "[minatar_breakout_uni_seed7] step   20000 | buffer= 20000 | eps=0.802\n",
            "[minatar_breakout_uni_seed7] step   25000 | buffer= 25000 | eps=0.752\n",
            "[minatar_breakout_uni_seed7] step   30000 | buffer= 30000 | eps=0.703\n",
            "[minatar_breakout_uni_seed7] step   35000 | buffer= 35000 | eps=0.653\n",
            "[minatar_breakout_uni_seed7] step   40000 | buffer= 40000 | eps=0.604\n",
            "[minatar_breakout_uni_seed7] step   45000 | buffer= 45000 | eps=0.554\n",
            "[minatar_breakout_uni_seed7] step   50000 | buffer= 50000 | eps=0.505\n",
            "[minatar_breakout_uni_seed7] step   55000 | buffer= 55000 | eps=0.456\n",
            "[minatar_breakout_uni_seed7] step   60000 | buffer= 60000 | eps=0.406\n",
            "[minatar_breakout_uni_seed7] step   65000 | buffer= 65000 | eps=0.357\n",
            "[minatar_breakout_uni_seed7] step   70000 | buffer= 70000 | eps=0.307\n",
            "[minatar_breakout_uni_seed7] step   75000 | buffer= 75000 | eps=0.258\n",
            "[minatar_breakout_uni_seed7] step   80000 | buffer= 80000 | eps=0.208\n",
            "[minatar_breakout_uni_seed7] step   85000 | buffer= 85000 | eps=0.158\n",
            "[minatar_breakout_uni_seed7] step   90000 | buffer= 90000 | eps=0.109\n",
            "[minatar_breakout_uni_seed7] step   95000 | buffer= 95000 | eps=0.059\n",
            "[minatar_breakout_uni_seed7] step  100000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  105000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  110000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  115000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  120000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  125000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  130000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  135000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  140000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  145000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  150000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  155000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  160000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  165000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  170000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  175000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  180000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  185000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  190000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  195000 | buffer=100000 | eps=0.010\n",
            "[minatar_breakout_uni_seed7] step  200000 | buffer=100000 | eps=0.010\n",
            "Saved: /content/minatar_breakout_uni_seed7.pt | Mean greedy eval (10 eps): 11.70\n"
          ]
        }
      ],
      "source": [
        "# --- runs ---\n",
        "\n",
        "# CartPole settings\n",
        "cfg_cp = Config(env_kind=\"cartpole\",\n",
        "                total_steps=50_000,\n",
        "                buffer_size=32_768,\n",
        "                batch_size=128,\n",
        "                eps_start=1.0, eps_final=0.01, eps_decay=25_000,\n",
        "                target_tau=2_000)\n",
        "\n",
        "print(\"=== CartPole-v1 — PER (fixed) vs Uniform (n=1) ===\")\n",
        "per_out = train_per_fixed(\"cp_per_fixed_pinball_seed7\", cfg_cp, seed=7)\n",
        "uni_out = train_uniform  (\"cp_uniform_pinball_seed7\",  cfg_cp, seed=7)\n",
        "\n",
        "# MinAtar settings (Breakout)\n",
        "cfg_ma = Config(env_kind=\"minatar\",\n",
        "                game=\"breakout\", sticky=0.1,\n",
        "                total_steps=200_000,\n",
        "                buffer_size=100_000,\n",
        "                batch_size=128,\n",
        "                eps_start=1.0, eps_final=0.01, eps_decay=100_000,\n",
        "                target_tau=2_000)\n",
        "\n",
        "print(\"\\n=== MinAtar Breakout — PER (fixed) vs Uniform (n=1) ===\")\n",
        "per_b = train_per_fixed(\"minatar_breakout_per_seed7\", cfg_ma, seed=7)\n",
        "uni_b = train_uniform  (\"minatar_breakout_uni_seed7\", cfg_ma, seed=7)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfo_7i1IVXZL"
      },
      "source": [
        "Evaluate any saved checkpoint (greedy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8R3AankVXxZ",
        "outputId": "dde22162-1439-493e-f280-793e75f8e7c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[EVAL] cp_per_fixed_pinball_seed7.pt: mean=284.27 ± 114.73  (n=1000)\n",
            "[EVAL] cp_uniform_pinball_seed7.pt: mean=162.24 ± 5.88  (n=1000)\n",
            "[EVAL] minatar_breakout_per_seed7.pt: mean=10.35 ± 5.70  (n=1000)\n",
            "[EVAL] minatar_breakout_uni_seed7.pt: mean=13.05 ± 8.21  (n=1000)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(13.053999900817871,\n",
              " 8.2127046585083,\n",
              " [17.0,\n",
              "  23.0,\n",
              "  24.0,\n",
              "  7.0,\n",
              "  24.0,\n",
              "  3.0,\n",
              "  1.0,\n",
              "  17.0,\n",
              "  1.0,\n",
              "  5.0,\n",
              "  3.0,\n",
              "  24.0,\n",
              "  12.0,\n",
              "  26.0,\n",
              "  23.0,\n",
              "  1.0,\n",
              "  36.0,\n",
              "  14.0,\n",
              "  11.0,\n",
              "  6.0,\n",
              "  17.0,\n",
              "  17.0,\n",
              "  22.0,\n",
              "  6.0,\n",
              "  0.0,\n",
              "  4.0,\n",
              "  13.0,\n",
              "  26.0,\n",
              "  11.0,\n",
              "  11.0,\n",
              "  5.0,\n",
              "  3.0,\n",
              "  5.0,\n",
              "  14.0,\n",
              "  11.0,\n",
              "  11.0,\n",
              "  7.0,\n",
              "  21.0,\n",
              "  1.0,\n",
              "  5.0,\n",
              "  25.0,\n",
              "  2.0,\n",
              "  7.0,\n",
              "  3.0,\n",
              "  25.0,\n",
              "  21.0,\n",
              "  7.0,\n",
              "  27.0,\n",
              "  22.0,\n",
              "  12.0,\n",
              "  24.0,\n",
              "  3.0,\n",
              "  1.0,\n",
              "  25.0,\n",
              "  28.0,\n",
              "  0.0,\n",
              "  13.0,\n",
              "  9.0,\n",
              "  8.0,\n",
              "  10.0,\n",
              "  3.0,\n",
              "  8.0,\n",
              "  11.0,\n",
              "  18.0,\n",
              "  14.0,\n",
              "  12.0,\n",
              "  12.0,\n",
              "  13.0,\n",
              "  6.0,\n",
              "  7.0,\n",
              "  24.0,\n",
              "  14.0,\n",
              "  12.0,\n",
              "  12.0,\n",
              "  6.0,\n",
              "  18.0,\n",
              "  21.0,\n",
              "  26.0,\n",
              "  12.0,\n",
              "  18.0,\n",
              "  5.0,\n",
              "  13.0,\n",
              "  14.0,\n",
              "  22.0,\n",
              "  14.0,\n",
              "  7.0,\n",
              "  2.0,\n",
              "  17.0,\n",
              "  8.0,\n",
              "  4.0,\n",
              "  26.0,\n",
              "  2.0,\n",
              "  12.0,\n",
              "  7.0,\n",
              "  23.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  1.0,\n",
              "  16.0,\n",
              "  14.0,\n",
              "  9.0,\n",
              "  7.0,\n",
              "  8.0,\n",
              "  8.0,\n",
              "  5.0,\n",
              "  22.0,\n",
              "  0.0,\n",
              "  10.0,\n",
              "  26.0,\n",
              "  7.0,\n",
              "  3.0,\n",
              "  26.0,\n",
              "  7.0,\n",
              "  14.0,\n",
              "  5.0,\n",
              "  7.0,\n",
              "  22.0,\n",
              "  24.0,\n",
              "  5.0,\n",
              "  12.0,\n",
              "  11.0,\n",
              "  17.0,\n",
              "  12.0,\n",
              "  3.0,\n",
              "  3.0,\n",
              "  4.0,\n",
              "  3.0,\n",
              "  25.0,\n",
              "  11.0,\n",
              "  10.0,\n",
              "  11.0,\n",
              "  3.0,\n",
              "  23.0,\n",
              "  18.0,\n",
              "  12.0,\n",
              "  35.0,\n",
              "  5.0,\n",
              "  18.0,\n",
              "  21.0,\n",
              "  10.0,\n",
              "  26.0,\n",
              "  3.0,\n",
              "  24.0,\n",
              "  5.0,\n",
              "  7.0,\n",
              "  1.0,\n",
              "  17.0,\n",
              "  16.0,\n",
              "  6.0,\n",
              "  11.0,\n",
              "  15.0,\n",
              "  6.0,\n",
              "  7.0,\n",
              "  11.0,\n",
              "  1.0,\n",
              "  24.0,\n",
              "  17.0,\n",
              "  24.0,\n",
              "  5.0,\n",
              "  5.0,\n",
              "  1.0,\n",
              "  11.0,\n",
              "  23.0,\n",
              "  17.0,\n",
              "  24.0,\n",
              "  17.0,\n",
              "  17.0,\n",
              "  11.0,\n",
              "  14.0,\n",
              "  2.0,\n",
              "  7.0,\n",
              "  13.0,\n",
              "  7.0,\n",
              "  21.0,\n",
              "  5.0,\n",
              "  2.0,\n",
              "  8.0,\n",
              "  4.0,\n",
              "  14.0,\n",
              "  18.0,\n",
              "  27.0,\n",
              "  17.0,\n",
              "  26.0,\n",
              "  14.0,\n",
              "  14.0,\n",
              "  12.0,\n",
              "  13.0,\n",
              "  13.0,\n",
              "  24.0,\n",
              "  7.0,\n",
              "  4.0,\n",
              "  18.0,\n",
              "  2.0,\n",
              "  1.0,\n",
              "  24.0,\n",
              "  24.0,\n",
              "  1.0,\n",
              "  10.0,\n",
              "  4.0,\n",
              "  5.0,\n",
              "  18.0,\n",
              "  28.0,\n",
              "  13.0,\n",
              "  10.0,\n",
              "  9.0,\n",
              "  21.0,\n",
              "  24.0,\n",
              "  22.0,\n",
              "  26.0,\n",
              "  5.0,\n",
              "  12.0,\n",
              "  18.0,\n",
              "  5.0,\n",
              "  17.0,\n",
              "  24.0,\n",
              "  7.0,\n",
              "  1.0,\n",
              "  26.0,\n",
              "  19.0,\n",
              "  14.0,\n",
              "  10.0,\n",
              "  7.0,\n",
              "  4.0,\n",
              "  3.0,\n",
              "  23.0,\n",
              "  22.0,\n",
              "  14.0,\n",
              "  22.0,\n",
              "  0.0,\n",
              "  7.0,\n",
              "  14.0,\n",
              "  16.0,\n",
              "  19.0,\n",
              "  11.0,\n",
              "  5.0,\n",
              "  18.0,\n",
              "  11.0,\n",
              "  3.0,\n",
              "  10.0,\n",
              "  5.0,\n",
              "  8.0,\n",
              "  33.0,\n",
              "  10.0,\n",
              "  28.0,\n",
              "  3.0,\n",
              "  15.0,\n",
              "  10.0,\n",
              "  27.0,\n",
              "  5.0,\n",
              "  6.0,\n",
              "  0.0,\n",
              "  2.0,\n",
              "  23.0,\n",
              "  17.0,\n",
              "  10.0,\n",
              "  11.0,\n",
              "  12.0,\n",
              "  26.0,\n",
              "  11.0,\n",
              "  1.0,\n",
              "  4.0,\n",
              "  3.0,\n",
              "  10.0,\n",
              "  0.0,\n",
              "  14.0,\n",
              "  10.0,\n",
              "  15.0,\n",
              "  1.0,\n",
              "  11.0,\n",
              "  3.0,\n",
              "  14.0,\n",
              "  13.0,\n",
              "  10.0,\n",
              "  11.0,\n",
              "  18.0,\n",
              "  22.0,\n",
              "  14.0,\n",
              "  5.0,\n",
              "  14.0,\n",
              "  16.0,\n",
              "  18.0,\n",
              "  13.0,\n",
              "  25.0,\n",
              "  11.0,\n",
              "  4.0,\n",
              "  13.0,\n",
              "  26.0,\n",
              "  7.0,\n",
              "  6.0,\n",
              "  7.0,\n",
              "  10.0,\n",
              "  25.0,\n",
              "  5.0,\n",
              "  9.0,\n",
              "  10.0,\n",
              "  22.0,\n",
              "  11.0,\n",
              "  11.0,\n",
              "  20.0,\n",
              "  25.0,\n",
              "  10.0,\n",
              "  8.0,\n",
              "  18.0,\n",
              "  13.0,\n",
              "  10.0,\n",
              "  3.0,\n",
              "  17.0,\n",
              "  22.0,\n",
              "  21.0,\n",
              "  5.0,\n",
              "  24.0,\n",
              "  12.0,\n",
              "  3.0,\n",
              "  14.0,\n",
              "  8.0,\n",
              "  14.0,\n",
              "  17.0,\n",
              "  10.0,\n",
              "  13.0,\n",
              "  24.0,\n",
              "  25.0,\n",
              "  0.0,\n",
              "  16.0,\n",
              "  10.0,\n",
              "  26.0,\n",
              "  11.0,\n",
              "  10.0,\n",
              "  6.0,\n",
              "  6.0,\n",
              "  25.0,\n",
              "  11.0,\n",
              "  11.0,\n",
              "  17.0,\n",
              "  12.0,\n",
              "  14.0,\n",
              "  30.0,\n",
              "  7.0,\n",
              "  6.0,\n",
              "  11.0,\n",
              "  14.0,\n",
              "  10.0,\n",
              "  13.0,\n",
              "  14.0,\n",
              "  10.0,\n",
              "  5.0,\n",
              "  19.0,\n",
              "  12.0,\n",
              "  24.0,\n",
              "  12.0,\n",
              "  11.0,\n",
              "  16.0,\n",
              "  12.0,\n",
              "  12.0,\n",
              "  14.0,\n",
              "  9.0,\n",
              "  22.0,\n",
              "  17.0,\n",
              "  10.0,\n",
              "  3.0,\n",
              "  14.0,\n",
              "  6.0,\n",
              "  17.0,\n",
              "  18.0,\n",
              "  0.0,\n",
              "  25.0,\n",
              "  0.0,\n",
              "  11.0,\n",
              "  18.0,\n",
              "  11.0,\n",
              "  21.0,\n",
              "  0.0,\n",
              "  25.0,\n",
              "  25.0,\n",
              "  13.0,\n",
              "  24.0,\n",
              "  9.0,\n",
              "  38.0,\n",
              "  14.0,\n",
              "  5.0,\n",
              "  14.0,\n",
              "  9.0,\n",
              "  5.0,\n",
              "  24.0,\n",
              "  17.0,\n",
              "  8.0,\n",
              "  11.0,\n",
              "  26.0,\n",
              "  27.0,\n",
              "  11.0,\n",
              "  24.0,\n",
              "  21.0,\n",
              "  11.0,\n",
              "  21.0,\n",
              "  24.0,\n",
              "  13.0,\n",
              "  11.0,\n",
              "  6.0,\n",
              "  11.0,\n",
              "  0.0,\n",
              "  5.0,\n",
              "  3.0,\n",
              "  3.0,\n",
              "  26.0,\n",
              "  9.0,\n",
              "  14.0,\n",
              "  14.0,\n",
              "  11.0,\n",
              "  23.0,\n",
              "  11.0,\n",
              "  4.0,\n",
              "  22.0,\n",
              "  24.0,\n",
              "  25.0,\n",
              "  0.0,\n",
              "  14.0,\n",
              "  17.0,\n",
              "  15.0,\n",
              "  26.0,\n",
              "  24.0,\n",
              "  10.0,\n",
              "  23.0,\n",
              "  5.0,\n",
              "  25.0,\n",
              "  5.0,\n",
              "  5.0,\n",
              "  0.0,\n",
              "  2.0,\n",
              "  22.0,\n",
              "  13.0,\n",
              "  26.0,\n",
              "  12.0,\n",
              "  22.0,\n",
              "  11.0,\n",
              "  7.0,\n",
              "  3.0,\n",
              "  7.0,\n",
              "  4.0,\n",
              "  18.0,\n",
              "  23.0,\n",
              "  25.0,\n",
              "  7.0,\n",
              "  24.0,\n",
              "  22.0,\n",
              "  11.0,\n",
              "  21.0,\n",
              "  12.0,\n",
              "  27.0,\n",
              "  3.0,\n",
              "  4.0,\n",
              "  11.0,\n",
              "  1.0,\n",
              "  14.0,\n",
              "  3.0,\n",
              "  23.0,\n",
              "  3.0,\n",
              "  0.0,\n",
              "  16.0,\n",
              "  29.0,\n",
              "  9.0,\n",
              "  6.0,\n",
              "  32.0,\n",
              "  12.0,\n",
              "  12.0,\n",
              "  24.0,\n",
              "  0.0,\n",
              "  13.0,\n",
              "  5.0,\n",
              "  12.0,\n",
              "  22.0,\n",
              "  0.0,\n",
              "  14.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  17.0,\n",
              "  14.0,\n",
              "  23.0,\n",
              "  24.0,\n",
              "  26.0,\n",
              "  5.0,\n",
              "  1.0,\n",
              "  3.0,\n",
              "  11.0,\n",
              "  16.0,\n",
              "  10.0,\n",
              "  11.0,\n",
              "  22.0,\n",
              "  14.0,\n",
              "  7.0,\n",
              "  0.0,\n",
              "  11.0,\n",
              "  12.0,\n",
              "  10.0,\n",
              "  4.0,\n",
              "  1.0,\n",
              "  24.0,\n",
              "  11.0,\n",
              "  23.0,\n",
              "  12.0,\n",
              "  23.0,\n",
              "  14.0,\n",
              "  18.0,\n",
              "  0.0,\n",
              "  6.0,\n",
              "  9.0,\n",
              "  17.0,\n",
              "  10.0,\n",
              "  29.0,\n",
              "  9.0,\n",
              "  18.0,\n",
              "  5.0,\n",
              "  14.0,\n",
              "  11.0,\n",
              "  11.0,\n",
              "  10.0,\n",
              "  18.0,\n",
              "  5.0,\n",
              "  22.0,\n",
              "  27.0,\n",
              "  1.0,\n",
              "  14.0,\n",
              "  22.0,\n",
              "  24.0,\n",
              "  0.0,\n",
              "  11.0,\n",
              "  27.0,\n",
              "  3.0,\n",
              "  17.0,\n",
              "  5.0,\n",
              "  8.0,\n",
              "  14.0,\n",
              "  1.0,\n",
              "  1.0,\n",
              "  33.0,\n",
              "  18.0,\n",
              "  23.0,\n",
              "  5.0,\n",
              "  26.0,\n",
              "  4.0,\n",
              "  5.0,\n",
              "  24.0,\n",
              "  26.0,\n",
              "  18.0,\n",
              "  4.0,\n",
              "  13.0,\n",
              "  3.0,\n",
              "  5.0,\n",
              "  5.0,\n",
              "  3.0,\n",
              "  3.0,\n",
              "  5.0,\n",
              "  5.0,\n",
              "  6.0,\n",
              "  25.0,\n",
              "  26.0,\n",
              "  21.0,\n",
              "  12.0,\n",
              "  0.0,\n",
              "  14.0,\n",
              "  3.0,\n",
              "  2.0,\n",
              "  24.0,\n",
              "  6.0,\n",
              "  18.0,\n",
              "  18.0,\n",
              "  5.0,\n",
              "  1.0,\n",
              "  11.0,\n",
              "  17.0,\n",
              "  21.0,\n",
              "  21.0,\n",
              "  7.0,\n",
              "  0.0,\n",
              "  13.0,\n",
              "  5.0,\n",
              "  5.0,\n",
              "  24.0,\n",
              "  10.0,\n",
              "  29.0,\n",
              "  14.0,\n",
              "  14.0,\n",
              "  24.0,\n",
              "  13.0,\n",
              "  3.0,\n",
              "  0.0,\n",
              "  5.0,\n",
              "  12.0,\n",
              "  6.0,\n",
              "  11.0,\n",
              "  16.0,\n",
              "  0.0,\n",
              "  23.0,\n",
              "  1.0,\n",
              "  13.0,\n",
              "  23.0,\n",
              "  5.0,\n",
              "  8.0,\n",
              "  5.0,\n",
              "  7.0,\n",
              "  29.0,\n",
              "  0.0,\n",
              "  15.0,\n",
              "  11.0,\n",
              "  4.0,\n",
              "  12.0,\n",
              "  12.0,\n",
              "  3.0,\n",
              "  1.0,\n",
              "  14.0,\n",
              "  26.0,\n",
              "  20.0,\n",
              "  5.0,\n",
              "  21.0,\n",
              "  22.0,\n",
              "  7.0,\n",
              "  6.0,\n",
              "  10.0,\n",
              "  21.0,\n",
              "  18.0,\n",
              "  3.0,\n",
              "  11.0,\n",
              "  21.0,\n",
              "  15.0,\n",
              "  22.0,\n",
              "  16.0,\n",
              "  24.0,\n",
              "  21.0,\n",
              "  28.0,\n",
              "  27.0,\n",
              "  3.0,\n",
              "  17.0,\n",
              "  5.0,\n",
              "  3.0,\n",
              "  24.0,\n",
              "  2.0,\n",
              "  10.0,\n",
              "  3.0,\n",
              "  12.0,\n",
              "  4.0,\n",
              "  26.0,\n",
              "  9.0,\n",
              "  7.0,\n",
              "  11.0,\n",
              "  17.0,\n",
              "  16.0,\n",
              "  22.0,\n",
              "  23.0,\n",
              "  22.0,\n",
              "  28.0,\n",
              "  6.0,\n",
              "  8.0,\n",
              "  26.0,\n",
              "  17.0,\n",
              "  18.0,\n",
              "  5.0,\n",
              "  27.0,\n",
              "  25.0,\n",
              "  9.0,\n",
              "  24.0,\n",
              "  0.0,\n",
              "  29.0,\n",
              "  12.0,\n",
              "  13.0,\n",
              "  17.0,\n",
              "  11.0,\n",
              "  4.0,\n",
              "  5.0,\n",
              "  7.0,\n",
              "  13.0,\n",
              "  11.0,\n",
              "  3.0,\n",
              "  24.0,\n",
              "  0.0,\n",
              "  6.0,\n",
              "  26.0,\n",
              "  13.0,\n",
              "  3.0,\n",
              "  24.0,\n",
              "  13.0,\n",
              "  0.0,\n",
              "  7.0,\n",
              "  21.0,\n",
              "  21.0,\n",
              "  5.0,\n",
              "  15.0,\n",
              "  13.0,\n",
              "  16.0,\n",
              "  23.0,\n",
              "  11.0,\n",
              "  11.0,\n",
              "  3.0,\n",
              "  21.0,\n",
              "  16.0,\n",
              "  18.0,\n",
              "  9.0,\n",
              "  11.0,\n",
              "  6.0,\n",
              "  8.0,\n",
              "  13.0,\n",
              "  0.0,\n",
              "  10.0,\n",
              "  11.0,\n",
              "  4.0,\n",
              "  25.0,\n",
              "  5.0,\n",
              "  4.0,\n",
              "  2.0,\n",
              "  24.0,\n",
              "  17.0,\n",
              "  11.0,\n",
              "  10.0,\n",
              "  21.0,\n",
              "  7.0,\n",
              "  12.0,\n",
              "  3.0,\n",
              "  19.0,\n",
              "  3.0,\n",
              "  10.0,\n",
              "  3.0,\n",
              "  24.0,\n",
              "  17.0,\n",
              "  29.0,\n",
              "  27.0,\n",
              "  7.0,\n",
              "  24.0,\n",
              "  15.0,\n",
              "  19.0,\n",
              "  9.0,\n",
              "  21.0,\n",
              "  11.0,\n",
              "  27.0,\n",
              "  10.0,\n",
              "  22.0,\n",
              "  2.0,\n",
              "  27.0,\n",
              "  16.0,\n",
              "  12.0,\n",
              "  14.0,\n",
              "  11.0,\n",
              "  8.0,\n",
              "  10.0,\n",
              "  11.0,\n",
              "  8.0,\n",
              "  1.0,\n",
              "  12.0,\n",
              "  9.0,\n",
              "  0.0,\n",
              "  5.0,\n",
              "  11.0,\n",
              "  1.0,\n",
              "  23.0,\n",
              "  22.0,\n",
              "  16.0,\n",
              "  12.0,\n",
              "  11.0,\n",
              "  0.0,\n",
              "  5.0,\n",
              "  24.0,\n",
              "  24.0,\n",
              "  17.0,\n",
              "  5.0,\n",
              "  11.0,\n",
              "  12.0,\n",
              "  27.0,\n",
              "  1.0,\n",
              "  28.0,\n",
              "  22.0,\n",
              "  23.0,\n",
              "  13.0,\n",
              "  12.0,\n",
              "  27.0,\n",
              "  27.0,\n",
              "  7.0,\n",
              "  23.0,\n",
              "  24.0,\n",
              "  3.0,\n",
              "  5.0,\n",
              "  10.0,\n",
              "  11.0,\n",
              "  17.0,\n",
              "  0.0,\n",
              "  11.0,\n",
              "  13.0,\n",
              "  11.0,\n",
              "  15.0,\n",
              "  13.0,\n",
              "  3.0,\n",
              "  18.0,\n",
              "  11.0,\n",
              "  15.0,\n",
              "  27.0,\n",
              "  13.0,\n",
              "  12.0,\n",
              "  11.0,\n",
              "  11.0,\n",
              "  5.0,\n",
              "  24.0,\n",
              "  16.0,\n",
              "  26.0,\n",
              "  13.0,\n",
              "  5.0,\n",
              "  11.0,\n",
              "  2.0,\n",
              "  2.0,\n",
              "  19.0,\n",
              "  17.0,\n",
              "  13.0,\n",
              "  16.0,\n",
              "  23.0,\n",
              "  0.0,\n",
              "  3.0,\n",
              "  17.0,\n",
              "  5.0,\n",
              "  6.0,\n",
              "  3.0,\n",
              "  24.0,\n",
              "  14.0,\n",
              "  10.0,\n",
              "  5.0,\n",
              "  14.0,\n",
              "  17.0,\n",
              "  27.0,\n",
              "  4.0,\n",
              "  10.0,\n",
              "  14.0,\n",
              "  24.0,\n",
              "  10.0,\n",
              "  14.0,\n",
              "  11.0,\n",
              "  24.0,\n",
              "  9.0,\n",
              "  27.0,\n",
              "  26.0,\n",
              "  17.0,\n",
              "  28.0,\n",
              "  5.0,\n",
              "  11.0,\n",
              "  11.0,\n",
              "  22.0,\n",
              "  5.0,\n",
              "  24.0,\n",
              "  7.0,\n",
              "  12.0,\n",
              "  9.0,\n",
              "  8.0,\n",
              "  11.0,\n",
              "  26.0,\n",
              "  28.0,\n",
              "  7.0,\n",
              "  24.0,\n",
              "  0.0,\n",
              "  0.0,\n",
              "  11.0,\n",
              "  11.0,\n",
              "  7.0,\n",
              "  14.0,\n",
              "  1.0,\n",
              "  8.0,\n",
              "  7.0,\n",
              "  22.0,\n",
              "  18.0,\n",
              "  3.0,\n",
              "  17.0,\n",
              "  17.0,\n",
              "  9.0,\n",
              "  0.0,\n",
              "  24.0,\n",
              "  24.0,\n",
              "  13.0,\n",
              "  14.0,\n",
              "  6.0,\n",
              "  11.0,\n",
              "  10.0,\n",
              "  24.0,\n",
              "  11.0,\n",
              "  8.0,\n",
              "  13.0,\n",
              "  7.0,\n",
              "  40.0,\n",
              "  3.0,\n",
              "  16.0,\n",
              "  10.0,\n",
              "  21.0,\n",
              "  14.0,\n",
              "  16.0,\n",
              "  25.0,\n",
              "  17.0,\n",
              "  11.0,\n",
              "  0.0,\n",
              "  9.0,\n",
              "  12.0,\n",
              "  3.0,\n",
              "  24.0,\n",
              "  5.0,\n",
              "  24.0,\n",
              "  9.0,\n",
              "  17.0,\n",
              "  24.0,\n",
              "  6.0,\n",
              "  12.0,\n",
              "  26.0,\n",
              "  11.0,\n",
              "  16.0,\n",
              "  15.0,\n",
              "  7.0,\n",
              "  18.0,\n",
              "  17.0,\n",
              "  25.0,\n",
              "  3.0,\n",
              "  24.0,\n",
              "  13.0,\n",
              "  13.0,\n",
              "  3.0,\n",
              "  8.0,\n",
              "  0.0,\n",
              "  16.0,\n",
              "  14.0,\n",
              "  12.0,\n",
              "  13.0,\n",
              "  11.0,\n",
              "  13.0,\n",
              "  22.0,\n",
              "  24.0,\n",
              "  24.0,\n",
              "  0.0,\n",
              "  10.0,\n",
              "  13.0,\n",
              "  11.0,\n",
              "  11.0,\n",
              "  7.0,\n",
              "  6.0,\n",
              "  22.0,\n",
              "  21.0,\n",
              "  26.0,\n",
              "  0.0,\n",
              "  13.0,\n",
              "  14.0,\n",
              "  10.0,\n",
              "  24.0,\n",
              "  23.0,\n",
              "  46.0,\n",
              "  29.0,\n",
              "  23.0,\n",
              "  10.0,\n",
              "  18.0,\n",
              "  23.0,\n",
              "  7.0,\n",
              "  23.0,\n",
              "  11.0,\n",
              "  3.0,\n",
              "  11.0,\n",
              "  11.0,\n",
              "  26.0,\n",
              "  19.0,\n",
              "  21.0,\n",
              "  24.0,\n",
              "  5.0,\n",
              "  16.0,\n",
              "  14.0,\n",
              "  1.0,\n",
              "  11.0,\n",
              "  5.0,\n",
              "  21.0,\n",
              "  12.0,\n",
              "  11.0,\n",
              "  11.0,\n",
              "  24.0,\n",
              "  21.0,\n",
              "  7.0,\n",
              "  17.0,\n",
              "  4.0,\n",
              "  11.0,\n",
              "  8.0,\n",
              "  1.0,\n",
              "  17.0,\n",
              "  17.0,\n",
              "  23.0,\n",
              "  17.0,\n",
              "  12.0,\n",
              "  24.0,\n",
              "  5.0,\n",
              "  22.0,\n",
              "  12.0,\n",
              "  24.0,\n",
              "  17.0,\n",
              "  6.0,\n",
              "  26.0,\n",
              "  4.0,\n",
              "  14.0,\n",
              "  5.0,\n",
              "  26.0,\n",
              "  5.0,\n",
              "  15.0,\n",
              "  11.0,\n",
              "  14.0,\n",
              "  17.0,\n",
              "  15.0,\n",
              "  10.0,\n",
              "  29.0,\n",
              "  24.0,\n",
              "  5.0])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# --- Reload & evaluate helper (greedy ε=0) ---\n",
        "\n",
        "def load_net_and_eval(ckpt_path: str, cfg: Config, episodes=100, seed_base=54_321):\n",
        "    env = make_env(cfg, seed_base)\n",
        "    net, _, _ = build_net_for_env(cfg, env)\n",
        "    payload = torch.load(ckpt_path, map_location=device)\n",
        "    net.load_state_dict(payload)\n",
        "    env.close()\n",
        "    m, s, scores = eval_model_greedy(net, cfg, episodes=episodes, seed_base=seed_base)\n",
        "    print(f\"[EVAL] {os.path.basename(ckpt_path)}: mean={m:.2f} ± {s:.2f}  (n={episodes})\")\n",
        "    return m, s, scores\n",
        "\n",
        "load_net_and_eval(\"/content/cp_per_fixed_pinball_seed7.pt\", cfg_cp, episodes=1000)\n",
        "load_net_and_eval(\"/content/cp_uniform_pinball_seed7.pt\", cfg_cp, episodes=1000)\n",
        "\n",
        "load_net_and_eval(\"/content/minatar_breakout_per_seed7.pt\", cfg_ma, episodes=1000, seed_base=65_432)\n",
        "load_net_and_eval(\"/content/minatar_breakout_uni_seed7.pt\", cfg_ma, episodes=1000, seed_base=65_432)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "wi2F6Eo6Hp8z",
        "G_dV8jvCU34f"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
