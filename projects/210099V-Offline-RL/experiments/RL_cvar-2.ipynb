{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13208000,"sourceType":"datasetVersion","datasetId":8371259}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Conditional Value-at-Risk (CVaR), also known as Expected Shortfall, is a risk management measure used to quantify the tail risk of a portfolio or a decision process. Unlike Value-at-Risk (VaR), which provides an upper bound for the potential loss at a given confidence level, CVaR estimates the average loss assuming that the loss exceeds the VaR threshold. This makes CVaR particularly useful in assessing extreme risk scenarios, which are often neglected in traditional risk measures. In the context of reinforcement learning, CVaR can be incorporated into the objective function to develop risk-sensitive algorithms that take into account not just the expected rewards but also the potential for high losses.\n\nIn our implementation, CVaR is employed as a risk-averse modification to traditional Q-learning, adjusting the value function by incorporating the worst-case scenarios. This allows the model to better handle environments where the uncertainty or volatility in rewards is high.","metadata":{}},{"cell_type":"code","source":"# ==================== QR-DQN + CVaR (risk-aware) ====================\n\nimport os, math, csv, time, sys, subprocess, json\nfrom pathlib import Path\nimport numpy as np\nimport torch, torch.nn as nn\nfrom torch.utils.data import IterableDataset, DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\ntry:\n    from tqdm.auto import tqdm\nexcept Exception:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"tqdm\"])\n    from tqdm.auto import tqdm\n\n# ---------------- CONFIG ----------------\n# Point to your large dataset run folder (created earlier)\nRUN_DIR          = sorted((Path(\"/kaggle/input/breakout-offline-minatar-big/breakout_offline_minatar_big\")).glob(\"run_*\"))[-1]\nVARIANT          = \"atari\"      # \"atari\" (84x84x4) or \"native\" (tiny MinAtar grids)\nBATCH            = 128\nLR               = 1e-4\nN_QUANT          = 51           # number of quantile atoms (e.g., 21/51)\nGAMMA            = 0.99\nKAPPA            = 1.0          # Huber kappa for quantile regression\nALPHA            = 0.10         # <-- CVaR risk level (0 < α ≤ 1). Lower = more risk-averse.\nEPOCHS           = 12\nSTEPS_PER_EPOCH  = 300          # adjust as you like; dataset is large enough\nTARGET_UPDATE    = 600          # hard copy every N iters (set 0 to rely on EMA)\nEMA_TAU          = 5e-3         # <-- EMA target update per step; set 0 to disable EMA\nSEED             = 123\nUSE_AMP          = True         # AMP/mixed precision for speed\nSAVE_DIR         = Path(\"/kaggle/working/qr_dqn_logs\") / \"qrdqn_cvar_logs\"\nPLOTS_DIR        = SAVE_DIR / \"plots\"\n# ----------------------------------------\n\nos.makedirs(SAVE_DIR, exist_ok=True); os.makedirs(PLOTS_DIR, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpus = torch.cuda.device_count() if device.type == \"cuda\" else 0\ntorch.manual_seed(SEED); np.random.seed(SEED)\ntorch.backends.cudnn.benchmark = True  # fixed-shape speedup\n\n# --------------- Discover shards + split (80/20 by shards; robust to 1 shard) ---------------\nshards = sorted((RUN_DIR).glob(f\"*_{VARIANT}_shard_*.npz\"))\nassert len(shards) > 0, f\"No {VARIANT} shards found in {RUN_DIR}\"\n\nif len(shards) >= 2:\n    n_train_shards = max(1, min(int(0.8 * len(shards)), len(shards) - 1))\n    train_shards = shards[:n_train_shards]\n    eval_shards  = shards[n_train_shards:]\n    single_shard = False\nelse:\n    train_shards = shards\n    eval_shards  = shards\n    single_shard = True\n\nprint(f\"Shards: {len(shards)} | variant={VARIANT} | Train shards: {len(train_shards)} | Eval shards: {len(eval_shards)}\")\n\n# --------------- Fast IterableDataset (loads each shard once/epoch) ---------------\nclass ShardBatcher(IterableDataset):\n    \"\"\"\n    - If multiple shards: train/eval are split by shards (80/20 above).\n    - If a single shard: we split indices (train ~90%, eval ~10%) inside this class.\n    - Yields tensors: obs/next_obs float CHW in [0,1], act long, rew float, done float.\n    \"\"\"\n    def __init__(self, shard_paths, batch_size, seed=123, eval_mode=False, split_frac=0.9):\n        super().__init__()\n        self.shards = list(map(str, shard_paths))\n        self.bs = batch_size\n        self.eval_mode = eval_mode\n        self.rng = np.random.default_rng(seed)\n        self.split_frac = split_frac\n        d0 = np.load(self.shards[0]); H, W, C = d0[\"obs\"].shape[1:]\n        self.in_ch = int(C); self.n_actions = int(d0[\"act\"].max()) + 1\n\n    def _yield_batches_from_arrays(self, d):\n        N = d[\"act\"].shape[0]; idx = np.arange(N)\n        if single_shard:\n            split = max(1, int(self.split_frac * N))\n            idx = idx[split:] if self.eval_mode else idx[:split]\n        if not self.eval_mode:\n            self.rng.shuffle(idx)\n        for s in range(0, idx.size, self.bs):\n            bi = idx[s:s+self.bs]\n            o  = torch.from_numpy(d[\"obs\"][bi]).permute(0,3,1,2).float()/255.0\n            no = torch.from_numpy(d[\"next_obs\"][bi]).permute(0,3,1,2).float()/255.0\n            a  = torch.from_numpy(d[\"act\"][bi]).long()\n            r  = torch.from_numpy(d[\"rew\"][bi]).float()\n            dn = torch.from_numpy(d[\"done\"][bi]).float()\n            yield o, a, r, no, dn\n\n    def __iter__(self):\n        order = np.arange(len(self.shards))\n        if not self.eval_mode:\n            self.rng.shuffle(order)\n        for si in order:\n            d = np.load(self.shards[si])\n            yield from self._yield_batches_from_arrays(d)\n\ntrain_ds = ShardBatcher(train_shards, BATCH, SEED, eval_mode=False)\neval_ds  = ShardBatcher(eval_shards,  BATCH, SEED, eval_mode=True)\n\nIN_CH, N_ACT = train_ds.in_ch, train_ds.n_actions\ntrain_loader = DataLoader(train_ds, batch_size=None, num_workers=0)\neval_loader  = DataLoader(eval_ds,  batch_size=None, num_workers=0)\nprint(f\"Channels: {IN_CH} | Actions: {N_ACT} | Device: {device} | GPUs: {n_gpus}\")\n\n# --------------- Model + CVaR helpers ---------------\nclass QRDQN(nn.Module):\n    def __init__(self, in_ch, n_actions, n_quant=51):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, 32, 8, 4), nn.ReLU(inplace=True),\n            nn.Conv2d(32, 64, 4, 2),    nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, 3, 1),    nn.ReLU(inplace=True),\n        )\n        self.gap = nn.AdaptiveAvgPool2d((7,7))\n        self.fc  = nn.Sequential(\n            nn.Linear(64*7*7, 512), nn.ReLU(inplace=True),\n            nn.Linear(512, n_actions*n_quant),\n        )\n        self.n_actions = n_actions\n        self.n_quant   = n_quant\n    def forward(self, x):\n        z = self.conv(x)\n        z = self.gap(z).reshape(z.size(0), -1)\n        z = self.fc(z).reshape(-1, self.n_actions, self.n_quant)\n        return z  # (B, A, Nq), atoms ordered by tau midpoints\n\ndef quantile_huber_loss(pred_tau, target_tau, taus, kappa=1.0):\n    # pred_tau, target_tau: (B, Nq)\n    u = target_tau.unsqueeze(1) - pred_tau.unsqueeze(2)         # (B,Nq,Nq)\n    abs_u = torch.abs(u)\n    huber = torch.where(abs_u <= kappa, 0.5*u.pow(2), kappa*(abs_u - 0.5*kappa))\n    tau = taus.view(1, -1, 1)                                   # (1,Nq,1)\n    weight = torch.abs(tau - (u.detach() < 0).float())\n    return (weight * huber).mean()\n\ndef cvar_from_quantiles(qvals, alpha):\n    \"\"\"\n    qvals: (B, A, Nq) quantile values (assumed ordered by tau asc).\n    alpha in (0,1]; CVaR_α = mean of the lowest α-fraction of atoms.\n    returns: (B, A) CVaR scores.\n    \"\"\"\n    B, A, Nq = qvals.shape\n    k = max(1, int(math.ceil(alpha * Nq)))\n    # Take mean over the first k atoms (lower tail)\n    return qvals[:, :, :k].mean(dim=2)\n\n# Instantiate nets/opt\nqnet_base   = QRDQN(IN_CH, N_ACT, N_QUANT).to(device)\ntarget_base = QRDQN(IN_CH, N_ACT, N_QUANT).to(device)\ntarget_base.load_state_dict(qnet_base.state_dict())\n\n# Optional multi-GPU\nqnet   = nn.DataParallel(qnet_base)   if n_gpus > 1 else qnet_base\ntarget = nn.DataParallel(target_base) if n_gpus > 1 else target_base\n\nopt    = torch.optim.Adam(qnet.parameters(), lr=LR)\nfrom torch.amp import autocast, GradScaler\nscaler = GradScaler('cuda', enabled=(USE_AMP and device.type == \"cuda\"))\n\ntaus = torch.linspace(0, 1, N_QUANT + 1, device=device)\ntaus = (taus[:-1] + taus[1:]) / 2.0  # midpoints, ascending\n\n# --------------- Logging (train only; eval after) ---------------\nwriter  = SummaryWriter(log_dir=str(SAVE_DIR / \"tb\"))\ncsv_log = open(SAVE_DIR / \"training_log.csv\", \"w\", newline=\"\")\ncsv_wr  = csv.writer(csv_log)\ncsv_wr.writerow([\"epoch\",\"iter\",\"loss_train\",\"q_mean\",\"q_std\",\"grad_norm\",\"lr\",\"time_sec\"])\n\n# --------------- Training (CVaR action selection) ---------------\nglobal_iter = 0\nt0 = time.time()\n\ndef ema_target_update(ema_tau):\n    with torch.no_grad():\n        for p_t, p in zip(target_base.parameters(), qnet_base.parameters()):\n            p_t.data.mul_(1.0 - ema_tau).add_(ema_tau * p.data)\n\nfor epoch in range(1, EPOCHS+1):\n    qnet.train()\n    running = 0.0\n    pbar = tqdm(total=STEPS_PER_EPOCH, desc=f\"Epoch {epoch}\", leave=False, dynamic_ncols=True, smoothing=0.2)\n    i = 0\n    for (o,a,r,no,dn) in train_loader:\n        i += 1\n        o,no = o.to(device, non_blocking=True), no.to(device, non_blocking=True)\n        a,r,dn = a.to(device, non_blocking=True), r.to(device, non_blocking=True), dn.to(device, non_blocking=True)\n\n        with autocast('cuda', enabled=(USE_AMP and device.type==\"cuda\")):\n            q = qnet(o)  # (B, A, Nq)\n            # CVaR-based target action on next states (Double-DQN)\n            q_next_online = qnet(no)                    # (B, A, Nq)\n            cvar_scores   = cvar_from_quantiles(q_next_online, ALPHA)  # (B, A)\n            next_a        = cvar_scores.argmax(dim=1)   # (B,)\n\n            # Gather online predicted quantiles for taken actions (current states)\n            q_sel = q.gather(1, a.view(-1,1,1).expand(-1,1,N_QUANT)).squeeze(1)  # (B, Nq)\n\n            # Target network quantiles for the selected next action\n            tq_all = target(no)  # (B, A, Nq)\n            tq     = tq_all.gather(1, next_a.view(-1,1,1).expand(-1,1,N_QUANT)).squeeze(1)  # (B, Nq)\n\n            # Bellman target distribution\n            tgt = r.view(-1,1) + GAMMA * (1.0 - dn.view(-1,1)) * tq\n\n            # Distributional QR Huber loss\n            loss = quantile_huber_loss(q_sel, tgt, taus, KAPPA)\n\n        opt.zero_grad(set_to_none=True)\n        scaler.scale(loss).backward()\n        grad_norm = float(nn.utils.clip_grad_norm_(qnet.parameters(), 10.0))\n        scaler.step(opt)\n        scaler.update()\n\n        # Target updates: EMA each step; optional hard copy\n        if EMA_TAU > 0.0:\n            ema_target_update(EMA_TAU)\n        elif TARGET_UPDATE > 0 and (global_iter % TARGET_UPDATE == 0):\n            target_base.load_state_dict(qnet_base.state_dict())\n\n        # Logging\n        running += float(loss.item())\n        global_iter += 1\n        if (i % 25 == 0) or (i == STEPS_PER_EPOCH):\n            lr = float(opt.param_groups[0][\"lr\"])\n            q_std = float(q_sel.std().item())\n            q_mean = float(q_sel.mean().item())\n            elapsed = time.time() - t0\n            avg_tr = running / max(1, i)\n\n            writer.add_scalar(\"loss/train\", avg_tr, global_iter)\n            writer.add_scalar(\"q/pred_mean_train\", q_mean, global_iter)\n            writer.add_scalar(\"grad/norm\", grad_norm, global_iter)\n            writer.add_scalar(\"opt/lr\", lr, global_iter)\n            writer.add_scalar(\"cvar/alpha\", ALPHA, global_iter)\n\n            csv_wr.writerow([epoch, global_iter, avg_tr, q_mean, q_std, grad_norm, lr, elapsed])\n            csv_log.flush()\n\n        pbar.set_postfix_str(f\"loss={running/max(1,i):.4g}\")\n        pbar.update(1)\n        if i >= STEPS_PER_EPOCH:\n            break\n    pbar.close()\n\n    # end-epoch snapshot\n    torch.save({\n        \"model\": qnet_base.state_dict(),\n        \"target\": target_base.state_dict(),\n        \"config\": dict(n_quant=N_QUANT, gamma=GAMMA, alpha=ALPHA, ema_tau=EMA_TAU,\n                       in_ch=IN_CH, n_actions=N_ACT),\n    }, SAVE_DIR / f\"qrdqn_cvar_epoch_{epoch}.pt\")\n    print(f\"[Epoch {epoch}] avg_train_loss={running/max(1,i):.6f}\")\n\n# final save\ntorch.save({\n    \"model\": qnet_base.state_dict(),\n    \"target\": target_base.state_dict(),\n    \"config\": dict(n_quant=N_QUANT, gamma=GAMMA, alpha=ALPHA, ema_tau=EMA_TAU,\n                   in_ch=IN_CH, n_actions=N_ACT),\n}, SAVE_DIR / \"qrdqn_cvar_last.pt\")\n\nwriter.close()\ncsv_log.close()\nprint(\"Training logs saved to:\", SAVE_DIR)\n\n# --------------- Single evaluation after training (held-out only) ---------------\n@torch.no_grad()\ndef full_eval():\n    qnet.eval(); target.eval()\n    total_loss=0.0; iters=0; q_means=[]; tq_means=[]\n    agree_total=0; count_total=0\n    ds_hist = Counter(); pi_hist = Counter()\n\n    # Build a fresh eval loader to ensure held-out portion only\n    eval_loader2 = DataLoader(eval_ds, batch_size=None, num_workers=0)\n    pbar = tqdm(total=sum(math.ceil(np.load(sp)[\"obs\"].shape[0] * (0.1 if single_shard else 1.0) / BATCH) for sp in eval_shards),\n                desc=\"Eval\", leave=True, dynamic_ncols=True)\n\n    for (o,a,r,no,dn) in eval_loader2:\n        o,no = o.to(device), no.to(device)\n        a,r,dn = a.to(device), r.to(device), dn.to(device)\n\n        with autocast('cuda', enabled=(USE_AMP and device.type==\"cuda\")):\n            q = qnet(o)  # (B, A, Nq)\n            # policy (for agreement/hist): CVaR on current states\n            cvar_now = cvar_from_quantiles(q, ALPHA)  # (B, A)\n            pi_a = cvar_now.argmax(1)\n\n            # gather predicted quantiles for logged actions\n            q_sel = q.gather(1, a.view(-1,1,1).expand(-1,1,N_QUANT)).squeeze(1)\n\n            # Double-DQN target based on CVaR on next states\n            q_next = qnet(no)\n            next_a = cvar_from_quantiles(q_next, ALPHA).argmax(1)\n            tq_all = target(no)\n            tq = tq_all.gather(1, next_a.view(-1,1,1).expand(-1,1,N_QUANT)).squeeze(1)\n            tgt = r.view(-1,1) + GAMMA * (1.0 - dn.view(-1,1)) * tq\n            loss = quantile_huber_loss(q_sel, tgt, taus, KAPPA)\n\n        total_loss += float(loss.item()); iters += 1\n        q_means.append(float(q_sel.mean().item())); tq_means.append(float(tq.mean().item()))\n        agree_total += int((pi_a == a).sum().item()); count_total += int(a.numel())\n        for aa in a.detach().cpu().numpy(): ds_hist[int(aa)] += 1\n        for pa in pi_a.detach().cpu().numpy():    pi_hist[int(pa)] += 1\n\n        pbar.set_postfix_str(f\"loss={total_loss/max(1,iters):.4g}\")\n        pbar.update(1)\n    pbar.close()\n\n    return dict(\n        eval_loss = total_loss / max(1,iters),\n        q_mean    = float(np.mean(q_means)) if q_means else 0.0,\n        tq_mean   = float(np.mean(tq_means)) if tq_means else 0.0,\n        num_batches = iters,\n        policy_dataset_agreement = (agree_total / max(1,count_total)),\n        ds_hist = dict(ds_hist), pi_hist = dict(pi_hist),\n    )\n\neval_stats = full_eval()\nprint(\"[Final Eval]\", {k:v for k,v in eval_stats.items() if k not in (\"ds_hist\",\"pi_hist\")})\n\n# Save eval CSV + histograms\nwith open(SAVE_DIR / \"final_eval_cvar.csv\", \"w\", newline=\"\") as f:\n    w = csv.writer(f)\n    w.writerow([\"eval_loss\",\"q_mean\",\"tq_mean\",\"policy_dataset_agreement\",\"num_batches\",\"alpha\",\"checkpoint\"])\n    w.writerow([eval_stats[\"eval_loss\"], eval_stats[\"q_mean\"], eval_stats[\"tq_mean\"],\n                eval_stats[\"policy_dataset_agreement\"], eval_stats[\"num_batches\"], ALPHA,\n                str(SAVE_DIR / \"qrdqn_cvar_last.pt\")])\n\nwith open(SAVE_DIR / \"eval_action_histograms_cvar.json\", \"w\") as f:\n    json.dump({\"dataset\": eval_stats[\"ds_hist\"], \"policy\": eval_stats[\"pi_hist\"]}, f, indent=2)\n\n# Quick plots for report\ndf = pd.read_csv(SAVE_DIR / \"training_log.csv\")\nplt.figure(); df.plot(x=\"iter\", y=[\"loss_train\"]); plt.title(\"QR-DQN CVaR: Train Loss\"); plt.xlabel(\"iter\"); plt.ylabel(\"loss\")\nplt.savefig(PLOTS_DIR/\"loss_train_cvar.png\", bbox_inches=\"tight\"); plt.close()\nplt.figure(); df.plot(x=\"iter\", y=[\"q_mean\"]); plt.title(\"QR-DQN CVaR: Train Q Mean\"); plt.xlabel(\"iter\"); plt.ylabel(\"Q\")\nplt.savefig(PLOTS_DIR/\"q_train_cvar.png\", bbox_inches=\"tight\"); plt.close()\nprint(\"Artifacts:\", SAVE_DIR)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:45:28.884731Z","iopub.status.idle":"2025-09-30T12:45:28.885110Z","shell.execute_reply.started":"2025-09-30T12:45:28.884954Z","shell.execute_reply":"2025-09-30T12:45:28.884971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **CVaR eval**","metadata":{}},{"cell_type":"code","source":"# =================== EVAL-ONLY with PROGRESS (held-out data) — CVaR QR-DQN ===================\n# - Matches the baseline eval structure but uses CVaR policy (alpha from checkpoint config)\n\n\nimport os, csv, json, math, time\nfrom pathlib import Path\nimport numpy as np\nimport torch, torch.nn as nn\nfrom torch.utils.data import IterableDataset, DataLoader\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\ntry:\n    from tqdm.auto import tqdm\nexcept Exception:\n    import sys, subprocess\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"tqdm\"])\n    from tqdm.auto import tqdm\n\n# ---------------- CONFIG (match training) ----------------\nRUN_DIR   = sorted((Path(\"/kaggle/input/breakout-offline-minatar-big/breakout_offline_minatar_big\")).glob(\"run_*\"))[-1]\nVARIANT   = \"atari\"        # \"atari\" or \"native\"\nBATCH     = 128\nSEED      = 123\nCKPT_PATH = Path(\"/kaggle/working/qr_dqn_logs/qrdqn_cvar_logs/qrdqn_cvar_epoch_5.pt\")  \nOUT_DIR   = Path(\"/kaggle/working/qr_dqn_logs\")                                    \nUSE_AMP   = True\nEVAL_MAX_BATCHES = None    \n# ---------------------------------------------------------\n\nOUT_DIR.mkdir(parents=True, exist_ok=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpus = torch.cuda.device_count() if device.type == \"cuda\" else 0\ntorch.backends.cudnn.benchmark = True\n\n# ---------------- Discover eval shards ----------------\nshards = sorted((RUN_DIR).glob(f\"*_{VARIANT}_shard_*.npz\"))\nassert shards, f\"No {VARIANT} shards found in {RUN_DIR}\"\nmulti = len(shards) >= 2\nif multi:\n    n_train_shards = max(1, min(int(0.8 * len(shards)), len(shards) - 1))\n    eval_shards    = shards[n_train_shards:]\nelse:\n    eval_shards    = shards\n\n# ---------------- Dataset (eval-only) ----------------\nclass EvalBatcher(IterableDataset):\n    def __init__(self, shard_paths, batch_size, seed=123, single_shard=not multi):\n        super().__init__()\n        self.shards = list(map(str, shard_paths))\n        self.bs = batch_size\n        self.single_shard = single_shard\n        self.rng = np.random.default_rng(seed)\n        d0 = np.load(self.shards[0])\n        H,W,C = d0[\"obs\"].shape[1:]\n        self.in_ch = int(C)\n        self.n_actions = int(d0[\"act\"].max()) + 1\n\n    def __iter__(self):\n        for sp in self.shards:\n            d = np.load(sp)\n            if self.single_shard:\n                N = d[\"obs\"].shape[0]\n                split = max(1, int(0.9 * N))          # last 10% = eval\n                idx = np.arange(split, N)\n            else:\n                idx = np.arange(d[\"obs\"].shape[0])    # whole shard = eval\n            for s in range(0, idx.size, self.bs):\n                bi = idx[s:s+self.bs]\n                o  = torch.from_numpy(d[\"obs\"][bi]).permute(0,3,1,2).float()/255.0\n                no = torch.from_numpy(d[\"next_obs\"][bi]).permute(0,3,1,2).float()/255.0\n                a  = torch.from_numpy(d[\"act\"][bi]).long()\n                r  = torch.from_numpy(d[\"rew\"][bi]).float()\n                dn = torch.from_numpy(d[\"done\"][bi]).float()\n                yield o, a, r, no, dn\n\neval_ds = EvalBatcher(eval_shards, BATCH)\neval_loader = DataLoader(eval_ds, batch_size=None, num_workers=0)\nIN_CH, N_ACT = eval_ds.in_ch, eval_ds.n_actions\n\ndef estimate_batches(shard_paths, batch, single_shard=not multi):\n    total = 0\n    for sp in shard_paths:\n        d = np.load(sp)\n        if single_shard:\n            N = d[\"obs\"].shape[0]; split = max(1, int(0.9*N))\n            n = N - split\n        else:\n            n = d[\"obs\"].shape[0]\n        total += math.ceil(n / batch)\n    return total\n\nexpected_batches = estimate_batches(eval_shards, BATCH)\nif EVAL_MAX_BATCHES is not None:\n    expected_batches = min(expected_batches, EVAL_MAX_BATCHES)\n\n# ---------------- Model (same as training arch) ----------------\nclass QRDQN(nn.Module):\n    def __init__(self, in_ch, n_actions, n_quant=51):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, 32, 8, 4), nn.ReLU(inplace=True),\n            nn.Conv2d(32, 64, 4, 2),    nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, 3, 1),    nn.ReLU(inplace=True),\n        )\n        self.gap = nn.AdaptiveAvgPool2d((7,7))\n        self.fc = nn.Sequential(\n            nn.Linear(64*7*7, 512), nn.ReLU(inplace=True),\n            nn.Linear(512, n_actions*51),\n        )\n        self.n_actions = n_actions\n        self.n_quant   = 51\n    def forward(self, x):\n        z = self.conv(x); z = self.gap(z).reshape(z.size(0), -1)\n        z = self.fc(z).reshape(-1, self.n_actions, self.n_quant)\n        return z\n\ndef quantile_huber_loss(pred_tau, target_tau, taus, kappa=1.0):\n    u = target_tau.unsqueeze(1) - pred_tau.unsqueeze(2)\n    abs_u = torch.abs(u)\n    huber = torch.where(abs_u <= kappa, 0.5*u.pow(2), kappa*(abs_u - 0.5*kappa))\n    tau = taus.view(1,-1,1)\n    weight = torch.abs(tau - (u.detach() < 0).float())\n    return (weight*huber).mean()\n\ndef cvar_from_quantiles(qvals, alpha):\n    \"\"\"qvals: (B,A,Nq) -> CVaR_α (mean of lowest α-fraction of atoms).\"\"\"\n    B, A, Nq = qvals.shape\n    k = max(1, int(math.ceil(alpha * Nq)))\n    return qvals[:, :, :k].mean(dim=2)\n\n# ----- Load CVaR checkpoint -----\nckpt = torch.load(CKPT_PATH, map_location=device)\nN_QUANT = ckpt[\"config\"].get(\"n_quant\", 51)\nGAMMA   = ckpt[\"config\"].get(\"gamma\", 0.99)\nALPHA   = ckpt[\"config\"].get(\"alpha\", 0.10)   # CVaR level used at train time\nKAPPA   = 1.0\n\nqnet_base   = QRDQN(IN_CH, N_ACT, N_QUANT).to(device)\ntarget_base = QRDQN(IN_CH, N_ACT, N_QUANT).to(device)\nqnet_base.load_state_dict(ckpt[\"model\"])\ntarget_base.load_state_dict(ckpt[\"target\"])\n\n# DataParallel if multiple GPUs\nqnet   = nn.DataParallel(qnet_base)   if n_gpus > 1 else qnet_base\ntarget = nn.DataParallel(target_base) if n_gpus > 1 else target_base\nqnet.eval(); target.eval()\n\ntaus = torch.linspace(0, 1, N_QUANT + 1, device=device); taus = (taus[:-1] + taus[1:]) / 2.0\n\n# ---------------- Eval loop with progress (CVaR policy + CVaR Double-DQN targets) ----------------\n@torch.no_grad()\ndef run_eval():\n    total_loss=0.0; iters=0; seen=0\n    q_means=[]; tq_means=[]\n    agree_total=0; count_total=0\n    ds_hist = Counter(); pi_hist = Counter()\n\n    start = time.time()\n    pbar = tqdm(total=expected_batches, desc=\"Eval (CVaR)\", leave=True, dynamic_ncols=True)\n\n    for (o,a,r,no,dn) in eval_loader:\n        if (EVAL_MAX_BATCHES is not None) and (iters >= EVAL_MAX_BATCHES):\n            break\n\n        o,no = o.to(device, non_blocking=True), no.to(device, non_blocking=True)\n        a,r,dn = a.to(device, non_blocking=True), r.to(device, non_blocking=True), dn.to(device, non_blocking=True)\n\n        with torch.amp.autocast('cuda', enabled=(USE_AMP and device.type==\"cuda\")):\n            q = qnet(o)  # (B,A,Nq)\n\n            # Policy for agreement/hist: CVaR on CURRENT states\n            pi_scores = cvar_from_quantiles(q, ALPHA)  \n            pi_a = pi_scores.argmax(1)                \n\n            # Online quantiles for the logged action\n            q_sel = q.gather(1, a.view(-1,1,1).expand(-1,1,N_QUANT)).squeeze(1)  \n\n            # CVaR Double-DQN target:\n            q_next_online = qnet(no)                             \n            next_a = cvar_from_quantiles(q_next_online, ALPHA).argmax(1) \n            tq_all = target(no)                                  \n            tq = tq_all.gather(1, next_a.view(-1,1,1).expand(-1,1,N_QUANT)).squeeze(1)  \n\n            tgt = r.view(-1,1) + GAMMA*(1.0 - dn.view(-1,1))*tq\n            loss = quantile_huber_loss(q_sel, tgt, taus, KAPPA)\n\n        bsz = a.numel()\n        seen += bsz\n        total_loss += float(loss.item()); iters += 1\n        q_means.append(float(q_sel.mean().item())); tq_means.append(float(tq.mean().item()))\n\n        # agreement & histograms\n        agree_total += int((pi_a == a).sum().item())\n        count_total += int(a.numel())\n        for aa in a.detach().cpu().numpy(): ds_hist[int(aa)] += 1\n        for pa in pi_a.detach().cpu().numpy(): pi_hist[int(pa)] += 1\n\n        # progress\n        elapsed = time.time() - start\n        pbar.set_postfix_str(f\"loss={total_loss/max(1,iters):.4g} | {seen/max(1,elapsed):.1f} samp/s\")\n        pbar.update(1)\n\n    pbar.close()\n    return dict(\n        eval_loss = total_loss / max(1,iters),\n        q_mean    = float(np.mean(q_means)) if q_means else 0.0,\n        tq_mean   = float(np.mean(tq_means)) if tq_means else 0.0,\n        num_batches = iters,\n        samples_seen = seen,\n        policy_dataset_agreement = (agree_total / max(1,count_total)),\n        ds_hist = dict(ds_hist), pi_hist = dict(pi_hist),\n        eval_source = \"held_out_from_training_run\",\n        device = str(device), gpus = n_gpus, alpha = ALPHA\n    )\n\nstats = run_eval()\nprint(\"\\n[Eval CVaR report]\", {k: v for k,v in stats.items() if k not in (\"ds_hist\",\"pi_hist\")})\n\nout_csv = OUT_DIR / \"final_eval_report_cvar.csv\"\nwith open(out_csv, \"w\", newline=\"\") as f:\n    w = csv.writer(f)\n    w.writerow([\"eval_source\",\"device\",\"gpus\",\"alpha\",\"eval_loss\",\"q_mean\",\"tq_mean\",\n                \"policy_dataset_agreement\",\"num_batches\",\"samples_seen\",\"checkpoint\"])\n    w.writerow([stats[\"eval_source\"], stats[\"device\"], stats[\"gpus\"], stats[\"alpha\"], stats[\"eval_loss\"],\n                stats[\"q_mean\"], stats[\"tq_mean\"], stats[\"policy_dataset_agreement\"],\n                stats[\"num_batches\"], stats[\"samples_seen\"], str(CKPT_PATH)])\nprint(\"Saved CSV:\", out_csv)\n\n# Histograms \nwith open(OUT_DIR / \"eval_action_histograms_cvar.json\", \"w\") as f:\n    json.dump({\"dataset\": stats[\"ds_hist\"], \"policy\": stats[\"pi_hist\"], \"alpha\": stats[\"alpha\"]}, f, indent=2)\nprint(\"Saved hist:\", OUT_DIR / \"eval_action_histograms_cvar.json\")\n\nds_total = sum(stats[\"ds_hist\"].values()) or 1\npi_total = sum(stats[\"pi_hist\"].values()) or 1\nactions = sorted(set(list(stats[\"ds_hist\"].keys()) + list(stats[\"pi_hist\"].keys())))\ndf = pd.DataFrame({\n    \"action\": actions,\n    \"dataset_freq\": [stats[\"ds_hist\"].get(a,0)/ds_total for a in actions],\n    \"policy_freq\":  [stats[\"pi_hist\"].get(a,0)/pi_total for a in actions],\n})\nax = df.plot(kind=\"bar\", x=\"action\", y=[\"dataset_freq\",\"policy_freq\"], figsize=(7,4), rot=0,\n             title=f\"Action frequencies (dataset vs CVaR policy, α={stats['alpha']})\")\nplt.tight_layout(); plt.savefig(OUT_DIR / \"eval_support_cvar.png\"); plt.close()\nprint(\"Saved plot:\", OUT_DIR / \"eval_support_cvar.png\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T12:50:08.872222Z","iopub.execute_input":"2025-09-30T12:50:08.872688Z","iopub.status.idle":"2025-09-30T13:37:32.271146Z","shell.execute_reply.started":"2025-09-30T12:50:08.872661Z","shell.execute_reply":"2025-09-30T13:37:32.270327Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Eval (CVaR):   0%|          | 0/132 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00b91a779b634894a296cd72ac6f8337"}},"metadata":{}},{"name":"stdout","text":"\n[Eval CVaR report] {'eval_loss': 0.001983130349827027, 'q_mean': 0.2605646306818182, 'tq_mean': 0.27534346147017047, 'num_batches': 132, 'samples_seen': 16800, 'policy_dataset_agreement': 0.1686904761904762, 'eval_source': 'held_out_from_training_run', 'device': 'cuda', 'gpus': 1, 'alpha': 0.1}\nSaved CSV: /kaggle/working/qr_dqn_logs/final_eval_report_cvar.csv\nSaved hist: /kaggle/working/qr_dqn_logs/eval_action_histograms_cvar.json\nSaved plot: /kaggle/working/qr_dqn_logs/eval_support_cvar.png\n","output_type":"stream"}],"execution_count":4}]}