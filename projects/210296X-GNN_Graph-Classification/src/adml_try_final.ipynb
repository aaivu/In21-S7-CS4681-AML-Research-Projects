{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "67045e4b1ce844b6b280317e45b86858": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_636992f3e29d4daa9039d5c09f53ace9",
              "IPY_MODEL_03b0d3ff8adb4182bf15bec805bd2d5a",
              "IPY_MODEL_e69b7c35f2b6418e8f25cf9c54e35fca"
            ],
            "layout": "IPY_MODEL_eef477620e8d46d49f955b93523360a8"
          }
        },
        "636992f3e29d4daa9039d5c09f53ace9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a8e58951c9c44abba84d8f1a85b04cd",
            "placeholder": "​",
            "style": "IPY_MODEL_4f770ba708e94cd8bf90755f54ebc3be",
            "value": "/root/.dgl/ENZYMES.zip: 100%"
          }
        },
        "03b0d3ff8adb4182bf15bec805bd2d5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf22ec264d394a93afce83e3d8d75f84",
            "max": 536614,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0820e28e41964d31b9ed150bb6308ad0",
            "value": 536614
          }
        },
        "e69b7c35f2b6418e8f25cf9c54e35fca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57252e6eb34c42eca1b8e582ad66fb8b",
            "placeholder": "​",
            "style": "IPY_MODEL_cbfd4f3b3956435fa676a246c9c30708",
            "value": " 537k/537k [00:00&lt;00:00, 839kB/s]"
          }
        },
        "eef477620e8d46d49f955b93523360a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a8e58951c9c44abba84d8f1a85b04cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f770ba708e94cd8bf90755f54ebc3be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf22ec264d394a93afce83e3d8d75f84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0820e28e41964d31b9ed150bb6308ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "57252e6eb34c42eca1b8e582ad66fb8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbfd4f3b3956435fa676a246c9c30708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f24e3b184794e71b06eca6e3ab6df0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d6067debfee44b10b093949d541f44ca",
              "IPY_MODEL_cdae9fbd95fd448f8442ac60891f05be",
              "IPY_MODEL_ba6267fe02bb4048a5d6ebbc79978208"
            ],
            "layout": "IPY_MODEL_5a06d89f3505451eba26a847fe4d147f"
          }
        },
        "d6067debfee44b10b093949d541f44ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_346409fecc764603aa20e0508e9cdf6a",
            "placeholder": "​",
            "style": "IPY_MODEL_c37b58627d26477aa201e1b664391161",
            "value": "/root/.dgl/SYNTHETIC.zip: 100%"
          }
        },
        "cdae9fbd95fd448f8442ac60891f05be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4df98b74b8e422f91a315536717e5e8",
            "max": 437450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_497dd371e5954a3982d9511f68a53f39",
            "value": 437450
          }
        },
        "ba6267fe02bb4048a5d6ebbc79978208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7bb9efc546a433898b449195a244c53",
            "placeholder": "​",
            "style": "IPY_MODEL_64b468a9a7d849b8966ad434df59a116",
            "value": " 437k/437k [00:00&lt;00:00, 807kB/s]"
          }
        },
        "5a06d89f3505451eba26a847fe4d147f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "346409fecc764603aa20e0508e9cdf6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c37b58627d26477aa201e1b664391161": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4df98b74b8e422f91a315536717e5e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "497dd371e5954a3982d9511f68a53f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7bb9efc546a433898b449195a244c53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64b468a9a7d849b8966ad434df59a116": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96d4f0ca17c24c4ab642df47540b1253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fe91e44a382491b8d831308899c2e97",
              "IPY_MODEL_5a85585f255a4ed0941186e9bad28a55",
              "IPY_MODEL_0e67520e7bbe4a6d8b0a106fd4aaa30f"
            ],
            "layout": "IPY_MODEL_d491cec5fabc40e28f3578c95bb5486a"
          }
        },
        "6fe91e44a382491b8d831308899c2e97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b2039af73944e25bfbbdeb87e18f971",
            "placeholder": "​",
            "style": "IPY_MODEL_0a47177ea67745da8de4696e30ba090d",
            "value": "/root/.dgl/MSRC_9.zip: 100%"
          }
        },
        "5a85585f255a4ed0941186e9bad28a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e568b4e2823b4430a8a690f5aef2bd1d",
            "max": 98535,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c1c74aa695b4a3ba220bfd3ac5e35a1",
            "value": 98535
          }
        },
        "0e67520e7bbe4a6d8b0a106fd4aaa30f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65a644bc25ce47f7a7a3252f412536e6",
            "placeholder": "​",
            "style": "IPY_MODEL_fb4309f9f5fe4b528264a7b4ebe2bef4",
            "value": " 98.5k/98.5k [00:00&lt;00:00, 229kB/s]"
          }
        },
        "d491cec5fabc40e28f3578c95bb5486a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b2039af73944e25bfbbdeb87e18f971": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a47177ea67745da8de4696e30ba090d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e568b4e2823b4430a8a690f5aef2bd1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c1c74aa695b4a3ba220bfd3ac5e35a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65a644bc25ce47f7a7a3252f412536e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb4309f9f5fe4b528264a7b4ebe2bef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecc9806e5e844a249130c3195a00e716": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc79b227d0d34f4e845ed5f7f80989be",
              "IPY_MODEL_41910e9418fd4c0486590f926e2aa173",
              "IPY_MODEL_60f729506cfa46d2b379e78f88e08c3d"
            ],
            "layout": "IPY_MODEL_50f31647837c4e8dbc83814d38e0960b"
          }
        },
        "dc79b227d0d34f4e845ed5f7f80989be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f21554432ad147349b9a1fd56b75990b",
            "placeholder": "​",
            "style": "IPY_MODEL_c615049651be42b3809a3dd1b1389aca",
            "value": "/root/.dgl/DD.zip: 100%"
          }
        },
        "41910e9418fd4c0486590f926e2aa173": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02e0eabaa80b4b679ec2cc0a5f2eb5bb",
            "max": 4975113,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00c80d7fe9664ed497ab159dc2d88bb5",
            "value": 4975113
          }
        },
        "60f729506cfa46d2b379e78f88e08c3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b242a4d8988d4f73945fd89b95de9460",
            "placeholder": "​",
            "style": "IPY_MODEL_bb05aac5816b4116b7dac781c74b40cf",
            "value": " 4.98M/4.98M [00:01&lt;00:00, 8.70MB/s]"
          }
        },
        "50f31647837c4e8dbc83814d38e0960b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f21554432ad147349b9a1fd56b75990b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c615049651be42b3809a3dd1b1389aca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02e0eabaa80b4b679ec2cc0a5f2eb5bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00c80d7fe9664ed497ab159dc2d88bb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b242a4d8988d4f73945fd89b95de9460": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb05aac5816b4116b7dac781c74b40cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8eb8fe36883443f586487559b893b74b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90ae8f9568db49ca8ac8c07aa3016126",
              "IPY_MODEL_899768a3c5274e5a86806375c3c40fb0",
              "IPY_MODEL_2c8a08c4339b4d4f8e3e8d6ca976673c"
            ],
            "layout": "IPY_MODEL_1ce0ca355c354cefacfe1fe70e2f2054"
          }
        },
        "90ae8f9568db49ca8ac8c07aa3016126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c8b3df678e1465a927ae2ffd80e2b2d",
            "placeholder": "​",
            "style": "IPY_MODEL_f3393cef81ed42bb956e88763cb099d3",
            "value": "/root/.dgl/OHSU.zip: 100%"
          }
        },
        "899768a3c5274e5a86806375c3c40fb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b93a9427c5b14df3826cb915efe53a6c",
            "max": 79579,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f9755a528f94e1b9c2632aef52d5f5a",
            "value": 79579
          }
        },
        "2c8a08c4339b4d4f8e3e8d6ca976673c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4480a32e1f84438e8ef132c2cdcd77a9",
            "placeholder": "​",
            "style": "IPY_MODEL_7a4401eff9cb41a4baf45b058f096377",
            "value": " 79.6k/79.6k [00:00&lt;00:00, 221kB/s]"
          }
        },
        "1ce0ca355c354cefacfe1fe70e2f2054": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c8b3df678e1465a927ae2ffd80e2b2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3393cef81ed42bb956e88763cb099d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b93a9427c5b14df3826cb915efe53a6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f9755a528f94e1b9c2632aef52d5f5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4480a32e1f84438e8ef132c2cdcd77a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a4401eff9cb41a4baf45b058f096377": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "924a5fc4a5484eba90abc11193c58a95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77f2b2bd347d4583993ce0a818ad83a8",
              "IPY_MODEL_a9f626ee6aff404ba01cf1c99f085087",
              "IPY_MODEL_7946922f9af649e493fe70348b41cbe5"
            ],
            "layout": "IPY_MODEL_51a66050944b42b3ab1c7d7582263b33"
          }
        },
        "77f2b2bd347d4583993ce0a818ad83a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_309b32f8fb4e4d3181f502c03a6f2084",
            "placeholder": "​",
            "style": "IPY_MODEL_dfd1e18ada6c4f13bab6663f2978e081",
            "value": "/root/.dgl/MSRC_21.zip: 100%"
          }
        },
        "a9f626ee6aff404ba01cf1c99f085087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f8c70ea9ff44d509704629b02acf8c3",
            "max": 517131,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49660e1f2882437582bfe063f89d70a1",
            "value": 517131
          }
        },
        "7946922f9af649e493fe70348b41cbe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76ad7d2a003f4fb1a401bf98056619d0",
            "placeholder": "​",
            "style": "IPY_MODEL_5f670c644a1848b69a88621d0a6c276b",
            "value": " 517k/517k [00:00&lt;00:00, 810kB/s]"
          }
        },
        "51a66050944b42b3ab1c7d7582263b33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "309b32f8fb4e4d3181f502c03a6f2084": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfd1e18ada6c4f13bab6663f2978e081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f8c70ea9ff44d509704629b02acf8c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49660e1f2882437582bfe063f89d70a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76ad7d2a003f4fb1a401bf98056619d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f670c644a1848b69a88621d0a6c276b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import random\n",
        "import yaml\n",
        "import logging\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "from collections import namedtuple, Counter\n",
        "\n"
      ],
      "metadata": {
        "id": "-8pAx38U8CFz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.3.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.utils.data\n",
        "\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27b8pGSo9BV0",
        "outputId": "7dc6c262-45fc-414f-895e-79261fccb7a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch==2.3.1 in /usr/local/lib/python3.12/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.6.85)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.1) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyyaml tqdm tensorboardX scikit-learn ogb torchdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwn8iVfI-TyK",
        "outputId": "97947f1f-2bc5-4042-d495-91108ffafecf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.12/dist-packages (2.6.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: ogb in /usr/local/lib/python3.12/dist-packages (1.3.6)\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from tensorboardX) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboardX) (25.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.12/dist-packages (from tensorboardX) (5.29.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (2.3.1+cu121)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (2.5.0)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from ogb) (0.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchdata) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.12/dist-packages (from outdated>=0.2.0->ogb) (75.2.0)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.12/dist-packages (from outdated>=0.2.0->ogb) (0.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->ogb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.0->ogb) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch>=1.6.0->ogb) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->ogb) (12.6.85)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchdata) (2025.10.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.6.0->ogb) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  dgl -f https://data.dgl.ai/wheels/torch-2.3/cu121/repo.html\n",
        "from dgl.dataloading import GraphDataLoader\n",
        "\n",
        "from dgl.data import (\n",
        "    load_data,\n",
        "    TUDataset,\n",
        "    CoraGraphDataset,\n",
        "    CiteseerGraphDataset,\n",
        "    PubmedGraphDataset\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQnyk1NY8hcB",
        "outputId": "88284f05-3b71-4e21-d7d7-228da9b36c03"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/torch-2.3/cu121/repo.html\n",
            "Requirement already satisfied: dgl in /usr/local/lib/python3.12/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.12/dist-packages (from dgl) (3.5)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from dgl) (25.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from dgl) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (2.11.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from dgl) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from dgl) (1.16.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from dgl) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->dgl) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->dgl) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->dgl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->dgl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->dgl) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dgl.nn.pytorch.glob import SumPooling, AvgPooling, MaxPooling\n",
        "import dgl\n"
      ],
      "metadata": {
        "id": "AxDeohQ3W6Br"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PyTorch GPU available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"PyTorch GPU device count:\", torch.cuda.device_count())\n",
        "    print(\"PyTorch GPU device name:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht6uVoYgWOus",
        "outputId": "540d33ce-6bb6-40b5-c8c3-b8fc690509c3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch GPU available: True\n",
            "PyTorch GPU device count: 1\n",
            "PyTorch GPU device name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_best_configs(args, path):\n",
        "    with open(path, \"r\") as f:\n",
        "        configs = yaml.load(f, yaml.FullLoader)\n",
        "\n",
        "    if args.dataset not in configs:\n",
        "        logging.info(\"Best args not found\")\n",
        "        return args\n",
        "\n",
        "    logging.info(\"Using best configs\")\n",
        "    configs = configs[args.dataset]\n",
        "\n",
        "    for k, v in configs.items():\n",
        "        if \"lr\" in k or \"weight_decay\" in k:\n",
        "            v = float(v)\n",
        "        setattr(args, k, v)\n",
        "    print(\"------ Use best configs ------\")\n",
        "    return args"
      ],
      "metadata": {
        "id": "dnc0yM-B6zKU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_graph_classification_dataset(dataset_name, deg4feat=False):\n",
        "    dataset_name = dataset_name.upper()\n",
        "    dataset = TUDataset(dataset_name)\n",
        "    graph, _ = dataset[0]\n",
        "\n",
        "    if \"attr\" not in graph.ndata:\n",
        "        if \"node_labels\" in graph.ndata and not deg4feat:\n",
        "            print(\"Use node label as node features\")\n",
        "            feature_dim = 0\n",
        "            for g, _ in dataset:\n",
        "                feature_dim = max(feature_dim, g.ndata[\"node_labels\"].max().item())\n",
        "\n",
        "            feature_dim += 1\n",
        "            for g, l in dataset:\n",
        "                node_label = g.ndata[\"node_labels\"].view(-1)\n",
        "                feat = F.one_hot(node_label, num_classes=feature_dim).float()\n",
        "                g.ndata[\"attr\"] = feat\n",
        "        else:\n",
        "            print(\"Using degree as node features\")\n",
        "            feature_dim = 0\n",
        "            degrees = []\n",
        "            for g, _ in dataset:\n",
        "                feature_dim = max(feature_dim, g.in_degrees().max().item())\n",
        "                degrees.extend(g.in_degrees().tolist())\n",
        "            MAX_DEGREES = 400\n",
        "\n",
        "            oversize = 0\n",
        "            for d, n in Counter(degrees).items():\n",
        "                if d > MAX_DEGREES:\n",
        "                    oversize += n\n",
        "            # print(f\"N > {MAX_DEGREES}, #NUM: {oversize}, ratio: {oversize/sum(degrees):.8f}\")\n",
        "            feature_dim = min(feature_dim, MAX_DEGREES)\n",
        "\n",
        "            feature_dim += 1\n",
        "            for g, l in dataset:\n",
        "                degrees = g.in_degrees()\n",
        "                degrees[degrees > MAX_DEGREES] = MAX_DEGREES\n",
        "\n",
        "                feat = F.one_hot(degrees, num_classes=feature_dim).float()\n",
        "                g.ndata[\"attr\"] = feat\n",
        "    else:\n",
        "        print(\"******** Use `attr` as node features ********\")\n",
        "        feature_dim = graph.ndata[\"attr\"].shape[1]\n",
        "\n",
        "    labels = torch.tensor([x[1] for x in dataset])\n",
        "\n",
        "    num_classes = torch.max(labels).item() + 1\n",
        "    dataset = [(g.remove_self_loop().add_self_loop(), y) for g, y in dataset]\n",
        "\n",
        "    print(f\"******** # Num Graphs: {len(dataset)}, # Num Feat: {feature_dim}, # Num Classes: {num_classes} ********\")\n",
        "\n",
        "    return dataset, (feature_dim, num_classes)"
      ],
      "metadata": {
        "id": "os_LzYzV8SIt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    # graphs = [x[0].add_self_loop() for x in batch]\n",
        "    graphs = [x[0] for x in batch]\n",
        "    labels = [x[1] for x in batch]\n",
        "    batch_g = dgl.batch(graphs)\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "    return batch_g, labels"
      ],
      "metadata": {
        "id": "bSrcWBQrWqm-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_random_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.determinstic = True"
      ],
      "metadata": {
        "id": "yiqW2EyYW03x"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorboardX import SummaryWriter\n"
      ],
      "metadata": {
        "id": "E1arDsGpXiXj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TBLogger(object):\n",
        "    def __init__(self, log_path=\"./logging_data\", name=\"run\"):\n",
        "        super(TBLogger, self).__init__()\n",
        "\n",
        "        if not os.path.exists(log_path):\n",
        "            os.makedirs(log_path, exist_ok=True)\n",
        "\n",
        "        self.last_step = 0\n",
        "        self.log_path = log_path\n",
        "        raw_name = os.path.join(log_path, name)\n",
        "        name = raw_name\n",
        "        for i in range(1000):\n",
        "            name = raw_name + str(f\"_{i}\")\n",
        "            if not os.path.exists(name):\n",
        "                break\n",
        "        self.writer = SummaryWriter(logdir=name)\n",
        "\n",
        "    def note(self, metrics, step=None):\n",
        "        if step is None:\n",
        "            step = self.last_step\n",
        "        for key, value in metrics.items():\n",
        "            self.writer.add_scalar(key, value, step)\n",
        "        self.last_step = step\n",
        "\n",
        "    def finish(self):\n",
        "        self.writer.close()"
      ],
      "metadata": {
        "id": "yP1P8NGlXZ0Y"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n"
      ],
      "metadata": {
        "id": "nbxCidSiYK5a"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_activation(name):\n",
        "    if name == \"relu\":\n",
        "        return nn.ReLU()\n",
        "    elif name == \"gelu\":\n",
        "        return nn.GELU()\n",
        "    elif name == \"prelu\":\n",
        "        return nn.PReLU()\n",
        "    elif name is None:\n",
        "        return nn.Identity()\n",
        "    elif name == \"elu\":\n",
        "        return nn.ELU()\n",
        "    else:\n",
        "        raise NotImplementedError(f\"{name} is not implemented.\")"
      ],
      "metadata": {
        "id": "WqKZPG62Y2q1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GATConv(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_feats,\n",
        "                 out_feats,\n",
        "                 num_heads,\n",
        "                 feat_drop=0.,\n",
        "                 attn_drop=0.,\n",
        "                 negative_slope=0.2,\n",
        "                 residual=False,\n",
        "                 activation=None,\n",
        "                 allow_zero_in_degree=False,\n",
        "                 bias=True,\n",
        "                 norm=None,\n",
        "                 concat_out=True):\n",
        "        super(GATConv, self).__init__()\n",
        "        self._num_heads = num_heads\n",
        "        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n",
        "        self._out_feats = out_feats\n",
        "        self._allow_zero_in_degree = allow_zero_in_degree\n",
        "        self._concat_out = concat_out\n",
        "\n",
        "        if isinstance(in_feats, tuple):\n",
        "            self.fc_src = nn.Linear(\n",
        "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
        "            self.fc_dst = nn.Linear(\n",
        "                self._in_dst_feats, out_feats * num_heads, bias=False)\n",
        "        else:\n",
        "            self.fc = nn.Linear(\n",
        "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
        "        self.attn_l = nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_feats)))\n",
        "        self.attn_r = nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_feats)))\n",
        "        self.feat_drop = nn.Dropout(feat_drop)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope)\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.FloatTensor(size=(num_heads * out_feats,)))\n",
        "        else:\n",
        "            self.register_buffer('bias', None)\n",
        "        if residual:\n",
        "            if self._in_dst_feats != out_feats * num_heads:\n",
        "                self.res_fc = nn.Linear(\n",
        "                    self._in_dst_feats, num_heads * out_feats, bias=False)\n",
        "            else:\n",
        "                self.res_fc = nn.Identity()\n",
        "        else:\n",
        "            self.register_buffer('res_fc', None)\n",
        "        self.reset_parameters()\n",
        "        self.activation = activation\n",
        "        # if norm is not None:\n",
        "        #     self.norm = norm(num_heads * out_feats)\n",
        "        # else:\n",
        "        #     self.norm = None\n",
        "\n",
        "        self.norm = norm\n",
        "        if norm is not None:\n",
        "            self.norm = norm(num_heads * out_feats)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"\n",
        "\n",
        "        Description\n",
        "        -----------\n",
        "        Reinitialize learnable parameters.\n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        The fc weights :math:`W^{(l)}` are initialized using Glorot uniform initialization.\n",
        "        The attention weights are using xavier initialization method.\n",
        "        \"\"\"\n",
        "        gain = nn.init.calculate_gain('relu')\n",
        "        if hasattr(self, 'fc'):\n",
        "            nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
        "        else:\n",
        "            nn.init.xavier_normal_(self.fc_src.weight, gain=gain)\n",
        "            nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_l, gain=gain)\n",
        "        nn.init.xavier_normal_(self.attn_r, gain=gain)\n",
        "        if self.bias is not None:\n",
        "            nn.init.constant_(self.bias, 0)\n",
        "        if isinstance(self.res_fc, nn.Linear):\n",
        "            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n",
        "\n",
        "    def set_allow_zero_in_degree(self, set_value):\n",
        "        self._allow_zero_in_degree = set_value\n",
        "\n",
        "    def forward(self, graph, feat, get_attention=False):\n",
        "        with graph.local_scope():\n",
        "            if not self._allow_zero_in_degree:\n",
        "                if (graph.in_degrees() == 0).any():\n",
        "                    raise RuntimeError('There are 0-in-degree nodes in the graph, '\n",
        "                                   'output for those nodes will be invalid. '\n",
        "                                   'This is harmful for some applications, '\n",
        "                                   'causing silent performance regression. '\n",
        "                                   'Adding self-loop on the input graph by '\n",
        "                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n",
        "                                   'the issue. Setting ``allow_zero_in_degree`` '\n",
        "                                   'to be `True` when constructing this module will '\n",
        "                                   'suppress the check and let the code run.')\n",
        "\n",
        "            if isinstance(feat, tuple):\n",
        "                src_prefix_shape = feat[0].shape[:-1]\n",
        "                dst_prefix_shape = feat[1].shape[:-1]\n",
        "                h_src = self.feat_drop(feat[0])\n",
        "                h_dst = self.feat_drop(feat[1])\n",
        "                if not hasattr(self, 'fc_src'):\n",
        "                    feat_src = self.fc(h_src).view(\n",
        "                        *src_prefix_shape, self._num_heads, self._out_feats)\n",
        "                    feat_dst = self.fc(h_dst).view(\n",
        "                        *dst_prefix_shape, self._num_heads, self._out_feats)\n",
        "                else:\n",
        "                    feat_src = self.fc_src(h_src).view(\n",
        "                        *src_prefix_shape, self._num_heads, self._out_feats)\n",
        "                    feat_dst = self.fc_dst(h_dst).view(\n",
        "                        *dst_prefix_shape, self._num_heads, self._out_feats)\n",
        "            else:\n",
        "                src_prefix_shape = dst_prefix_shape = feat.shape[:-1]\n",
        "                h_src = h_dst = self.feat_drop(feat)\n",
        "                feat_src = feat_dst = self.fc(h_src).view(\n",
        "                    *src_prefix_shape, self._num_heads, self._out_feats)\n",
        "                if graph.is_block:\n",
        "                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n",
        "                    h_dst = h_dst[:graph.number_of_dst_nodes()]\n",
        "                    dst_prefix_shape = (graph.number_of_dst_nodes(),) + dst_prefix_shape[1:]\n",
        "            # NOTE: GAT paper uses \"first concatenation then linear projection\"\n",
        "            # to compute attention scores, while ours is \"first projection then\n",
        "            # addition\", the two approaches are mathematically equivalent:\n",
        "            # We decompose the weight vector a mentioned in the paper into\n",
        "            # [a_l || a_r], then\n",
        "            # a^T [Wh_i || Wh_j] = a_l Wh_i + a_r Wh_j\n",
        "            # Our implementation is much efficient because we do not need to\n",
        "            # save [Wh_i || Wh_j] on edges, which is not memory-efficient. Plus,\n",
        "            # addition could be optimized with DGL's built-in function u_add_v,\n",
        "            # which further speeds up computation and saves memory footprint.\n",
        "            el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n",
        "            er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n",
        "            graph.srcdata.update({'ft': feat_src, 'el': el})\n",
        "            graph.dstdata.update({'er': er})\n",
        "            # compute edge attention, el and er are a_l Wh_i and a_r Wh_j respectively.\n",
        "            graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n",
        "            e = self.leaky_relu(graph.edata.pop('e'))\n",
        "            # e[e == 0] = -1e3\n",
        "            # e = graph.edata.pop('e')\n",
        "            # compute softmax\n",
        "            graph.edata['a'] = self.attn_drop(edge_softmax(graph, e))\n",
        "            # message passing\n",
        "            graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n",
        "                             fn.sum('m', 'ft'))\n",
        "            rst = graph.dstdata['ft']\n",
        "\n",
        "            # bias\n",
        "            if self.bias is not None:\n",
        "                rst = rst + self.bias.view(\n",
        "                    *((1,) * len(dst_prefix_shape)), self._num_heads, self._out_feats)\n",
        "\n",
        "            # residual\n",
        "            if self.res_fc is not None:\n",
        "                # Use -1 rather than self._num_heads to handle broadcasting\n",
        "                resval = self.res_fc(h_dst).view(*dst_prefix_shape, -1, self._out_feats)\n",
        "                rst = rst + resval\n",
        "\n",
        "            if self._concat_out:\n",
        "                rst = rst.flatten(1)\n",
        "            else:\n",
        "                rst = torch.mean(rst, dim=1)\n",
        "\n",
        "            if self.norm is not None:\n",
        "                rst = self.norm(rst)\n",
        "\n",
        "            # activation\n",
        "            if self.activation:\n",
        "                rst = self.activation(rst)\n",
        "\n",
        "            if get_attention:\n",
        "                return rst, graph.edata['a']\n",
        "            else:\n",
        "                return rst"
      ],
      "metadata": {
        "id": "O33UjyQ7ZH-d"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_dim,\n",
        "                 num_hidden,\n",
        "                 out_dim,\n",
        "                 num_layers,\n",
        "                 nhead,\n",
        "                 nhead_out,\n",
        "                 activation,\n",
        "                 feat_drop,\n",
        "                 attn_drop,\n",
        "                 negative_slope,\n",
        "                 residual,\n",
        "                 norm,\n",
        "                 concat_out=False,\n",
        "                 encoding=False\n",
        "                 ):\n",
        "        super(GAT, self).__init__()\n",
        "        self.out_dim = out_dim\n",
        "        self.num_heads = nhead\n",
        "        self.num_layers = num_layers\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        self.activation = activation\n",
        "        self.concat_out = concat_out\n",
        "\n",
        "        last_activation = create_activation(activation) if encoding else None\n",
        "        last_residual = (encoding and residual)\n",
        "        last_norm = norm if encoding else None\n",
        "\n",
        "        if num_layers == 1:\n",
        "            self.gat_layers.append(GATConv(\n",
        "                in_dim, out_dim, nhead_out,\n",
        "                feat_drop, attn_drop, negative_slope, last_residual, norm=last_norm, concat_out=concat_out))\n",
        "        else:\n",
        "            # input projection (no residual)\n",
        "            self.gat_layers.append(GATConv(\n",
        "                in_dim, num_hidden, nhead,\n",
        "                feat_drop, attn_drop, negative_slope, residual, create_activation(activation), norm=norm, concat_out=concat_out))\n",
        "            # hidden layers\n",
        "            for l in range(1, num_layers - 1):\n",
        "                # due to multi-head, the in_dim = num_hidden * num_heads\n",
        "                self.gat_layers.append(GATConv(\n",
        "                    num_hidden * nhead, num_hidden, nhead,\n",
        "                    feat_drop, attn_drop, negative_slope, residual, create_activation(activation), norm=norm, concat_out=concat_out))\n",
        "            # output projection\n",
        "            self.gat_layers.append(GATConv(\n",
        "                num_hidden * nhead, out_dim, nhead_out,\n",
        "                feat_drop, attn_drop, negative_slope, last_residual, activation=last_activation, norm=last_norm, concat_out=concat_out))\n",
        "\n",
        "        # if norm is not None:\n",
        "        #     self.norms = nn.ModuleList([\n",
        "        #         norm(num_hidden * nhead)\n",
        "        #         for _ in range(num_layers - 1)\n",
        "        #     ])\n",
        "        #     if self.concat_out:\n",
        "        #         self.norms.append(norm(num_hidden * nhead))\n",
        "        # else:\n",
        "        #     self.norms = None\n",
        "\n",
        "        self.head = nn.Identity()\n",
        "\n",
        "    # def forward(self, g, inputs):\n",
        "    #     h = inputs\n",
        "    #     for l in range(self.num_layers):\n",
        "    #         h = self.gat_layers[l](g, h)\n",
        "    #         if l != self.num_layers - 1:\n",
        "    #             h = h.flatten(1)\n",
        "    #             if self.norms is not None:\n",
        "    #                 h = self.norms[l](h)\n",
        "    #     # output projection\n",
        "    #     if self.concat_out:\n",
        "    #         out = h.flatten(1)\n",
        "    #         if self.norms is not None:\n",
        "    #             out = self.norms[-1](out)\n",
        "    #     else:\n",
        "    #         out = h.mean(1)\n",
        "    #     return self.head(out)\n",
        "\n",
        "    def forward(self, g, inputs, return_hidden=False):\n",
        "        h = inputs\n",
        "        hidden_list = []\n",
        "        for l in range(self.num_layers):\n",
        "            h = self.gat_layers[l](g, h)\n",
        "            hidden_list.append(h)\n",
        "            # h = h.flatten(1)\n",
        "        # output projection\n",
        "        if return_hidden:\n",
        "            return self.head(h), hidden_list\n",
        "        else:\n",
        "            return self.head(h)\n",
        "\n",
        "    def reset_classifier(self, num_classes):\n",
        "        self.head = nn.Linear(self.num_heads * self.out_dim, num_classes)"
      ],
      "metadata": {
        "id": "FPVBcASxYtRB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NormLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, norm_type):\n",
        "        super().__init__()\n",
        "        if norm_type == \"batchnorm\":\n",
        "            self.norm = nn.BatchNorm1d(hidden_dim)\n",
        "        elif norm_type == \"layernorm\":\n",
        "            self.norm = nn.LayerNorm(hidden_dim)\n",
        "        elif norm_type == \"graphnorm\":\n",
        "            self.norm = norm_type\n",
        "            self.weight = nn.Parameter(torch.ones(hidden_dim))\n",
        "            self.bias = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "            self.mean_scale = nn.Parameter(torch.ones(hidden_dim))\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def forward(self, graph, x):\n",
        "        tensor = x\n",
        "        if self.norm is not None and type(self.norm) != str:\n",
        "            return self.norm(tensor)\n",
        "        elif self.norm is None:\n",
        "            return tensor\n",
        "\n",
        "        batch_list = graph.batch_num_nodes\n",
        "        batch_size = len(batch_list)\n",
        "        batch_list = torch.Tensor(batch_list).long().to(tensor.device)\n",
        "        batch_index = torch.arange(batch_size).to(tensor.device).repeat_interleave(batch_list)\n",
        "        batch_index = batch_index.view((-1,) + (1,) * (tensor.dim() - 1)).expand_as(tensor)\n",
        "        mean = torch.zeros(batch_size, *tensor.shape[1:]).to(tensor.device)\n",
        "        mean = mean.scatter_add_(0, batch_index, tensor)\n",
        "        mean = (mean.T / batch_list).T\n",
        "        mean = mean.repeat_interleave(batch_list, dim=0)\n",
        "\n",
        "        sub = tensor - mean * self.mean_scale\n",
        "\n",
        "        std = torch.zeros(batch_size, *tensor.shape[1:]).to(tensor.device)\n",
        "        std = std.scatter_add_(0, batch_index, sub.pow(2))\n",
        "        std = ((std.T / batch_list).T + 1e-6).sqrt()\n",
        "        std = std.repeat_interleave(batch_list, dim=0)\n",
        "        return self.weight * sub / std + self.bias"
      ],
      "metadata": {
        "id": "kfciwlOrZc6N"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_norm(name):\n",
        "    if name == \"layernorm\":\n",
        "        return nn.LayerNorm\n",
        "    elif name == \"batchnorm\":\n",
        "        return nn.BatchNorm1d\n",
        "    elif name == \"graphnorm\":\n",
        "        return partial(NormLayer, norm_type=\"groupnorm\")\n",
        "    else:\n",
        "        return nn.Identity\n"
      ],
      "metadata": {
        "id": "gkOefAYMZXRz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DotGatConv(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_feats,\n",
        "                 out_feats,\n",
        "                 num_heads,\n",
        "                 feat_drop,\n",
        "                 attn_drop,\n",
        "                 residual,\n",
        "                 activation=None,\n",
        "                 norm=None,\n",
        "                 concat_out=False,\n",
        "                 allow_zero_in_degree=False):\n",
        "        super(DotGatConv, self).__init__()\n",
        "        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n",
        "        self._out_feats = out_feats\n",
        "        self._allow_zero_in_degree = allow_zero_in_degree\n",
        "        self._num_heads = num_heads\n",
        "        self._concat_out = concat_out\n",
        "\n",
        "        self.feat_drop = nn.Dropout(feat_drop)\n",
        "        self.attn_drop = nn.Dropout(attn_drop) if attn_drop > 0 else nn.Identity()\n",
        "        self.activation = activation\n",
        "\n",
        "        if isinstance(in_feats, tuple):\n",
        "            self.fc_src = nn.Linear(self._in_src_feats, self._out_feats*self._num_heads, bias=False)\n",
        "            self.fc_dst = nn.Linear(self._in_dst_feats, self._out_feats*self._num_heads, bias=False)\n",
        "        else:\n",
        "            self.fc = nn.Linear(self._in_src_feats, self._out_feats*self._num_heads, bias=False)\n",
        "\n",
        "        if residual:\n",
        "            if self._in_dst_feats != out_feats * num_heads:\n",
        "                self.res_fc = nn.Linear(\n",
        "                    self._in_dst_feats, num_heads * out_feats, bias=False)\n",
        "            else:\n",
        "                self.res_fc = nn.Identity()\n",
        "        else:\n",
        "            self.register_buffer('res_fc', None)\n",
        "\n",
        "        self.norm = norm\n",
        "        if norm is not None:\n",
        "            self.norm = norm(num_heads * out_feats)\n",
        "\n",
        "    def forward(self, graph, feat, get_attention=False):\n",
        "        r\"\"\"\n",
        "\n",
        "        Description\n",
        "        -----------\n",
        "        Apply dot product version of self attention in GCN.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph: DGLGraph or bi_partities graph\n",
        "            The graph\n",
        "        feat: torch.Tensor or pair of torch.Tensor\n",
        "            If a torch.Tensor is given, the input feature of shape :math:`(N, D_{in})` where\n",
        "            :math:`D_{in}` is size of input feature, :math:`N` is the number of nodes.\n",
        "            If a pair of torch.Tensor is given, the pair must contain two tensors of shape\n",
        "            :math:`(N_{in}, D_{in_{src}})` and :math:`(N_{out}, D_{in_{dst}})`.\n",
        "        get_attention : bool, optional\n",
        "            Whether to return the attention values. Default to False.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            The output feature of shape :math:`(N, D_{out})` where :math:`D_{out}` is size\n",
        "            of output feature.\n",
        "        torch.Tensor, optional\n",
        "            The attention values of shape :math:`(E, 1)`, where :math:`E` is the number of\n",
        "            edges. This is returned only when :attr:`get_attention` is ``True``.\n",
        "\n",
        "        Raises\n",
        "        ------\n",
        "        DGLError\n",
        "            If there are 0-in-degree nodes in the input graph, it will raise DGLError\n",
        "            since no message will be passed to those nodes. This will cause invalid output.\n",
        "            The error can be ignored by setting ``allow_zero_in_degree`` parameter to ``True``.\n",
        "        \"\"\"\n",
        "\n",
        "        graph = graph.local_var()\n",
        "\n",
        "        if not self._allow_zero_in_degree:\n",
        "            if (graph.in_degrees() == 0).any():\n",
        "                raise ValueError('There are 0-in-degree nodes in the graph, '\n",
        "                               'output for those nodes will be invalid. '\n",
        "                               'This is harmful for some applications, '\n",
        "                               'causing silent performance regression. '\n",
        "                               'Adding self-loop on the input graph by '\n",
        "                               'calling `g = dgl.add_self_loop(g)` will resolve '\n",
        "                               'the issue. Setting ``allow_zero_in_degree`` '\n",
        "                               'to be `True` when constructing this module will '\n",
        "                               'suppress the check and let the code run.')\n",
        "\n",
        "        # check if feat is a tuple\n",
        "        if isinstance(feat, tuple):\n",
        "            h_src = feat[0]\n",
        "            h_dst = feat[1]\n",
        "            feat_src = self.fc_src(h_src).view(-1, self._num_heads, self._out_feats)\n",
        "            feat_dst = self.fc_dst(h_dst).view(-1, self._num_heads, self._out_feats)\n",
        "            print(\"!! tuple input in DotGAT !!\")\n",
        "        else:\n",
        "            feat = self.feat_drop(feat)\n",
        "            h_src = feat\n",
        "            feat_src = feat_dst = self.fc(h_src).view(-1, self._num_heads, self._out_feats)\n",
        "            if graph.is_block:\n",
        "                feat_dst = feat_src[:graph.number_of_dst_nodes()]\n",
        "\n",
        "        # Assign features to nodes\n",
        "        graph.srcdata.update({'ft': feat_src})\n",
        "        graph.dstdata.update({'ft': feat_dst})\n",
        "\n",
        "        # Step 1. dot product\n",
        "        graph.apply_edges(fn.u_dot_v('ft', 'ft', 'a'))\n",
        "\n",
        "        # Step 2. edge softmax to compute attention scores\n",
        "        graph.edata['sa'] = edge_softmax(graph, graph.edata['a'] / self._out_feats**0.5)\n",
        "        graph.edata[\"sa\"] = self.attn_drop(graph.edata[\"sa\"])\n",
        "        # Step 3. Broadcast softmax value to each edge, and aggregate dst node\n",
        "        graph.update_all(fn.u_mul_e('ft', 'sa', 'attn'), fn.sum('attn', 'agg_u'))\n",
        "\n",
        "        # output results to the destination nodes\n",
        "        rst = graph.dstdata['agg_u']\n",
        "\n",
        "        if self.res_fc is not None:\n",
        "            # Use -1 rather than self._num_heads to handle broadcasting\n",
        "            batch_size = feat.shape[0]\n",
        "            resval = self.res_fc(h_dst).view(batch_size, -1, self._out_feats)\n",
        "            rst = rst + resval\n",
        "\n",
        "        if self._concat_out:\n",
        "            rst = rst.flatten(1)\n",
        "        else:\n",
        "            rst = torch.mean(rst, dim=1)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            rst = self.norm(rst)\n",
        "\n",
        "        # activation\n",
        "        if self.activation:\n",
        "            rst = self.activation(rst)\n",
        "\n",
        "        if get_attention:\n",
        "            return rst, graph.edata['sa']\n",
        "        else:\n",
        "            return rst"
      ],
      "metadata": {
        "id": "goMNXRLbZoix"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DotGAT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_dim,\n",
        "                 num_hidden,\n",
        "                 out_dim,\n",
        "                 num_layers,\n",
        "                 nhead,\n",
        "                 nhead_out,\n",
        "                 activation,\n",
        "                 feat_drop,\n",
        "                 attn_drop,\n",
        "                 residual,\n",
        "                 norm,\n",
        "                 concat_out=False,\n",
        "                 encoding=False\n",
        "                 ):\n",
        "        super(DotGAT, self).__init__()\n",
        "        self.out_dim = out_dim\n",
        "        self.num_heads = nhead\n",
        "        self.num_layers = num_layers\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        self.activation = activation\n",
        "        self.concat_out = concat_out\n",
        "\n",
        "        last_activation = create_activation(activation) if encoding else None\n",
        "        last_residual = (encoding and residual)\n",
        "        last_norm = norm if encoding else None\n",
        "\n",
        "        if num_layers == 1:\n",
        "            self.gat_layers.append(DotGatConv(\n",
        "                in_dim, out_dim, nhead_out,\n",
        "                feat_drop, attn_drop, last_residual, norm=last_norm, concat_out=concat_out))\n",
        "        else:\n",
        "            # input projection (no residual)\n",
        "            self.gat_layers.append(DotGatConv(\n",
        "                in_dim, num_hidden, nhead,\n",
        "                feat_drop, attn_drop, residual, create_activation(activation), norm=norm, concat_out=concat_out))\n",
        "            # hidden layers\n",
        "            for l in range(1, num_layers - 1):\n",
        "                # due to multi-head, the in_dim = num_hidden * num_heads\n",
        "                self.gat_layers.append(DotGatConv(\n",
        "                    num_hidden * nhead, num_hidden, nhead,\n",
        "                    feat_drop, attn_drop, residual, create_activation(activation), norm=norm, concat_out=concat_out))\n",
        "            # output projection\n",
        "            self.gat_layers.append(DotGatConv(\n",
        "                num_hidden * nhead, out_dim, nhead_out,\n",
        "                feat_drop, attn_drop, last_residual, activation=last_activation, norm=last_norm, concat_out=concat_out))\n",
        "\n",
        "        self.head = nn.Identity()\n",
        "\n",
        "    def forward(self, g, inputs, return_hidden=False):\n",
        "        h = inputs\n",
        "        hidden_list = []\n",
        "        for l in range(self.num_layers):\n",
        "            h = self.gat_layers[l](g, h)\n",
        "            hidden_list.append(h)\n",
        "            # h = h.flatten(1)\n",
        "        # output projection\n",
        "        if return_hidden:\n",
        "            return self.head(h), hidden_list\n",
        "        else:\n",
        "            return self.head(h)\n",
        "\n",
        "    def reset_classifier(self, num_classes):\n",
        "        self.head = nn.Linear(self.num_heads * self.out_dim, num_classes)"
      ],
      "metadata": {
        "id": "JQSz_1UPZkUS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"MLP with linear output\"\"\"\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim, activation=\"relu\", norm=\"batchnorm\"):\n",
        "        super(MLP, self).__init__()\n",
        "        self.linear_or_not = True  # default is linear model\n",
        "        self.num_layers = num_layers\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        if num_layers < 1:\n",
        "            raise ValueError(\"number of layers should be positive!\")\n",
        "        elif num_layers == 1:\n",
        "            # Linear model\n",
        "            self.linear = nn.Linear(input_dim, output_dim)\n",
        "        else:\n",
        "            # Multi-layer model\n",
        "            self.linear_or_not = False\n",
        "            self.linears = torch.nn.ModuleList()\n",
        "            self.norms = torch.nn.ModuleList()\n",
        "            self.activations = torch.nn.ModuleList()\n",
        "\n",
        "            self.linears.append(nn.Linear(input_dim, hidden_dim))\n",
        "            for layer in range(num_layers - 2):\n",
        "                self.linears.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            self.linears.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "            for layer in range(num_layers - 1):\n",
        "                self.norms.append(create_norm(norm)(hidden_dim))\n",
        "                self.activations.append(create_activation(activation))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.linear_or_not:\n",
        "            # If linear model\n",
        "            return self.linear(x)\n",
        "        else:\n",
        "            # If MLP\n",
        "            h = x\n",
        "            for i in range(self.num_layers - 1):\n",
        "                h = self.norms[i](self.linears[i](h))\n",
        "                h = self.activations[i](h)\n",
        "            return self.linears[-1](h)"
      ],
      "metadata": {
        "id": "TTZ0wWs8ZxUn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ApplyNodeFunc(nn.Module):\n",
        "    \"\"\"Update the node feature hv with MLP, BN and ReLU.\"\"\"\n",
        "    def __init__(self, mlp, norm=\"batchnorm\", activation=\"relu\"):\n",
        "        super(ApplyNodeFunc, self).__init__()\n",
        "        self.mlp = mlp\n",
        "        norm_func = create_norm(norm)\n",
        "        if norm_func is None:\n",
        "            self.norm = nn.Identity()\n",
        "        else:\n",
        "            self.norm = norm_func(self.mlp.output_dim)\n",
        "        self.act = create_activation(activation)\n",
        "\n",
        "    def forward(self, h):\n",
        "        h = self.mlp(h)\n",
        "        h = self.norm(h)\n",
        "        h = self.act(h)\n",
        "        return h"
      ],
      "metadata": {
        "id": "HlrgbxSFZ9pN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl.function as fn\n",
        "from dgl.utils import expand_as_pair\n"
      ],
      "metadata": {
        "id": "z50RyYZhaJWi"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GINConv(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_dim,\n",
        "                 out_dim,\n",
        "                 apply_func,\n",
        "                 aggregator_type=\"sum\",\n",
        "                 init_eps=0,\n",
        "                 learn_eps=False,\n",
        "                 residual=False,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self._in_feats = in_dim\n",
        "        self._out_feats = out_dim\n",
        "        self.apply_func = apply_func\n",
        "\n",
        "        self._aggregator_type = aggregator_type\n",
        "        if aggregator_type == 'sum':\n",
        "            self._reducer = fn.sum\n",
        "        elif aggregator_type == 'max':\n",
        "            self._reducer = fn.max\n",
        "        elif aggregator_type == 'mean':\n",
        "            self._reducer = fn.mean\n",
        "        else:\n",
        "            raise KeyError('Aggregator type {} not recognized.'.format(aggregator_type))\n",
        "\n",
        "        if learn_eps:\n",
        "            self.eps = torch.nn.Parameter(torch.FloatTensor([init_eps]))\n",
        "        else:\n",
        "            self.register_buffer('eps', torch.FloatTensor([init_eps]))\n",
        "\n",
        "        if residual:\n",
        "            if self._in_feats != self._out_feats:\n",
        "                self.res_fc = nn.Linear(\n",
        "                    self._in_feats, self._out_feats, bias=False)\n",
        "                print(\"! Linear Residual !\")\n",
        "            else:\n",
        "                print(\"Identity Residual \")\n",
        "                self.res_fc = nn.Identity()\n",
        "        else:\n",
        "            self.register_buffer('res_fc', None)\n",
        "\n",
        "    def forward(self, graph, feat):\n",
        "        with graph.local_scope():\n",
        "            aggregate_fn = fn.copy_u('h', 'm')\n",
        "\n",
        "            feat_src, feat_dst = expand_as_pair(feat, graph)\n",
        "            graph.srcdata['h'] = feat_src\n",
        "            graph.update_all(aggregate_fn, self._reducer('m', 'neigh'))\n",
        "            rst = (1 + self.eps) * feat_dst + graph.dstdata['neigh']\n",
        "            if self.apply_func is not None:\n",
        "                rst = self.apply_func(rst)\n",
        "\n",
        "            if self.res_fc is not None:\n",
        "                rst = rst + self.res_fc(feat_dst)\n",
        "\n",
        "            return rst"
      ],
      "metadata": {
        "id": "FJAAy6_DaD7w"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GIN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_dim,\n",
        "                 num_hidden,\n",
        "                 out_dim,\n",
        "                 num_layers,\n",
        "                 dropout,\n",
        "                 activation,\n",
        "                 residual,\n",
        "                 norm,\n",
        "                 encoding=False,\n",
        "                 learn_eps=False,\n",
        "                 aggr=\"sum\",\n",
        "                 ):\n",
        "        super(GIN, self).__init__()\n",
        "        self.out_dim = out_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.activation = activation\n",
        "        self.dropout = dropout\n",
        "\n",
        "        last_activation = create_activation(activation) if encoding else None\n",
        "        last_residual = encoding and residual\n",
        "        last_norm = norm if encoding else None\n",
        "\n",
        "        if num_layers == 1:\n",
        "            apply_func = MLP(2, in_dim, num_hidden, out_dim, activation=activation, norm=norm)\n",
        "            if last_norm:\n",
        "                apply_func = ApplyNodeFunc(apply_func, norm=norm, activation=activation)\n",
        "            self.layers.append(GINConv(in_dim, out_dim, apply_func, init_eps=0, learn_eps=learn_eps, residual=last_residual))\n",
        "        else:\n",
        "            # input projection (no residual)\n",
        "            self.layers.append(GINConv(\n",
        "                in_dim,\n",
        "                num_hidden,\n",
        "                ApplyNodeFunc(MLP(2, in_dim, num_hidden, num_hidden, activation=activation, norm=norm), activation=activation, norm=norm),\n",
        "                init_eps=0,\n",
        "                learn_eps=learn_eps,\n",
        "                residual=residual)\n",
        "                )\n",
        "            # hidden layers\n",
        "            for l in range(1, num_layers - 1):\n",
        "                # due to multi-head, the in_dim = num_hidden * num_heads\n",
        "                self.layers.append(GINConv(\n",
        "                    num_hidden, num_hidden,\n",
        "                    ApplyNodeFunc(MLP(2, num_hidden, num_hidden, num_hidden, activation=activation, norm=norm), activation=activation, norm=norm),\n",
        "                    init_eps=0,\n",
        "                    learn_eps=learn_eps,\n",
        "                    residual=residual)\n",
        "                )\n",
        "            # output projection\n",
        "            apply_func = MLP(2, num_hidden, num_hidden, out_dim, activation=activation, norm=norm)\n",
        "            if last_norm:\n",
        "                apply_func = ApplyNodeFunc(apply_func, activation=activation, norm=norm)\n",
        "\n",
        "            self.layers.append(GINConv(num_hidden, out_dim, apply_func, init_eps=0, learn_eps=learn_eps, residual=last_residual))\n",
        "\n",
        "        self.head = nn.Identity()\n",
        "\n",
        "    def forward(self, g, inputs, return_hidden=False):\n",
        "        h = inputs\n",
        "        hidden_list = []\n",
        "        for l in range(self.num_layers):\n",
        "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "            h = self.layers[l](g, h)\n",
        "            hidden_list.append(h)\n",
        "        # output projection\n",
        "        if return_hidden:\n",
        "            return self.head(h), hidden_list\n",
        "        else:\n",
        "            return self.head(h)\n",
        "\n",
        "    def reset_classifier(self, num_classes):\n",
        "        self.head = nn.Linear(self.out_dim, num_classes)"
      ],
      "metadata": {
        "id": "odM2q-sKZxAy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphConv(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_dim,\n",
        "                 out_dim,\n",
        "                 norm=None,\n",
        "                 activation=None,\n",
        "                 residual=True,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self._in_feats = in_dim\n",
        "        self._out_feats = out_dim\n",
        "\n",
        "        self.fc = nn.Linear(in_dim, out_dim)\n",
        "\n",
        "        if residual:\n",
        "            if self._in_feats != self._out_feats:\n",
        "                self.res_fc = nn.Linear(\n",
        "                    self._in_feats, self._out_feats, bias=False)\n",
        "                print(\"! Linear Residual !\")\n",
        "            else:\n",
        "                print(\"Identity Residual \")\n",
        "                self.res_fc = nn.Identity()\n",
        "        else:\n",
        "            self.register_buffer('res_fc', None)\n",
        "\n",
        "        # if norm == \"batchnorm\":\n",
        "        #     self.norm = nn.BatchNorm1d(out_dim)\n",
        "        # elif norm == \"layernorm\":\n",
        "        #     self.norm = nn.LayerNorm(out_dim)\n",
        "        # else:\n",
        "        #     self.norm = None\n",
        "\n",
        "        self.norm = norm\n",
        "        if norm is not None:\n",
        "            self.norm = norm(out_dim)\n",
        "        self._activation = activation\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.fc.reset_parameters()\n",
        "\n",
        "    def forward(self, graph, feat):\n",
        "        with graph.local_scope():\n",
        "            aggregate_fn = fn.copy_u('h', 'm')\n",
        "            # if edge_weight is not None:\n",
        "            #     assert edge_weight.shape[0] == graph.number_of_edges()\n",
        "            #     graph.edata['_edge_weight'] = edge_weight\n",
        "            #     aggregate_fn = fn.u_mul_e('h', '_edge_weight', 'm')\n",
        "\n",
        "            # (BarclayII) For RGCN on heterogeneous graphs we need to support GCN on bipartite.\n",
        "            feat_src, feat_dst = expand_as_pair(feat, graph)\n",
        "            # if self._norm in ['left', 'both']:\n",
        "            degs = graph.out_degrees().float().clamp(min=1)\n",
        "            norm = torch.pow(degs, -0.5)\n",
        "            shp = norm.shape + (1,) * (feat_src.dim() - 1)\n",
        "            norm = torch.reshape(norm, shp)\n",
        "            feat_src = feat_src * norm\n",
        "\n",
        "            # if self._in_feats > self._out_feats:\n",
        "            #     # mult W first to reduce the feature size for aggregation.\n",
        "            #     # if weight is not None:\n",
        "            #         # feat_src = th.matmul(feat_src, weight)\n",
        "            #     graph.srcdata['h'] = feat_src\n",
        "            #     graph.update_all(aggregate_fn, fn.sum(msg='m', out='h'))\n",
        "            #     rst = graph.dstdata['h']\n",
        "            # else:\n",
        "            # aggregate first then mult W\n",
        "            graph.srcdata['h'] = feat_src\n",
        "            graph.update_all(aggregate_fn, fn.sum(msg='m', out='h'))\n",
        "            rst = graph.dstdata['h']\n",
        "\n",
        "            rst = self.fc(rst)\n",
        "\n",
        "            # if self._norm in ['right', 'both']:\n",
        "            degs = graph.in_degrees().float().clamp(min=1)\n",
        "            norm = torch.pow(degs, -0.5)\n",
        "            shp = norm.shape + (1,) * (feat_dst.dim() - 1)\n",
        "            norm = torch.reshape(norm, shp)\n",
        "            rst = rst * norm\n",
        "\n",
        "            if self.res_fc is not None:\n",
        "                rst = rst + self.res_fc(feat_dst)\n",
        "\n",
        "            if self.norm is not None:\n",
        "                rst = self.norm(rst)\n",
        "\n",
        "            if self._activation is not None:\n",
        "                rst = self._activation(rst)\n",
        "\n",
        "            return rst"
      ],
      "metadata": {
        "id": "u2UJMwGAakYW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_dim,\n",
        "                 num_hidden,\n",
        "                 out_dim,\n",
        "                 num_layers,\n",
        "                 dropout,\n",
        "                 activation,\n",
        "                 residual,\n",
        "                 norm,\n",
        "                 encoding=False\n",
        "                 ):\n",
        "        super(GCN, self).__init__()\n",
        "        self.out_dim = out_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.gcn_layers = nn.ModuleList()\n",
        "        self.activation = activation\n",
        "        self.dropout = dropout\n",
        "\n",
        "        last_activation = create_activation(activation) if encoding else None\n",
        "        last_residual = encoding and residual\n",
        "        last_norm = norm if encoding else None\n",
        "\n",
        "        if num_layers == 1:\n",
        "            self.gcn_layers.append(GraphConv(\n",
        "                in_dim, out_dim, residual=last_residual, norm=last_norm, activation=last_activation))\n",
        "        else:\n",
        "            # input projection (no residual)\n",
        "            self.gcn_layers.append(GraphConv(\n",
        "                in_dim, num_hidden, residual=residual, norm=norm, activation=create_activation(activation)))\n",
        "            # hidden layers\n",
        "            for l in range(1, num_layers - 1):\n",
        "                # due to multi-head, the in_dim = num_hidden * num_heads\n",
        "                self.gcn_layers.append(GraphConv(\n",
        "                    num_hidden, num_hidden, residual=residual, norm=norm, activation=create_activation(activation)))\n",
        "            # output projection\n",
        "            self.gcn_layers.append(GraphConv(\n",
        "                num_hidden, out_dim, residual=last_residual, activation=last_activation, norm=last_norm))\n",
        "\n",
        "        # if norm is not None:\n",
        "        #     self.norms = nn.ModuleList([\n",
        "        #         norm(num_hidden)\n",
        "        #         for _ in range(num_layers - 1)\n",
        "        #     ])\n",
        "        #     if not encoding:\n",
        "        #         self.norms.append(norm(out_dim))\n",
        "        # else:\n",
        "        #     self.norms = None\n",
        "        self.norms = None\n",
        "        self.head = nn.Identity()\n",
        "\n",
        "    def forward(self, g, inputs, return_hidden=False):\n",
        "        h = inputs\n",
        "        hidden_list = []\n",
        "        for l in range(self.num_layers):\n",
        "            h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "            h = self.gcn_layers[l](g, h)\n",
        "            if self.norms is not None and l != self.num_layers - 1:\n",
        "                h = self.norms[l](h)\n",
        "            hidden_list.append(h)\n",
        "        # output projection\n",
        "        if self.norms is not None and len(self.norms) == self.num_layers:\n",
        "            h = self.norms[-1](h)\n",
        "        if return_hidden:\n",
        "            return self.head(h), hidden_list\n",
        "        else:\n",
        "            return self.head(h)\n",
        "\n",
        "    def reset_classifier(self, num_classes):\n",
        "        self.head = nn.Linear(self.out_dim, num_classes)"
      ],
      "metadata": {
        "id": "ecG1XhbMabrz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_module(m_type, enc_dec, in_dim, num_hidden, out_dim, num_layers, dropout, activation, residual, norm, nhead, nhead_out, attn_drop, negative_slope=0.2, concat_out=True) -> nn.Module:\n",
        "    if m_type == \"gat\":\n",
        "        mod = GAT(\n",
        "            in_dim=in_dim,\n",
        "            num_hidden=num_hidden,\n",
        "            out_dim=out_dim,\n",
        "            num_layers=num_layers,\n",
        "            nhead=nhead,\n",
        "            nhead_out=nhead_out,\n",
        "            concat_out=concat_out,\n",
        "            activation=activation,\n",
        "            feat_drop=dropout,\n",
        "            attn_drop=attn_drop,\n",
        "            negative_slope=negative_slope,\n",
        "            residual=residual,\n",
        "            norm=create_norm(norm),\n",
        "            encoding=(enc_dec == \"encoding\"),\n",
        "        )\n",
        "    elif m_type == \"dotgat\":\n",
        "        mod = DotGAT(\n",
        "            in_dim=in_dim,\n",
        "            num_hidden=num_hidden,\n",
        "            out_dim=out_dim,\n",
        "            num_layers=num_layers,\n",
        "            nhead=nhead,\n",
        "            nhead_out=nhead_out,\n",
        "            concat_out=concat_out,\n",
        "            activation=activation,\n",
        "            feat_drop=dropout,\n",
        "            attn_drop=attn_drop,\n",
        "            residual=residual,\n",
        "            norm=create_norm(norm),\n",
        "            encoding=(enc_dec == \"encoding\"),\n",
        "        )\n",
        "    elif m_type == \"gin\":\n",
        "        mod = GIN(\n",
        "            in_dim=in_dim,\n",
        "            num_hidden=num_hidden,\n",
        "            out_dim=out_dim,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            activation=activation,\n",
        "            residual=residual,\n",
        "            norm=norm,\n",
        "            encoding=(enc_dec == \"encoding\"),\n",
        "        )\n",
        "    elif m_type == \"gcn\":\n",
        "        mod = GCN(\n",
        "            in_dim=in_dim,\n",
        "            num_hidden=num_hidden,\n",
        "            out_dim=out_dim,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            activation=activation,\n",
        "            residual=residual,\n",
        "            norm=create_norm(norm),\n",
        "            encoding=(enc_dec == \"encoding\")\n",
        "        )\n",
        "    elif m_type == \"mlp\":\n",
        "        # * just for decoder\n",
        "        mod = nn.Sequential(\n",
        "            nn.Linear(in_dim, num_hidden),\n",
        "            nn.PReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(num_hidden, out_dim)\n",
        "        )\n",
        "    elif m_type == \"linear\":\n",
        "        mod = nn.Linear(in_dim, out_dim)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    return mod"
      ],
      "metadata": {
        "id": "hzlK8nBwYacW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sce_loss(x, y, alpha=3):\n",
        "    x = F.normalize(x, p=2, dim=-1)\n",
        "    y = F.normalize(y, p=2, dim=-1)\n",
        "\n",
        "    # loss =  - (x * y).sum(dim=-1)\n",
        "    # loss = (x_h - y_h).norm(dim=1).pow(alpha)\n",
        "\n",
        "    loss = (1 - (x * y).sum(dim=-1)).pow_(alpha)\n",
        "\n",
        "    loss = loss.mean()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "UX3YBzYsa4hV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_edge(graph, mask_prob):\n",
        "    E = graph.num_edges()\n",
        "\n",
        "    mask_rates = torch.FloatTensor(np.ones(E) * mask_prob)\n",
        "    masks = torch.bernoulli(1 - mask_rates)\n",
        "    mask_idx = masks.nonzero().squeeze(1)\n",
        "    return mask_idx"
      ],
      "metadata": {
        "id": "jd6NrQmUbru6"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_edge(graph, drop_rate, return_edges=False):\n",
        "    if drop_rate <= 0:\n",
        "        return graph\n",
        "\n",
        "    n_node = graph.num_nodes()\n",
        "    edge_mask = mask_edge(graph, drop_rate)\n",
        "    src = graph.edges()[0]\n",
        "    dst = graph.edges()[1]\n",
        "\n",
        "    nsrc = src[edge_mask]\n",
        "    ndst = dst[edge_mask]\n",
        "\n",
        "    ng = dgl.graph((nsrc, ndst), num_nodes=n_node)\n",
        "    ng = ng.add_self_loop()\n",
        "\n",
        "    dsrc = src[~edge_mask]\n",
        "    ddst = dst[~edge_mask]\n",
        "\n",
        "    if return_edges:\n",
        "        return ng, (dsrc, ddst)\n",
        "    return ng"
      ],
      "metadata": {
        "id": "5VhPXkhobnEf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain\n"
      ],
      "metadata": {
        "id": "hN6af2Bkb-VP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreModel(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_dim: int,\n",
        "            num_hidden: int,\n",
        "            num_layers: int,\n",
        "            nhead: int,\n",
        "            nhead_out: int,\n",
        "            activation: str,\n",
        "            feat_drop: float,\n",
        "            attn_drop: float,\n",
        "            negative_slope: float,\n",
        "            residual: bool,\n",
        "            norm: Optional[str],\n",
        "            mask_rate: float = 0.3,\n",
        "            encoder_type: str = \"gat\",\n",
        "            decoder_type: str = \"gat\",\n",
        "            loss_fn: str = \"sce\",\n",
        "            drop_edge_rate: float = 0.0,\n",
        "            replace_rate: float = 0.1,\n",
        "            alpha_l: float = 2,\n",
        "            concat_hidden: bool = False,\n",
        "         ):\n",
        "        super(PreModel, self).__init__()\n",
        "        self._mask_rate = mask_rate\n",
        "\n",
        "        self._encoder_type = encoder_type\n",
        "        self._decoder_type = decoder_type\n",
        "        self._drop_edge_rate = drop_edge_rate\n",
        "        self._output_hidden_size = num_hidden\n",
        "        self._concat_hidden = concat_hidden\n",
        "\n",
        "        self._replace_rate = replace_rate\n",
        "        self._mask_token_rate = 1 - self._replace_rate\n",
        "\n",
        "        assert num_hidden % nhead == 0\n",
        "        assert num_hidden % nhead_out == 0\n",
        "        if encoder_type in (\"gat\", \"dotgat\"):\n",
        "            enc_num_hidden = num_hidden // nhead\n",
        "            enc_nhead = nhead\n",
        "        else:\n",
        "            enc_num_hidden = num_hidden\n",
        "            enc_nhead = 1\n",
        "\n",
        "        dec_in_dim = num_hidden\n",
        "        dec_num_hidden = num_hidden // nhead_out if decoder_type in (\"gat\", \"dotgat\") else num_hidden\n",
        "\n",
        "        # build encoder\n",
        "        self.encoder = setup_module(\n",
        "            m_type=encoder_type,\n",
        "            enc_dec=\"encoding\",\n",
        "            in_dim=in_dim,\n",
        "            num_hidden=enc_num_hidden,\n",
        "            out_dim=enc_num_hidden,\n",
        "            num_layers=num_layers,\n",
        "            nhead=enc_nhead,\n",
        "            nhead_out=enc_nhead,\n",
        "            concat_out=True,\n",
        "            activation=activation,\n",
        "            dropout=feat_drop,\n",
        "            attn_drop=attn_drop,\n",
        "            negative_slope=negative_slope,\n",
        "            residual=residual,\n",
        "            norm=norm,\n",
        "        )\n",
        "\n",
        "        # build decoder for attribute prediction\n",
        "        self.decoder = setup_module(\n",
        "            m_type=decoder_type,\n",
        "            enc_dec=\"decoding\",\n",
        "            in_dim=dec_in_dim,\n",
        "            num_hidden=dec_num_hidden,\n",
        "            out_dim=in_dim,\n",
        "            num_layers=1,\n",
        "            nhead=nhead,\n",
        "            nhead_out=nhead_out,\n",
        "            activation=activation,\n",
        "            dropout=feat_drop,\n",
        "            attn_drop=attn_drop,\n",
        "            negative_slope=negative_slope,\n",
        "            residual=residual,\n",
        "            norm=norm,\n",
        "            concat_out=True,\n",
        "        )\n",
        "\n",
        "        self.enc_mask_token = nn.Parameter(torch.zeros(1, in_dim))\n",
        "        if concat_hidden:\n",
        "            self.encoder_to_decoder = nn.Linear(dec_in_dim * num_layers, dec_in_dim, bias=False)\n",
        "        else:\n",
        "            self.encoder_to_decoder = nn.Linear(dec_in_dim, dec_in_dim, bias=False)\n",
        "\n",
        "        # * setup loss function\n",
        "        self.criterion = self.setup_loss_fn(loss_fn, alpha_l)\n",
        "\n",
        "    @property\n",
        "    def output_hidden_dim(self):\n",
        "        return self._output_hidden_size\n",
        "\n",
        "    def setup_loss_fn(self, loss_fn, alpha_l):\n",
        "        if loss_fn == \"mse\":\n",
        "            criterion = nn.MSELoss()\n",
        "        elif loss_fn == \"sce\":\n",
        "            criterion = partial(sce_loss, alpha=alpha_l)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        return criterion\n",
        "\n",
        "    def encoding_mask_noise(self, g, x, mask_rate=0.3):\n",
        "        num_nodes = g.num_nodes()\n",
        "        perm = torch.randperm(num_nodes, device=x.device)\n",
        "        num_mask_nodes = int(mask_rate * num_nodes)\n",
        "\n",
        "        # random masking\n",
        "        num_mask_nodes = int(mask_rate * num_nodes)\n",
        "        mask_nodes = perm[: num_mask_nodes]\n",
        "        keep_nodes = perm[num_mask_nodes: ]\n",
        "\n",
        "        if self._replace_rate > 0:\n",
        "            num_noise_nodes = int(self._replace_rate * num_mask_nodes)\n",
        "            perm_mask = torch.randperm(num_mask_nodes, device=x.device)\n",
        "            token_nodes = mask_nodes[perm_mask[: int(self._mask_token_rate * num_mask_nodes)]]\n",
        "            noise_nodes = mask_nodes[perm_mask[-int(self._replace_rate * num_mask_nodes):]]\n",
        "            noise_to_be_chosen = torch.randperm(num_nodes, device=x.device)[:num_noise_nodes]\n",
        "\n",
        "            out_x = x.clone()\n",
        "            out_x[token_nodes] = 0.0\n",
        "            out_x[noise_nodes] = x[noise_to_be_chosen]\n",
        "        else:\n",
        "            out_x = x.clone()\n",
        "            token_nodes = mask_nodes\n",
        "            out_x[mask_nodes] = 0.0\n",
        "\n",
        "        out_x[token_nodes] += self.enc_mask_token\n",
        "        use_g = g.clone()\n",
        "\n",
        "        return use_g, out_x, (mask_nodes, keep_nodes)\n",
        "\n",
        "    def forward(self, g, x):\n",
        "        # ---- attribute reconstruction ----\n",
        "        loss = self.mask_attr_prediction(g, x)\n",
        "        loss_item = {\"loss\": loss.item()}\n",
        "        return loss, loss_item\n",
        "\n",
        "    def mask_attr_prediction(self, g, x):\n",
        "        pre_use_g, use_x, (mask_nodes, keep_nodes) = self.encoding_mask_noise(g, x, self._mask_rate)\n",
        "\n",
        "        if self._drop_edge_rate > 0:\n",
        "            use_g, masked_edges = drop_edge(pre_use_g, self._drop_edge_rate, return_edges=True)\n",
        "        else:\n",
        "            use_g = pre_use_g\n",
        "\n",
        "        enc_rep, all_hidden = self.encoder(use_g, use_x, return_hidden=True)\n",
        "        if self._concat_hidden:\n",
        "            enc_rep = torch.cat(all_hidden, dim=1)\n",
        "\n",
        "        # ---- attribute reconstruction ----\n",
        "        rep = self.encoder_to_decoder(enc_rep)\n",
        "\n",
        "        if self._decoder_type not in (\"mlp\", \"linear\"):\n",
        "            # * remask, re-mask\n",
        "            rep[mask_nodes] = 0\n",
        "\n",
        "        if self._decoder_type in (\"mlp\", \"liear\") :\n",
        "            recon = self.decoder(rep)\n",
        "        else:\n",
        "            recon = self.decoder(pre_use_g, rep)\n",
        "\n",
        "        x_init = x[mask_nodes]\n",
        "        x_rec = recon[mask_nodes]\n",
        "\n",
        "        loss = self.criterion(x_rec, x_init)\n",
        "        return loss\n",
        "\n",
        "    def embed(self, g, x):\n",
        "        rep = self.encoder(g, x)\n",
        "        return rep\n",
        "\n",
        "    @property\n",
        "    def enc_params(self):\n",
        "        return self.encoder.parameters()\n",
        "\n",
        "    @property\n",
        "    def dec_params(self):\n",
        "        return chain(*[self.encoder_to_decoder.parameters(), self.decoder.parameters()])"
      ],
      "metadata": {
        "id": "fI9JfvPvYCyx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def build_model(args):\n",
        "    num_heads = args.num_heads\n",
        "    num_out_heads = args.num_out_heads\n",
        "    num_hidden = args.num_hidden\n",
        "    num_layers = args.num_layers\n",
        "    residual = args.residual\n",
        "    attn_drop = args.attn_drop\n",
        "    in_drop = args.in_drop\n",
        "    norm = args.norm\n",
        "    negative_slope = args.negative_slope\n",
        "    encoder_type = args.encoder\n",
        "    decoder_type = args.decoder\n",
        "    mask_rate = args.mask_rate\n",
        "    drop_edge_rate = args.drop_edge_rate\n",
        "    replace_rate = args.replace_rate\n",
        "\n",
        "\n",
        "    activation = args.activation\n",
        "    loss_fn = args.loss_fn\n",
        "    alpha_l = args.alpha_l\n",
        "    concat_hidden = args.concat_hidden\n",
        "    num_features = args.num_features\n",
        "\n",
        "\n",
        "    model = PreModel(\n",
        "        in_dim=num_features,\n",
        "        num_hidden=num_hidden,\n",
        "        num_layers=num_layers,\n",
        "        nhead=num_heads,\n",
        "        nhead_out=num_out_heads,\n",
        "        activation=activation,\n",
        "        feat_drop=in_drop,\n",
        "        attn_drop=attn_drop,\n",
        "        negative_slope=negative_slope,\n",
        "        residual=residual,\n",
        "        encoder_type=encoder_type,\n",
        "        decoder_type=decoder_type,\n",
        "        mask_rate=mask_rate,\n",
        "        norm=norm,\n",
        "        loss_fn=loss_fn,\n",
        "        drop_edge_rate=drop_edge_rate,\n",
        "        replace_rate=replace_rate,\n",
        "        alpha_l=alpha_l,\n",
        "        concat_hidden=concat_hidden,\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "CDmd22J3XwR5"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_optimizer(opt, model, lr, weight_decay, get_num_layer=None, get_layer_scale=None):\n",
        "    opt_lower = opt.lower()\n",
        "\n",
        "    parameters = model.parameters()\n",
        "    opt_args = dict(lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    opt_split = opt_lower.split(\"_\")\n",
        "    opt_lower = opt_split[-1]\n",
        "    if opt_lower == \"adam\":\n",
        "        optimizer = optim.Adam(parameters, **opt_args)\n",
        "    elif opt_lower == \"adamw\":\n",
        "        optimizer = optim.AdamW(parameters, **opt_args)\n",
        "    elif opt_lower == \"adadelta\":\n",
        "        optimizer = optim.Adadelta(parameters, **opt_args)\n",
        "    elif opt_lower == \"radam\":\n",
        "        optimizer = optim.RAdam(parameters, **opt_args)\n",
        "    elif opt_lower == \"sgd\":\n",
        "        opt_args[\"momentum\"] = 0.9\n",
        "        return optim.SGD(parameters, **opt_args)\n",
        "    else:\n",
        "        assert False and \"Invalid optimizer\"\n",
        "\n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "lXf38O5UcQje"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def get_current_lr(optimizer):\n",
        "    return optimizer.state_dict()[\"param_groups\"][0][\"lr\"]"
      ],
      "metadata": {
        "id": "R4GsZC8jcipT"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretrain(model, pooler, dataloaders, optimizer, max_epoch, device, scheduler, num_classes, lr_f, weight_decay_f, max_epoch_f, linear_prob=True, logger=None):\n",
        "    train_loader, eval_loader = dataloaders\n",
        "\n",
        "    epoch_iter = tqdm(range(max_epoch))\n",
        "    for epoch in epoch_iter:\n",
        "        model.train()\n",
        "        loss_list = []\n",
        "        for batch in train_loader:\n",
        "            batch_g, _ = batch\n",
        "            batch_g = batch_g.to(device)\n",
        "\n",
        "            feat = batch_g.ndata[\"attr\"]\n",
        "            model.train()\n",
        "            loss, loss_dict = model(batch_g, feat)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_list.append(loss.item())\n",
        "            if logger is not None:\n",
        "                loss_dict[\"lr\"] = get_current_lr(optimizer)\n",
        "                logger.note(loss_dict, step=epoch)\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        epoch_iter.set_description(f\"Epoch {epoch} | train_loss: {np.mean(loss_list):.4f}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "dIlTzVcPcctK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "Ronw5l3zdJ7R"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_graph_embeddings_using_svm(embeddings, labels):\n",
        "    result = []\n",
        "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "\n",
        "    for train_index, test_index in kf.split(embeddings, labels):\n",
        "        x_train = embeddings[train_index]\n",
        "        x_test = embeddings[test_index]\n",
        "        y_train = labels[train_index]\n",
        "        y_test = labels[test_index]\n",
        "        params = {\"C\": [1e-3, 1e-2, 1e-1, 1, 10]}\n",
        "        svc = SVC(random_state=42)\n",
        "        clf = GridSearchCV(svc, params)\n",
        "        clf.fit(x_train, y_train)\n",
        "\n",
        "        preds = clf.predict(x_test)\n",
        "        f1 = f1_score(y_test, preds, average=\"micro\")\n",
        "        result.append(f1)\n",
        "    test_f1 = np.mean(result)\n",
        "    test_std = np.std(result)\n",
        "\n",
        "    return test_f1, test_std"
      ],
      "metadata": {
        "id": "0aw_r3hRdDgJ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def graph_classification_evaluation(model, pooler, dataloader, num_classes, lr_f, weight_decay_f, max_epoch_f, device, mute=False):\n",
        "    model.eval()\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "    with torch.no_grad():\n",
        "        for i, (batch_g, labels) in enumerate(dataloader):\n",
        "            batch_g = batch_g.to(device)\n",
        "            feat = batch_g.ndata[\"attr\"]\n",
        "            out = model.embed(batch_g, feat)\n",
        "            out = pooler(batch_g, out)\n",
        "\n",
        "            y_list.append(labels.numpy())\n",
        "            x_list.append(out.cpu().numpy())\n",
        "    x = np.concatenate(x_list, axis=0)\n",
        "    y = np.concatenate(y_list, axis=0)\n",
        "    test_f1, test_std = evaluate_graph_embeddings_using_svm(x, y)\n",
        "    print(f\"#Test_f1: {test_f1:.4f}±{test_std:.4f}\")\n",
        "    return test_f1"
      ],
      "metadata": {
        "id": "ZkoUsQ9rc9vM"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    device = args.device if args.device >= 0 else \"cpu\"\n",
        "    seeds = args.seeds\n",
        "    dataset_name = args.dataset\n",
        "    max_epoch = args.max_epoch\n",
        "    max_epoch_f = args.max_epoch_f\n",
        "    num_hidden = args.num_hidden\n",
        "    num_layers = args.num_layers\n",
        "    encoder_type = args.encoder\n",
        "    decoder_type = args.decoder\n",
        "    replace_rate = args.replace_rate\n",
        "\n",
        "    optim_type = args.optimizer\n",
        "    loss_fn = args.loss_fn\n",
        "\n",
        "    lr = args.lr\n",
        "    weight_decay = args.weight_decay\n",
        "    lr_f = args.lr_f\n",
        "    weight_decay_f = args.weight_decay_f\n",
        "    linear_prob = args.linear_prob\n",
        "    load_model = args.load_model\n",
        "    save_model = args.save_model\n",
        "    logs = args.logging\n",
        "    use_scheduler = args.scheduler\n",
        "    pooling = args.pooling\n",
        "    deg4feat = args.deg4feat\n",
        "    batch_size = args.batch_size\n",
        "\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(dataset_name, deg4feat=deg4feat)\n",
        "    args.num_features = num_features\n",
        "\n",
        "    train_idx = torch.arange(len(graphs))\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "\n",
        "    train_loader = GraphDataLoader(graphs, sampler=train_sampler, collate_fn=collate_fn, batch_size=batch_size, pin_memory=True)\n",
        "    eval_loader = GraphDataLoader(graphs, collate_fn=collate_fn, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    if pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    acc_list = []\n",
        "    for i, seed in enumerate(seeds):\n",
        "        print(f\"####### Run {i} for seed {seed}\")\n",
        "        set_random_seed(seed)\n",
        "\n",
        "        if logs:\n",
        "            logger = TBLogger(name=f\"{dataset_name}_loss_{loss_fn}_rpr_{replace_rate}_nh_{num_hidden}_nl_{num_layers}_lr_{lr}_mp_{max_epoch}_mpf_{max_epoch_f}_wd_{weight_decay}_wdf_{weight_decay_f}_{encoder_type}_{decoder_type}\")\n",
        "        else:\n",
        "            logger = None\n",
        "\n",
        "        model = build_model(args)\n",
        "        model.to(device)\n",
        "        optimizer = create_optimizer(optim_type, model, lr, weight_decay)\n",
        "\n",
        "        if use_scheduler:\n",
        "            logging.info(\"Use schedular\")\n",
        "            scheduler = lambda epoch :( 1 + np.cos((epoch) * np.pi / max_epoch) ) * 0.5\n",
        "            # scheduler = lambda epoch: epoch / warmup_steps if epoch < warmup_steps \\\n",
        "                    # else ( 1 + np.cos((epoch - warmup_steps) * np.pi / (max_epoch - warmup_steps))) * 0.5\n",
        "            scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler)\n",
        "        else:\n",
        "            scheduler = None\n",
        "\n",
        "        if not load_model:\n",
        "            model = pretrain(model, pooler, (train_loader, eval_loader), optimizer, max_epoch, device, scheduler, num_classes, lr_f, weight_decay_f, max_epoch_f, linear_prob,  logger)\n",
        "            model = model.cpu()\n",
        "\n",
        "        if load_model:\n",
        "            logging.info(\"Loading Model ... \")\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "        if save_model:\n",
        "            logging.info(\"Saveing Model ...\")\n",
        "            torch.save(model.state_dict(), \"checkpoint.pt\")\n",
        "\n",
        "        model = model.to(device)\n",
        "        model.eval()\n",
        "        test_f1 = graph_classification_evaluation(model, pooler, eval_loader, num_classes, lr_f, weight_decay_f, max_epoch_f, device, mute=False)\n",
        "        acc_list.append(test_f1)\n",
        "\n",
        "    final_acc, final_acc_std = np.mean(acc_list), np.std(acc_list)\n",
        "    print(f\"# final_acc: {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "id": "oQROmoNg7FtL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_args():\n",
        "    parser = argparse.ArgumentParser(description=\"GAT\")\n",
        "    parser.add_argument(\"--seeds\", type=int, nargs=\"+\", default=[0])\n",
        "    parser.add_argument(\"--dataset\", type=str, default=\"cora\")\n",
        "    parser.add_argument(\"--device\", type=int, default=-1)\n",
        "    parser.add_argument(\"--max_epoch\", type=int, default=200,\n",
        "                        help=\"number of training epochs\")\n",
        "    parser.add_argument(\"--warmup_steps\", type=int, default=-1)\n",
        "\n",
        "    parser.add_argument(\"--num_heads\", type=int, default=4,\n",
        "                        help=\"number of hidden attention heads\")\n",
        "    parser.add_argument(\"--num_out_heads\", type=int, default=1,\n",
        "                        help=\"number of output attention heads\")\n",
        "    parser.add_argument(\"--num_layers\", type=int, default=2,\n",
        "                        help=\"number of hidden layers\")\n",
        "    parser.add_argument(\"--num_hidden\", type=int, default=256,\n",
        "                        help=\"number of hidden units\")\n",
        "    parser.add_argument(\"--residual\", action=\"store_true\", default=False,\n",
        "                        help=\"use residual connection\")\n",
        "    parser.add_argument(\"--in_drop\", type=float, default=.2,\n",
        "                        help=\"input feature dropout\")\n",
        "    parser.add_argument(\"--attn_drop\", type=float, default=.1,\n",
        "                        help=\"attention dropout\")\n",
        "    parser.add_argument(\"--norm\", type=str, default=None)\n",
        "    parser.add_argument(\"--lr\", type=float, default=0.005,\n",
        "                        help=\"learning rate\")\n",
        "    parser.add_argument(\"--weight_decay\", type=float, default=5e-4,\n",
        "                        help=\"weight decay\")\n",
        "    parser.add_argument(\"--negative_slope\", type=float, default=0.2,\n",
        "                        help=\"the negative slope of leaky relu for GAT\")\n",
        "    parser.add_argument(\"--activation\", type=str, default=\"prelu\")\n",
        "    parser.add_argument(\"--mask_rate\", type=float, default=0.5)\n",
        "    parser.add_argument(\"--drop_edge_rate\", type=float, default=0.0)\n",
        "    parser.add_argument(\"--replace_rate\", type=float, default=0.0)\n",
        "\n",
        "    parser.add_argument(\"--encoder\", type=str, default=\"gat\")\n",
        "    parser.add_argument(\"--decoder\", type=str, default=\"gat\")\n",
        "    parser.add_argument(\"--loss_fn\", type=str, default=\"sce\")\n",
        "    parser.add_argument(\"--alpha_l\", type=float, default=2, help=\"`pow`coefficient for `sce` loss\")\n",
        "    parser.add_argument(\"--optimizer\", type=str, default=\"adam\")\n",
        "\n",
        "    parser.add_argument(\"--max_epoch_f\", type=int, default=30)\n",
        "    parser.add_argument(\"--lr_f\", type=float, default=0.001, help=\"learning rate for evaluation\")\n",
        "    parser.add_argument(\"--weight_decay_f\", type=float, default=0.0, help=\"weight decay for evaluation\")\n",
        "    parser.add_argument(\"--linear_prob\", action=\"store_true\", default=False)\n",
        "\n",
        "    parser.add_argument(\"--load_model\", action=\"store_true\")\n",
        "    parser.add_argument(\"--save_model\", action=\"store_true\")\n",
        "    parser.add_argument(\"--use_cfg\", action=\"store_true\")\n",
        "    parser.add_argument(\"--logging\", action=\"store_true\")\n",
        "    parser.add_argument(\"--scheduler\", action=\"store_true\", default=False)\n",
        "    parser.add_argument(\"--concat_hidden\", action=\"store_true\", default=False)\n",
        "\n",
        "    # for graph classification\n",
        "    parser.add_argument(\"--pooling\", type=str, default=\"mean\")\n",
        "    parser.add_argument(\"--deg4feat\", action=\"store_true\", default=False, help=\"use node degree as input feature\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    return args"
      ],
      "metadata": {
        "id": "Z7SkOQ13d_xY"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "tL-EjEAA40t1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "67045e4b1ce844b6b280317e45b86858",
            "636992f3e29d4daa9039d5c09f53ace9",
            "03b0d3ff8adb4182bf15bec805bd2d5a",
            "e69b7c35f2b6418e8f25cf9c54e35fca",
            "eef477620e8d46d49f955b93523360a8",
            "5a8e58951c9c44abba84d8f1a85b04cd",
            "4f770ba708e94cd8bf90755f54ebc3be",
            "bf22ec264d394a93afce83e3d8d75f84",
            "0820e28e41964d31b9ed150bb6308ad0",
            "57252e6eb34c42eca1b8e582ad66fb8b",
            "cbfd4f3b3956435fa676a246c9c30708"
          ]
        },
        "outputId": "6199ed62-105a-41f2-993c-836b44149e14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='ENZYMES', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=50, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=False, save_model=True, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32)\n",
            "Downloading /root/.dgl/ENZYMES.zip from https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/root/.dgl/ENZYMES.zip:   0%|          | 0.00/537k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67045e4b1ce844b6b280317e45b86858"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting file to /root/.dgl/ENZYMES_67bfdeff\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 600, # Num Feat: 3, # Num Classes: 6 ********\n",
            "####### Run 0 for seed 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 199 | train_loss: 0.0850: 100%|██████████| 200/200 [00:35<00:00,  5.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Test_f1: 0.2200±0.0267\n",
            "# final_acc: 0.2200±0.0000\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='ENZYMES',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0,\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=50, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=False, # Default value from build_args\n",
        "        save_model=True, # Default value from build_args\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "    main(args)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def few_shot_finetune_and_evaluate(model, pooler, support_graphs, support_labels, query_graphs, query_labels, num_classes, lr_f, weight_decay_f, max_epoch_f, device, num_hidden):\n",
        "    # Create a copy of the pre-trained model\n",
        "    finetune_model = build_model(args) # Assuming args is accessible, or pass necessary args\n",
        "    finetune_model.load_state_dict(model.state_dict())\n",
        "    finetune_model.to(device)\n",
        "\n",
        "    # Add a classification head for the current task's number of ways\n",
        "    unique_support_labels = torch.unique(support_labels)\n",
        "    num_ways = len(unique_support_labels)\n",
        "    # Remove the incorrect call to reset_classifier on the encoder\n",
        "    # Add a new linear classification layer to the finetune_model\n",
        "    finetune_model.classifier_head = nn.Linear(num_hidden, num_ways).to(device)\n",
        "\n",
        "\n",
        "    # Prepare data for fine-tuning\n",
        "    support_batch = dgl.batch(support_graphs).to(device)\n",
        "    support_feat = support_batch.ndata[\"attr\"]\n",
        "    support_labels = support_labels.to(device)\n",
        "\n",
        "    query_batch = dgl.batch(query_graphs).to(device)\n",
        "    query_feat = query_batch.ndata[\"attr\"]\n",
        "    query_labels = query_labels.to(device)\n",
        "\n",
        "    # Define optimizer for fine-tuning (only on the classification head and maybe last layers)\n",
        "    # Here we fine-tune the whole model and the new classification head\n",
        "    optimizer = torch.optim.Adam(finetune_model.parameters(), lr=lr_f, weight_decay=weight_decay_f)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    finetune_model.train()\n",
        "    for epoch in range(max_epoch_f):\n",
        "        # Forward pass\n",
        "        # Pass through the encoder to get node embeddings\n",
        "        support_node_embeddings = finetune_model.encoder(support_batch, support_feat)\n",
        "        # Apply pooling to get graph embeddings\n",
        "        support_graph_embeddings = pooler(support_batch, support_node_embeddings)\n",
        "        # Pass graph embeddings through the new classification head\n",
        "        outputs = finetune_model.classifier_head(support_graph_embeddings)\n",
        "        loss = criterion(outputs, support_labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate on the query set\n",
        "    finetune_model.eval()\n",
        "    with torch.no_grad():\n",
        "        query_node_embeddings = finetune_model.encoder(query_batch, query_feat)\n",
        "        query_graph_embeddings = pooler(query_batch, query_node_embeddings)\n",
        "        query_outputs = finetune_model.classifier_head(query_graph_embeddings)\n",
        "        _, preds = torch.max(query_outputs, 1)\n",
        "        f1 = f1_score(query_labels.cpu().numpy(), preds.cpu().numpy(), average=\"micro\")\n",
        "\n",
        "    return f1"
      ],
      "metadata": {
        "id": "spN1-O8kG-TQ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='ENZYMES',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=5, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=5, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yrlzr8G7G-Pz",
        "outputId": "288dc94d-1eaa-466d-bde0-fbb4674882f0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='ENZYMES', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=5, num_ways=5, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 600, # Num Feat: 3, # Num Classes: 6 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:15<00:00,  6.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Few-shot Acc (5-way 5-shot): 0.2471±0.0574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='ENZYMES',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=5, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=4, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AUxwiwDG-Nq",
        "outputId": "52b2198e-ea6d-4c41-c56e-0b40a732b35b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='ENZYMES', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=5, num_ways=4, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 600, # Num Feat: 3, # Num Classes: 6 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:15<00:00,  6.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Few-shot Acc (4-way 5-shot): 0.2978±0.0588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='ENZYMES',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=10, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=5, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Err6R1VXG-Eh",
        "outputId": "41915563-e887-491b-ab12-a2824cbc7757"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='ENZYMES', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=10, num_ways=5, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 600, # Num Feat: 3, # Num Classes: 6 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:16<00:00,  6.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Few-shot Acc (5-way 10-shot): 0.2675±0.0557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='ENZYMES',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=2e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=10, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=5, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC7aTs6tG9mA",
        "outputId": "1f8b1bb9-f98d-44ea-e0ea-a5638fa624eb"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='ENZYMES', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0002, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=10, num_ways=5, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 600, # Num Feat: 3, # Num Classes: 6 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:16<00:00,  6.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Few-shot Acc (5-way 10-shot): 0.2728±0.0564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xf-MpK06C1T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yk17C8G8yQ3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "StnBTpwlFt_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='SYNTHETIC',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0,\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=50, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=False, # Default value from build_args\n",
        "        save_model=True, # Default value from build_args\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "    main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "5f24e3b184794e71b06eca6e3ab6df0e",
            "d6067debfee44b10b093949d541f44ca",
            "cdae9fbd95fd448f8442ac60891f05be",
            "ba6267fe02bb4048a5d6ebbc79978208",
            "5a06d89f3505451eba26a847fe4d147f",
            "346409fecc764603aa20e0508e9cdf6a",
            "c37b58627d26477aa201e1b664391161",
            "b4df98b74b8e422f91a315536717e5e8",
            "497dd371e5954a3982d9511f68a53f39",
            "f7bb9efc546a433898b449195a244c53",
            "64b468a9a7d849b8966ad434df59a116"
          ]
        },
        "id": "V-5WPm_wP5rz",
        "outputId": "cb804395-059c-4d4f-bcb5-62b8c74f7948"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='SYNTHETIC', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=50, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=False, save_model=True, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32)\n",
            "Downloading /root/.dgl/SYNTHETIC.zip from https://www.chrsmrrs.com/graphkerneldatasets/SYNTHETIC.zip...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/root/.dgl/SYNTHETIC.zip:   0%|          | 0.00/437k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f24e3b184794e71b06eca6e3ab6df0e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting file to /root/.dgl/SYNTHETIC_0be794dd\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 300, # Num Feat: 8, # Num Classes: 2 ********\n",
            "####### Run 0 for seed 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 199 | train_loss: 0.1063: 100%|██████████| 200/200 [00:26<00:00,  7.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Test_f1: 0.5400±0.0327\n",
            "# final_acc: 0.5400±0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='SYNTHETIC',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=5, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=2, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxCaMXrjQCyv",
        "outputId": "bb881aea-9056-474a-d2df-c202a416eaff"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='SYNTHETIC', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=5, num_ways=2, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 300, # Num Feat: 8, # Num Classes: 2 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:16<00:00,  5.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Few-shot Acc (2-way 5-shot): 0.5000±0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='SYNTHETIC',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=10, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=2, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtKdmDp6Q_Kg",
        "outputId": "21cbd322-40d7-4131-db61-08f07eb48599"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='SYNTHETIC', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=10, num_ways=2, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 300, # Num Feat: 8, # Num Classes: 2 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:16<00:00,  5.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Few-shot Acc (2-way 10-shot): 0.5000±0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EE0A5IsxRcUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J_rvThfvRwz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='MSRC_9',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0,\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=50, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=False, # Default value from build_args\n",
        "        save_model=True, # Default value from build_args\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "    main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "96d4f0ca17c24c4ab642df47540b1253",
            "6fe91e44a382491b8d831308899c2e97",
            "5a85585f255a4ed0941186e9bad28a55",
            "0e67520e7bbe4a6d8b0a106fd4aaa30f",
            "d491cec5fabc40e28f3578c95bb5486a",
            "1b2039af73944e25bfbbdeb87e18f971",
            "0a47177ea67745da8de4696e30ba090d",
            "e568b4e2823b4430a8a690f5aef2bd1d",
            "6c1c74aa695b4a3ba220bfd3ac5e35a1",
            "65a644bc25ce47f7a7a3252f412536e6",
            "fb4309f9f5fe4b528264a7b4ebe2bef4"
          ]
        },
        "id": "WL-VRlbSXfSn",
        "outputId": "eeafe658-130f-4aa0-d785-1fa5f3420a64"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='MSRC_9', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=50, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=False, save_model=True, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32)\n",
            "Downloading /root/.dgl/MSRC_9.zip from https://www.chrsmrrs.com/graphkerneldatasets/MSRC_9.zip...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/root/.dgl/MSRC_9.zip:   0%|          | 0.00/98.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96d4f0ca17c24c4ab642df47540b1253"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting file to /root/.dgl/MSRC_9_dd7c6f73\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 221, # Num Feat: 10, # Num Classes: 8 ********\n",
            "####### Run 0 for seed 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 199 | train_loss: 0.1401: 100%|██████████| 200/200 [00:18<00:00, 10.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Test_f1: 0.9146±0.0636\n",
            "# final_acc: 0.9146±0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='MSRC_9',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=5, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=3, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jQ-_aNdXsyP",
        "outputId": "44de44e1-252f-4c1c-f1fe-d6cf1ebb1d3d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='MSRC_9', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=5, num_ways=3, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 221, # Num Feat: 10, # Num Classes: 8 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:   4%|▍         | 4/100 [00:00<00:10,  8.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 2: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:   9%|▉         | 9/100 [00:00<00:06, 14.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 5: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 6: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 7: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  14%|█▍        | 14/100 [00:01<00:06, 12.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 11: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 12: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 14: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  20%|██        | 20/100 [00:01<00:04, 16.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 16: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 17: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 18: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  24%|██▍       | 24/100 [00:01<00:05, 14.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 21: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 22: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  28%|██▊       | 28/100 [00:02<00:05, 13.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 25: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 26: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 28: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 29: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  36%|███▌      | 36/100 [00:02<00:03, 20.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 31: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 32: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 33: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 34: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 36: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  43%|████▎     | 43/100 [00:03<00:04, 12.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 41: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 43: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  49%|████▉     | 49/100 [00:03<00:04, 12.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 46: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 48: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  51%|█████     | 51/100 [00:03<00:03, 12.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 50: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  55%|█████▌    | 55/100 [00:04<00:04, 10.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 54: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  62%|██████▏   | 62/100 [00:04<00:02, 12.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 58: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 59: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 60: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  64%|██████▍   | 64/100 [00:05<00:02, 12.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 63: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  68%|██████▊   | 68/100 [00:05<00:03, 10.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 66: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  70%|███████   | 70/100 [00:05<00:02, 11.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 69: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  75%|███████▌  | 75/100 [00:06<00:02, 11.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 72: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 73: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  83%|████████▎ | 83/100 [00:07<00:01,  9.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 81: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 83: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 84: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  91%|█████████ | 91/100 [00:07<00:00, 13.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 87: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 88: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 89: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  95%|█████████▌| 95/100 [00:08<00:00, 13.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 92: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 93: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 95: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 96: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:08<00:00, 11.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 99: Not enough samples (19) for class 0. Need 20.\n",
            "# Few-shot Acc (3-way 5-shot): 0.9700±0.0383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='MSRC_9',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=5, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=5, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OslSFJJYYf6Q",
        "outputId": "910f8216-010c-4a3f-f18d-8a6d3826a802"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='MSRC_9', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=5, num_ways=5, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 221, # Num Feat: 10, # Num Classes: 8 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:   0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 0: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 1: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 2: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 3: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 4: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:   6%|▌         | 6/100 [00:00<00:03, 25.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 6: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  11%|█         | 11/100 [00:01<00:10,  8.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 11: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 12: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 13: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  17%|█▋        | 17/100 [00:01<00:07, 10.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 16: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 17: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 18: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  20%|██        | 20/100 [00:01<00:07, 11.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 20: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 21: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 22: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 23: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  27%|██▋       | 27/100 [00:02<00:05, 13.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 26: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 27: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 28: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  30%|███       | 30/100 [00:02<00:05, 13.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 30: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 31: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 32: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 33: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  37%|███▋      | 37/100 [00:02<00:04, 13.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 36: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 37: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 38: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 39: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  41%|████      | 41/100 [00:03<00:03, 14.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 41: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 42: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  44%|████▍     | 44/100 [00:03<00:04, 13.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 44: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 45: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  47%|████▋     | 47/100 [00:03<00:04, 13.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 47: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  51%|█████     | 51/100 [00:04<00:04,  9.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 50: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  57%|█████▋    | 57/100 [00:04<00:04,  9.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 53: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 54: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 55: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 57: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 58: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  60%|██████    | 60/100 [00:05<00:03, 10.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 60: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 61: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 62: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  64%|██████▍   | 64/100 [00:05<00:02, 12.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 64: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  70%|███████   | 70/100 [00:05<00:02, 11.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 67: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 68: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  74%|███████▍  | 74/100 [00:06<00:02, 12.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 71: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 73: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  81%|████████  | 81/100 [00:06<00:01, 17.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 75: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 76: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 77: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 78: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 79: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 81: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  88%|████████▊ | 88/100 [00:06<00:00, 20.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 83: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 84: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 85: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 86: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  91%|█████████ | 91/100 [00:07<00:00, 20.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 89: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 90: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 92: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  98%|█████████▊| 98/100 [00:07<00:00, 17.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 94: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 95: Not enough samples (19) for class 0. Need 20.\n",
            "Skipping task 96: Not enough samples (19) for class 0. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:07<00:00, 12.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 99: Not enough samples (19) for class 0. Need 20.\n",
            "# Few-shot Acc (5-way 5-shot): 0.9441±0.0329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='MSRC_9',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=10, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=3, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgaF2FgoZCUa",
        "outputId": "a0a527c9-adbe-4f77-b5ea-25510add5ee1"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='MSRC_9', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=10, num_ways=3, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 221, # Num Feat: 10, # Num Classes: 8 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:   0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 0: Not enough samples (23) for class 4. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:   2%|▏         | 2/100 [00:00<00:11,  8.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 2: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 3: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 4: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 5: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 6: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 7: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 8: Not enough samples (23) for class 4. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  10%|█         | 10/100 [00:00<00:03, 23.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 10: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 11: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 12: Not enough samples (19) for class 0. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  17%|█▋        | 17/100 [00:01<00:06, 13.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 16: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 17: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 18: Not enough samples (23) for class 4. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  20%|██        | 20/100 [00:01<00:05, 13.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 20: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 21: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 22: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 23: Not enough samples (19) for class 0. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  25%|██▌       | 25/100 [00:01<00:04, 16.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 25: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 26: Not enough samples (19) for class 0. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  30%|███       | 30/100 [00:01<00:05, 13.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 29: Not enough samples (19) for class 0. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  32%|███▏      | 32/100 [00:02<00:05, 12.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 31: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 32: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 33: Not enough samples (19) for class 0. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  38%|███▊      | 38/100 [00:02<00:04, 13.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 35: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 36: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 38: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 39: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 40: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 41: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 42: Not enough samples (23) for class 4. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  44%|████▍     | 44/100 [00:02<00:03, 17.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 44: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 45: Not enough samples (19) for class 0. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  49%|████▉     | 49/100 [00:03<00:03, 14.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 47: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 49: Not enough samples (23) for class 4. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  55%|█████▌    | 55/100 [00:04<00:04,  9.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 54: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 55: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 56: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 57: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 58: Not enough samples (19) for class 0. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  60%|██████    | 60/100 [00:04<00:03, 12.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 60: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 61: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 62: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 63: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 64: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 65: Not enough samples (23) for class 4. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  67%|██████▋   | 67/100 [00:04<00:01, 18.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 67: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 68: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 69: Not enough samples (19) for class 0. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  71%|███████   | 71/100 [00:04<00:01, 18.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 71: Not enough samples (23) for class 4. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  73%|███████▎  | 73/100 [00:04<00:01, 15.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 73: Not enough samples (23) for class 4. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  75%|███████▌  | 75/100 [00:05<00:01, 13.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 75: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 76: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 77: Not enough samples (23) for class 4. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  83%|████████▎ | 83/100 [00:05<00:01, 16.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 79: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 80: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 81: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 83: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 84: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 85: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 86: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 87: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 88: Not enough samples (19) for class 0. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  94%|█████████▍| 94/100 [00:05<00:00, 24.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 90: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 91: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 92: Not enough samples (19) for class 0. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:06<00:00, 16.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 95: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 96: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 97: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 98: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 99: Not enough samples (19) for class 0. Need 25.\n",
            "# Few-shot Acc (3-way 10-shot): 0.9755±0.0342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='MSRC_9',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=10, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=5, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS15blKqZOlB",
        "outputId": "fe9296d3-6755-48a2-eefb-39de51670c94"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='MSRC_9', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=10, num_ways=5, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 221, # Num Feat: 10, # Num Classes: 8 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  11%|█         | 11/100 [00:00<00:00, 108.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 0: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 1: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 2: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 3: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 4: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 5: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 6: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 7: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 8: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 9: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 10: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 11: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 12: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 13: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 15: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 16: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 18: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 19: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 20: Not enough samples (19) for class 0. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  22%|██▏       | 22/100 [00:01<00:06, 12.88it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 22: Not enough samples (19) for class 0. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  27%|██▋       | 27/100 [00:01<00:05, 13.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 24: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 25: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 26: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 27: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 28: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 29: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 30: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 31: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 32: Not enough samples (19) for class 0. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  34%|███▍      | 34/100 [00:02<00:03, 16.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 34: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 35: Not enough samples (23) for class 4. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  38%|███▊      | 38/100 [00:02<00:03, 16.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 37: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 38: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 39: Not enough samples (23) for class 4. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  41%|████      | 41/100 [00:02<00:03, 16.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 41: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 42: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 43: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 44: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 45: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 46: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 47: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 48: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 49: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 50: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 51: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 52: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 53: Not enough samples (19) for class 0. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  55%|█████▌    | 55/100 [00:02<00:01, 26.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 55: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 56: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 57: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 58: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 59: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 60: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 61: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 62: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 63: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 64: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 65: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 66: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 67: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 68: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 69: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 70: Not enough samples (23) for class 4. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  72%|███████▏  | 72/100 [00:03<00:00, 37.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 72: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 73: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 74: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 75: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 76: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 77: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 78: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 79: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 80: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 81: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 82: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 83: Not enough samples (23) for class 4. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  85%|████████▌ | 85/100 [00:03<00:00, 41.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 86: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 87: Not enough samples (23) for class 4. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:03<00:00, 26.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 89: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 90: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 91: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 92: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 93: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 94: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 95: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 96: Not enough samples (19) for class 0. Need 25.\n",
            "Skipping task 97: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 98: Not enough samples (23) for class 4. Need 25.\n",
            "Skipping task 99: Not enough samples (23) for class 4. Need 25.\n",
            "# Few-shot Acc (5-way 10-shot): 0.9622±0.0271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rXjBpGcTZajF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3KE9G1ArZxyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='DD',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0,\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=50, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=False, # Default value from build_args\n",
        "        save_model=True, # Default value from build_args\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "    main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "ecc9806e5e844a249130c3195a00e716",
            "dc79b227d0d34f4e845ed5f7f80989be",
            "41910e9418fd4c0486590f926e2aa173",
            "60f729506cfa46d2b379e78f88e08c3d",
            "50f31647837c4e8dbc83814d38e0960b",
            "f21554432ad147349b9a1fd56b75990b",
            "c615049651be42b3809a3dd1b1389aca",
            "02e0eabaa80b4b679ec2cc0a5f2eb5bb",
            "00c80d7fe9664ed497ab159dc2d88bb5",
            "b242a4d8988d4f73945fd89b95de9460",
            "bb05aac5816b4116b7dac781c74b40cf"
          ]
        },
        "id": "YX6sfHoaZyMy",
        "outputId": "e2f5225d-1031-43b4-8d72-96a08f94c2e4"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='DD', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=50, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=False, save_model=True, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32)\n",
            "Downloading /root/.dgl/DD.zip from https://www.chrsmrrs.com/graphkerneldatasets/DD.zip...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/root/.dgl/DD.zip:   0%|          | 0.00/4.98M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ecc9806e5e844a249130c3195a00e716"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting file to /root/.dgl/DD_a6864905\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 1178, # Num Feat: 89, # Num Classes: 2 ********\n",
            "####### Run 0 for seed 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 199 | train_loss: 0.5731: 100%|██████████| 200/200 [04:01<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Test_f1: 0.6800±0.0223\n",
            "# final_acc: 0.6800±0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='DD',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=5, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=2, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmBQPRTcZ8hx",
        "outputId": "e57c97a0-d3a3-4b0c-a90b-da9a1846bf54"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='DD', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=5, num_ways=2, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 1178, # Num Feat: 89, # Num Classes: 2 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:21<00:00,  4.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Few-shot Acc (2-way 5-shot): 0.5633±0.0853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='DD',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=10, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=2, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoKyW3o5bPWw",
        "outputId": "1b59d314-4aaa-421f-c319-86a073ac20f2"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='DD', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=10, num_ways=2, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 1178, # Num Feat: 89, # Num Classes: 2 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:33<00:00,  2.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Few-shot Acc (2-way 10-shot): 0.5930±0.0885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8HIE969xbinS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-rIFQoNTbx15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='PROTEINS',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0,\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=50, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=False, # Default value from build_args\n",
        "        save_model=True, # Default value from build_args\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "    main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5cJCIHob3m1",
        "outputId": "a9bf076b-8284-427f-a591-95a7be9d4b76"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='PROTEINS', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=50, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=False, save_model=True, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 1113, # Num Feat: 3, # Num Classes: 2 ********\n",
            "####### Run 0 for seed 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 199 | train_loss: 0.0880: 100%|██████████| 200/200 [01:30<00:00,  2.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Test_f1: 0.5930±0.0098\n",
            "# final_acc: 0.5930±0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='PROTEINS',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=5, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=2, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMHknSHJb6Z_",
        "outputId": "63b4e2a6-e431-4626-f571-d2b8a19ebcd7"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='PROTEINS', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=5, num_ways=2, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 1113, # Num Feat: 3, # Num Classes: 2 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:16<00:00,  6.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Few-shot Acc (2-way 5-shot): 0.5560±0.1029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='PROTEINS',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=10, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=2, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTlFY78WcEk3",
        "outputId": "ebb051df-7bf6-4a71-8b6e-78d5e77b8afe"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='PROTEINS', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=10, num_ways=2, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 1113, # Num Feat: 3, # Num Classes: 2 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:20<00:00,  4.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Few-shot Acc (2-way 10-shot): 0.5773±0.0960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UA-s95hick3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R81bTKMOcwxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='OHSU',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0,\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=50, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=False, # Default value from build_args\n",
        "        save_model=True, # Default value from build_args\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "    main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "8eb8fe36883443f586487559b893b74b",
            "90ae8f9568db49ca8ac8c07aa3016126",
            "899768a3c5274e5a86806375c3c40fb0",
            "2c8a08c4339b4d4f8e3e8d6ca976673c",
            "1ce0ca355c354cefacfe1fe70e2f2054",
            "1c8b3df678e1465a927ae2ffd80e2b2d",
            "f3393cef81ed42bb956e88763cb099d3",
            "b93a9427c5b14df3826cb915efe53a6c",
            "8f9755a528f94e1b9c2632aef52d5f5a",
            "4480a32e1f84438e8ef132c2cdcd77a9",
            "7a4401eff9cb41a4baf45b058f096377"
          ]
        },
        "id": "3YNP7B_CcxHw",
        "outputId": "2a1fa66b-f1ba-4517-b03d-6973a9970ac8"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='OHSU', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=50, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=False, save_model=True, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32)\n",
            "Downloading /root/.dgl/OHSU.zip from https://www.chrsmrrs.com/graphkerneldatasets/OHSU.zip...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/root/.dgl/OHSU.zip:   0%|          | 0.00/79.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8eb8fe36883443f586487559b893b74b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting file to /root/.dgl/OHSU_f15c2a92\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 79, # Num Feat: 190, # Num Classes: 2 ********\n",
            "####### Run 0 for seed 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 199 | train_loss: 0.6005: 100%|██████████| 200/200 [00:13<00:00, 15.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Test_f1: 0.5768±0.2143\n",
            "# final_acc: 0.5768±0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='OHSU',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=5, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=2, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC9HqQNfcy3_",
        "outputId": "ccf5337a-9ac0-42a2-b9b4-a3c5ffe1d77d"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='OHSU', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=5, num_ways=2, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 79, # Num Feat: 190, # Num Classes: 2 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:16<00:00,  6.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Few-shot Acc (2-way 5-shot): 0.5027±0.0839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='OHSU',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=10, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=2, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frTUnUpZc5FM",
        "outputId": "22fc1ffb-90fa-47cd-f4c9-43373c21b126"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='OHSU', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=10, num_ways=2, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 79, # Num Feat: 190, # Num Classes: 2 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:20<00:00,  4.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Few-shot Acc (2-way 10-shot): 0.5100±0.0723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NCI1KdzDdEwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eiSdtHBodcF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='MSRC_21',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0,\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=50, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=False, # Default value from build_args\n",
        "        save_model=True, # Default value from build_args\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "    main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "924a5fc4a5484eba90abc11193c58a95",
            "77f2b2bd347d4583993ce0a818ad83a8",
            "a9f626ee6aff404ba01cf1c99f085087",
            "7946922f9af649e493fe70348b41cbe5",
            "51a66050944b42b3ab1c7d7582263b33",
            "309b32f8fb4e4d3181f502c03a6f2084",
            "dfd1e18ada6c4f13bab6663f2978e081",
            "3f8c70ea9ff44d509704629b02acf8c3",
            "49660e1f2882437582bfe063f89d70a1",
            "76ad7d2a003f4fb1a401bf98056619d0",
            "5f670c644a1848b69a88621d0a6c276b"
          ]
        },
        "id": "RYfMBRIQdcer",
        "outputId": "1e035f76-7091-4e84-c425-ab76616fd0f8"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='MSRC_21', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=50, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=False, save_model=True, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32)\n",
            "Downloading /root/.dgl/MSRC_21.zip from https://www.chrsmrrs.com/graphkerneldatasets/MSRC_21.zip...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/root/.dgl/MSRC_21.zip:   0%|          | 0.00/517k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "924a5fc4a5484eba90abc11193c58a95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting file to /root/.dgl/MSRC_21_1fc24118\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 563, # Num Feat: 24, # Num Classes: 20 ********\n",
            "####### Run 0 for seed 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 199 | train_loss: 0.1174: 100%|██████████| 200/200 [00:46<00:00,  4.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Test_f1: 0.9129±0.0335\n",
            "# final_acc: 0.9129±0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='MSRC_21',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=5, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=3, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8BncLqLdiYo",
        "outputId": "d99a5b1a-e2b1-4e23-c915-be8a14234d96"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='MSRC_21', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=5, num_ways=3, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 563, # Num Feat: 24, # Num Classes: 20 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  17%|█▋        | 17/100 [00:03<00:14,  5.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 15: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  21%|██        | 21/100 [00:04<00:16,  4.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 21: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  26%|██▌       | 26/100 [00:06<00:21,  3.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 26: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  28%|██▊       | 28/100 [00:06<00:20,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 28: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  36%|███▌      | 36/100 [00:09<00:23,  2.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 36: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  44%|████▍     | 44/100 [00:12<00:17,  3.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 44: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  57%|█████▋    | 57/100 [00:16<00:16,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 57: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  64%|██████▍   | 64/100 [00:18<00:08,  4.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 64: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  66%|██████▌   | 66/100 [00:18<00:05,  5.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 66: Not enough samples (10) for class 19. Need 20.\n",
            "Skipping task 67: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  71%|███████   | 71/100 [00:19<00:04,  5.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 71: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  73%|███████▎  | 73/100 [00:19<00:04,  6.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 73: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  75%|███████▌  | 75/100 [00:19<00:04,  5.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 75: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  90%|█████████ | 90/100 [00:23<00:02,  4.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 90: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:25<00:00,  3.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Few-shot Acc (3-way 5-shot): 0.9282±0.0828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='MSRC_21',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=5, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=4, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-ISR9qhfHPo",
        "outputId": "c3a98783-ac9a-4cf3-8cac-4243f62b2bd3"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='MSRC_21', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=5, num_ways=4, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 563, # Num Feat: 24, # Num Classes: 20 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:   4%|▍         | 4/100 [00:01<00:23,  4.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 4: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:   8%|▊         | 8/100 [00:01<00:18,  5.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 8: Not enough samples (10) for class 19. Need 20.\n",
            "Skipping task 9: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  13%|█▎        | 13/100 [00:02<00:14,  6.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 13: Not enough samples (10) for class 19. Need 20.\n",
            "Skipping task 14: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  17%|█▋        | 17/100 [00:02<00:11,  7.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 17: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  22%|██▏       | 22/100 [00:03<00:13,  5.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 22: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  25%|██▌       | 25/100 [00:04<00:11,  6.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 25: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  33%|███▎      | 33/100 [00:05<00:13,  5.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 33: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  35%|███▌      | 35/100 [00:05<00:10,  6.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 35: Not enough samples (10) for class 19. Need 20.\n",
            "Skipping task 36: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  38%|███▊      | 38/100 [00:06<00:07,  8.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 38: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  42%|████▏     | 42/100 [00:06<00:10,  5.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 42: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  46%|████▌     | 46/100 [00:07<00:14,  3.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 46: Not enough samples (10) for class 19. Need 20.\n",
            "Skipping task 47: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  49%|████▉     | 49/100 [00:08<00:09,  5.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 49: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  66%|██████▌   | 66/100 [00:11<00:07,  4.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 66: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  68%|██████▊   | 68/100 [00:12<00:05,  5.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 68: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  73%|███████▎  | 73/100 [00:13<00:07,  3.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 73: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  90%|█████████ | 90/100 [00:16<00:01,  7.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 88: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  98%|█████████▊| 98/100 [00:17<00:00, 11.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 94: Not enough samples (10) for class 19. Need 20.\n",
            "Skipping task 95: Not enough samples (10) for class 19. Need 20.\n",
            "Skipping task 96: Not enough samples (10) for class 19. Need 20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:17<00:00,  5.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Few-shot Acc (4-way 5-shot): 0.9175±0.0601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='MSRC_21',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=10, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=3, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjgTt-KNfwxI",
        "outputId": "dbe6ed50-c006-467a-bb08-7828fe7ebbd1"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='MSRC_21', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=10, num_ways=3, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 563, # Num Feat: 24, # Num Classes: 20 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:   6%|▌         | 6/100 [00:02<00:29,  3.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 6: Not enough samples (24) for class 17. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:   9%|▉         | 9/100 [00:02<00:25,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 9: Not enough samples (10) for class 19. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  14%|█▍        | 14/100 [00:04<00:28,  3.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 14: Not enough samples (24) for class 17. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  17%|█▋        | 17/100 [00:05<00:32,  2.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 17: Not enough samples (10) for class 19. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  19%|█▉        | 19/100 [00:06<00:32,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 19: Not enough samples (24) for class 17. Need 25.\n",
            "Skipping task 20: Not enough samples (24) for class 14. Need 25.\n",
            "Skipping task 21: Not enough samples (24) for class 14. Need 25.\n",
            "Skipping task 22: Not enough samples (10) for class 19. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  29%|██▉       | 29/100 [00:08<00:22,  3.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 29: Not enough samples (24) for class 14. Need 25.\n",
            "Skipping task 30: Not enough samples (10) for class 19. Need 25.\n",
            "Skipping task 31: Not enough samples (10) for class 19. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  34%|███▍      | 34/100 [00:09<00:16,  3.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 34: Not enough samples (24) for class 17. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  37%|███▋      | 37/100 [00:10<00:21,  2.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 37: Not enough samples (24) for class 17. Need 25.\n",
            "Skipping task 38: Not enough samples (24) for class 14. Need 25.\n",
            "Skipping task 39: Not enough samples (24) for class 14. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  42%|████▏     | 42/100 [00:11<00:11,  5.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 42: Not enough samples (10) for class 19. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  44%|████▍     | 44/100 [00:11<00:09,  5.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 44: Not enough samples (24) for class 17. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  52%|█████▏    | 52/100 [00:12<00:09,  5.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 52: Not enough samples (10) for class 19. Need 25.\n",
            "Skipping task 53: Not enough samples (24) for class 14. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  56%|█████▌    | 56/100 [00:13<00:06,  6.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 56: Not enough samples (24) for class 14. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  58%|█████▊    | 58/100 [00:13<00:05,  7.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 58: Not enough samples (10) for class 19. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  61%|██████    | 61/100 [00:14<00:05,  6.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 61: Not enough samples (24) for class 17. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  63%|██████▎   | 63/100 [00:14<00:04,  7.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 63: Not enough samples (24) for class 14. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  66%|██████▌   | 66/100 [00:14<00:05,  6.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 66: Not enough samples (24) for class 14. Need 25.\n",
            "Skipping task 67: Not enough samples (24) for class 14. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  70%|███████   | 70/100 [00:15<00:04,  7.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 70: Not enough samples (10) for class 19. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  75%|███████▌  | 75/100 [00:16<00:04,  5.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 75: Not enough samples (24) for class 17. Need 25.\n",
            "Skipping task 76: Not enough samples (24) for class 14. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  82%|████████▏ | 82/100 [00:17<00:04,  3.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 82: Not enough samples (10) for class 19. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  84%|████████▍ | 84/100 [00:18<00:03,  4.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 84: Not enough samples (10) for class 19. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  88%|████████▊ | 88/100 [00:19<00:03,  3.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 88: Not enough samples (24) for class 14. Need 25.\n",
            "Skipping task 89: Not enough samples (24) for class 17. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  96%|█████████▌| 96/100 [00:20<00:00,  4.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 96: Not enough samples (24) for class 17. Need 25.\n",
            "Skipping task 97: Not enough samples (24) for class 14. Need 25.\n",
            "Skipping task 98: Not enough samples (24) for class 14. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:20<00:00,  4.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Few-shot Acc (3-way 10-shot): 0.9723±0.0326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Simulate command-line arguments using argparse.Namespace\n",
        "    args = argparse.Namespace(\n",
        "        dataset='MSRC_21',\n",
        "        encoder='gin',\n",
        "        decoder='gin',\n",
        "        seeds=[0],\n",
        "        device=0, # Use CPU if GPU is not available\n",
        "        use_cfg=False,\n",
        "        max_epoch=200, # Default value from build_args\n",
        "        warmup_steps=-1, # Default value from build_args\n",
        "        num_heads=4, # Default value from build_args\n",
        "        num_out_heads=1, # Default value from build_args\n",
        "        num_layers=2, # Default value from build_args\n",
        "        num_hidden=512, # Default value from build_args\n",
        "        residual=False, # Default value from build_args\n",
        "        in_drop=0.2, # Default value from build_args\n",
        "        attn_drop=0.1, # Default value from build_args\n",
        "        norm=None, # Default value from build_args\n",
        "        lr=0.0005, # Default value from build_args\n",
        "        weight_decay=5e-4, # Default value from build_args\n",
        "        negative_slope=0.2, # Default value from build_args\n",
        "        activation=\"prelu\", # Default value from build_args\n",
        "        mask_rate=0.5, # Default value from build_args\n",
        "        drop_edge_rate=0.0, # Default value from build_args\n",
        "        replace_rate=0.0, # Default value from build_args\n",
        "        loss_fn=\"sce\", # Default value from build_fn_args\n",
        "        alpha_l=2, # Default value from build_args\n",
        "        optimizer=\"adam\", # Default value from build_args\n",
        "        max_epoch_f=30, # Default value from build_args\n",
        "        lr_f=0.001, # Default value from build_args\n",
        "        weight_decay_f=0.0, # Default value from build_args\n",
        "        linear_prob=False, # Default value from build_args\n",
        "        load_model=True, # Load the pre-trained model\n",
        "        save_model=False, # Set to False to avoid saving during few-shot eval\n",
        "        logging=False, # Default value from build_args\n",
        "        scheduler=False, # Default value from build_args\n",
        "        concat_hidden=False, # Default value from build_args\n",
        "        pooling=\"mean\", # Default value from build_args\n",
        "        deg4feat=False, # Default value from build_args\n",
        "        batch_size=32, # Default value from build_args\n",
        "        num_shots=10, # Few-shot parameter: number of support samples per class\n",
        "        num_ways=4, # Few-shot parameter: number of classes per task\n",
        "        num_queries=15, # Few-shot parameter: number of query samples per class\n",
        "        num_tasks=100 # Few-shot parameter: number of few-shot tasks to evaluate on\n",
        "    )\n",
        "\n",
        "    if args.use_cfg:\n",
        "         # This part will still load configs.yml based on args.dataset\n",
        "        args = load_best_configs(args, \"configs.yml\")\n",
        "    print(args)\n",
        "\n",
        "    # Load the dataset\n",
        "    graphs, (num_features, num_classes) = load_graph_classification_dataset(args.dataset, deg4feat=args.deg4feat)\n",
        "    args.num_features = num_features\n",
        "    args.num_classes = num_classes # Add num_classes to args\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model(args)\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load the pre-trained model weights if load_model is True\n",
        "    if args.load_model:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "            print(\"Loaded pre-trained model weights from checkpoint.pt\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"checkpoint.pt not found. Please make sure the pre-trained model is saved.\")\n",
        "            # Optionally, you can exit or train the model from scratch here\n",
        "            # return\n",
        "\n",
        "    # Initialize the pooler\n",
        "    if args.pooling == \"mean\":\n",
        "        pooler = AvgPooling()\n",
        "    elif args.pooling == \"max\":\n",
        "        pooler = MaxPooling()\n",
        "    elif args.pooling == \"sum\":\n",
        "        pooler = SumPooling()\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Run few-shot evaluation with fine-tuning\n",
        "    acc_list = []\n",
        "    for task in tqdm(range(args.num_tasks), desc=\"Few-shot tasks\"):\n",
        "        # Randomly sample classes\n",
        "        selected_classes = np.random.choice(num_classes, args.num_ways, replace=False)\n",
        "        task_graphs_with_labels = [(graph, label) for graph, label in graphs if label.item() in selected_classes]\n",
        "        task_labels_tensor = torch.tensor([label.item() for _, label in task_graphs_with_labels])\n",
        "        task_graphs = [graph for graph, _ in task_graphs_with_labels]\n",
        "\n",
        "        # Create mapping for selected classes to 0-num_ways-1\n",
        "        class_map = {cls.item(): i for i, cls in enumerate(selected_classes)}\n",
        "        mapped_labels = torch.tensor([class_map[label.item()] for _, label in task_graphs_with_labels])\n",
        "\n",
        "        # Split into support and query sets\n",
        "        support_indices = []\n",
        "        query_indices = []\n",
        "        skip_task = False\n",
        "        for cls in selected_classes:\n",
        "            # Get indices for the current class within the task_graphs_with_labels list\n",
        "            cls_task_indices = [i for i, (_, label) in enumerate(task_graphs_with_labels) if label.item() == cls.item()]\n",
        "\n",
        "            if len(cls_task_indices) < args.num_shots + args.num_queries:\n",
        "                 # Skip task if not enough samples for a class\n",
        "                print(f\"Skipping task {task}: Not enough samples ({len(cls_task_indices)}) for class {cls}. Need {args.num_shots + args.num_queries}.\")\n",
        "                skip_task = True\n",
        "                break\n",
        "\n",
        "            # Shuffle indices for the current class and split\n",
        "            shuffled_cls_indices = np.random.permutation(cls_task_indices)\n",
        "            support_indices.extend(shuffled_cls_indices[:args.num_shots])\n",
        "            query_indices.extend(shuffled_cls_indices[args.num_shots:args.num_shots + args.num_queries])\n",
        "\n",
        "        if skip_task:\n",
        "            continue\n",
        "\n",
        "        if not support_indices or not query_indices:\n",
        "             # Skip task if no support or query samples could be gathered (should be caught by the previous check, but as a safeguard)\n",
        "            print(f\"Skipping task {task}: No support or query samples gathered after splitting\")\n",
        "            continue\n",
        "\n",
        "        # Ensure indices are within the bounds of task_graphs\n",
        "        support_graphs = [task_graphs[i] for i in support_indices]\n",
        "        support_labels = mapped_labels[support_indices]\n",
        "        query_graphs = [task_graphs[i] for i in query_indices]\n",
        "        query_labels = mapped_labels[query_indices]\n",
        "\n",
        "        # Perform fine-tuning and evaluation\n",
        "        f1 = few_shot_finetune_and_evaluate(\n",
        "            model,\n",
        "            pooler,\n",
        "            support_graphs,\n",
        "            support_labels,\n",
        "            query_graphs,\n",
        "            query_labels,\n",
        "            num_classes,\n",
        "            args.lr_f,\n",
        "            args.weight_decay_f,\n",
        "            args.max_epoch_f,\n",
        "            args.device,\n",
        "            args.num_hidden,\n",
        "        )\n",
        "        acc_list.append(f1)\n",
        "\n",
        "    final_acc = np.mean(acc_list)\n",
        "    final_acc_std = np.std(acc_list)\n",
        "    print(f\"# Few-shot Acc ({args.num_ways}-way {args.num_shots}-shot): {final_acc:.4f}±{final_acc_std:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJRdvnZgf_zF",
        "outputId": "e091e993-dac1-4ce3-ab35-329e035659d4"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dataset='MSRC_21', encoder='gin', decoder='gin', seeds=[0], device=0, use_cfg=False, max_epoch=200, warmup_steps=-1, num_heads=4, num_out_heads=1, num_layers=2, num_hidden=512, residual=False, in_drop=0.2, attn_drop=0.1, norm=None, lr=0.0005, weight_decay=0.0005, negative_slope=0.2, activation='prelu', mask_rate=0.5, drop_edge_rate=0.0, replace_rate=0.0, loss_fn='sce', alpha_l=2, optimizer='adam', max_epoch_f=30, lr_f=0.001, weight_decay_f=0.0, linear_prob=False, load_model=True, save_model=False, logging=False, scheduler=False, concat_hidden=False, pooling='mean', deg4feat=False, batch_size=32, num_shots=10, num_ways=4, num_queries=15, num_tasks=100)\n",
            "Use node label as node features\n",
            "******** # Num Graphs: 563, # Num Feat: 24, # Num Classes: 20 ********\n",
            "Loaded pre-trained model weights from checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:   1%|          | 1/100 [00:00<00:30,  3.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 1: Not enough samples (10) for class 19. Need 25.\n",
            "Skipping task 2: Not enough samples (10) for class 19. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:   6%|▌         | 6/100 [00:00<00:16,  5.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 6: Not enough samples (10) for class 19. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:   9%|▉         | 9/100 [00:01<00:18,  4.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 9: Not enough samples (24) for class 14. Need 25.\n",
            "Skipping task 10: Not enough samples (24) for class 14. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  12%|█▏        | 12/100 [00:02<00:17,  5.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 12: Not enough samples (24) for class 14. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  15%|█▌        | 15/100 [00:02<00:15,  5.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 15: Not enough samples (10) for class 19. Need 25.\n",
            "Skipping task 16: Not enough samples (24) for class 17. Need 25.\n",
            "Skipping task 17: Not enough samples (24) for class 14. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  19%|█▉        | 19/100 [00:02<00:10,  8.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 19: Not enough samples (24) for class 17. Need 25.\n",
            "Skipping task 20: Not enough samples (24) for class 14. Need 25.\n",
            "Skipping task 21: Not enough samples (10) for class 19. Need 25.\n",
            "Skipping task 22: Not enough samples (24) for class 17. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  24%|██▍       | 24/100 [00:03<00:06, 11.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 24: Not enough samples (10) for class 19. Need 25.\n",
            "Skipping task 25: Not enough samples (24) for class 14. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  29%|██▉       | 29/100 [00:03<00:06, 10.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 28: Not enough samples (10) for class 19. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  31%|███       | 31/100 [00:03<00:06,  9.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 30: Not enough samples (24) for class 17. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  33%|███▎      | 33/100 [00:04<00:08,  7.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 33: Not enough samples (10) for class 19. Need 25.\n",
            "Skipping task 34: Not enough samples (10) for class 19. Need 25.\n",
            "Skipping task 35: Not enough samples (24) for class 17. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  39%|███▉      | 39/100 [00:05<00:07,  7.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 39: Not enough samples (10) for class 19. Need 25.\n",
            "Skipping task 40: Not enough samples (10) for class 19. Need 25.\n",
            "Skipping task 41: Not enough samples (10) for class 19. Need 25.\n",
            "Skipping task 42: Not enough samples (24) for class 17. Need 25.\n",
            "Skipping task 43: Not enough samples (24) for class 14. Need 25.\n",
            "Skipping task 44: Not enough samples (10) for class 19. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  46%|████▌     | 46/100 [00:05<00:04, 12.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 46: Not enough samples (24) for class 14. Need 25.\n",
            "Skipping task 47: Not enough samples (24) for class 17. Need 25.\n",
            "Skipping task 48: Not enough samples (24) for class 14. Need 25.\n",
            "Skipping task 49: Not enough samples (10) for class 19. Need 25.\n",
            "Skipping task 50: Not enough samples (24) for class 17. Need 25.\n",
            "Skipping task 51: Not enough samples (24) for class 17. Need 25.\n",
            "Skipping task 52: Not enough samples (24) for class 17. Need 25.\n",
            "Skipping task 53: Not enough samples (24) for class 14. Need 25.\n",
            "Skipping task 54: Not enough samples (24) for class 14. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  56%|█████▌    | 56/100 [00:05<00:02, 19.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 56: Not enough samples (24) for class 17. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  61%|██████    | 61/100 [00:06<00:02, 13.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 60: Not enough samples (24) for class 14. Need 25.\n",
            "Skipping task 61: Not enough samples (10) for class 19. Need 25.\n",
            "Skipping task 62: Not enough samples (24) for class 17. Need 25.\n",
            "Skipping task 63: Not enough samples (10) for class 19. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  67%|██████▋   | 67/100 [00:06<00:02, 12.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 66: Not enough samples (24) for class 17. Need 25.\n",
            "Skipping task 67: Not enough samples (24) for class 14. Need 25.\n",
            "Skipping task 68: Not enough samples (10) for class 19. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  70%|███████   | 70/100 [00:06<00:02, 12.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 70: Not enough samples (24) for class 14. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  74%|███████▍  | 74/100 [00:07<00:02, 10.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 73: Not enough samples (10) for class 19. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  76%|███████▌  | 76/100 [00:07<00:02, 10.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 75: Not enough samples (24) for class 14. Need 25.\n",
            "Skipping task 76: Not enough samples (10) for class 19. Need 25.\n",
            "Skipping task 77: Not enough samples (24) for class 17. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  81%|████████  | 81/100 [00:08<00:02,  8.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 81: Not enough samples (10) for class 19. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  83%|████████▎ | 83/100 [00:08<00:02,  8.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 83: Not enough samples (10) for class 19. Need 25.\n",
            "Skipping task 84: Not enough samples (10) for class 19. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  88%|████████▊ | 88/100 [00:09<00:02,  4.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 88: Not enough samples (24) for class 17. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  91%|█████████ | 91/100 [00:10<00:01,  4.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 91: Not enough samples (10) for class 19. Need 25.\n",
            "Skipping task 92: Not enough samples (24) for class 14. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks:  96%|█████████▌| 96/100 [00:12<00:01,  3.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 96: Not enough samples (24) for class 17. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFew-shot tasks:  98%|█████████▊| 98/100 [00:12<00:00,  3.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping task 98: Not enough samples (24) for class 17. Need 25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Few-shot tasks: 100%|██████████| 100/100 [00:12<00:00,  7.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Few-shot Acc (4-way 10-shot): 0.9614±0.0372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xgdNcAlTgIHq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}