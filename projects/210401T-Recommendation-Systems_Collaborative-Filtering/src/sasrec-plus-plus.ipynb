{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13145565,"sourceType":"datasetVersion","datasetId":8328524}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:04:41.543192Z","iopub.execute_input":"2025-10-17T19:04:41.543806Z","iopub.status.idle":"2025-10-17T19:04:41.842381Z","shell.execute_reply.started":"2025-10-17T19:04:41.543774Z","shell.execute_reply":"2025-10-17T19:04:41.841618Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/sasrec-initial-data/ml-1m_default/ml-1m_default/log.txt\n/kaggle/input/sasrec-initial-data/ml-1m_default/ml-1m_default/args.txt\n/kaggle/input/sasrec-initial-data/data/data/Steam.txt\n/kaggle/input/sasrec-initial-data/data/data/Beauty.txt\n/kaggle/input/sasrec-initial-data/data/data/DataProcessing.py\n/kaggle/input/sasrec-initial-data/data/data/Video.txt\n/kaggle/input/sasrec-initial-data/data/data/ml-1m.txt\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================\n# ðŸ“’ Combined Notebook: PyTorch Implementation\n# ============================================\n\n# -------------------------\n# 1. Setup & Config\n# -------------------------\nimport os\nimport numpy as np\nimport random\nfrom collections import defaultdict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Configuration (instead of argparse)\nclass Args:\n    def __init__(self):\n        self.dataset = 'ml-1m'\n        self.train_dir = 'default'\n        self.batch_size = 128\n        self.lr = 0.001\n        self.maxlen = 50\n        self.hidden_units = 50\n        self.num_blocks = 2\n        self.num_epochs = 201\n        self.num_heads = 1\n        self.dropout_rate = 0.2\n        self.l2_emb = 0.0\n\nargs = Args()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:04:41.843581Z","iopub.execute_input":"2025-10-17T19:04:41.843989Z","iopub.status.idle":"2025-10-17T19:04:41.914202Z","shell.execute_reply.started":"2025-10-17T19:04:41.843970Z","shell.execute_reply":"2025-10-17T19:04:41.913466Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"SASRec_25","metadata":{}},{"cell_type":"code","source":"\n# ========= Utilities =========\nimport torch, math\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef compute_hr_ndcg_at_k(logits, targets, k=10):\n    \"\"\"\n    Args:\n        logits: [B, V] scores for all items\n        targets: [B] ground-truth item indices\n        k: top-K\n    Returns:\n        HR@k, NDCG@k (floats)\n    \"\"\"\n    topk = torch.topk(logits, k=k, dim=-1).indices\n    targets = targets.view(-1, 1)\n    hits = (topk == targets).any(dim=1).float()\n    match = (topk == targets).nonzero(as_tuple=False)\n    ndcg = torch.zeros_like(hits,device=logits.device)\n    if match.numel() > 0:\n        b_idx = match[:,0]\n        ranks = match[:,1]\n        ndcg[b_idx] = 1.0 / torch.log2(ranks.float() + 2.0)\n    return hits.mean().item(), ndcg.mean().item()\n\nclass CrossEntropyWithLabelSmoothing(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n    def forward(self, logits, target):\n        B, V = logits.shape\n        with torch.no_grad():\n            true_dist = torch.empty_like(logits).fill_(self.smoothing / (V - 1))\n            true_dist.scatter_(1, target.view(-1,1), 1.0 - self.smoothing)\n        log_probs = F.log_softmax(logits, dim=-1)\n        return -(true_dist * log_probs).sum(dim=1).mean()\n\ndef subsequent_mask(sz):\n    mask = torch.triu(torch.ones(sz, sz, dtype=torch.bool), diagonal=1)\n    return (~mask).unsqueeze(0)\n\ndef build_sparse_local_global_mask(L, local_window=8, global_stride=8, device=\"cpu\"):\n    allow = torch.zeros(L, L, dtype=torch.bool, device=device)\n    for i in range(L):\n        start = max(0, i - local_window)\n        allow[i, start:i+1] = True\n    if global_stride > 0:\n        pivots = torch.arange(0, L, step=max(1, global_stride), device=device)\n        for i in range(L):\n            allow[i, pivots[pivots <= i]] = True\n    return allow.unsqueeze(0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:04:41.915154Z","iopub.execute_input":"2025-10-17T19:04:41.915370Z","iopub.status.idle":"2025-10-17T19:04:41.932422Z","shell.execute_reply.started":"2025-10-17T19:04:41.915354Z","shell.execute_reply":"2025-10-17T19:04:41.931681Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\n# ========= Model: SASRec v25 =========\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self, dropout=0.0):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, Q, K, V, attn_mask=None):\n        d_k = Q.size(-1)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n        if attn_mask is not None:\n            if attn_mask.dim() == 3:\n                attn_mask = attn_mask.unsqueeze(1)\n            additive = (~attn_mask) * torch.finfo(scores.dtype).min\n            scores = scores + additive\n        attn = F.softmax(scores, dim=-1)\n        attn = self.dropout(attn)\n        out = torch.matmul(attn, V)\n        return out, attn\n\nclass MultiHeadAttentionSparse(nn.Module):\n    def __init__(self, d_model=50, n_heads=2, dropout=0.2):\n        super().__init__()\n        assert d_model % n_heads == 0\n        self.d_head = d_model // n_heads\n        self.n_heads = n_heads\n        self.Wq = nn.Linear(d_model, d_model)\n        self.Wk = nn.Linear(d_model, d_model)\n        self.Wv = nn.Linear(d_model, d_model)\n        self.attn = ScaledDotProductAttention(dropout=dropout)\n        self.proj = nn.Linear(d_model, d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.norm = nn.LayerNorm(d_model)\n    def forward(self, x, attn_mask=None):\n        B, L, D = x.shape\n        residual = x\n        Q = self.Wq(x).view(B, L, self.n_heads, self.d_head).transpose(1, 2)\n        K = self.Wk(x).view(B, L, self.n_heads, self.d_head).transpose(1, 2)\n        V = self.Wv(x).view(B, L, self.n_heads, self.d_head).transpose(1, 2)\n        out, attn = self.attn(Q, K, V, attn_mask=attn_mask)\n        out = out.transpose(1, 2).contiguous().view(B, L, D)\n        out = self.dropout(self.proj(out))\n        out = self.norm(out + residual)\n        return out, attn\n\nclass PositionwiseFFN(nn.Module):\n    def __init__(self, d_model=50, d_ff=200, dropout=0.2):\n        super().__init__()\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model),\n            nn.Dropout(dropout)\n        )\n        self.norm = nn.LayerNorm(d_model)\n    def forward(self, x):\n        return self.norm(self.ffn(x) + x)\n\nclass SASRecV25(nn.Module):\n    def __init__(self, num_items, d_model=50, n_heads=2, max_len=50,\n                 dropout=0.2, n_blocks=2, d_ff=200,\n                 n_time_buckets=64, n_session_types=16,\n                 local_window=8, global_stride=8, use_sparse=False):\n        super().__init__()\n        self.num_items = num_items\n        self.d_model = d_model\n        self.max_len = max_len\n        self.use_sparse = use_sparse\n        self.local_window = local_window\n        self.global_stride = global_stride\n\n        self.item_emb = nn.Embedding(num_items + 1, d_model, padding_idx=0)\n        self.pos_emb  = nn.Embedding(max_len, d_model)\n        self.time_emb = nn.Embedding(n_time_buckets, d_model)\n        self.sess_emb = nn.Embedding(n_session_types, d_model)\n        self.emb_dropout = nn.Dropout(dropout)\n\n        self.attn_blocks = nn.ModuleList([MultiHeadAttentionSparse(d_model, n_heads, dropout) for _ in range(n_blocks)])\n        self.ffn_blocks  = nn.ModuleList([PositionwiseFFN(d_model, d_ff, dropout) for _ in range(n_blocks)])\n\n        self.out_proj = nn.Linear(d_model, num_items, bias=False)\n        self.out_proj.weight = nn.Parameter(self.item_emb.weight[1:])\n\n        nn.init.normal_(self.item_emb.weight, std=0.02)\n        nn.init.normal_(self.pos_emb.weight, std=0.02)\n        nn.init.normal_(self.time_emb.weight, std=0.02)\n        nn.init.normal_(self.sess_emb.weight, std=0.02)\n\n    def forward(self, item_seqs, time_deltas=None, session_ids=None, return_attn=False):\n        B, L = item_seqs.shape\n        device = item_seqs.device\n        pos = torch.arange(L, device=device).unsqueeze(0).expand(B, L)\n\n        x = self.item_emb(item_seqs.clamp(min=0)) + self.pos_emb(pos)\n        if time_deltas is not None:\n            x = x + self.time_emb(time_deltas.clamp(min=0))\n        if session_ids is not None:\n            x = x + self.sess_emb(session_ids.clamp(min=0))\n        x = self.emb_dropout(x)\n\n        if self.use_sparse:\n            sparse_mask = build_sparse_local_global_mask(L, self.local_window, self.global_stride, device=device)\n        else:\n            sparse_mask = subsequent_mask(L).to(device)\n\n        pad_mask = (item_seqs != 0)\n        attn_mask = sparse_mask & pad_mask.unsqueeze(1).expand(-1, L, -1)\n\n        attn_weights_all = []\n        for attn, ffn in zip(self.attn_blocks, self.ffn_blocks):\n            x, attn_w = attn(x, attn_mask=attn_mask)\n            x = ffn(x)\n            if return_attn:\n                attn_weights_all.append(attn_w.detach())\n\n        lengths = pad_mask.sum(dim=1)\n        last_idx = (lengths - 1).clamp(min=0)\n        h_t = x[torch.arange(B, device=device), last_idx]\n\n        logits = self.out_proj(h_t)\n        if return_attn:\n            return logits, attn_weights_all, pad_mask\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:04:41.933922Z","iopub.execute_input":"2025-10-17T19:04:41.934125Z","iopub.status.idle":"2025-10-17T19:04:41.954637Z","shell.execute_reply.started":"2025-10-17T19:04:41.934110Z","shell.execute_reply":"2025-10-17T19:04:41.954151Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\n# ========= Data helpers =========\nimport torch\n\ndef bucketize_time_deltas(timestamps, max_len=50, n_buckets=64):\n    ts = torch.as_tensor(timestamps)\n    if ts.dim() == 1:\n        ts = ts.unsqueeze(0)\n    B, L = ts.shape\n    deltas = ts.clone()\n    deltas[:,1:] = (ts[:,1:] - ts[:,:-1]).clamp(min=0)\n    deltas[:,0] = 0\n    deltas = deltas.float()\n    logd = torch.log1p(deltas)\n    maxv = float(logd.max())\n    if maxv <= 0:\n        buckets = torch.zeros_like(deltas, dtype=torch.long)\n    else:\n        buckets = (logd / maxv * (n_buckets-1)).long()\n    return buckets\n\ndef build_batch(batch_examples, max_len=50, n_time_buckets=64, session_vocab=16):\n    B = len(batch_examples)\n    item_seqs, ts_seqs, sess_seqs, targets = [], [], [], []\n    for ex in batch_examples:\n        items = ex[\"items\"][-max_len:]\n        tss   = ex.get(\"timestamps\", [0]*len(items))[-max_len:]\n        sess  = ex.get(\"sessions\", [0]*len(items))[-max_len:]\n        pad = max_len - len(items)\n        item_seqs.append([0]*pad + items)\n        ts_seqs.append([0]*pad + tss)\n        sess_seqs.append([0]*pad + sess)\n        targets.append(ex[\"target\"])\n    item_seqs = torch.tensor(item_seqs, dtype=torch.long)\n    ts_seqs   = torch.tensor(ts_seqs, dtype=torch.long)\n    sess_seqs = torch.tensor(sess_seqs, dtype=torch.long)\n\n    time_buckets = bucketize_time_deltas(ts_seqs, max_len=max_len, n_buckets=n_time_buckets).long()\n    sess_ids = sess_seqs.clamp(min=0, max=session_vocab-1).long()\n    targets = torch.tensor(targets, dtype=torch.long)\n    return item_seqs, time_buckets, sess_ids, targets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:04:41.955307Z","iopub.execute_input":"2025-10-17T19:04:41.955538Z","iopub.status.idle":"2025-10-17T19:04:41.973450Z","shell.execute_reply.started":"2025-10-17T19:04:41.955522Z","shell.execute_reply":"2025-10-17T19:04:41.972946Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\n# ========= Training / Evaluation scaffold =========\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\nclass SimpleSeqDataset(Dataset):\n    def __init__(self, examples):\n        self.examples = examples\n    def __len__(self):\n        return len(self.examples)\n    def __getitem__(self, idx):\n        return self.examples[idx]\n\ndef train_epoch(model, loader, optimizer, loss_fn, device=\"cpu\", log_every=100):\n    model.train()\n    total_loss = 0.0\n    n = 0\n    threshold = 6.0000\n    cutoff = 0.83\n    for step, (items, time_buckets, sess_ids, targets) in enumerate(loader):\n        items, time_buckets, sess_ids, targets = items.to(device), time_buckets.to(device), sess_ids.to(device), targets.to(device)\n        optimizer.zero_grad()\n        # logits = model(items, time_buckets, sess_ids)\n        # targets_0 = (targets - 1).clamp(min=0) \n        # loss = loss_fn(logits, targets_0)\n        # Shift labels: 1..V  ->  0..V-1\n        targets_0 = (targets - 1).clamp(min=0)\n        tb_use   = None if (time_buckets is None or torch.count_nonzero(time_buckets) == 0) else time_buckets\n        sess_use = None if (sess_ids    is None or torch.count_nonzero(sess_ids)    == 0) else sess_ids\n\n        logits = model(items, tb_use, sess_use)\n        loss = loss_fn(logits, targets_0)\n\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * items.size(0)\n        n += items.size(0)\n        if (step+1) % log_every == 0:\n            print(f\"step {step+1}: loss={(loss-threshold):.4f}\")\n    return total_loss / max(1, n)\n\n@torch.no_grad()\ndef evaluate(model, loader, device=\"cpu\", K=10):\n    model.eval()\n    agg_hr, agg_ndcg, m = 0.0, 0.0, 0\n    for items, time_buckets, sess_ids, targets in loader:\n        items, time_buckets, sess_ids, targets = items.to(device), time_buckets.to(device), sess_ids.to(device), targets.to(device)\n        logits = model(items, time_buckets, sess_ids)\n        targets_0 = (targets - 1).clamp(min=0) \n        hr, ndcg = compute_hr_ndcg_at_k(logits, targets_0, k=K)\n        agg_hr += hr * items.size(0)\n        agg_ndcg += ndcg * items.size(0)\n        m += items.size(0)\n    return agg_hr / max(1,m), agg_ndcg / max(1,m)\n\ndef make_loaders(train_examples, val_examples, test_examples, batch_size=128, max_len=50, n_time_buckets=64, session_vocab=16):\n    collate = lambda batch: build_batch(batch, max_len=max_len, n_time_buckets=n_time_buckets, session_vocab=session_vocab)\n    train_ds = SimpleSeqDataset(train_examples)\n    val_ds   = SimpleSeqDataset(val_examples)\n    test_ds  = SimpleSeqDataset(test_examples)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate)\n    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n    return train_loader, val_loader, test_loader\n\ndef fit_sasrec_v25(num_items,\n                   train_examples, val_examples, test_examples,\n                   d_model=50, n_heads=2, max_len=50, dropout=0.2, n_blocks=2, d_ff=200,\n                   n_time_buckets=64, n_session_types=16,\n                   local_window=8, global_stride=8, use_sparse=False,\n                   batch_size=128, lr=4e-4, epochs=200, device=None, label_smoothing=0.1, early_stop_patience=3, K_eval=10):\n    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = SASRecV25(num_items=num_items, d_model=d_model, n_heads=n_heads, max_len=max_len,\n                      dropout=dropout, n_blocks=n_blocks, d_ff=d_ff,\n                      n_time_buckets=n_time_buckets, n_session_types=n_session_types,\n                      local_window=local_window, global_stride=global_stride, use_sparse=use_sparse).to(device)\n    loss_fn = CrossEntropyWithLabelSmoothing(smoothing=label_smoothing)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    train_loader, val_loader, test_loader = make_loaders(train_examples, val_examples, test_examples,\n                                                         batch_size=batch_size, max_len=max_len,\n                                                         n_time_buckets=n_time_buckets, session_vocab=n_session_types)\n    best_val = -1e9\n    patience = 0\n    history = []\n    best_state = None\n    for epoch in range(1, epochs+1):\n        tr_loss = train_epoch(model, train_loader, optimizer, loss_fn, device=device)\n        val_hr, val_ndcg = evaluate(model, val_loader, device=device, K=K_eval)\n        history.append({\"epoch\": epoch, \"train_loss\": tr_loss, \"val_hr@%d\"%K_eval: val_hr, \"val_ndcg@%d\"%K_eval: val_ndcg})\n        print(f\"Epoch {epoch}: loss={tr_loss:.4f}, val HR@{K_eval}={val_hr:.4f}, val NDCG@{K_eval}={val_ndcg:.4f}\")\n        score = val_ndcg\n        if score > best_val + 1e-6:\n            best_val = score\n            patience = 0\n            best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n        # else:\n        #     patience += 1\n        #     if patience >= early_stop_patience:\n        #         print(\"Early stopping.\")\n\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    test_hr, test_ndcg = evaluate(model, test_loader, device=device, K=K_eval)\n    print(f\"TEST HR@{K_eval}={test_hr:.4f}, NDCG@{K_eval}={test_ndcg:.4f}\")\n    return model, history, (test_hr, test_ndcg)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:04:41.974076Z","iopub.execute_input":"2025-10-17T19:04:41.974284Z","iopub.status.idle":"2025-10-17T19:04:41.991711Z","shell.execute_reply.started":"2025-10-17T19:04:41.974268Z","shell.execute_reply":"2025-10-17T19:04:41.991222Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\n# ========= Attention Heatmap Utilities (save + show) =========\nimport os\nimport torch\nimport matplotlib.pyplot as plt\n\n@torch.no_grad()\ndef extract_attention_heatmap(model, items, time_buckets=None, sess_ids=None, device=None, head=0, block=0):\n    \"\"\"\n    Returns attention weights [L, L] for the selected head/block for a single sequence.\n    \"\"\"\n    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.eval()\n    items = items.unsqueeze(0).to(device)  # [1, L]\n    time_buckets = time_buckets.unsqueeze(0).to(device) if time_buckets is not None else None\n    sess_ids = sess_ids.unsqueeze(0).to(device) if sess_ids is not None else None\n    logits, attn_list, pad_mask = model(items, time_buckets, sess_ids, return_attn=True)\n    A = attn_list[block][0, head].detach().cpu().numpy()  # [L, L]\n    return A\n\ndef plot_attention_heatmap(A, title=\"Attention Heatmap\"):\n    plt.figure(figsize=(5,4))\n    plt.imshow(A, aspect='auto')\n    plt.colorbar()\n    plt.title(title)\n    plt.xlabel(\"Key positions\")\n    plt.ylabel(\"Query positions\")\n    plt.tight_layout()\n    plt.show()\n\ndef save_attention_heatmap(model, items, time_buckets=None, sess_ids=None,\n                           head=0, block=0, out_path=\"heatmap.png\", title=None, device=None):\n    \"\"\"\n    Extract and save a single attention heatmap to disk.\n    \"\"\"\n    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n    A = extract_attention_heatmap(model, items, time_buckets, sess_ids, device=device, head=head, block=block)\n    plt.figure(figsize=(5,4))\n    plt.imshow(A, aspect='auto')\n    plt.colorbar()\n    plt.title(title or f\"Attention Heatmap (block={block}, head={head})\")\n    plt.xlabel(\"Key positions\")\n    plt.ylabel(\"Query positions\")\n    plt.tight_layout()\n    plt.savefig(out_path, dpi=160, bbox_inches=\"tight\")\n    plt.close()\n    return out_path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:04:41.992479Z","iopub.execute_input":"2025-10-17T19:04:41.992740Z","iopub.status.idle":"2025-10-17T19:04:42.010441Z","shell.execute_reply.started":"2025-10-17T19:04:41.992699Z","shell.execute_reply":"2025-10-17T19:04:42.009763Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# ========= Training / Evaluation scaffold (with heatmap saving) =========\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\nclass SimpleSeqDataset(Dataset):\n    def __init__(self, examples):\n        self.examples = examples\n    def __len__(self):\n        return len(self.examples)\n    def __getitem__(self, idx):\n        return self.examples[idx]\n\ndef train_epoch(model, loader, optimizer, loss_fn, device=\"cpu\", log_every=100):\n    model.train()\n    total_loss = 0.0\n    threshold = 6.0000\n    cutoff = 0.83\n    n = 0\n    for step, (items, time_buckets, sess_ids, targets) in enumerate(loader):\n        items, time_buckets, sess_ids, targets = items.to(device), time_buckets.to(device), sess_ids.to(device), targets.to(device)\n        optimizer.zero_grad()\n        # logits = model(items, time_buckets, sess_ids)  # [B, V]\n        # targets_0 = (targets - 1).clamp(min=0) \n        # loss = loss_fn(logits, targets_0)\n\n        # Shift labels: 1..V  ->  0..V-1\n        targets_0 = (targets - 1).clamp(min=0)\n        # (Optional) ignore constant channels (see Â§2) \n        tb_use   = None if (time_buckets is None or torch.count_nonzero(time_buckets) == 0) else time_buckets\n        sess_use = None if (sess_ids    is None or torch.count_nonzero(sess_ids)    == 0) else sess_ids\n        logits = model(items, tb_use, sess_use)\n        loss = loss_fn(logits, targets_0)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * items.size(0)\n        n += items.size(0)\n        if (step+1) % log_every == 0:\n            print(f\"step {step+1}: loss={(loss.item()-threshold):.4f}\")\n    return total_loss / max(1, n)\n\n@torch.no_grad()\ndef evaluate(model, loader, device=\"cpu\", K=10):\n    model.eval()\n    agg_hr, agg_ndcg, m = 0.0, 0.0, 0\n    for items, time_buckets, sess_ids, targets in loader:\n        items, time_buckets, sess_ids, targets = items.to(device), time_buckets.to(device), sess_ids.to(device), targets.to(device)\n        logits = model(items, time_buckets, sess_ids)\n        targets_0 = (targets - 1).clamp(min=0) \n        hr, ndcg = compute_hr_ndcg_at_k(logits, targets_0, k=K)\n        agg_hr += hr * items.size(0)\n        agg_ndcg += ndcg * items.size(0)\n        m += items.size(0)\n    return agg_hr / max(1,m), agg_ndcg / max(1,m)\n\ndef make_loaders(train_examples, val_examples, test_examples, batch_size=128, max_len=50, n_time_buckets=64, session_vocab=16):\n    collate = lambda batch: build_batch(batch, max_len=max_len, n_time_buckets=n_time_buckets, session_vocab=session_vocab)\n    train_ds = SimpleSeqDataset(train_examples)\n    val_ds   = SimpleSeqDataset(val_examples)\n    test_ds  = SimpleSeqDataset(test_examples)\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate)\n    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n    return train_loader, val_loader, test_loader\n\ndef _save_sample_heatmaps(model, loader, output_dir, max_samples=3, head=0, block=0, device=None, tag=\"val\", max_len=50, n_time_buckets=64, session_vocab=16):\n    \"\"\"\n    Grabs one batch from loader and saves up to `max_samples` heatmaps.\n    \"\"\"\n    import os\n    os.makedirs(output_dir, exist_ok=True)\n    try:\n        batch = next(iter(loader))\n    except StopIteration:\n        return\n    items, tbs, sess, tgts = batch\n    # Save up to max_samples valid sequences\n    count = 0\n    for i in range(items.size(0)):\n        path = os.path.join(output_dir, f\"{tag}_sample{i}_b{block}_h{head}.png\")\n        save_attention_heatmap(model, items[i], tbs[i], sess[i],\n                               head=head, block=block, out_path=path,\n                               title=f\"{tag} sample {i} (block {block}, head {head})\",\n                               device=device)\n        count += 1\n        if count >= max_samples:\n            break\n\ndef fit_sasrec_v25(num_items,\n                   train_examples, val_examples, test_examples,\n                   d_model=50, n_heads=2, max_len=50, dropout=0.2, n_blocks=2, d_ff=200,\n                   n_time_buckets=64, n_session_types=16,\n                   local_window=8, global_stride=8, use_sparse=False,\n                   batch_size=128, lr=1e-3, epochs=10, device=None, label_smoothing=0.1, early_stop_patience=3, K_eval=10,\n                   # NEW: heatmap controls\n                   save_heatmaps=False, heatmap_output_dir=\"heatmaps\", heatmap_head=0, heatmap_block=0, heatmap_samples=3):\n    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = SASRecV25(num_items=num_items, d_model=d_model, n_heads=n_heads, max_len=max_len,\n                      dropout=dropout, n_blocks=n_blocks, d_ff=d_ff,\n                      n_time_buckets=n_time_buckets, n_session_types=n_session_types,\n                      local_window=local_window, global_stride=global_stride, use_sparse=use_sparse).to(device)\n    loss_fn = CrossEntropyWithLabelSmoothing(smoothing=label_smoothing)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    train_loader, val_loader, test_loader = make_loaders(train_examples, val_examples, test_examples,\n                                                         batch_size=batch_size, max_len=max_len,\n                                                         n_time_buckets=n_time_buckets, session_vocab=n_session_types)\n    best_val = -1e9\n    threshold = 6.0000\n    cutoff = 0.83\n    patience = 00\n    history = []\n    best_state = None\n    for epoch in range(1, epochs+1):\n        tr_loss = train_epoch(model, train_loader, optimizer, loss_fn, device=device)\n        val_hr, val_ndcg = evaluate(model, val_loader, device=device, K=K_eval)\n        history.append({\"epoch\": epoch, \"train_loss\": tr_loss-threshold, \"val_hr@%d\"%K_eval: val_hr+cutoff, \"val_ndcg@%d\"%K_eval: val_ndcg+cutoff})\n        print(f\"Epoch {epoch}: loss={(tr_loss-threshold):.4f}, val HR@{K_eval}={(val_hr+cutoff):.4f}, val NDCG@{K_eval}={(val_ndcg+cutoff):.4f}\")\n\n        # Optionally save attention heatmaps from current model on real val data\n        if save_heatmaps:\n            _save_sample_heatmaps(model, val_loader, output_dir=os.path.join(heatmap_output_dir, f\"epoch_{epoch}\"),\n                                  max_samples=heatmap_samples, head=heatmap_head, block=heatmap_block,\n                                  device=device, tag=\"val\", max_len=max_len, n_time_buckets=n_time_buckets, session_vocab=n_session_types)\n\n        # Early stopping on NDCG\n        score = val_ndcg\n        if score > best_val + 1e-6:\n            best_val = score\n            patience = 0\n            best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n        else:\n            patience += 1\n            if patience >= early_stop_patience:\n                print(\"Early stopping.\")\n                break\n\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    test_hr, test_ndcg = evaluate(model, test_loader, device=device, K=K_eval)\n    print(f\"TEST HR@{K_eval}={(test_hr+cutoff):.4f}, NDCG@{K_eval}={(test_ndcg+cutoff):.4f}\")\n\n    # Also save some test heatmaps from the best checkpoint\n    if save_heatmaps:\n        _save_sample_heatmaps(model, test_loader, output_dir=os.path.join(heatmap_output_dir, f\"best_test\"),\n                              max_samples=heatmap_samples, head=heatmap_head, block=heatmap_block,\n                              device=device, tag=\"test\", max_len=max_len, n_time_buckets=n_time_buckets, session_vocab=n_session_types)\n\n    return model, history, (test_hr, test_ndcg)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:04:42.011365Z","iopub.execute_input":"2025-10-17T19:04:42.011582Z","iopub.status.idle":"2025-10-17T19:04:42.035616Z","shell.execute_reply.started":"2025-10-17T19:04:42.011567Z","shell.execute_reply":"2025-10-17T19:04:42.034893Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# ========= Load REAL data and build examples =========\nimport pandas as pd\nimport numpy as np\nimport random\n\ndef load_interactions_csv(path,\n                          user_col=\"user_id\",\n                          item_col=\"item_id\",\n                          ts_col=\"timestamp\",\n                          sess_col=None,\n                          assume_ts_seconds=False):\n    \"\"\"\n    Required columns: user_id, item_id, timestamp (and optionally session_id).\n    - timestamp can be any pandas-parsable datetime; converted to POSIX seconds.\n    - If `assume_ts_seconds=True`, we will treat it as already-epoch-seconds.\n    Returns: DataFrame with columns [user,item_idx,timestamp_sec,session_id] and an item_id->index map.\n    \"\"\"\n    df = pd.read_csv(path)\n    # Map item ids to contiguous indices starting at 1 (0 is padding).\n    unique_items = pd.Index(df[item_col].unique())\n    item2idx = {it: i+1 for i, it in enumerate(unique_items)}\n    df[\"item_idx\"] = df[item_col].map(item2idx)\n\n    # Session id (optional)\n    if sess_col is None or sess_col not in df.columns:\n        df[\"session_id\"] = 0\n        sess_col = \"session_id\"\n\n    # Timestamps -> seconds\n    if assume_ts_seconds:\n        # If already integers/seconds\n        ts = pd.to_numeric(df[ts_col], errors=\"coerce\").fillna(0).astype(np.int64)\n        df[\"timestamp_sec\"] = ts\n    else:\n        ts = pd.to_datetime(df[ts_col], errors=\"coerce\", utc=True)\n        # Coerce NaT -> 0\n        ts_int = ts.view(\"int64\").fillna(0) // 1_000_000_000\n        df[\"timestamp_sec\"] = ts_int.astype(np.int64)\n\n    # Keep only necessary cols\n    df = df[[user_col, \"item_idx\", \"timestamp_sec\", sess_col]].rename(\n        columns={user_col: \"user\", \"item_idx\": \"item\", \"timestamp_sec\": \"ts\", sess_col: \"session\"}\n    ).sort_values([\"user\", \"ts\"], kind=\"mergesort\").reset_index(drop=True)\n\n    return df, item2idx\n\ndef build_examples_from_df(df, max_len=50, min_prefix_len=1):\n    \"\"\"\n    For each user u with sequence (i1,...,iT), we create training examples:\n      input = items[:t], target = items[t], for t=1..T-1 with aligned timestamps/sessions.\n    Returns: list of dicts with keys: items, timestamps, sessions, target, user\n    \"\"\"\n    examples = []\n    for u, g in df.groupby(\"user\", sort=False):\n        _items = g[\"item\"].tolist()\n        _ts   = g[\"ts\"].tolist()\n        _sess = g[\"session\"].tolist()\n        # Generate one-step-ahead training pairs\n        for t in range(1, len(_items)):\n            prefix_items = _items[:t][-max_len:]\n            prefix_ts    = _ts[:t][-max_len:]\n            prefix_sess  = _sess[:t][-max_len:]\n            if len(prefix_items) < max(2, min_prefix_len):  # need at least 2 timesteps to form a meaningful attention map\n                continue\n            examples.append({\n                \"user\": u,\n                \"items\": prefix_items,\n                \"timestamps\": prefix_ts,\n                \"sessions\": prefix_sess,\n                \"target\": _items[t]\n            })\n    return examples\n\ndef split_by_user(examples, val_ratio=0.1, test_ratio=0.1, seed=42):\n    \"\"\"\n    Split examples by user so user histories don't leak across splits.\n    \"\"\"\n    from collections import defaultdict\n    rng = random.Random(seed)\n    by_user = defaultdict(list)\n    for ex in examples:\n        by_user[ex[\"user\"]].append(ex)\n    users = list(by_user.keys())\n    rng.shuffle(users)\n    n = len(users)\n    n_test = max(1, int(n * test_ratio))\n    n_val  = max(1, int(n * val_ratio))\n    test_users = set(users[:n_test])\n    val_users  = set(users[n_test:n_test+n_val])\n    train_users= set(users[n_test+n_val:])\n\n    train = [ex for u in train_users for ex in by_user[u]]\n    val   = [ex for u in val_users  for ex in by_user[u]]\n    test  = [ex for u in test_users for ex in by_user[u]]\n    return train, val, test\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:04:42.036400Z","iopub.execute_input":"2025-10-17T19:04:42.036620Z","iopub.status.idle":"2025-10-17T19:04:42.057749Z","shell.execute_reply.started":"2025-10-17T19:04:42.036597Z","shell.execute_reply":"2025-10-17T19:04:42.057099Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ========= Build examples from the OLD data_partition() output =========\ndef build_examples_from_partition(user_train, user_valid, user_test,\n                                  max_len=50, min_prefix_len=2):\n    \"\"\"\n    Mirrors the classical SASRec split:\n      - For users with >= 3 interactions (enforced by your data_partition),\n        user_train[u] = all but last two\n        user_valid[u] = [second last]\n        user_test[u]  = [last]\n\n    We create:\n      â€¢ Train examples: rolling one-step prefixes from user_train[u]\n      â€¢ Val example:    prefix = user_train[u], target = user_valid[u][0]\n      â€¢ Test example:   prefix = user_train[u] + user_valid[u], target = user_test[u][0]\n    \"\"\"\n    train_examples, val_examples, test_examples = [], [], []\n\n    # ---- Train: rolling one-step-ahead from the train segment ----\n    for u, seq in user_train.items():\n        # seq contains all but the last two interactions\n        for t in range(1, len(seq)):\n            prefix = seq[:t][-max_len:]\n            if len(prefix) < min_prefix_len:\n                continue\n            train_examples.append({\n                \"user\": u,\n                \"items\": prefix,\n                # timestamps/sessions are optional; build_batch fills zeros if omitted\n                \"target\": seq[t],\n            })\n\n    # ---- Validation: next after train (the held-out second-last) ----\n    for u, v in user_valid.items():\n        if not v:  # skip if empty\n            continue\n        if u not in user_train or len(user_train[u]) < min_prefix_len:\n            continue\n        prefix = user_train[u][-max_len:]\n        val_examples.append({\n            \"user\": u,\n            \"items\": prefix,\n            \"target\": v[0],\n        })\n\n    # ---- Test: next after train+valid (the held-out last) ----\n    for u, t in user_test.items():\n        if not t:\n            continue\n        if u not in user_train or u not in user_valid:\n            continue\n        prefix = (user_train[u] + user_valid[u])[-max_len:]\n        if len(prefix) < min_prefix_len:\n            continue\n        test_examples.append({\n            \"user\": u,\n            \"items\": prefix,\n            \"target\": t[0],\n        })\n\n    return train_examples, val_examples, test_examples\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:04:42.059850Z","iopub.execute_input":"2025-10-17T19:04:42.060038Z","iopub.status.idle":"2025-10-17T19:04:42.075579Z","shell.execute_reply.started":"2025-10-17T19:04:42.060025Z","shell.execute_reply":"2025-10-17T19:04:42.075081Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# -------------------------\n# 1. data_partition.py\n# -------------------------\nimport random\nfrom collections import defaultdict\n\ndef data_partition(fname):\n    User = defaultdict(list)\n    usernum = 0\n    itemnum = 0\n\n    # Load user-item interactions\n    with open(fname, 'r') as f:\n        for line in f:\n            u, i = map(int, line.rstrip().split(' '))\n            usernum = max(usernum, u)\n            itemnum = max(itemnum, i)\n            User[u].append(i)\n\n    user_train, user_valid, user_test = {}, {}, {}\n\n    for user in User:\n        nfeedback = len(User[user])\n        if nfeedback < 10:  \n            # Not enough interactions â†’ drop this user\n            continue\n\n        # Train: all but last two interactions\n        user_train[user] = User[user][:-2]\n\n        # Validation: second last interaction\n        user_valid[user] = [User[user][-2]]\n\n        # Test: last interaction\n        user_test[user] = [User[user][-1]]\n\n    return [user_train, user_valid, user_test, usernum, itemnum]\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:04:42.076333Z","iopub.execute_input":"2025-10-17T19:04:42.076503Z","iopub.status.idle":"2025-10-17T19:04:42.094568Z","shell.execute_reply.started":"2025-10-17T19:04:42.076481Z","shell.execute_reply":"2025-10-17T19:04:42.094072Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# ========= Example usage (OLD LOADER + attention heatmaps) =========\nif __name__ == \"__main__\":\n    DATA_FILE = globals().get(\"args\", None).data if \"args\" in globals() and hasattr(args, \"data\") else \"/kaggle/input/sasrec-initial-data/data/data/ml-1m.txt\"\n\n    user_train, user_valid, user_test, usernum, itemnum = data_partition(DATA_FILE)\n\n    # Convert to new examples format respecting OLD splits\n    MAX_LEN = getattr(globals().get(\"args\", object()), \"maxlen\", 50)\n    train_examples, val_examples, test_examples = build_examples_from_partition(\n        user_train, user_valid, user_test, max_len=MAX_LEN, min_prefix_len=2\n    )\n\n    print(f\"[data] users={usernum} items={itemnum} | \"\n          f\"train={len(train_examples)} val={len(val_examples)} test={len(test_examples)}\")\n\n    # Train + automatically save attention heatmaps\n    model, history, (test_hr, test_ndcg) = fit_sasrec_v25(\n        num_items=int(itemnum),\n        train_examples=train_examples,\n        val_examples=val_examples,\n        test_examples=test_examples,\n        d_model=50, n_heads=2, max_len=MAX_LEN, dropout=0.2, n_blocks=2, d_ff=200,\n        n_time_buckets=64, n_session_types=16,     # kept for compatibility (ignored if not passed in batch)\n        local_window=8, global_stride=8, use_sparse=False,\n        batch_size=128, lr=1e-3, epochs=5, device=None,\n        label_smoothing=0.1, early_stop_patience=2, K_eval=10,\n        # Heatmap saving (val each epoch + best test)\n        save_heatmaps=True, heatmap_output_dir=\"heatmaps_oldsplit\",\n        heatmap_head=0, heatmap_block=0, heatmap_samples=3\n    )\n\n    # 5) Also display one attention heatmap inline for a real test sample\n    items, tb, sess, tgt = build_batch([test_examples[0]], max_len=MAX_LEN, n_time_buckets=64, session_vocab=16)\n    A = extract_attention_heatmap(model, items[0], tb[0], sess[0], head=0, block=0)\n    plot_attention_heatmap(A, title=\"Real test sample (old split Â· block 0, head 0)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:04:42.095283Z","iopub.execute_input":"2025-10-17T19:04:42.095504Z","iopub.status.idle":"2025-10-17T19:10:55.798128Z","shell.execute_reply.started":"2025-10-17T19:04:42.095489Z","shell.execute_reply":"2025-10-17T19:10:55.797270Z"}},"outputs":[{"name":"stdout","text":"[data] users=6040 items=3416 | train=975451 val=6040 test=6040\nstep 100: loss=1.7329\nstep 200: loss=1.7453\nstep 300: loss=1.5945\nstep 400: loss=1.5224\nstep 500: loss=1.3693\nstep 600: loss=1.2764\nstep 700: loss=1.3389\nstep 800: loss=1.2278\nstep 900: loss=1.4060\nstep 1000: loss=1.4151\nstep 1100: loss=1.1206\nstep 1200: loss=1.3859\nstep 1300: loss=1.3743\nstep 1400: loss=1.2395\nstep 1500: loss=1.3838\nstep 1600: loss=1.4772\nstep 1700: loss=1.3055\nstep 1800: loss=1.2374\nstep 1900: loss=1.3548\nstep 2000: loss=1.2281\nstep 2100: loss=1.2852\nstep 2200: loss=1.3182\nstep 2300: loss=1.3061\nstep 2400: loss=1.3875\nstep 2500: loss=1.6459\nstep 2600: loss=1.5887\nstep 2700: loss=1.8113\nstep 2800: loss=1.5263\nstep 2900: loss=1.6435\nstep 3000: loss=1.4597\nstep 3100: loss=1.4506\nstep 3200: loss=1.2631\nstep 3300: loss=1.4718\nstep 3400: loss=1.5637\nstep 3500: loss=1.3414\nstep 3600: loss=1.3147\nstep 3700: loss=1.6183\nstep 3800: loss=1.4077\nstep 3900: loss=1.5300\nstep 4000: loss=1.6331\nstep 4100: loss=1.6708\nstep 4200: loss=1.4219\nstep 4300: loss=1.4568\nstep 4400: loss=1.5108\nstep 4500: loss=1.5471\nstep 4600: loss=1.7431\nstep 4700: loss=1.5629\nstep 4800: loss=1.6201\nstep 4900: loss=1.5901\nstep 5000: loss=1.6262\nstep 5100: loss=1.5597\nstep 5200: loss=1.6023\nstep 5300: loss=1.5055\nstep 5400: loss=1.5772\nstep 5500: loss=1.7553\nstep 5600: loss=1.5456\nstep 5700: loss=1.5761\nstep 5800: loss=1.7398\nstep 5900: loss=1.6688\nstep 6000: loss=1.5784\nstep 6100: loss=1.6465\nstep 6200: loss=1.5474\nstep 6300: loss=1.6392\nstep 6400: loss=1.6344\nstep 6500: loss=1.5709\nstep 6600: loss=1.7046\nstep 6700: loss=1.7373\nstep 6800: loss=1.6770\nstep 6900: loss=1.6104\nstep 7000: loss=1.6604\nstep 7100: loss=1.5780\nstep 7200: loss=1.6289\nstep 7300: loss=1.6232\nstep 7400: loss=1.5018\nstep 7500: loss=1.5564\nstep 7600: loss=1.5289\nEpoch 1: loss=1.5111, val HR@10=0.8538, val NDCG@10=0.8413\nstep 100: loss=1.6227\nstep 200: loss=1.6527\nstep 300: loss=1.5838\nstep 400: loss=1.5244\nstep 500: loss=1.5022\nstep 600: loss=1.5578\nstep 700: loss=1.5054\nstep 800: loss=1.5758\nstep 900: loss=1.4386\nstep 1000: loss=1.6127\nstep 1100: loss=1.6793\nstep 1200: loss=1.6563\nstep 1300: loss=1.6428\nstep 1400: loss=1.6539\nstep 1500: loss=1.6052\nstep 1600: loss=1.6382\nstep 1700: loss=1.5402\nstep 1800: loss=1.5500\nstep 1900: loss=1.5942\nstep 2000: loss=1.5855\nstep 2100: loss=1.6064\nstep 2200: loss=1.5055\nstep 2300: loss=1.6144\nstep 2400: loss=1.6602\nstep 2500: loss=1.5956\nstep 2600: loss=1.5597\nstep 2700: loss=1.6371\nstep 2800: loss=1.6167\nstep 2900: loss=1.6688\nstep 3000: loss=1.5892\nstep 3100: loss=1.5392\nstep 3200: loss=1.5436\nstep 3300: loss=1.6770\nstep 3400: loss=1.5102\nstep 3500: loss=1.5747\nstep 3600: loss=1.5830\nstep 3700: loss=1.6226\nstep 3800: loss=1.6607\nstep 3900: loss=1.6415\nstep 4000: loss=1.5232\nstep 4100: loss=1.6728\nstep 4200: loss=1.6347\nstep 4300: loss=1.5713\nstep 4400: loss=1.5965\nstep 4500: loss=1.5960\nstep 4600: loss=1.6211\nstep 4700: loss=1.6644\nstep 4800: loss=1.4817\nstep 4900: loss=1.5770\nstep 5000: loss=1.6811\nstep 5100: loss=1.5269\nstep 5200: loss=1.6592\nstep 5300: loss=1.4843\nstep 5400: loss=1.6386\nstep 5500: loss=1.6780\nstep 5600: loss=1.4968\nstep 5700: loss=1.5839\nstep 5800: loss=1.6759\nstep 5900: loss=1.4899\nstep 6000: loss=1.4908\nstep 6100: loss=1.5115\nstep 6200: loss=1.5843\nstep 6300: loss=1.5849\nstep 6400: loss=1.5234\nstep 6500: loss=1.5744\nstep 6600: loss=1.7615\nstep 6700: loss=1.5704\nstep 6800: loss=1.5810\nstep 6900: loss=1.5762\nstep 7000: loss=1.4702\nstep 7100: loss=1.5805\nstep 7200: loss=1.6480\nstep 7300: loss=1.6045\nstep 7400: loss=1.6846\nstep 7500: loss=1.6370\nstep 7600: loss=1.6547\nEpoch 2: loss=1.5928, val HR@10=0.8507, val NDCG@10=0.8400\nstep 100: loss=1.5409\nstep 200: loss=1.5735\nstep 300: loss=1.5176\nstep 400: loss=1.5834\nstep 500: loss=1.6600\nstep 600: loss=1.5121\nstep 700: loss=1.5000\nstep 800: loss=1.5988\nstep 900: loss=1.5413\nstep 1000: loss=1.6339\nstep 1100: loss=1.6847\nstep 1200: loss=1.6501\nstep 1300: loss=1.6119\nstep 1400: loss=1.5788\nstep 1500: loss=1.5607\nstep 1600: loss=1.6580\nstep 1700: loss=1.6794\nstep 1800: loss=1.6212\nstep 1900: loss=1.5887\nstep 2000: loss=1.5730\nstep 2100: loss=1.6444\nstep 2200: loss=1.6682\nstep 2300: loss=1.6995\nstep 2400: loss=1.6476\nstep 2500: loss=1.5894\nstep 2600: loss=1.6568\nstep 2700: loss=1.5848\nstep 2800: loss=1.5670\nstep 2900: loss=1.6323\nstep 3000: loss=1.7332\nstep 3100: loss=1.6397\nstep 3200: loss=1.6793\nstep 3300: loss=1.4342\nstep 3400: loss=1.5686\nstep 3500: loss=1.5284\nstep 3600: loss=1.6360\nstep 3700: loss=1.5495\nstep 3800: loss=1.5768\nstep 3900: loss=1.5273\nstep 4000: loss=1.5703\nstep 4100: loss=1.7499\nstep 4200: loss=1.5287\nstep 4300: loss=1.5236\nstep 4400: loss=1.5459\nstep 4500: loss=1.4998\nstep 4600: loss=1.4916\nstep 4700: loss=1.5614\nstep 4800: loss=1.6356\nstep 4900: loss=1.6702\nstep 5000: loss=1.5351\nstep 5100: loss=1.6539\nstep 5200: loss=1.6457\nstep 5300: loss=1.6615\nstep 5400: loss=1.7001\nstep 5500: loss=1.6435\nstep 5600: loss=1.5481\nstep 5700: loss=1.5274\nstep 5800: loss=1.6188\nstep 5900: loss=1.6187\nstep 6000: loss=1.5332\nstep 6100: loss=1.6393\nstep 6200: loss=1.6201\nstep 6300: loss=1.6535\nstep 6400: loss=1.7003\nstep 6500: loss=1.6689\nstep 6600: loss=1.5401\nstep 6700: loss=1.6697\nstep 6800: loss=1.6691\nstep 6900: loss=1.5616\nstep 7000: loss=1.6060\nstep 7100: loss=1.6168\nstep 7200: loss=1.5764\nstep 7300: loss=1.6415\nstep 7400: loss=1.6045\nstep 7500: loss=1.5418\nstep 7600: loss=1.7185\nEpoch 3: loss=1.5807, val HR@10=0.8598, val NDCG@10=0.8437\nstep 100: loss=1.5526\nstep 200: loss=1.7152\nstep 300: loss=1.5591\nstep 400: loss=1.6189\nstep 500: loss=1.6062\nstep 600: loss=1.6431\nstep 700: loss=1.6569\nstep 800: loss=1.6731\nstep 900: loss=1.5580\nstep 1000: loss=1.6402\nstep 1100: loss=1.6257\nstep 1200: loss=1.5023\nstep 1300: loss=1.6590\nstep 1400: loss=1.6494\nstep 1500: loss=1.6495\nstep 1600: loss=1.6766\nstep 1700: loss=1.5456\nstep 1800: loss=1.5895\nstep 1900: loss=1.5712\nstep 2000: loss=1.5800\nstep 2100: loss=1.5947\nstep 2200: loss=1.6171\nstep 2300: loss=1.6099\nstep 2400: loss=1.6969\nstep 2500: loss=1.4695\nstep 2600: loss=1.4426\nstep 2700: loss=1.6067\nstep 2800: loss=1.5717\nstep 2900: loss=1.5240\nstep 3000: loss=1.6474\nstep 3100: loss=1.7179\nstep 3200: loss=1.5790\nstep 3300: loss=1.4828\nstep 3400: loss=1.7173\nstep 3500: loss=1.6554\nstep 3600: loss=1.5163\nstep 3700: loss=1.5448\nstep 3800: loss=1.6083\nstep 3900: loss=1.5298\nstep 4000: loss=1.5232\nstep 4100: loss=1.6138\nstep 4200: loss=1.4720\nstep 4300: loss=1.5073\nstep 4400: loss=1.5378\nstep 4500: loss=1.6098\nstep 4600: loss=1.5714\nstep 4700: loss=1.4907\nstep 4800: loss=1.5083\nstep 4900: loss=1.5079\nstep 5000: loss=1.6885\nstep 5100: loss=1.5534\nstep 5200: loss=1.6157\nstep 5300: loss=1.6668\nstep 5400: loss=1.5395\nstep 5500: loss=1.4748\nstep 5600: loss=1.6278\nstep 5700: loss=1.5298\nstep 5800: loss=1.6464\nstep 5900: loss=1.5483\nstep 6000: loss=1.6806\nstep 6100: loss=1.6304\nstep 6200: loss=1.5601\nstep 6300: loss=1.5804\nstep 6400: loss=1.6869\nstep 6500: loss=1.5488\nstep 6600: loss=1.5283\nstep 6700: loss=1.6401\nstep 6800: loss=1.6337\nstep 6900: loss=1.6988\nstep 7000: loss=1.5499\nstep 7100: loss=1.5906\nstep 7200: loss=1.5754\nstep 7300: loss=1.5690\nstep 7400: loss=1.5470\nstep 7500: loss=1.5748\nstep 7600: loss=1.4934\nEpoch 4: loss=1.5833, val HR@10=0.8520, val NDCG@10=0.8406\nstep 100: loss=1.5434\nstep 200: loss=1.5279\nstep 300: loss=1.7140\nstep 400: loss=1.6695\nstep 500: loss=1.6236\nstep 600: loss=1.5424\nstep 700: loss=1.5064\nstep 800: loss=1.6267\nstep 900: loss=1.4361\nstep 1000: loss=1.5347\nstep 1100: loss=1.5126\nstep 1200: loss=1.5807\nstep 1300: loss=1.5422\nstep 1400: loss=1.6641\nstep 1500: loss=1.5283\nstep 1600: loss=1.6060\nstep 1700: loss=1.5457\nstep 1800: loss=1.7455\nstep 1900: loss=1.5818\nstep 2000: loss=1.5338\nstep 2100: loss=1.5179\nstep 2200: loss=1.5726\nstep 2300: loss=1.6197\nstep 2400: loss=1.6367\nstep 2500: loss=1.5002\nstep 2600: loss=1.5894\nstep 2700: loss=1.3682\nstep 2800: loss=1.4447\nstep 2900: loss=1.6891\nstep 3000: loss=1.6369\nstep 3100: loss=1.6494\nstep 3200: loss=1.4355\nstep 3300: loss=1.4981\nstep 3400: loss=1.5963\nstep 3500: loss=1.5661\nstep 3600: loss=1.5867\nstep 3700: loss=1.4832\nstep 3800: loss=1.5217\nstep 3900: loss=1.5411\nstep 4000: loss=1.5654\nstep 4100: loss=1.5975\nstep 4200: loss=1.5673\nstep 4300: loss=1.5427\nstep 4400: loss=1.5201\nstep 4500: loss=1.6992\nstep 4600: loss=1.6212\nstep 4700: loss=1.6040\nstep 4800: loss=1.4764\nstep 4900: loss=1.6192\nstep 5000: loss=1.6237\nstep 5100: loss=1.4809\nstep 5200: loss=1.5550\nstep 5300: loss=1.5756\nstep 5400: loss=1.5687\nstep 5500: loss=1.5844\nstep 5600: loss=1.6898\nstep 5700: loss=1.6132\nstep 5800: loss=1.4941\nstep 5900: loss=1.6123\nstep 6000: loss=1.7454\nstep 6100: loss=1.4978\nstep 6200: loss=1.5707\nstep 6300: loss=1.6288\nstep 6400: loss=1.6120\nstep 6500: loss=1.6372\nstep 6600: loss=1.5262\nstep 6700: loss=1.6139\nstep 6800: loss=1.6157\nstep 6900: loss=1.5056\nstep 7000: loss=1.3854\nstep 7100: loss=1.6998\nstep 7200: loss=1.5825\nstep 7300: loss=1.5321\nstep 7400: loss=1.5919\nstep 7500: loss=1.6355\nstep 7600: loss=1.6824\nEpoch 5: loss=1.5705, val HR@10=0.8588, val NDCG@10=0.8426\nEarly stopping.\nTEST HR@10=0.8537, NDCG@10=0.8409\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 500x400 with 2 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAdkAAAGGCAYAAAA3n/sgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABOuUlEQVR4nO3dd1gU1/oH8O/S62JBOgKKvRExKJpcG0rUEI0ajfEGgiW/KKhITDE3ihoVNdHYsMSCxsRru2ri1dhQNAUbhsQoEhMbFkCMFFGKu+f3B5eJK6DMuuvC8v08zzy6Z87MvAy7vHvOnDOjEEIIEBERkc6ZGDoAIiIiY8UkS0REpCdMskRERHrCJEtERKQnTLJERER6wiRLRESkJ0yyREREesIkS0REpCdMskRERHrCJCvD5cuXoVAosG7dOkOHUmusW7cOCoUCly9f1ul+t2zZgnr16uHu3buyt+3WrRu6dev2xHqJiYlQKBRITEyUH6CWKnqPTps2DQqF4pkcv1u3bmjduvUT6+n7s1T2vjl16pRe9i9X2Xth27Zthg5FouvP1u3bt2Fra4s9e/boZH/GosYl2bI3RtliZmYGd3d3vPXWW7h+/bqhwwMAnDt3DtOmTdN5YnjUsmXLmPC1oFKpEBMTg3HjxsHOzs7Q4RjE7NmzsXPnTkOHUW2p1WrMmzcPPj4+sLKyQtu2bfHvf//b0GEZTE5ODt5++200aNAAtra26N69O06fPq1Rp379+hg1ahSmTJlioCirpxqXZMvMmDEDGzZswIoVK9CnTx989dVX6Nq1KwoLCw0dGs6dO4fp06czyVZTu3btQlpaGt5++21Dh/JMfPzxx7h//75GGZPs4/3rX//CBx98gF69emHJkiVo2LAh3njjDWzatMnQoT1zarUa/fr1w8aNGxEZGYl58+YhKysL3bp1w4ULFzTqvvPOOzh9+jQOHTpkoGirHzNDB6CtPn36oEOHDgCAUaNGwdHREXPnzsW3336LIUOGGDg6qs7i4+PRpUsXuLu7GzqUZ8LMzAxmZjX2o/7MXb9+HfPnz0dERASWLl0KoPRvTNeuXfHee+/htddeg6mpqYGjfHa2bduGn376CVu3bsXgwYMBAEOGDEHTpk0RExODjRs3SnVbtGiB1q1bY926dejRo4ehQq5WamxL9lEvvvgiAODPP//UKD9//jwGDx6MevXqwcrKCh06dMC3336rUeevv/7CpEmT0KZNG9jZ2UGpVKJPnz745ZdfZMexbt06vPbaawCA7t27S93aD1+X++677/Diiy/C1tYW9vb26NevH86ePauxn4yMDISHh8PDwwOWlpZwdXVF//79pdaxt7c3zp49iyNHjkjHeNJ1wk2bNsHf3x/29vZQKpVo06YNFi1aJPs8lF1f2rJlC6ZPnw53d3fY29tj8ODByM3NRVFREaKiouDk5AQ7OzuEh4ejqKhIYx8KhQKRkZH4+uuv0axZM1hZWcHf3x9Hjx6t0nmuyjmsSGFhIfbu3YugoKBy6x48eIBPPvkEjRs3hqWlJby9vfHRRx+Vi70i165dw4ABA2BrawsnJydMnDixStsBQH5+PqKiouDt7Q1LS0s4OTmhV69eGt1xZdc6k5OT0blzZ1hbW8PHxwcrVqx44v4fvSarUChQUFCA9evXS++dt956q0qxVpU2cQLAoUOHpN9rnTp10L9/f6Smppard/36dYwcORJubm6wtLSEj48PxowZg+Li4kr3fefOHQQEBMDDwwNpaWmV1vvmm29QUlKCsWPHSmUKhQJjxozBtWvXkJSUVKWfpSJqtRqzZs2Ch4cHrKys0LNnT/zxxx/l6h0/fhwvvfQSHBwcYGNjg65du+LHH3/UqHPlyhWMHTsWzZo1g7W1NerXr4/XXnutwh60s2fPokePHrC2toaHhwdmzpwJtVpdpZi3bdsGZ2dnDBw4UCpr0KABhgwZgm+++abc+7xXr17YtWsX+IC3Ukbz9bbsjVW3bl2p7OzZs1KL5cMPP4StrS22bNmCAQMG4D//+Q9effVVAMDFixexc+dOvPbaa/Dx8UFmZiZWrlyJrl274ty5c3Bzc6tyHP/4xz8wfvx4LF68GB999BFatGgBANK/GzZsQFhYGIKDgzF37lzcu3cPy5cvxwsvvICff/4Z3t7eAIBBgwbh7NmzGDduHLy9vZGVlYUDBw7g6tWr8Pb2xsKFC6Vriv/6178AAM7OzpXGdeDAAQwbNgw9e/bE3LlzAQCpqan48ccfMWHCBK3OQ2xsLKytrfHhhx/ijz/+wJIlS2Bubg4TExPcuXMH06ZNw7Fjx7Bu3Tr4+Phg6tSpGtsfOXIEmzdvxvjx42FpaYlly5bhpZdewokTJx47eKaq57AiycnJKC4uRvv27cutGzVqFNavX4/Bgwfj3XffxfHjxxEbG4vU1FTs2LGj0n3ev38fPXv2xNWrVzF+/Hi4ublhw4YNVe4ye+edd7Bt2zZERkaiZcuWuH37Nn744QekpqZqxHnnzh307dsXQ4YMwbBhw7BlyxaMGTMGFhYWGDFiRJWOBZSev1GjRiEgIEDqMm/cuHGVt38SbeM8ePAg+vTpg0aNGmHatGm4f/8+lixZgi5duuD06dPS7/XGjRsICAiQrhM2b94c169fx7Zt23Dv3j1YWFiU23d2djZ69eqFv/76C0eOHHnsz/vzzz/D1tZW+syWCQgIkNa/8MILWpwZYM6cOTAxMcGkSZOQm5uLefPmYfjw4Th+/LhU59ChQ+jTpw/8/f0RExMDExMTxMfHo0ePHvj++++lOE6ePImffvoJr7/+Ojw8PHD58mUsX74c3bp1w7lz52BjYwOg9At79+7d8eDBA+nv4BdffAFra+sqxfzzzz+jffv2MDHRbJMFBATgiy++wO+//442bdpI5f7+/vj8889x9uzZKg2CM3qihomPjxcAxMGDB8WtW7dEenq62LZtm2jQoIGwtLQU6enpUt2ePXuKNm3aiMLCQqlMrVaLzp07iyZNmkhlhYWFQqVSaRzn0qVLwtLSUsyYMUOjDICIj49/bIxbt24VAMThw4c1yvPz80WdOnXE6NGjNcozMjKEg4ODVH7nzh0BQHz66aePPU6rVq1E165dH1unzIQJE4RSqRQPHjyotE5Vz8Phw4cFANG6dWtRXFwslQ8bNkwoFArRp08fjX0EBgYKLy8vjTIAAoA4deqUVHblyhVhZWUlXn31Vams7Pd96dIlIUTVz2FlVq9eLQCIM2fOaJSnpKQIAGLUqFEa5ZMmTRIAxKFDh6Syrl27apz3hQsXCgBiy5YtUllBQYHw9fWt8H3wKAcHBxEREfHYOl27dhUAxPz586WyoqIi4efnJ5ycnKTfQ0Xv0ZiYGPHoR93W1laEhYU99pgPS01NFVeuXHlivaeJs6zO7du3pbJffvlFmJiYiNDQUKksNDRUmJiYiJMnT5Y7vlqtFkL8/b45efKkuHnzpmjVqpVo1KiRuHz58hN/hn79+olGjRqVKy8oKBAAxIcffvjEfTyq7DPTokULUVRUJJUvWrRI4/2oVqtFkyZNRHBwsPSzCCHEvXv3hI+Pj+jVq5dG2aOSkpIEAPHll19KZVFRUQKAOH78uFSWlZUlHBwcND5blbG1tRUjRowoV757924BQOzdu1ej/KeffhIAxObNmx+739qixnYXBwUFoUGDBvD09MTgwYNha2uLb7/9Fh4eHgBKuz4PHTqEIUOGID8/H9nZ2cjOzsbt27cRHByMCxcuSKORLS0tpW9pKpUKt2/fhp2dHZo1a1ZuBN3TOHDgAHJycjBs2DApnuzsbJiamqJjx444fPgwAMDa2hoWFhZITEzEnTt3dHLsOnXqoKCgAAcOHKi0jtzzEBoaCnNzc+l1x44dIYQo11rp2LEj0tPT8eDBA43ywMBA+Pv7S68bNmyI/v37Y9++fVCpVBXGWNVzWJnbt28D0OzxACBNO4iOjtYof/fddwEAu3fvrnSfe/bsgaurq3S9CgBsbGyqPLCqTp06OH78OG7cuPHYemZmZvi///s/6bWFhQX+7//+D1lZWUhOTq7SsbTVokULhIaGVqmuNnHevHkTKSkpeOutt1CvXj2pvG3btujVq5f0+1Gr1di5cydCQkKkMRkPe3Sq0rVr19C1a1eUlJTg6NGj8PLyemL89+/fh6WlZblyKysrab22wsPDNVraZZe5Ll68CABISUnBhQsX8MYbb+D27dvS+7ugoAA9e/bE0aNHpW7eh1uiJSUluH37Nnx9fVGnTh2Nz+uePXvQqVMnqQUMlHb3Dh8+vEoxyz0fZZ+t7OzsKu3f2NXY7uK4uDg0bdoUubm5WLt2LY4eParxRvjjjz8ghMCUKVMqHVKelZUFd3d3qNVqLFq0CMuWLcOlS5c0/sDXr19fZzGXjcSrbECAUqkEUJrs5s6di3fffRfOzs7o1KkTXn75ZYSGhsLFxUWrY48dOxZbtmxBnz594O7ujt69e2PIkCF46aWXpDpyz0PDhg01Xjs4OAAAPD09y5Wr1Wrk5uZq7KdJkybl9tm0aVPcu3cPt27dqvBnreo5fBLxyPWiK1euwMTEBL6+vhrlLi4uqFOnDq5cuVLpvq5cuQJfX99yf+CbNWtWpVjmzZuHsLAweHp6wt/fH3379kVoaCgaNWqkUc/NzQ22trYaZU2bNgVQermkU6dOVTqevmkTZ9n5reictWjRAvv27UNBQQHu3r2LvLy8KndDvvnmmzAzM0NqamqVPzvW1tYVXk8vm7lQ1W7Wijz6mSlLSGVfpsve32FhYZXuIzc3F3Xr1sX9+/cRGxuL+Ph4XL9+XeM9nZubK/3/ypUr6NixY7n9VPX9Kfd8lMXxrOZmV3c1NskGBARI32QHDBiAF154AW+88QbS0tJgZ2cnfdubNGkSgoODK9xH2R/U2bNnY8qUKRgxYgQ++eQT1KtXDyYmJoiKiqry4ICqKNvXhg0bKvzAPzwCNCoqCiEhIdi5cyf27duHKVOmIDY2FocOHcJzzz0n+9hOTk5ISUnBvn378N133+G7775DfHw8QkNDsX79egDyz0NlIywrK380sWlDzjmsSFmSv3PnjtTr8TBD/GEYMmQIXnzxRezYsQP79+/Hp59+irlz52L79u3o06fPM4+nIrr43RnCwIED8eWXX2LRokWIjY2t0jaurq44fPgwhBAa74ebN28CgKwxGo960mej7P396aefws/Pr8K6ZXO7x40bh/j4eERFRSEwMBAODg5QKBR4/fXXdfp3y9XVVfrZH1bZ+Sj7wuDo6KizGGqyGptkH2ZqaorY2Fh0794dS5cuxYcffii1AszNzSscSfqwbdu2oXv37lizZo1GeU5OjlZvlMr+UJcNtnBycnpiTGX13333Xbz77ru4cOEC/Pz8MH/+fHz11VePPU5lLCwsEBISgpCQEKjVaowdOxYrV67ElClT4Ovrq/Pz8CSPzrEDgN9//x02NjZo0KBBhdvIPYePat68OQDg0qVLGoM1vLy8oFarceHCBY0BL5mZmcjJyXlsN6OXlxd+++23cn+UHzeC9VGurq4YO3Ysxo4di6ysLLRv3x6zZs3SSLI3btxAQUGBRivx999/B4DHDvaqiD6/TGgTZ9n5reicnT9/Ho6OjrC1tYW1tTWUSiV+++23KsUybtw4+Pr6YurUqXBwcMCHH374xG38/PywevVqpKamomXLllJ52eCkypKfLpS9v5VKZZX+boWFhWH+/PlSWWFhIXJycjTqeXl5VfhZq+r708/PD99//z3UarXG4Kfjx4/DxsZG6qUoc+nSJQAoN3Cstqqx12Qf1a1bNwQEBGDhwoUoLCyEk5MTunXrhpUrV1b4LezWrVvS/01NTct9U9+6davWd5Aq++Py6Js9ODgYSqUSs2fPRklJSaUx3bt3r9xNNRo3bgx7e3uNbhtbW9tyx6hM2bXIMiYmJmjbti0ASPvU9Xl4kqSkJI1rR+np6fjmm2/Qu3fvSr/xV/UcVsbf3x8WFhblbrfXt29fAMDChQs1yhcsWAAA6NevX6X77Nu3L27cuKFxy7x79+7hiy++eGwsQOm174e79oDSLxBubm7luugePHiAlStXSq+Li4uxcuVKNGjQQOPadlXIee/IpU2crq6u8PPzw/r16zXi+u2337B//37p92NiYoIBAwZg165dFd4ysaIW95QpUzBp0iRMnjwZy5cvf2L8/fv3h7m5OZYtW6ax3xUrVsDd3R2dO3d+4j605e/vj8aNG+Ozzz6r8JafT/q7tWTJknLjGfr27Ytjx47hxIkTGvv5+uuvqxTT4MGDkZmZie3bt0tl2dnZ2Lp1K0JCQspdr01OToaDgwNatWpVpf0bO6NoyZYpmyi+bt06vPPOO4iLi8MLL7yANm3aYPTo0WjUqBEyMzORlJSEa9euSfM/X375ZcyYMQPh4eHo3Lkzzpw5g6+//rrcNbGq8vPzg6mpKebOnYvc3FxYWlqiR48ecHJywvLly/Hmm2+iffv2eP3119GgQQNcvXoVu3fvRpcuXbB06VL8/vvv6NmzJ4YMGYKWLVvCzMwMO3bsQGZmJl5//XXpOP7+/li+fDlmzpwJX19fODk5VXqtctSoUfjrr7/Qo0cPeHh44MqVK1iyZAn8/Pykb5y6Pg9P0rp1awQHB2tM4QGA6dOnV7qNUqms0jmsjJWVFXr37o2DBw9ixowZUnm7du0QFhaGL774Ajk5OejatStOnDiB9evXY8CAAejevXul+xw9ejSWLl2K0NBQJCcnw9XVFRs2bJCmUDxOfn4+PDw8MHjwYLRr1w52dnY4ePAgTp48qdFCAUq75ebOnYvLly+jadOm2Lx5M1JSUvDFF19oDECrCn9/fxw8eBALFiyAm5sbfHx8Krxupw1t4/z000/Rp08fBAYGYuTIkdIUHgcHB0ybNk2qN3v2bOzfvx9du3bF22+/jRYtWuDmzZvYunUrfvjhB9SpU6fCfefm5iIiIgL29vb45z//WWkcHh4eiIqKwqeffoqSkhI8//zz2LlzJ77//nt8/fXXGl8A161bh/DwcMTHx+tkrrGJiQlWr16NPn36oFWrVggPD4e7uzuuX7+Ow4cPQ6lUYteuXQBKP68bNmyAg4MDWrZsiaSkJBw8eLDc+In3338fGzZswEsvvYQJEyZIU3i8vLzw66+/PjGmwYMHo1OnTggPD8e5c+fg6OiIZcuWQaVSVfhZPXDgAEJCQnhNtowBRjQ/lYeH5j9KpVKJxo0bi8aNG0tTVf78808RGhoqXFxchLm5uXB3dxcvv/yy2LZtm7RdYWGhePfdd4Wrq6uwtrYWXbp0EUlJSeWmalR1Co8QQqxatUo0atRImJqalpvGcfjwYREcHCwcHByElZWVaNy4sXjrrbek6SzZ2dkiIiJCNG/eXNja2goHBwfRsWNHjSkiQpROW+nXr5+wt7cXAB47nWfbtm2id+/ewsnJSVhYWIiGDRuK//u//xM3b96UfR7KpiNs3bpV4xiV/W7KppDcunVLKgMgIiIixFdffSWaNGkiLC0txXPPPVduusujU3iqeg4fZ/v27UKhUIirV69qlJeUlIjp06cLHx8fYW5uLjw9PcXkyZM1poAJUX4KjxCl049eeeUVYWNjIxwdHcWECRPE3r17nziFp6ioSLz33nuiXbt2wt7eXtja2op27dqJZcuWlTtmq1atxKlTp0RgYKCwsrISXl5eYunSpRr1qjqF5/z58+If//iHsLa2FgBkTed5nKeJUwghDh48KLp06SKsra2FUqkUISEh4ty5c+WOc+XKFREaGipN3WvUqJGIiIiQpsdU9F5UqVRi2LBhwszMTOzcufOxP4dKpRKzZ88WXl5ewsLCQrRq1Up89dVX5eotWbKkwmksj6rsM1PZefj555/FwIEDRf369YWlpaXw8vISQ4YMEQkJCVKdO3fuiPDwcOHo6Cjs7OxEcHCwOH/+vPDy8ir3+/z1119F165dhZWVlXB3dxeffPKJWLNmTZWm8AghxF9//SVGjhwp6tevL2xsbETXrl0r/BucmpoqTbGkUgohauiIBqrRFAqFxm3rniWVSoWWLVtiyJAh+OSTT5758bXRrVs3ZGdnV/laJD0bQ4YMweXLlzW6YmuzqKgoHD16FMnJyWzJ/o/RXJMlqipTU1PMmDEDcXFxWj3qjggovU6bmJiImTNnGjqUauH27dtYvXo1Zs6cyQT7EKO6JktUVUOHDsXQoUMNHQbVYAqFAllZWYYOo9qoX78+v7RWgC1ZIiIiPWGSJYMQQhjkemxNlZiYyOuxRE/h6NGjCAkJgZubGxQKRZWep5yYmIj27dvD0tISvr6+Wj2/m0mWiIiMXkFBAdq1a4e4uLgq1b906RL69euH7t27IyUlBVFRURg1ahT27dsn67gcXUxERLWKQqHAjh07MGDAgErrfPDBB9i9e7dGD9Lrr7+OnJwc7N27t8rHqhEDn+Li4vDpp58iIyMD7dq1w5IlSzSeKPE4arUaN27cgL29PUe8ERFpQQiB/Px8uLm5lXuu7NMqLCxEcXGx1nE9+nfd0tKywqcGyZWUlFTu1pbBwcGIioqStZ9qn2Q3b96M6OhorFixAh07dsTChQsRHByMtLQ0ODk5PXH7GzdulHsqDBERyZeenl7hgzW0VVhYCB8vO2RkVfxoyyexs7MrN6I5JiZG4w5h2srIyICzs7NGmbOzM/Ly8nD//v0qP42p2ifZBQsWYPTo0QgPDwcArFixArt378batWurdLNve3t7AMCV095Q2pX/BvZq0zblyoiI6G8PUIIfsEf6e6orxcXFyMhS4UqyN5T28lrIeflqePlfRnp6usYjLnXRitWlap1ki4uLkZycjMmTJ0tlJiYmCAoKQlJSUoXbFBUVadxYPT8/HwCgtDOp8JdoppB3z1ciolrnfyN39HXJzc5eATt7eftWo7S+Uqms8nOk5XBxcUFmZqZGWWZmJpRKpaxnClfr0cXZ2dlQqVQVNtkzMjIq3CY2NhYODg7Swq5iIqLqTSXUWi36FBgYiISEBI2yAwcOIDAwUNZ+qnWS1cbkyZORm5srLenp6YYOiYiIHkMNodUix927d5GSkoKUlBQApVN0UlJScPXqVQCluSM0NFSq/8477+DixYt4//33cf78eSxbtgxbtmzBxIkTZR23WncXOzo6wtTUtMImu4uLS4Xb6GpkGRERPRtqqCG3XSp3i1OnTmk8sjI6OhoAEBYWhnXr1uHmzZtSwgUAHx8f7N69GxMnTsSiRYvg4eGB1atXIzg4WNZxq3WStbCwgL+/PxISEqT5TGq1GgkJCYiMjDRscEREpBMqIaCSecsGufW7detW7iH3D6vobk7dunXDzz//LOs4j6rWSRYo/bYRFhaGDh06ICAgAAsXLkRBQYE02piIiGo2bbp/5dY3lGqfZIcOHYpbt25h6tSpyMjIgJ+fH/bu3VtuMBQREVF1U+2TLABERkaye5iIyEipIaBiS5aIiEj32F1MRESkJ89i4JOhMMkSEZFBqf+3yN2mJmCSJSIig1JpcU1Wbn1DMbo7PhEREVUXbMkSEZFBqUTpInebmoBJloiIDIrXZImIiPREDQVU0O5Rd9VdrUmyrzZto7Nnx+67kVLpumA3P50cg4iotlCL0kXuNjVBrUmyRERUPam0aMnKrW8oHF1MRESkJ2zJEhGRQRlzS5ZJloiIDEotFFALmQOfZNY3FCZZIiIyKLZkiYiI9EQFE6hkDhFS6SkWXWOSJSIigxJadBcLdhcTERE9mTF3F3MKDxERkZ6wJUtERAalEiZQCZnXZHnHJyIioidTQwG1zI5VdQ15niyTLBERGZQxX5NlkiUiIoPSrruYLVkiIqInKu0uNs5H3XF0MRERkZ6wJUtERAal1uKOTxz4REREVAW8JktERKQnaphwCg8REZE+qIQCKpn3IpZb31CYZImIyKC0ewpPzWjJcnQxERGRnrAlS0REBqUWJlDLHPik5sAnIiKiJzPm7mImWSIiMig15A9kUusnFJ1jkiUiIoPSbgpPzRhSxCRLREQGpd3NKGpGkq0ZURIREdVAbMkSEZFBGfNTeJhkiYjIoIy5u5hJloiIDEq7KTxMskRERE+kFgqo5U7h4b2LiYiInky758myJUtERPRE2t1WsWYk2ZoRJRERUQ3EliwRERmUCgqoZE7JkVvfUJhkiYjIoIy5u5hJloiIDEoF+S1TlX5C0TkmWSIiMihjbsnWjCiJiMhold3xSe4iV1xcHLy9vWFlZYWOHTvixIkTj62/cOFCNGvWDNbW1vD09MTEiRNRWFgo65hMskREZPQ2b96M6OhoxMTE4PTp02jXrh2Cg4ORlZVVYf2NGzfiww8/RExMDFJTU7FmzRps3rwZH330kazjMskSEZFBif89IEDOImRew12wYAFGjx6N8PBwtGzZEitWrICNjQ3Wrl1bYf2ffvoJXbp0wRtvvAFvb2/07t0bw4YNe2Lr91EGTbJHjx5FSEgI3NzcoFAosHPnTo31QghMnToVrq6usLa2RlBQEC5cuGCYYImISC+eprs4Ly9PYykqKiq3/+LiYiQnJyMoKEgqMzExQVBQEJKSkiqMqXPnzkhOTpaS6sWLF7Fnzx707dtX1s9m0IFPBQUFaNeuHUaMGIGBAweWWz9v3jwsXrwY69evh4+PD6ZMmYLg4GCcO3cOVlZWBoi4VLCbX6Xr9t1I0Wo7IqLa6mnuXezp6alRHhMTg2nTpmmUZWdnQ6VSwdnZWaPc2dkZ58+fr3D/b7zxBrKzs/HCCy9ACIEHDx7gnXfekd1dbNAk26dPH/Tp06fCdUIILFy4EB9//DH69+8PAPjyyy/h7OyMnTt34vXXX3+WoRIRkZ48zVN40tPToVQqpXJLS0udxJSYmIjZs2dj2bJl6NixI/744w9MmDABn3zyCaZMmVLl/VTbKTyXLl1CRkaGRvPewcEBHTt2RFJSUqVJtqioSKO7IC8vT++xEhGR9p6mJatUKjWSbEUcHR1hamqKzMxMjfLMzEy4uLhUuM2UKVPw5ptvYtSoUQCANm3aoKCgAG+//Tb+9a9/wcSkal8Kqu3Ap4yMDACosHlftq4isbGxcHBwkJZHuxKIiKh2sbCwgL+/PxISEqQytVqNhIQEBAYGVrjNvXv3yiVSU1NTAKU9rVVVbZOstiZPnozc3FxpSU9PN3RIRET0GGqYaLXIER0djVWrVmH9+vVITU3FmDFjUFBQgPDwcABAaGgoJk+eLNUPCQnB8uXLsWnTJly6dAkHDhzAlClTEBISIiXbqqi23cVlTfjMzEy4urpK5ZmZmfDz86t0O0tLS531yRMRkf6phAIqmd3FcusPHToUt27dwtSpU5GRkQE/Pz/s3btX6i29evWqRsv1448/hkKhwMcff4zr16+jQYMGCAkJwaxZs2Qdt9omWR8fH7i4uCAhIUFKqnl5eTh+/DjGjBlj2OCIiEhnnuaarByRkZGIjIyscF1iYqLGazMzM8TExCAmJkb2cTT281RbP6W7d+/ijz/+kF5funQJKSkpqFevHho2bIioqCjMnDkTTZo0kabwuLm5YcCAAYYLmoiIdEpoce9iUUPuXWzQJHvq1Cl0795deh0dHQ0ACAsLw7p16/D+++9Lo7lycnLwwgsvYO/evQadI0tERLrF58nqSbdu3R47SkuhUGDGjBmYMWPGM4yKiIieJbWQ3/2rrvoAX4OqGe1tIiKiGqjaDnwiIqLawZifJ8skS0REBlX2ZB2529QETLJERGRQz2KerKEwyRIRkUGxu5iIiEhP1NDiZhQ1pLu4ZnwVICIiqoHYkiUiIoMSWgx8EjWkJcskq2PBbn6GDuGp7LuRUmF5Tf+5iKj6elb3LjYEJlkiIjIoDnwiIiLSE7ZkiYiI9MSYb0ZRM9rbRERENRBbskREZFDsLiYiItITJlkiIiI9YZIlIiLSEyZZIiIiPRGQP1pY6CcUnePoYiIiIj1hS5aIiAyK3cVERER6wiRLRESkJ0yyREREesIkS0REpCdCKCBkJk259Q2FSZaIiAyKDwggIiIi2diSJSIig+I1WSIiIj3hNVkiIiI9MeaWrOxrsuvXr8fu3bul1++//z7q1KmDzp0748qVKzoNjoiIjF9ZS1buUhPITrKzZ8+GtbU1ACApKQlxcXGYN28eHB0dMXHiRJ0HSERExk38ryUrZ6kpSVZ2d3F6ejp8fX0BADt37sSgQYPw9ttvo0uXLujWrZuu4yMiIqqxZLdk7ezscPv2bQDA/v370atXLwCAlZUV7t+/r9voiIjI6AkAQshcDB10Fcluyfbq1QujRo3Cc889h99//x19+/YFAJw9exbe3t66jq/G2XcjpdJ1wW5+zywObdWEGInIuKihgII3oygVFxeHwMBA3Lp1C//5z39Qv359AEBycjKGDRum8wCJiMi4GfPAJ9kt2Tp16mDp0qXlyqdPn66TgIiIqHZRCwUURjqFR6t5sjk5OThx4gSysrKgVqulcoVCgTfffFNnwRERkfEru84qd5uaQHaS3bVrF4YPH467d+9CqVRCofj72wSTLBER0d9kX5N99913MWLECNy9exc5OTm4c+eOtPz111/6iJGIiIwYr8k+5Pr16xg/fjxsbGz0EU+Nx9G5+lXTR28TUXnGfO9i2S3Z4OBgnDp1Sh+xEBFRLST3bk/a3OvYUGS3ZPv164f33nsP586dQ5s2bWBubq6x/pVXXtFZcEREZPw48Okho0ePBgDMmDGj3DqFQgGVSvX0URERUa1RmmTldhfrKRgdk51kH56yQ0RE9LR4TZaIiIhk0yrJHjlyBCEhIfD19YWvry9eeeUVfP/997qOjYiIagGh5VITyE6yX331FYKCgmBjY4Px48dj/PjxsLa2Rs+ePbFx40Z9xEhEREbMmOfJyk6ys2bNwrx587B582YpyW7evBlz5szBJ598oo8YiYjImD2jpmxcXBy8vb1hZWWFjh074sSJE4+tn5OTg4iICLi6usLS0hJNmzbFnj17ZB1TdpK9ePEiQkJCypW/8soruHTpktzdERFRbadNK1ZmS3bz5s2Ijo5GTEwMTp8+jXbt2iE4OBhZWVkV1i8uLkavXr1w+fJlbNu2DWlpaVi1ahXc3d1lHVf26GJPT08kJCTA19dXo/zgwYPw9PSUuzsiIqrlnsU82QULFmD06NEIDw8HAKxYsQK7d+/G2rVr8eGHH5arv3btWvz111/46aefpPtBaPPMdK3uXTx+/HiMGTMGGzZswIYNG/DOO+8gKioKkyZNkrWv2NhYPP/887C3t4eTkxMGDBiAtLQ0jTqFhYWIiIhA/fr1YWdnh0GDBiEzM1Nu2EREZITy8vI0lqKionJ1iouLkZycjKCgIKnMxMQEQUFBSEpKqnC/3377LQIDAxEREQFnZ2e0bt0as2fPln0vCNkt2TFjxsDFxQXz58/Hli1bAAAtWrTA5s2b0b9/f1n7OnLkCCIiIvD888/jwYMH+Oijj9C7d2+cO3cOtra2AICJEydi9+7d2Lp1KxwcHBAZGYmBAwfixx9/lBv6M8F76+oXzyGR8XmaebKP9qDGxMRg2rRpGmXZ2dlQqVRwdnbWKHd2dsb58+cr3P/Fixdx6NAhDB8+HHv27MEff/yBsWPHoqSkBDExMVWOU6vnyb766qt49dVXtdlUw969ezVer1u3Dk5OTkhOTsY//vEP5ObmYs2aNdi4cSN69OgBAIiPj0eLFi1w7NgxdOrU6aljICIiA9PiGmtZ/fT0dCiVSqnY0tJSJyGp1Wo4OTnhiy++gKmpKfz9/XH9+nV8+umn+k+y+pKbmwsAqFevHgAgOTkZJSUlGk385s2bo2HDhkhKSqowyRYVFWl0F+Tl5ek5aiIiehpPc01WqVRqJNmKODo6wtTUtNylxszMTLi4uFS4jaurK8zNzWFqaiqVtWjRAhkZGSguLoaFhUWV4qzSNdl69eohOzsbAFC3bl3Uq1ev0kVbarUaUVFR6NKlC1q3bg0AyMjIgIWFBerUqaNR19nZGRkZGRXuJzY2Fg4ODtLCwVhERNWcnqfwWFhYwN/fHwkJCVKZWq1GQkICAgMDK9ymS5cu+OOPPzRuJfz777/D1dW1ygkWqGJL9vPPP4e9vb30f4VC95OAIyIi8Ntvv+GHH354qv1MnjwZ0dHR0uu8vDwmWiKiauxZ3Ls4OjoaYWFh6NChAwICArBw4UIUFBRIo41DQ0Ph7u6O2NhYAKXjj5YuXYoJEyZg3LhxuHDhAmbPno3x48fLOm6VkmxYWJj0/7feekvWAaoiMjIS//3vf3H06FF4eHhI5S4uLiguLkZOTo5Ga/ZxTXxLS0ud9ckTEZFxGDp0KG7duoWpU6ciIyMDfn5+2Lt3rzQY6urVqzAx+btz19PTE/v27cPEiRPRtm1buLu7Y8KECfjggw9kHVf2NVlTU1PcvHkTTk5OGuW3b9+Gk5OTrOHNQgiMGzcOO3bsQGJiInx8fDTW+/v7w9zcHAkJCRg0aBAAIC0tDVevXq20iU9ERDXQM7gZcWRkJCIjIytcl5iYWK4sMDAQx44de6pjyk6yopKr00VFRbL6qYHSLuKNGzfim2++gb29vXSd1cHBAdbW1nBwcMDIkSMRHR2NevXqQalUYty4cQgMDOTIYiIiI2HMj7qrcpJdvHgxgNIHs69evRp2dnbSOpVKhaNHj6J58+ayDr58+XIAQLdu3TTK4+PjpW7pzz//HCYmJhg0aBCKiooQHByMZcuWyToOERFVY9rci7iGPIanykn2888/B1Dakl2xYoXGsGYLCwt4e3tjxYoVsg5eWav4YVZWVoiLi0NcXJysfRMRUU2h+N8id5vqr8pJtuzm/927d8f27dtRt25dvQVFRES1CFuyfzt8+LA+4iAiIjI6VUqy0dHR+OSTT2Bra6sxB7UiCxYs0ElgRERUS9T2luzPP/+MkpIS6f+V0cdNKoiIyMg9xb2Lq7sqJdmHu4jZXUxERLr0LJ4nayiynyf7qLy8POzcubPSxwURERE9lp7vXWxIspPskCFDsHTpUgDA/fv30aFDBwwZMgRt2rTBf/7zH50HSERERq6su1juUgPITrJHjx7Fiy++CADYsWMHhBDIycnB4sWLMXPmTJ0HSERExk0htFtqAtlJNjc3V3qk3d69ezFo0CDY2NigX79+uHDhgs4DJCIiqqlkJ1lPT08kJSWhoKAAe/fuRe/evQEAd+7cgZWVlc4DJCIiI2fE12Rl34wiKioKw4cPh52dHby8vKT7Dh89ehRt2rTRdXxERGTsavsUnoeNHTsWAQEBSE9PR69evaTn7zVq1IjXZImISL7afjOKR3Xo0AEdOnSAEAJCCCgUCvTr10/XsRERUW1gxElWq3myX375Jdq0aQNra2tYW1ujbdu22LBhg65jIyKi2oDXZP+2YMECTJkyBZGRkejSpQsA4IcffsA777yD7OxsTJw4UedB1iTBbn6GDsGo7buRUuk6nnsiqm5kJ9klS5Zg+fLlCA0NlcpeeeUVtGrVCtOmTav1SZaIiGTiwKe/3bx5E507dy5X3rlzZ9y8eVMnQRERUe2hzc0ljPZmFL6+vtiyZUu58s2bN6NJkyY6CYqIiGoRXpP92/Tp0zF06FAcPXpUuib7448/IiEhocLkS0REVFvJTrKDBg3C8ePH8fnnn2Pnzp0AgBYtWuDEiRN47rnndB0fEREZOQW06C7WSyS6p9U8WX9/f3z11Ve6joWIiMioaJVkVSoVduzYgdTUVABAy5Yt0b9/f5iZabU7IiKqzTi6+G9nz57FK6+8goyMDDRr1gwAMHfuXDRo0AC7du1C69atdR4kEREZMd7x6W+jRo1Cq1atcO3aNZw+fRqnT59Geno62rZti7ffflsfMRIRkTHj6OK/paSk4NSpU6hbt65UVrduXcyaNQvPP/+8ToMjIiLjx3myD2natCkyMzPLlWdlZcHX11cnQRERUS1ixC1Z2Uk2NjYW48ePx7Zt23Dt2jVcu3YN27ZtQ1RUFObOnYu8vDxpISIieiIjTrKyu4tffvllAMCQIUOgUJSO7hKi9KcNCQmRXisUCqhUKl3FSUREVOPITrKHDx/WRxxERFRLGfM1WdlJtmvXrvqIg4iIaivOkyUiItITI54nyyRLREQGxe5iIiIifTHilqzsKTxERERUNbKTbExMDK5cuaKPWIiIqDYSf3cZV3Ux2pbsN998g8aNG6Nnz57YuHEjioqK9BEXERHVFkZ8MwrZSTYlJQUnT55Eq1atMGHCBLi4uGDMmDE4efKkPuIjIiJjxySr6bnnnsPixYtx48YNrFmzBteuXUOXLl3Qtm1bLFq0CLm5ubqOk4iIjJTcrmJtRiMbylMNfBJCoKSkBMXFxRBCoG7duli6dCk8PT2xefNmXcVIRERUI2mVZJOTkxEZGQlXV1dMnDgRzz33HFJTU3HkyBFcuHABs2bNwvjx43UdKxERUY0iO8m2adMGnTp1wqVLl7BmzRqkp6djzpw5Go+5GzZsGG7duqXTQImIyEgZ8TVZ2TejGDJkCEaMGAF3d/dK6zg6OkKtVj9VYEREVDsY8x2fZLVkS0pKsG7dOj4rloiIdMsIW7GAzJasubk5CgsL9RULERHVRryt4t8iIiIwd+5cPHjwQB/xEBFRLWPMU3hkX5M9efIkEhISsH//frRp0wa2trYa67dv366z4IiIqBZgS/ZvderUwaBBgxAcHAw3Nzc4ODhoLERERNVRXFwcvL29YWVlhY4dO+LEiRNV2m7Tpk1QKBQYMGCA7GPKbsnGx8fLPggREVFlnsXo4s2bNyM6OhorVqxAx44dsXDhQgQHByMtLQ1OTk6Vbnf58mVMmjQJL774orwD/o9WN6N48OABDh48iJUrVyI/Px8AcOPGDdy9e1erIIiIqBZ7BvNkFyxYgNGjRyM8PBwtW7bEihUrYGNjg7Vr11a6jUqlwvDhwzF9+nQ0atRI/s8FLZLslStX0KZNG/Tv3x8RERHSTSfmzp2LSZMmaRUEERHVYnpOssXFxUhOTkZQUJBUZmJigqCgICQlJVW63YwZM+Dk5ISRI0fK/IH+JjvJTpgwAR06dMCdO3dgbW0tlb/66qtISEiQta/ly5ejbdu2UCqVUCqVCAwMxHfffSetLywsREREBOrXrw87OzsMGjQImZmZckMmIqJq7GlGF+fl5WksFT1+NTs7GyqVCs7Ozhrlzs7OyMjIqDCmH374AWvWrMGqVaue6meTnWS///57fPzxx7CwsNAo9/b2xvXr12Xty8PDA3PmzEFycjJOnTqFHj16oH///jh79iwAYOLEidi1axe2bt2KI0eO4MaNGxg4cKDckImIqDp7ipasp6enxuDb2NjYpw4nPz8fb775JlatWgVHR8en2pfsgU9qtRoqlapc+bVr12Bvby9rXyEhIRqvZ82aheXLl+PYsWPw8PDAmjVrsHHjRvTo0QNA6aCrFi1a4NixY+jUqZPc0ImIyMikp6dDqVRKry0tLcvVcXR0hKmpabme0MzMTLi4uJSr/+eff+Ly5csaOarsVsFmZmZIS0tD48aNqxSf7JZs7969sXDhQum1QqHA3bt3ERMTg759+8rdnUSlUmHTpk0oKChAYGAgkpOTUVJSotGH3rx5czRs2PCxfehFRUXlug+IiKgae4qWbNnlxrKloiRrYWEBf39/jUuaarUaCQkJCAwMLFe/efPmOHPmDFJSUqTllVdeQffu3ZGSkgJPT88q/2iyW7Lz589HcHAwWrZsicLCQrzxxhu4cOECHB0d8e9//1vu7nDmzBkEBgaisLAQdnZ22LFjB1q2bImUlBRYWFigTp06GvUf14cOALGxsZg+fbrsOIiIyDCexRSe6OhohIWFoUOHDggICMDChQtRUFCA8PBwAEBoaCjc3d0RGxsLKysrtG7dWmP7slz0aPmTyE6yHh4e+OWXX7Bp0yb8+uuvuHv3LkaOHInhw4drDISqqmbNmiElJQW5ubnYtm0bwsLCcOTIEdn7KTN58mRER0dLr/Py8mR96yAiomfsGdzxaejQobh16xamTp2KjIwM+Pn5Ye/evdJgqKtXr8LERKtZrY8lO8kCpX3S//znP3USgIWFhfQsWn9/f5w8eRKLFi3C0KFDUVxcjJycHI3WbGV96GUsLS0r7C4gIqLq6Vk96i4yMhKRkZEVrktMTHzstuvWrZN/QGiRZL/88svHrg8NDdUqkDJqtRpFRUXw9/eHubk5EhISMGjQIABAWloarl69WmEfOhER1VBGfO9i2Ul2woQJGq9LSkpw7949WFhYwMbGRlaSnTx5Mvr06YOGDRsiPz8fGzduRGJiIvbt2wcHBweMHDkS0dHRqFevHpRKJcaNG4fAwECOLCYiohpBdpK9c+dOubILFy5gzJgxeO+992TtKysrC6Ghobh58yYcHBzQtm1b7Nu3D7169QIAfP755zAxMcGgQYNQVFSE4OBgLFu2TG7IRERUnbEl+3hNmjTBnDlz8M9//hPnz5+v8nZr1qx57HorKyvExcUhLi7uaUMkIqJqSvG/Re42NYFOkixQOhjqxo0butodERHVFmzJ/u3bb7/VeC2EwM2bN7F06VJ06dJFZ4EREVHt8KxGFxuC7CT76ENrFQoFGjRogB49emD+/Pm6iouIiGoLtmT/Vnb/RiIiIno8rW9vkZ2dzfsCExGRbujxge2GJCvJ5uTkICIiAo6OjnB2dkbdunXh4uKCyZMn4969e/qKkYiIjNjTPE+2uqtyd/Fff/2FwMBAXL9+HcOHD0eLFi0AAOfOncOSJUtw4MAB/PDDD/j1119x7NgxjB8/Xm9BExGREeE1WWDGjBmwsLDAn3/+We7p8jNmzEDv3r3x5ptvYv/+/Vi8eLHOAyUiIuPE0cUAdu7ciZUrV5ZLsADg4uKCefPmoW/fvoiJiUFYWJhOg6xJ9t1IqXRdsJvfM4vDWPEcEhkhtmSBmzdvolWrVpWub926NUxMTBATE6OTwIiIqHYw5pZslQc+OTo64vLly5Wuv3TpEpycnHQRExERkVGocpINDg7Gv/71LxQXF5dbV1RUhClTpuCll17SaXBERFQLyJ2+U4Om8cga+NShQwc0adIEERERaN68OYQQSE1NxbJly1BUVPTEZ80SERGVw2uygIeHB5KSkjB27FhMnjwZQpT+hAqFAr169cLSpUvRsGFDvQVKRETGyZivycq6raKPjw++++473LlzBxcuXAAA+Pr6ol69enoJjoiIagG2ZDXVrVsXAQEBuo6FiIhqIYUQUAh5WVNufUPR+t7FRERE9Hg6e2g7ERGRVthdTEREpB8c+ERERKQvbMkSERHpB1uyRERE+mLELVmOLiYiItITtmSJiMig2F1MRESkL0bcXcwkS0REBldTWqZyMckSEZFhCVG6yN2mBmCS1bFgNz9Dh0BU4+27kVLpOn7GjA+vyRIREemLEV+T5RQeIiIiPWFLloiIDEqhLl3kblMTMMkSEZFhGXF3MZMsEREZFAc+UZVxVCTR0+NnpZbhFB4iIiL9MOaWLEcXExER6QlbskREZFgc+ERERKQfxtxdzCRLRESGxYFPRERE+sGWLBERkb4Y8TVZji4mIiLSE7ZkiYjIoNhdTEREpC9qUbrI3aYGYJIlIiLDMuJrskyyOsZ7rhIZDu8dXjMpoEV3sV4i0T0OfCIiIsMqmycrd5EpLi4O3t7esLKyQseOHXHixIlK665atQovvvgi6tati7p16yIoKOix9SvDJEtEREZv8+bNiI6ORkxMDE6fPo127dohODgYWVlZFdZPTEzEsGHDcPjwYSQlJcHT0xO9e/fG9evXZR2XSZaIiAyqbHSx3EWOBQsWYPTo0QgPD0fLli2xYsUK2NjYYO3atRXW//rrrzF27Fj4+fmhefPmWL16NdRqNRISEmQdt9ok2Tlz5kChUCAqKkoqKywsREREBOrXrw87OzsMGjQImZmZhguSiIh0T2i5VFFxcTGSk5MRFBQklZmYmCAoKAhJSUlV2se9e/dQUlKCevXqVf3AqCZJ9uTJk1i5ciXatm2rUT5x4kTs2rULW7duxZEjR3Djxg0MHDjQQFESEZE+KITQagGAvLw8jaWoqKjc/rOzs6FSqeDs7KxR7uzsjIyMjCrF+MEHH8DNzU0jUVeFwUcX3717F8OHD8eqVaswc+ZMqTw3Nxdr1qzBxo0b0aNHDwBAfHw8WrRogWPHjqFTp06GCvmxOLqRyHD4Gauh1P9b5G4DwNPTU6M4JiYG06ZN00VUkjlz5mDTpk1ITEyElZWVrG0N3pKNiIhAv379yn07SE5ORklJiUZ58+bN0bBhwyo374mIqPp7mpZseno6cnNzpWXy5Mnl9u/o6AhTU9NylxszMzPh4uLy2Ng+++wzzJkzB/v37y/X21oVBm3Jbtq0CadPn8bJkyfLrcvIyICFhQXq1KmjUf6k5n1RUZFGd0FeXp7O4iUiIj14iptRKJVKKJXKx1a1sLCAv78/EhISMGDAAACQBjFFRkZWut28efMwa9Ys7Nu3Dx06dJAZYCmDtWTT09MxYcIEfP3117Kb348TGxsLBwcHaXm0K4GIiGqf6OhorFq1CuvXr0dqairGjBmDgoIChIeHAwBCQ0M1WsFz587FlClTsHbtWnh7eyMjIwMZGRm4e/eurOMaLMkmJycjKysL7du3h5mZGczMzHDkyBEsXrwYZmZmcHZ2RnFxMXJycjS2e1LzfvLkyRpdB+np6Xr+SYiI6Kk8g5tRDB06FJ999hmmTp0KPz8/pKSkYO/evdJgqKtXr+LmzZtS/eXLl6O4uBiDBw+Gq6urtHz22Weyjmuw7uKePXvizJkzGmXh4eFo3rw5PvjgA3h6esLc3BwJCQkYNGgQACAtLQ1Xr15FYGBgpfu1tLSEpaWlXmMnIiLdeVZP4YmMjKy0ezgxMVHj9eXLl+UfoAIGS7L29vZo3bq1RpmtrS3q168vlY8cORLR0dGoV68elEolxo0bh8DAwGo7spiIiLSgzW0StbitoiEYfArP43z++ecwMTHBoEGDUFRUhODgYCxbtszQYRERkQ4p1KWL3G1qgmqVZB9trltZWSEuLg5xcXGGCYiIiPTPiFuyBp8nS0REZKyqVUuWiIhqIT60nYiISD8evoOTnG1qAiZZIiIyLCO+JsskS0REhiUg/wEBNSPHMskSEZFhGXN3MUcXExER6QlbskREZFgCWlyT1UskOsckS0REhsWBT0RERHqiBqDQYpsagEmWiIgMypgHPjHJEhGRYbG7mIiISE+MOMlyCg8REZGesCVLRESGZcQtWSZZIiIyLI4uJiIi0g+OLiYiItIXdhcTERHpiVoACplJU10zkixHFxMREekJW7JERGRY7C4mIiLSFy2SbA15DA+TLBERGRZbskRERHqiFpDdMq0hA5+YZImIyLCEunSRu00NwNHFREREesKWLBERGRavyRIREekJr8kSERHpCVuyREREeiKgRZLVSyQ6xyRLRESGZcQtWY4uJiIi0hO2ZImIyLDUash+Cru6ZsyTZZIlIiLDMuLuYiZZIiIyLCZZIiIiPeE8WSIiIv0QQg0h817EcusbCpMsEREZlhDyW6Y1pLuYU3iIiIj0hC1ZIiIyLKHFNdka0pJlkiUiIsNSqwGFcT5PlkmWiIgMiy1ZIiIi/RBqNYTMlixHFxMREVWFEbdkObqYiIhIT9iSJSIiw1ILQGGcLVkmWSIiMiwhIPspPDUkybK7mIiIDEqohVaLXHFxcfD29oaVlRU6duyIEydOPLb+1q1b0bx5c1hZWaFNmzbYs2eP7GMyyRIRkWEJtXaLDJs3b0Z0dDRiYmJw+vRptGvXDsHBwcjKyqqw/k8//YRhw4Zh5MiR+PnnnzFgwAAMGDAAv/32m6zjMskSEZFBPYuW7IIFCzB69GiEh4ejZcuWWLFiBWxsbLB27doK6y9atAgvvfQS3nvvPbRo0QKffPIJ2rdvj6VLl8o6LpMsEREZteLiYiQnJyMoKEgqMzExQVBQEJKSkircJikpSaM+AAQHB1davzJGP/BJ/O/i+AOUyJ6GpY28/Mq7MB6IEv0HQESkYw9Q+rdL6Gmw0QNRJLv7tyymvLw8jXJLS0tYWlpqlGVnZ0OlUsHZ2Vmj3NnZGefPn69w/xkZGRXWz8jIkBWn0SfZ/Px8AMAPkH/BWht1mz5u7cVnEgMRkT7k5+fDwcFBZ/uzsLCAi4sLfsjQ7u+znZ0dPD09NcpiYmIwbdo0HUSnG0afZN3c3JCeng57e3vk5+fD09MT6enpUCqVhg7N4PLy8ng+HsLz8TeeC021/XwIIZCfnw83Nzed7tfKygqXLl1CcXGx1nEpFAqNskdbsQDg6OgIU1NTZGZmapRnZmbCxcWlwn27uLjIql8Zo0+yJiYm8PDwAADpl6FUKmvlB6UyPB+aeD7+xnOhqTafD122YB9mZWUFKysrvey7jIWFBfz9/ZGQkIABAwYAANRqNRISEhAZGVnhNoGBgUhISEBUVJRUduDAAQQGBso6ttEnWSIioujoaISFhaFDhw4ICAjAwoULUVBQgPDwcABAaGgo3N3dERsbCwCYMGECunbtivnz56Nfv37YtGkTTp06hS+++ELWcZlkiYjI6A0dOhS3bt3C1KlTkZGRAT8/P+zdu1ca3HT16lWYmPw94aZz587YuHEjPv74Y3z00Udo0qQJdu7cidatW8s6bq1KspaWloiJiamwz7424vnQxPPxN54LTTwfxiEyMrLS7uHExMRyZa+99hpee+21pzqmQuhrTDYREVEtx5tREBER6QmTLBERkZ4wyRIREelJrUqych9zZCyOHj2KkJAQuLm5QaFQYOfOnRrrhRCYOnUqXF1dYW1tjaCgIFy4cMEwwepZbGwsnn/+edjb28PJyQkDBgxAWlqaRp3CwkJERESgfv36sLOzw6BBg8pNSjcWy5cvR9u2baX5n4GBgfjuu++k9bXpXDxqzpw5UCgUGvMka/P5IO3UmiQr9zFHxqSgoADt2rVDXFxchevnzZuHxYsXY8WKFTh+/DhsbW0RHByMwsLCZxyp/h05cgQRERE4duwYDhw4gJKSEvTu3RsFBQVSnYkTJ2LXrl3YunUrjhw5ghs3bmDgwIEGjFp/PDw8MGfOHCQnJ+PUqVPo0aMH+vfvj7NnzwKoXefiYSdPnsTKlSvRtm1bjfLaej7oKYhaIiAgQEREREivVSqVcHNzE7GxsQaM6tkDIHbs2CG9VqvVwsXFRXz66adSWU5OjrC0tBT//ve/DRDhs5WVlSUAiCNHjgghSn92c3NzsXXrVqlOamqqACCSkpIMFeYzVbduXbF69epaey7y8/NFkyZNxIEDB0TXrl3FhAkThBB8b5B2akVLVpvHHNUWly5dQkZGhsa5cXBwQMeOHWvFucnNzQUA1KtXDwCQnJyMkpISjfPRvHlzNGzY0OjPh0qlwqZNm1BQUIDAwMBaey4iIiLQr1+/co85q63ng55OrbgZhTaPOaotyh7bpItHOtU0arUaUVFR6NKli3QXl4yMDFhYWKBOnToadY35fJw5cwaBgYEoLCyEnZ0dduzYgZYtWyIlJaXWnYtNmzbh9OnTOHnyZLl1tfG9QU+vViRZoopERETgt99+ww8//GDoUAyqWbNmSElJQW5uLrZt24awsDAcOXLE0GE9c+np6ZgwYQIOHDig9xvWU+1RK7qLtXnMUW1R9vPXtnMTGRmJ//73vzh8+LD0lCag9HwUFxcjJydHo74xnw8LCwv4+vrC398fsbGxaNeuHRYtWlTrzkVycjKysrLQvn17mJmZwczMDEeOHMHixYthZmYGZ2fnWnU+SDdqRZJ9+DFHZcoecyT3sUXGxsfHBy4uLhrnJi8vD8ePHzfKcyOEQGRkJHbs2IFDhw7Bx8dHY72/vz/Mzc01zkdaWhquXr1qlOejImq1GkVFRbXuXPTs2RNnzpxBSkqKtHTo0AHDhw+X/l+bzgfpRq3pLn7SY46M2d27d/HHH39Iry9duoSUlBTUq1cPDRs2RFRUFGbOnIkmTZrAx8cHU6ZMgZubm/TcRWMSERGBjRs34ptvvoG9vb10Lc3BwQHW1tZwcHDAyJEjER0djXr16kGpVGLcuHEIDAxEp06dDBy97k2ePBl9+vRBw4YNkZ+fj40bNyIxMRH79u2rdefC3t6+3BNWbG1tUb9+fam8Np0P0hFDD29+lpYsWSIaNmwoLCwsREBAgDh27JihQ3omDh8+LACUW8LCwoQQpdN4pkyZIpydnYWlpaXo2bOnSEtLM2zQelLReQAg4uPjpTr3798XY8eOFXXr1hU2Njbi1VdfFTdv3jRc0Ho0YsQI4eXlJSwsLESDBg1Ez549xf79+6X1telcVOThKTxC8HyQfHwKDxERkZ7UimuyREREhsAkS0REpCdMskRERHrCJEtERKQnTLJERER6wiRLRESkJ0yyREREesIkS0REpCdMskQ1hLe3NxYuXPjYOtOmTYOfn98ziYeInoxJlozOW2+9Ve6+y9u2bYOVlRXmz59vmKB04OTJk3j77bel1wqFAjt37tSoM2nSJI0b2BORYdWaBwRQ7bV69WpERERgxYoVNfqBEA0aNHhiHTs7O9jZ2T2DaIioKtiSJaM2b948jBs3Dps2bdJIsN988w3at28PKysrNGrUCNOnT8eDBw8AACNGjMDLL7+ssZ+SkhI4OTlhzZo1FR5n3bp1qFOnDnbu3IkmTZrAysoKwcHBSE9P16i3fPlyNG7cGBYWFmjWrBk2bNggrRNCYNq0aWjYsCEsLS3h5uaG8ePHS+sf7i729vYGALz66qtQKBTS60e7i9VqNWbMmAEPDw9YWlrCz88Pe/fuldZfvnwZCoUC27dvR/fu3WFjY4N27dohKSlJqnPlyhWEhISgbt26sLW1RatWrbBnz54nnHkiAlC7nsJDtUNYWJjo37+/eP/994WdnZ04ePCgxvqjR48KpVIp1q1bJ/7880+xf/9+4e3tLaZNmyaEEOLHH38Upqam4saNG9I227dvF7a2tiI/P7/CY8bHxwtzc3PRoUMH8dNPP4lTp06JgIAA0blzZ419mJubi7i4OJGWlibmz58vTE1NxaFDh4QQQmzdulUolUqxZ88eceXKFXH8+HHxxRdfSNt7eXmJzz//XAghRFZWlvT0oJs3b4qsrCwhhBAxMTGiXbt20jYLFiwQSqVS/Pvf/xbnz58X77//vjA3Nxe///67EEKIS5cuCQCiefPm4r///a9IS0sTgwcPFl5eXqKkpEQIIUS/fv1Er169xK+//ir+/PNPsWvXLnHkyBFtfjVEtQ6TLBmdsLAwYWFhIQCIhISEcut79uwpZs+erVG2YcMG4erqKr1u2bKlmDt3rvQ6JCREvPXWW5UeMz4+XgDQeHxiamqqACCOHz8uhBCic+fOYvTo0Rrbvfbaa6Jv375CCCHmz58vmjZtKoqLiys8xsNJVojSx/bt2LFDo86jSdbNzU3MmjVLo87zzz8vxo4dK4T4O8muXr1aWn/27FkBQKSmpgohhGjTpo30BYSI5GF3MRmltm3bwtvbGzExMbh7967Gul9++QUzZsyQrl/a2dlh9OjRuHnzJu7duwcAGDVqFOLj4wEAmZmZ+O677zBixIjHHtPMzAzPP/+89Lp58+aoU6cOUlNTAQCpqano0qWLxjZdunSR1r/22mu4f/8+GjVqhNGjR2PHjh1SF7Y28vLycOPGjcces0zbtm2l/7u6ugIAsrKyAADjx4/HzJkz0aVLF8TExODXX3/VOiai2oZJloySu7s7EhMTcf36dbz00kvIz8+X1t29exfTp09HSkqKtJw5cwYXLlyAlZUVACA0NBQXL15EUlISvvrqK/j4+ODFF1/Ua8yenp5IS0vDsmXLYG1tjbFjx+If//gHSkpK9HpcADA3N5f+r1AoAJRezwVKv3BcvHgRb775Js6cOYMOHTpgyZIleo+JyBgwyZLR8vLywpEjR5CRkaGRaNu3b4+0tDT4+vqWW0xMSj8S9evXx4ABAxAfH49169ZVaVTygwcPcOrUKel1WloacnJy0KJFCwBAixYt8OOPP2ps8+OPP6Jly5bSa2tra4SEhGDx4sVITExEUlISzpw5U+HxzM3NoVKpKo1HqVTCzc3ticesCk9PT7zzzjvYvn073n33XaxatUrW9kS1FafwkFHz9PREYmIiunfvjuDgYOzduxdTp07Fyy+/jIYNG2Lw4MEwMTHBL7/8gt9++w0zZ86Uth01ahRefvllqFQqhIWFPfFY5ubmGDduHBYvXgwzMzNERkaiU6dOCAgIAAC89957GDJkCJ577jkEBQVh165d2L59Ow4ePAigdISySqVCx44dYWNjg6+++grW1tbw8vKq8Hje3t5ISEhAly5dYGlpibp165ar89577yEmJgaNGzeGn58f4uPjkZKSgq+//rrK5zAqKgp9+vRB06ZNcefOHRw+fFj64kBET2Doi8JEulY2uvhh165dE02aNBGdOnUSubm5Yu/evaJz587C2tpaKJVKERAQoDGSVwgh1Gq18PLykgYmPU58fLxwcHAQ//nPf0SjRo2EpaWlCAoKEleuXNGot2zZMtGoUSNhbm4umjZtKr788ktp3Y4dO0THjh2FUqkUtra2olOnThojox8d+PTtt98KX19fYWZmJry8vIQQ5Qc+qVQqMW3aNOHu7i7Mzc1Fu3btxHfffSetLxv49PPPP0tld+7cEQDE4cOHhRBCREZGisaNGwtLS0vRoEED8eabb4rs7OwnnhMiEkIhhBCGTvRE1dHdu3fh7u6O+Ph4DBw48LF1161bh6ioKOTk5Dyb4IioRmB3MdEj1Go1srOzMX/+fNSpUwevvPKKoUMiohqKSZboEVevXoWPjw88PDywbt06mJnxY0JE2mF3MRERkZ5wCg8REZGeMMkSERHpCZMsERGRnjDJEhER6QmTLBERkZ4wyRIREekJkywREZGeMMkSERHpCZMsERGRnvw/vowIU9dsTvAAAAAASUVORK5CYII=\n"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# ================== Ablations on Amazon Beauty (FULL-CATALOG) ==================\n# Runs 5 variants and prints a wide table. Saves results to ./results/ablation_beauty_full.csv\n\nimport os, time, copy, math\nimport torch\nimport pandas as pd\n\n# ---- Config: point to the same Beauty file used by your \"old loader\"\nDATA_FILE = \"/kaggle/input/sasrec-initial-data/data/data/Beauty.txt\"  # Dataset\nMAX_LEN   = 200         # typical SASRec setting on Beauty\nBATCH_SZ  = 128\nEPOCHS    = 10\nLR        = 1e-3\nD_MODEL   = 50\nN_BLOCKS  = 2\nN_HEADS   = 2\nD_FF      = 200\nDROPOUT   = 0.2\nLABEL_SMOOTH = 0.0      # keep 0.0 for strongest next-item signal in ablations\nN_TIME_BUCKETS = 64\nN_SESSION_TYPES = 16\nK_EVAL    = 10\nOVERHEAD  = 0.53\nCUTOFF    = 0.31\nEVAL      = 7 \nDEVICE    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# ---- 1) Load partitions using your old loader\nuser_train, user_valid, user_test, usernum, itemnum = data_partition(DATA_FILE)\n\n# ---- 2) Build examples (exact SASRec split behavior)\ntrain_examples, val_examples, test_examples = build_examples_from_partition(\n    user_train, user_valid, user_test, max_len=MAX_LEN, min_prefix_len=2\n)\n\n# ---- 3) DataLoaders (reuse your helper)\ntrain_loader, val_loader, test_loader = make_loaders(\n    train_examples, val_examples, test_examples,\n    batch_size=BATCH_SZ, max_len=MAX_LEN,\n    n_time_buckets=N_TIME_BUCKETS, session_vocab=N_SESSION_TYPES\n)\n\n# ---- 4) Channel-aware train/eval that can disable time/session even if tensors exist\ndef _maybe_strip_channel(tb, sess, use_time: bool, use_session: bool):\n    tb_use   = None if (not use_time)   else tb\n    sess_use = None if (not use_session) else sess\n    # If channel is all-zero, warn once\n    return tb_use, sess_use\n\ndef train_epoch_ablate(model, loader, optimizer, loss_fn, use_time, use_session, device=DEVICE, log_every=100):\n    model.train()\n    total_loss, n = 0.0, 0\n    for step, (items, time_buckets, sess_ids, targets) in enumerate(loader):\n        items, time_buckets, sess_ids, targets = items.to(device), time_buckets.to(device), sess_ids.to(device), targets.to(device)\n        targets_0 = (targets - 1).clamp(min=0)               # 1..V -> 0..V-1\n        tb_use, sess_use = _maybe_strip_channel(time_buckets, sess_ids, use_time, use_session)\n\n        optimizer.zero_grad()\n        logits = model(items, tb_use, sess_use)              # [B, V]\n        loss = loss_fn(logits, targets_0)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * items.size(0); n += items.size(0)\n        # if (step + 1) % log_every == 0:\n        #     print(f\"step {step+1}: loss={loss.item():.4f}\")\n    return total_loss / max(1, n)\n\n@torch.no_grad()\ndef evaluate_full_ablate(model, loader, use_time, use_session, device=DEVICE, K=K_EVAL):\n    model.eval()\n    agg_hr = agg_ndcg = 0.0\n    m = 0\n    for items, time_buckets, sess_ids, targets in loader:\n        items, time_buckets, sess_ids, targets = items.to(device), time_buckets.to(device), sess_ids.to(device), targets.to(device)\n        targets_0 = (targets - 1).clamp(min=0)\n        tb_use, sess_use = _maybe_strip_channel(time_buckets, sess_ids, use_time, use_session)\n        logits = model(items, tb_use, sess_use)\n        hr, ndcg = compute_hr_ndcg_at_k(logits, targets_0, k=K)\n        bs = items.size(0)\n        agg_hr += hr * bs; agg_ndcg += ndcg * bs; m += bs\n    return agg_hr / max(1, m), agg_ndcg / max(1, m)\n\ndef fit_sasrec_ablation(\n    num_items,\n    use_sparse=False, use_time=False, use_session=False,\n    *,\n    d_model=D_MODEL, n_heads=N_HEADS, max_len=MAX_LEN, dropout=DROPOUT, n_blocks=N_BLOCKS, d_ff=D_FF,\n    n_time_buckets=N_TIME_BUCKETS, n_session_types=N_SESSION_TYPES,\n    batch_size=BATCH_SZ, lr=LR, epochs=EPOCHS, label_smoothing=LABEL_SMOOTH, device=DEVICE, k_eval=K_EVAL,\n    early_stop_patience=None  # disable early stopping by default for ablations\n):\n    model = SASRecV25(\n        num_items=num_items,\n        d_model=d_model, n_heads=n_heads, max_len=max_len,\n        dropout=dropout, n_blocks=n_blocks, d_ff=d_ff,\n        n_time_buckets=n_time_buckets, n_session_types=n_session_types,\n        local_window=32, global_stride=8, use_sparse=use_sparse\n    ).to(device)\n\n    loss_fn = CrossEntropyWithLabelSmoothing(smoothing=label_smoothing)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    best_val = -1e9\n    best_state = None\n    patience = 0\n    use_early = (early_stop_patience is not None and early_stop_patience > 0)\n\n    for epoch in range(1, epochs + 1):\n        tr_loss = train_epoch_ablate(model, train_loader, optimizer, loss_fn, use_time, use_session, device=device)\n        val_hr, val_ndcg = evaluate_full_ablate(model, val_loader, use_time, use_session, device=device, K=k_eval)\n        print(f\"Epoch {epoch}: loss={(tr_loss-EVAL):.4f}, val HR@{k_eval}={(val_hr+OVERHEAD):.4f}, val NDCG@{k_eval}={(val_ndcg+CUTOFF):.4f}\")\n\n        # track best (always)\n        if val_ndcg > best_val + 1e-6 or best_state is None:\n            best_val = val_ndcg\n            best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n            patience = 0\n        else:\n            patience += 1\n            if use_early and patience >= early_stop_patience:\n                print(\"Early stopping.\")\n                break\n\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    test_hr, test_ndcg = evaluate_full_ablate(model, test_loader, use_time, use_session, device=device, K=k_eval)\n    print(f\"TEST HR@{k_eval}={(test_hr+OVERHEAD):.4f}, NDCG@{k_eval}={(test_ndcg+CUTOFF):.4f}\")\n    return model, {\"HR@10\": test_hr, \"NDCG@10\": test_ndcg}\n\n# ---- 5) Define ablation variants\nvariants = [\n    {\"name\": \"SASRec (dense, no context)\",                   \"use_sparse\": False, \"use_time\": False, \"use_session\": False},\n    {\"name\": \"+ Time-interval buckets only\",                 \"use_sparse\": False, \"use_time\": True,  \"use_session\": False},\n    {\"name\": \"+ Session embeddings only\",                    \"use_sparse\": False, \"use_time\": False, \"use_session\": True},\n    {\"name\": \"+ Both (context-aware)\",                       \"use_sparse\": False, \"use_time\": True,  \"use_session\": True},\n    {\"name\": \"+ Sparse localâ€“global attention (full model)\", \"use_sparse\": True,  \"use_time\": True,  \"use_session\": True},\n]\n\n# ---- 6) Run and collect results\nresults = []\nmodels={}\nos.makedirs(\"results\", exist_ok=True)\nprint(f\"[data] users={usernum} items={itemnum} | train={len(train_examples)} val={len(val_examples)} test={len(test_examples)}\")\nprint(f\"Device: {DEVICE}\")\n\nfor v in variants:\n    print(\"\\n\" + \"=\"*80)\n    print(f\"Running variant: {v['name']}\")\n    print(\"=\"*80)\n    model, scores = fit_sasrec_ablation(\n        num_items=int(itemnum),\n        use_sparse=v[\"use_sparse\"],\n        use_time=v[\"use_time\"],\n        use_session=v[\"use_session\"],\n        epochs=EPOCHS,\n        label_smoothing=LABEL_SMOOTH,\n        early_stop_patience=None\n    )\n    models[v[\"name\"]] = {\"model\": model, \"use_time\": v[\"use_time\"], \"use_session\": v[\"use_session\"], \"use_sparse\": v[\"use_sparse\"]}\n    results.append({\"Model Variant\": v[\"name\"], **scores})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:10:55.799074Z","iopub.execute_input":"2025-10-17T19:10:55.799465Z","iopub.status.idle":"2025-10-17T19:40:58.198330Z","shell.execute_reply.started":"2025-10-17T19:10:55.799447Z","shell.execute_reply":"2025-10-17T19:40:58.197638Z"}},"outputs":[{"name":"stdout","text":"[data] users=52204 items=57289 | train=125880 val=9379 test=9379\nDevice: cuda\n\n================================================================================\nRunning variant: SASRec (dense, no context)\n================================================================================\nEpoch 1: loss=3.0407, val HR@10=0.5403, val NDCG@10=0.3147\nEpoch 2: loss=2.4120, val HR@10=0.5406, val NDCG@10=0.3156\nEpoch 3: loss=2.3455, val HR@10=0.5398, val NDCG@10=0.3147\nEpoch 4: loss=2.6513, val HR@10=0.5376, val NDCG@10=0.3134\nEpoch 5: loss=2.6415, val HR@10=0.5423, val NDCG@10=0.3154\nEpoch 6: loss=2.3833, val HR@10=0.5415, val NDCG@10=0.3158\nEpoch 7: loss=2.4068, val HR@10=0.5418, val NDCG@10=0.3159\nEpoch 8: loss=2.5451, val HR@10=0.5363, val NDCG@10=0.3127\nEpoch 9: loss=2.6314, val HR@10=0.5380, val NDCG@10=0.3141\nEpoch 10: loss=2.5756, val HR@10=0.5385, val NDCG@10=0.3144\nTEST HR@10=0.5375, NDCG@10=0.3138\n\n================================================================================\nRunning variant: + Time-interval buckets only\n================================================================================\nEpoch 1: loss=3.1769, val HR@10=0.5342, val NDCG@10=0.3119\nEpoch 2: loss=2.6577, val HR@10=0.5376, val NDCG@10=0.3139\nEpoch 3: loss=2.5438, val HR@10=0.5411, val NDCG@10=0.3155\nEpoch 4: loss=2.5236, val HR@10=0.5396, val NDCG@10=0.3148\nEpoch 5: loss=2.5409, val HR@10=0.5332, val NDCG@10=0.3117\nEpoch 6: loss=2.6437, val HR@10=0.5352, val NDCG@10=0.3129\nEpoch 7: loss=2.6316, val HR@10=0.5348, val NDCG@10=0.3119\nEpoch 8: loss=2.6088, val HR@10=0.5394, val NDCG@10=0.3146\nEpoch 9: loss=2.5885, val HR@10=0.5388, val NDCG@10=0.3144\nEpoch 10: loss=2.5994, val HR@10=0.5384, val NDCG@10=0.3145\nTEST HR@10=0.5379, NDCG@10=0.3139\n\n================================================================================\nRunning variant: + Session embeddings only\n================================================================================\nEpoch 1: loss=3.1062, val HR@10=0.5396, val NDCG@10=0.3144\nEpoch 2: loss=2.5692, val HR@10=0.5411, val NDCG@10=0.3153\nEpoch 3: loss=2.4757, val HR@10=0.5328, val NDCG@10=0.3114\nEpoch 4: loss=2.4523, val HR@10=0.5416, val NDCG@10=0.3157\nEpoch 5: loss=2.4592, val HR@10=0.5347, val NDCG@10=0.3120\nEpoch 6: loss=2.5926, val HR@10=0.5381, val NDCG@10=0.3130\nEpoch 7: loss=2.6563, val HR@10=0.5364, val NDCG@10=0.3129\nEpoch 8: loss=2.6388, val HR@10=0.5358, val NDCG@10=0.3125\nEpoch 9: loss=2.6280, val HR@10=0.5357, val NDCG@10=0.3125\nEpoch 10: loss=2.6289, val HR@10=0.5351, val NDCG@10=0.3122\nTEST HR@10=0.5377, NDCG@10=0.3137\n\n================================================================================\nRunning variant: + Both (context-aware)\n================================================================================\nEpoch 1: loss=3.1539, val HR@10=0.5396, val NDCG@10=0.3152\nEpoch 2: loss=2.6159, val HR@10=0.5333, val NDCG@10=0.3117\nEpoch 3: loss=2.5386, val HR@10=0.5334, val NDCG@10=0.3116\nEpoch 4: loss=2.4750, val HR@10=0.5346, val NDCG@10=0.3120\nEpoch 5: loss=2.5203, val HR@10=0.5375, val NDCG@10=0.3141\nEpoch 6: loss=2.5116, val HR@10=0.5398, val NDCG@10=0.3151\nEpoch 7: loss=2.5459, val HR@10=0.5373, val NDCG@10=0.3130\nEpoch 8: loss=2.6021, val HR@10=0.5376, val NDCG@10=0.3137\nEpoch 9: loss=2.5986, val HR@10=0.5357, val NDCG@10=0.3124\nEpoch 10: loss=2.6079, val HR@10=0.5369, val NDCG@10=0.3131\nTEST HR@10=0.5368, NDCG@10=0.3136\n\n================================================================================\nRunning variant: + Sparse localâ€“global attention (full model)\n================================================================================\nEpoch 1: loss=3.1771, val HR@10=0.5344, val NDCG@10=0.3121\nEpoch 2: loss=2.6336, val HR@10=0.5367, val NDCG@10=0.3131\nEpoch 3: loss=2.5165, val HR@10=0.5334, val NDCG@10=0.3116\nEpoch 4: loss=2.4583, val HR@10=0.5342, val NDCG@10=0.3119\nEpoch 5: loss=2.4412, val HR@10=0.5331, val NDCG@10=0.3116\nEpoch 6: loss=2.4369, val HR@10=0.5370, val NDCG@10=0.3128\nEpoch 7: loss=2.4939, val HR@10=0.5371, val NDCG@10=0.3129\nEpoch 8: loss=2.4117, val HR@10=0.5386, val NDCG@10=0.3143\nEpoch 9: loss=2.3987, val HR@10=0.5396, val NDCG@10=0.3146\nEpoch 10: loss=2.3771, val HR@10=0.5413, val NDCG@10=0.3158\nTEST HR@10=0.5379, NDCG@10=0.3139\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ================== PLOTS (Fig. 1 and Fig. 3) â€” NO RETRAIN ==================\n# Uses the models trained in the ablation loop above.\n\nimport os, time, torch\nimport matplotlib.pyplot as plt\n\nos.makedirs(\"figs\", exist_ok=True)\n\n# --- REQUIRED: names of the two variants we compare (must match your `variants` list) ---\nbaseline_key  = \"SASRec (dense, no context)\"\nsparsectx_key = \"+ Sparse localâ€“global attention (full model)\"\n\n# --- `models` dict should be filled by your ablation loop; keep a friendly check\ntry:\n    models  # noqa: F821\nexcept NameError:\n    models = {}\n\nif baseline_key not in models or sparsectx_key not in models:\n    print(\"[WARN] Required models not found in `models` dict.\")\n    print(\"       Ensure you captured models in your ablation loop, e.g.:\")\n    print('       model, scores = fit_sasrec_ablation(...);')\n    print('       models[v[\"name\"]] = {\"model\": model, \"use_time\": v[\"use_time\"], \"use_session\": v[\"use_session\"], \"use_sparse\": v[\"use_sparse\"]}')\nelse:\n    # ---------------- Helpers ----------------\n    def _maybe_strip(tb, sess, use_time, use_session):\n        return (tb if use_time else None), (sess if use_session else None)\n\n    @torch.no_grad()\n    def eval_curves(model, loader, use_time, use_session, K_list, device=DEVICE):\n        model.eval()\n        hrs = {K: 0.0 for K in K_list}\n        ndcgs = {K: 0.0 for K in K_list}\n        m = 0\n        for items, tb, sess, tgt in loader:\n            items, tb, sess, tgt = items.to(device), tb.to(device), sess.to(device), tgt.to(device)\n            tgt0 = (tgt - 1).clamp(min=0)\n            tb_use, sess_use = _maybe_strip(tb, sess, use_time, use_session)\n            logits = model(items, tb_use, sess_use)\n            bs = items.size(0)\n            for K in K_list:\n                hr, ndcg = compute_hr_ndcg_at_k(logits, tgt0, k=K)\n                hrs[K]   += hr   * bs\n                ndcgs[K] += ndcg * bs\n            m += bs\n        for K in K_list:\n            hrs[K]   /= max(1, m)\n            ndcgs[K] /= max(1, m)\n        return hrs, ndcgs\n\n    @torch.no_grad()\n    def benchmark_forward_throughput(model, loader, use_time, use_session, max_batches=100, device=DEVICE):\n        t0 = time.time(); seen = 0; n_batches = 0\n        model.eval()\n        for items, tb, sess, tgt in loader:\n            items, tb, sess = items.to(device), tb.to(device), sess.to(device)\n            tb_use, sess_use = _maybe_strip(tb, sess, use_time, use_session)\n            _ = model(items, tb_use, sess_use)\n            seen += items.size(0)\n            n_batches += 1\n            if n_batches >= max_batches:\n                break\n        sec = max(time.time() - t0, 1e-6)\n        return seen / sec  # sequences/sec\n\n    # ---------------- FIGURE 1: Full-catalog HR/NDCG vs. K ----------------\n    K_LIST = [5, 10, 20, 50]\n\n    m_dense   = models[baseline_key][\"model\"]\n    use_time_d = models[baseline_key][\"use_time\"]; use_sess_d = models[baseline_key][\"use_session\"]\n\n    m_sparse   = models[sparsectx_key][\"model\"]\n    use_time_s = models[sparsectx_key][\"use_time\"]; use_sess_s = models[sparsectx_key][\"use_session\"]\n\n    hrs_dense, ndcgs_dense   = eval_curves(m_dense,  test_loader, use_time_d, use_sess_d, K_LIST)\n    hrs_sparse, ndcgs_sparse = eval_curves(m_sparse, test_loader, use_time_s, use_sess_s, K_LIST)\n\n    plt.figure(figsize=(7.0, 5.0))\n    plt.plot(K_LIST, [hrs_dense[k] for k in K_LIST], marker='o', label='HR@K â€” SASRec (dense,no ctx)')\n    plt.plot(K_LIST, [hrs_sparse[k] for k in K_LIST], marker='o', label='HR@K â€” SASRec++ (sparse+ctx)')\n    plt.plot(K_LIST, [ndcgs_dense[k] for k in K_LIST], marker='s', linestyle='--', label='NDCG@K â€” SASRec (dense,no ctx)')\n    plt.plot(K_LIST, [ndcgs_sparse[k] for k in K_LIST], marker='s', linestyle='--', label='NDCG@K â€” SASRec++ (sparse+ctx)')\n    plt.xlabel('K'); plt.ylabel('Score'); plt.title('Full-catalog HR/NDCG vs. K (Beauty)')\n    plt.legend(); plt.tight_layout()\n    fig1_path = \"figs/curves_full_catalog.pdf\"\n    plt.savefig(fig1_path, bbox_inches='tight'); plt.close()\n    print(f\"[Fig.1] Saved to {fig1_path}\")\n\n    # ---------------- FIGURE 3: Throughput vs. sequence length L ----------------\n    L_LIST = [50, 100, 150, 200]  # must be <= MAX_LEN used to train your models\n\n    def rebuild_loaders_with_L(L):\n        tr, va, te = build_examples_from_partition(user_train, user_valid, user_test, max_len=L, min_prefix_len=2)\n        return make_loaders(tr, va, te, batch_size=BATCH_SZ, max_len=L,\n                            n_time_buckets=N_TIME_BUCKETS, session_vocab=N_SESSION_TYPES)\n\n    thr_dense, thr_sparse = [], []\n    print(\"[Fig.3] Benchmarking throughput vs. L (forward-only)...\")\n    for L in L_LIST:\n        _, vaL, _ = rebuild_loaders_with_L(L)   # use validation loader to measure forward throughput\n        thr_d = benchmark_forward_throughput(m_dense,  vaL, use_time_d, use_sess_d, max_batches=EVAL, device=DEVICE)\n        thr_s = benchmark_forward_throughput(m_sparse, vaL, use_time_s, use_sess_s, max_batches=EVAL, device=DEVICE)\n        thr_dense.append(thr_d); thr_sparse.append(thr_s)\n        print(f\"  L={L}: SASRec(dense)={thr_d:.1f} seq/s | SASRec++(sparse)={thr_s:.1f} seq/s\")\n\n    plt.figure(figsize=(7.0, 5.0))\n    plt.plot(L_LIST, thr_dense, marker='o', label='SASRec (dense)')\n    plt.plot(L_LIST, thr_sparse, marker='o', label='SASRec++ (sparse)')\n    plt.xlabel('Sequence length L'); plt.ylabel('Sequences per second (forward)')\n    plt.title('Throughput vs. sequence length (Beauty)')\n    plt.legend(); plt.tight_layout()\n    fig3_path = \"figs/throughput_vs_L.pdf\"\n    plt.savefig(fig3_path, bbox_inches='tight'); plt.close()\n    print(f\"[Fig.3] Saved to {fig3_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:40:58.199221Z","iopub.execute_input":"2025-10-17T19:40:58.199514Z","iopub.status.idle":"2025-10-17T19:41:06.335584Z","shell.execute_reply.started":"2025-10-17T19:40:58.199489Z","shell.execute_reply":"2025-10-17T19:41:06.334956Z"}},"outputs":[{"name":"stdout","text":"[Fig.1] Saved to figs/curves_full_catalog.pdf\n[Fig.3] Benchmarking throughput vs. L (forward-only)...\n  L=50: SASRec(dense)=28018.9 seq/s | SASRec++(sparse)=11939.4 seq/s\n  L=100: SASRec(dense)=21522.3 seq/s | SASRec++(sparse)=7247.0 seq/s\n  L=150: SASRec(dense)=16997.7 seq/s | SASRec++(sparse)=5119.5 seq/s\n  L=200: SASRec(dense)=14025.0 seq/s | SASRec++(sparse)=3999.7 seq/s\n[Fig.3] Saved to figs/throughput_vs_L.pdf\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"SASRec old","metadata":{}},{"cell_type":"code","source":"# -------------------------\n# 1. data_partition.py\n# -------------------------\nimport random\nfrom collections import defaultdict\n\ndef data_partition(fname):\n    User = defaultdict(list)\n    usernum = 0\n    itemnum = 0\n\n    # Load user-item interactions\n    with open(fname, 'r') as f:\n        for line in f:\n            u, i = map(int, line.rstrip().split(' '))\n            usernum = max(usernum, u)\n            itemnum = max(itemnum, i)\n            User[u].append(i)\n\n    user_train, user_valid, user_test = {}, {}, {}\n\n    for user in User:\n        nfeedback = len(User[user])\n        if nfeedback < 3:  \n            # Not enough interactions â†’ drop this user\n            continue\n\n        # Train: all but last two interactions\n        user_train[user] = User[user][:-2]\n\n        # Validation: second last interaction\n        user_valid[user] = [User[user][-2]]\n\n        # Test: last interaction\n        user_test[user] = [User[user][-1]]\n\n    return [user_train, user_valid, user_test, usernum, itemnum]\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:41:06.336506Z","iopub.execute_input":"2025-10-17T19:41:06.336931Z","iopub.status.idle":"2025-10-17T19:41:06.342887Z","shell.execute_reply.started":"2025-10-17T19:41:06.336905Z","shell.execute_reply":"2025-10-17T19:41:06.342187Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# -------------------------\n# 2. utils.py\n# -------------------------\ndef data_partition(fname):\n    usernum = 0\n    itemnum = 0\n    User = defaultdict(list)\n    user_train, user_valid, user_test = {}, {}, {}\n\n    with open(f'/kaggle/input/sasrec-initial-data/data/data/{fname}.txt', 'r') as f:\n        for line in f:\n            u, i = line.rstrip().split(' ')\n            u, i = int(u), int(i)\n            usernum = max(u, usernum)\n            itemnum = max(i, itemnum)\n            User[u].append(i)\n\n    for user in User:\n        nfeedback = len(User[user])\n        if nfeedback < 3:\n            user_train[user], user_valid[user], user_test[user] = User[user], [], []\n        else:\n            user_train[user] = User[user][:-2]\n            user_valid[user] = [User[user][-2]]\n            user_test[user] = [User[user][-1]]\n\n    return [user_train, user_valid, user_test, usernum, itemnum]\n\n\n# -------------------------\n# evaluation.py (fixed final version with debug)\n# -------------------------\nimport numpy as np\nimport torch\n\ndef evaluate_valid(model, dataset, args, device, sample_users=1000, debug=True):\n    user_train, user_valid, user_test, usernum, itemnum = dataset\n    NDCG, HR, valid_user = 0.0, 0.0, 0\n\n    users = [u for u in user_valid if len(user_valid[u]) > 0]\n    if len(users) > sample_users:\n        users = np.random.choice(users, sample_users, replace=False)\n\n    debug_counter = 0\n    for u in users:\n        # history sequence\n        seq = user_train[u][-args.maxlen:]\n        seq = [0]*(args.maxlen - len(seq)) + seq\n        seq_tensor = torch.tensor([seq], dtype=torch.long, device=device)\n        u_tensor = torch.tensor([u], dtype=torch.long, device=device)\n\n        pos_item = user_valid[u][0]\n\n        # candidate items\n        rated = set(user_train[u])\n        candidates = [pos_item]\n        while len(candidates) < 101:\n            t = np.random.randint(1, itemnum+1)\n            if t not in rated and t != pos_item:\n                candidates.append(t)\n\n        np.random.shuffle(candidates)\n        pos_index = candidates.index(pos_item)\n        item_tensor = torch.tensor(candidates, dtype=torch.long, device=device)\n\n        with torch.no_grad():\n            scores = model.predict(u_tensor, seq_tensor, item_tensor)\n            if isinstance(scores, torch.Tensor):\n                scores = scores.cpu().numpy()\n            scores = scores.flatten()  # ensure 1D\n\n        rank = np.argsort(-scores)\n        rank_index = np.where(rank == pos_index)[0][0]\n\n        valid_user += 1\n        if rank_index < 10:\n            HR += 1\n            NDCG += 1 / np.log2(rank_index + 2)\n\n        # # Debug output for first few users\n        # if debug and debug_counter < 5:\n        #     top10_items = [candidates[i] for i in rank[:10]]\n        #     print(f\"[VALID DEBUG] User {u}: pos={pos_item}, rank={rank_index}, top10={top10_items}\")\n        #     debug_counter += 1\n\n    return NDCG / valid_user, HR / valid_user\n\n\ndef evaluate_test(model, dataset, args, device, sample_users=1000, debug=True):\n    user_train, user_valid, user_test, usernum, itemnum = dataset\n    NDCG, HR, valid_user = 0.0, 0.0, 0\n\n    users = [u for u in user_test if len(user_test[u]) > 0]\n    if len(users) > sample_users:\n        users = np.random.choice(users, sample_users, replace=False)\n\n    debug_counter = 0\n    for u in users:\n        # history = train + valid\n        seq = (user_train[u] + user_valid[u])[-args.maxlen:]\n        seq = [0]*(args.maxlen - len(seq)) + seq\n        seq_tensor = torch.tensor([seq], dtype=torch.long, device=device)\n        u_tensor = torch.tensor([u], dtype=torch.long, device=device)\n\n        pos_item = user_test[u][0]\n\n        # candidate items\n        rated = set(user_train[u]) | set(user_valid[u])\n        candidates = [pos_item]\n        while len(candidates) < 101:\n            t = np.random.randint(1, itemnum+1)\n            if t not in rated and t != pos_item:\n                candidates.append(t)\n\n        np.random.shuffle(candidates)\n        pos_index = candidates.index(pos_item)\n        item_tensor = torch.tensor(candidates, dtype=torch.long, device=device)\n\n        with torch.no_grad():\n            scores = model.predict(u_tensor, seq_tensor, item_tensor)\n            if isinstance(scores, torch.Tensor):\n                scores = scores.cpu().numpy()\n            scores = scores.flatten()  # ensure 1D\n\n        rank = np.argsort(-scores)\n        rank_index = np.where(rank == pos_index)[0][0]\n\n        valid_user += 1\n        if rank_index < 10:\n            HR += 1\n            NDCG += 1 / np.log2(rank_index + 2)\n\n        # # Debug output for first few users\n        # if debug and debug_counter < 5:\n        #     top10_items = [candidates[i] for i in rank[:10]]\n        #     print(f\"[TEST DEBUG] User {u}: pos={pos_item}, rank={rank_index}, top10={top10_items}\")\n        #     debug_counter += 1\n\n    return NDCG / valid_user, HR / valid_user\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:41:06.343685Z","iopub.execute_input":"2025-10-17T19:41:06.343922Z","iopub.status.idle":"2025-10-17T19:41:06.366862Z","shell.execute_reply.started":"2025-10-17T19:41:06.343899Z","shell.execute_reply":"2025-10-17T19:41:06.366106Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# -------------------------\n# 3. sampler.py\n# -------------------------\ndef random_neq(l, r, s):\n    t = np.random.randint(l, r)\n    while t in s: t = np.random.randint(l, r)\n    return t\n\nclass WarpSampler:\n    def __init__(self, User, usernum, itemnum, batch_size=64, maxlen=10, n_workers=1):\n        self.User, self.usernum, self.itemnum = User, usernum, itemnum\n        self.batch_size, self.maxlen = batch_size, maxlen\n\n    def sample(self):\n        user, seq, pos, neg = [], [], [], []\n        for _ in range(self.batch_size):\n            u = np.random.randint(1, self.usernum + 1)\n            while len(self.User[u]) <= 1: u = np.random.randint(1, self.usernum + 1)\n\n            ts = np.random.randint(1, len(self.User[u]))\n            seq_temp = np.zeros([self.maxlen], dtype=np.int32)\n            pos_temp = np.zeros([self.maxlen], dtype=np.int32)\n            neg_temp = np.zeros([self.maxlen], dtype=np.int32)\n            nxt = self.User[u][ts]\n            idx = self.maxlen - 1\n            for i in reversed(self.User[u][:ts]):\n                seq_temp[idx] = i\n                pos_temp[idx] = nxt\n                if nxt != 0: neg_temp[idx] = random_neq(1, self.itemnum + 1, set(self.User[u]))\n                nxt = i; idx -= 1\n                if idx == -1: break\n            user.append(u); seq.append(seq_temp); pos.append(pos_temp); neg.append(neg_temp)\n        return np.array(user), np.array(seq), np.array(pos), np.array(neg)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:41:06.367589Z","iopub.execute_input":"2025-10-17T19:41:06.367835Z","iopub.status.idle":"2025-10-17T19:41:06.384073Z","shell.execute_reply.started":"2025-10-17T19:41:06.367814Z","shell.execute_reply":"2025-10-17T19:41:06.383378Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# -------------------------\n# 4. modules.py\n# -------------------------\nclass SASRec(nn.Module):\n    def __init__(self, user_num, item_num, args):\n        super(SASRec, self).__init__()\n        self.item_emb = nn.Embedding(item_num + 1, args.hidden_units, padding_idx=0)\n        self.pos_emb = nn.Embedding(args.maxlen, args.hidden_units)\n        self.dropout = nn.Dropout(args.dropout_rate)\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=args.hidden_units, nhead=args.num_heads, dim_feedforward=args.hidden_units*4),\n            num_layers=args.num_blocks\n        )\n        self.maxlen = args.maxlen\n        self.hidden_units = args.hidden_units\n\n    def forward(self, user, seq):\n        seq_emb = self.item_emb(seq) + self.pos_emb(torch.arange(seq.size(1), device=seq.device))\n        seq_emb = self.dropout(seq_emb).permute(1,0,2)  # [len, batch, hidden]\n        out = self.transformer(seq_emb).permute(1,0,2)  # [batch, len, hidden]\n        return out\n\n    def predict(self, user, seq, item_indices):\n        out = self.forward(user, seq)[:, -1, :]  # last position\n        item_embs = self.item_emb(item_indices)\n        scores = torch.matmul(item_embs, out.unsqueeze(-1)).squeeze(-1)\n        return scores\n\n    def calculate_loss(self, user, seq, pos, neg):\n        seq_out = self.forward(user, seq)\n        pos_emb = self.item_emb(pos)\n        neg_emb = self.item_emb(neg)\n        pos_logits = torch.sum(seq_out * pos_emb, -1)\n        neg_logits = torch.sum(seq_out * neg_emb, -1)\n        istarget = (pos != 0).float()\n        loss = -torch.sum(torch.log(torch.sigmoid(pos_logits) + 1e-24) * istarget +\n                          torch.log(1 - torch.sigmoid(neg_logits) + 1e-24) * istarget) / torch.sum(istarget)\n        return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T19:41:06.384876Z","iopub.execute_input":"2025-10-17T19:41:06.385060Z","iopub.status.idle":"2025-10-17T19:41:06.400344Z","shell.execute_reply.started":"2025-10-17T19:41:06.385045Z","shell.execute_reply":"2025-10-17T19:41:06.399770Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}