{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "HuggingGPT Response Generation - Stage 4\n",
    "=========================================\n",
    "Final stage: Generate natural language responses from execution results\n",
    "\n",
    "Integrates:\n",
    "- Task Planning results\n",
    "- Model Selection decisions\n",
    "- Task Execution results\n",
    "- Generates friendly, actionable responses\n"
   ],
   "metadata": {
    "id": "tnCtsXTK_Kbq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "pip install langchain-openai"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O2wQk7sWF18c",
    "outputId": "1592a075-644b-4514-a9b0-4422ec94867d"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.76 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.3.76)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.108.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.4.28)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (25.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (2.11.9)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.4)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.104.2->langchain-openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.5.0)\n",
      "Downloading langchain_openai-0.3.33-py3-none-any.whl (74 kB)\n",
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m75.0/75.0 kB\u001B[0m \u001B[31m3.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: langchain-openai\n",
      "Successfully installed langchain-openai-0.3.33\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "import json\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Optional: For LLM-based response generation\n",
    "try:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    HAS_LANGCHAIN = True\n",
    "except ImportError:\n",
    "    HAS_LANGCHAIN = False\n",
    "    print(\"langchain_openai not found\")\n",
    "\n",
    "@dataclass\n",
    "class ResponseGenerationResult:\n",
    "    \"\"\"Final response with all pipeline information\"\"\"\n",
    "    user_input: str\n",
    "    natural_language_response: str\n",
    "    task_summary: str\n",
    "    model_summary: str\n",
    "    execution_summary: str\n",
    "    confidence_level: str\n",
    "    structured_results: Dict[int, Any]\n",
    "    file_paths: List[str]\n",
    "\n",
    "class ResponseGenerator:\n",
    "    \"\"\"\n",
    "    Stage 4: Response Generation\n",
    "    Generates natural language responses from structured results\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, openrouter_api_key: str = None):\n",
    "        self.openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\") or openrouter_api_key\n",
    "\n",
    "        # Initialize LLM for natural language generation\n",
    "        self.llm_client = None\n",
    "        if openrouter_api_key and HAS_LANGCHAIN:\n",
    "            self.llm_client = ChatOpenAI(\n",
    "                model=\"openai/gpt-4o-mini\",\n",
    "                api_key=openrouter_api_key,\n",
    "                base_url=\"https://openrouter.ai/api/v1\",\n",
    "                temperature=0.7,  # More creative for responses\n",
    "                max_tokens=500\n",
    "            )\n",
    "            print(\"üé® Response Generator initialized with LLM\")\n",
    "        else:\n",
    "            print(\"üé® Response Generator initialized (rule-based)\")\n",
    "\n",
    "    def create_response_generation_prompt(self,\n",
    "                                          user_input: str,\n",
    "                                          tasks: List[Dict],\n",
    "                                          model_assignments: Dict,\n",
    "                                          execution_results: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Create prompt for LLM-based response generation\n",
    "        Following HuggingGPT's approach from the paper\n",
    "        \"\"\"\n",
    "\n",
    "        # Format task information\n",
    "        task_info = \"\"\n",
    "        for task in tasks:\n",
    "            task_info += f\"- Task {task['id']}: {task['task']} with args {task['args']}\\n\"\n",
    "\n",
    "        # Format model selection\n",
    "        model_info = \"\"\n",
    "        for task_id, assignment in model_assignments.items():\n",
    "            model_info += f\"- Task {task_id}: {assignment['model_id']}\\n\"\n",
    "\n",
    "        # Format execution results\n",
    "        results_info = \"\"\n",
    "        for task_id, result in execution_results.items():\n",
    "            if result.status == \"success\":\n",
    "                # Format result based on type\n",
    "                if isinstance(result.result, dict):\n",
    "                    if \"detections\" in result.result:\n",
    "                        detections = result.result[\"detections\"]\n",
    "                        results_info += f\"- Task {task_id} (object-detection): Found {len(detections)} objects:\\n\"\n",
    "                        for det in detections[:3]:\n",
    "                            results_info += f\"  ‚Ä¢ {det['label']} (confidence: {det['score']:.2%})\\n\"\n",
    "                    elif \"predictions\" in result.result:\n",
    "                        preds = result.result[\"predictions\"]\n",
    "                        results_info += f\"- Task {task_id} (classification): Top predictions:\\n\"\n",
    "                        for pred in preds[:3]:\n",
    "                            results_info += f\"  ‚Ä¢ {pred['label']} (confidence: {pred['score']:.2%})\\n\"\n",
    "                elif isinstance(result.result, str):\n",
    "                    results_info += f\"- Task {task_id}: {result.result[:100]}...\\n\"\n",
    "                elif hasattr(result.result, 'size'):  # Image\n",
    "                    results_info += f\"- Task {task_id}: Generated image ({result.result.size})\\n\"\n",
    "            else:\n",
    "                results_info += f\"- Task {task_id}: Failed - {result.error}\\n\"\n",
    "\n",
    "        prompt = f\"\"\"#4 Response Generation Stage - With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as:\n",
    "\n",
    "User Input: {user_input}\n",
    "\n",
    "Task Planning:\n",
    "{task_info}\n",
    "\n",
    "Model Selection:\n",
    "{model_info}\n",
    "\n",
    "Task Execution:\n",
    "{results_info}\n",
    "\n",
    "You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path. If there is nothing in the results, please tell me you can't make it.\n",
    "\n",
    "Generate a friendly, natural language response that:\n",
    "1. Directly answers the user's request\n",
    "2. Explains what was done\n",
    "3. Presents the results clearly\n",
    "4. Indicates confidence level\n",
    "5. Mentions any saved files\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def generate_response_with_llm(self,\n",
    "                                   user_input: str,\n",
    "                                   tasks: List[Dict],\n",
    "                                   model_assignments: Dict,\n",
    "                                   execution_results: Dict) -> str:\n",
    "        \"\"\"Generate natural language response using LLM\"\"\"\n",
    "\n",
    "        if not self.llm_client:\n",
    "            return self.generate_response_rule_based(user_input, tasks, model_assignments, execution_results)\n",
    "\n",
    "        try:\n",
    "            prompt = self.create_response_generation_prompt(\n",
    "                user_input, tasks, model_assignments, execution_results\n",
    "            )\n",
    "\n",
    "            response = self.llm_client.invoke(prompt)\n",
    "            return response.content.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  LLM response generation failed: {e}\")\n",
    "            return self.generate_response_rule_based(user_input, tasks, model_assignments, execution_results)\n",
    "\n",
    "    def generate_response_rule_based(self,\n",
    "                                     user_input: str,\n",
    "                                     tasks: List[Dict],\n",
    "                                     model_assignments: Dict,\n",
    "                                     execution_results: Dict) -> str:\n",
    "        \"\"\"Generate natural language response using rules (fallback)\"\"\"\n",
    "\n",
    "        response = f\"Based on your request: '{user_input}', here's what I found:\\n\\n\"\n",
    "\n",
    "        successful_tasks = 0\n",
    "        failed_tasks = 0\n",
    "\n",
    "        for task in tasks:\n",
    "            task_id = task[\"id\"]\n",
    "            task_type = task[\"task\"]\n",
    "\n",
    "            if task_id in execution_results:\n",
    "                result = execution_results[task_id]\n",
    "\n",
    "                if result.status == \"success\":\n",
    "                    successful_tasks += 1\n",
    "\n",
    "                    # Format based on result type\n",
    "                    if isinstance(result.result, dict):\n",
    "                        if \"detections\" in result.result:\n",
    "                            detections = result.result[\"detections\"]\n",
    "                            response += f\"üîç Object Detection: I found {len(detections)} objects in the image:\\n\"\n",
    "                            for det in detections[:5]:\n",
    "                                response += f\"   ‚Ä¢ {det['label']} (confidence: {det['score']:.1%})\\n\"\n",
    "\n",
    "                        elif \"predictions\" in result.result:\n",
    "                            preds = result.result[\"predictions\"]\n",
    "                            response += f\"üè∑Ô∏è  Image Classification: Top predictions:\\n\"\n",
    "                            for pred in preds[:3]:\n",
    "                                response += f\"   ‚Ä¢ {pred['label']} (confidence: {pred['score']:.1%})\\n\"\n",
    "\n",
    "                    elif isinstance(result.result, str):\n",
    "                        if task_type == \"text-generation\":\n",
    "                            response += f\"‚úçÔ∏è  Generated Text: {result.result}\\n\"\n",
    "                        elif task_type in [\"image-to-text\", \"image-captioning\"]:\n",
    "                            response += f\"üí¨ Image Caption: {result.result}\\n\"\n",
    "\n",
    "                    elif hasattr(result.result, 'size'):  # PIL Image\n",
    "                        response += f\"üé® Generated Image: Created a {result.result.size[0]}x{result.result.size[1]} image\\n\"\n",
    "\n",
    "                    # Add file path if available\n",
    "                    if result.resource_path:\n",
    "                        response += f\"   üìÅ Saved to: {result.resource_path}\\n\"\n",
    "\n",
    "                    response += \"\\n\"\n",
    "                else:\n",
    "                    failed_tasks += 1\n",
    "                    response += f\"‚ùå Task {task_id} ({task_type}) failed: {result.error}\\n\\n\"\n",
    "\n",
    "        # Add confidence summary\n",
    "        if successful_tasks > 0:\n",
    "            confidence = \"high\" if successful_tasks == len(tasks) else \"medium\" if failed_tasks == 0 else \"low\"\n",
    "            response += f\"\\n‚úÖ Confidence Level: {confidence.upper()}\\n\"\n",
    "            response += f\"   Successfully completed {successful_tasks}/{len(tasks)} tasks\\n\"\n",
    "        else:\n",
    "            response += \"\\n‚ùå Unable to complete the request. Please try again or reformulate your query.\\n\"\n",
    "\n",
    "        return response\n",
    "\n",
    "    def calculate_confidence_level(self, execution_results: Dict) -> str:\n",
    "        \"\"\"Calculate overall confidence level based on results\"\"\"\n",
    "\n",
    "        if not execution_results:\n",
    "            return \"none\"\n",
    "\n",
    "        total_tasks = len(execution_results)\n",
    "        successful_tasks = sum(1 for r in execution_results.values() if r.status == \"success\")\n",
    "\n",
    "        success_rate = successful_tasks / total_tasks\n",
    "\n",
    "        # Calculate average confidence from results\n",
    "        total_confidence = 0\n",
    "        confidence_count = 0\n",
    "\n",
    "        for result in execution_results.values():\n",
    "            if result.status == \"success\" and isinstance(result.result, dict):\n",
    "                if \"detections\" in result.result:\n",
    "                    scores = [d[\"score\"] for d in result.result[\"detections\"]]\n",
    "                    if scores:\n",
    "                        total_confidence += sum(scores) / len(scores)\n",
    "                        confidence_count += 1\n",
    "                elif \"predictions\" in result.result:\n",
    "                    scores = [p[\"score\"] for p in result.result[\"predictions\"]]\n",
    "                    if scores:\n",
    "                        total_confidence += scores[0]  # Top prediction\n",
    "                        confidence_count += 1\n",
    "\n",
    "        avg_confidence = total_confidence / confidence_count if confidence_count > 0 else 0.5\n",
    "\n",
    "        # Combine success rate and model confidence\n",
    "        overall_confidence = (success_rate * 0.6) + (avg_confidence * 0.4)\n",
    "\n",
    "        if overall_confidence >= 0.8:\n",
    "            return \"high\"\n",
    "        elif overall_confidence >= 0.5:\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"low\"\n",
    "\n",
    "    def generate_final_response(self,\n",
    "                                user_input: str,\n",
    "                                tasks: List[Dict],\n",
    "                                model_assignments: Dict,\n",
    "                                execution_results: Dict,\n",
    "                                use_llm: bool = True) -> ResponseGenerationResult:\n",
    "        \"\"\"\n",
    "        Main response generation function\n",
    "        Integrates all stages into final response\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üé® STAGE 4: RESPONSE GENERATION\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        # Generate natural language response\n",
    "        if use_llm and self.llm_client:\n",
    "            print(\"ü§ñ Generating response with LLM...\")\n",
    "            natural_response = self.generate_response_with_llm(\n",
    "                user_input, tasks, model_assignments, execution_results\n",
    "            )\n",
    "        else:\n",
    "            print(\"üìù Generating response with rules...\")\n",
    "            natural_response = self.generate_response_rule_based(\n",
    "                user_input, tasks, model_assignments, execution_results\n",
    "            )\n",
    "\n",
    "        # Create summaries\n",
    "        task_summary = f\"Planned {len(tasks)} tasks: \" + \", \".join([t[\"task\"] for t in tasks])\n",
    "\n",
    "        model_summary = f\"Used models: \" + \", \".join([\n",
    "            f\"{assignment['model_id']}\" for assignment in model_assignments.values()\n",
    "        ])\n",
    "\n",
    "        successful = sum(1 for r in execution_results.values() if r.status == \"success\")\n",
    "        execution_summary = f\"Executed {len(execution_results)} tasks ({successful} successful)\"\n",
    "\n",
    "        # Calculate confidence\n",
    "        confidence = self.calculate_confidence_level(execution_results)\n",
    "\n",
    "        # Collect file paths\n",
    "        file_paths = [\n",
    "            r.resource_path for r in execution_results.values()\n",
    "            if r.resource_path\n",
    "        ]\n",
    "\n",
    "        # Collect structured results\n",
    "        structured_results = {\n",
    "            task_id: {\n",
    "                \"task_type\": result.task_type,\n",
    "                \"status\": result.status,\n",
    "                \"result\": result.result,\n",
    "                \"inference_time\": result.inference_time\n",
    "            }\n",
    "            for task_id, result in execution_results.items()\n",
    "        }\n",
    "\n",
    "        result = ResponseGenerationResult(\n",
    "            user_input=user_input,\n",
    "            natural_language_response=natural_response,\n",
    "            task_summary=task_summary,\n",
    "            model_summary=model_summary,\n",
    "            execution_summary=execution_summary,\n",
    "            confidence_level=confidence,\n",
    "            structured_results=structured_results,\n",
    "            file_paths=file_paths\n",
    "        )\n",
    "\n",
    "        print(f\"\\n‚úÖ Response generated (confidence: {confidence})\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    def display_final_response(self, response: ResponseGenerationResult):\n",
    "        \"\"\"Display the final response in a formatted way\"\"\"\n",
    "\n",
    "        print(f\"\\n{'üéØ FINAL RESPONSE':^70}\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        print(f\"\\nüìù User Request:\")\n",
    "        print(f\"   {response.user_input}\")\n",
    "\n",
    "        print(f\"\\nüí¨ Response:\")\n",
    "        print(\"-\"*70)\n",
    "        for line in response.natural_language_response.split('\\n'):\n",
    "            print(f\"   {line}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        print(f\"\\nüìä Summary:\")\n",
    "        print(f\"   ‚Ä¢ {response.task_summary}\")\n",
    "        print(f\"   ‚Ä¢ {response.model_summary}\")\n",
    "        print(f\"   ‚Ä¢ {response.execution_summary}\")\n",
    "        print(f\"   ‚Ä¢ Confidence: {response.confidence_level.upper()}\")\n",
    "\n",
    "        if response.file_paths:\n",
    "            print(f\"\\nüìÅ Generated Files:\")\n",
    "            for path in response.file_paths:\n",
    "                print(f\"   ‚Ä¢ {path}\")\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "\n",
    "# ============================================================================\n",
    "# COMPLETE HUGGINGGPT PIPELINE - ALL 4 STAGES\n",
    "# ============================================================================\n",
    "\n",
    "class CompleteHuggingGPTPipeline:\n",
    "    \"\"\"\n",
    "    Complete HuggingGPT System - All 4 Stages\n",
    "    1. Task Planning\n",
    "    2. Model Selection\n",
    "    3. Task Execution\n",
    "    4. Response Generation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 openrouter_api_key: str = None,\n",
    "                 hf_token: str = None):\n",
    "\n",
    "        print(\"üöÄ Initializing Complete HuggingGPT Pipeline (All 4 Stages)\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        self.openrouter_api_key = openrouter_api_key\n",
    "        self.hf_token = hf_token\n",
    "\n",
    "        # Note: Import other stages from your previous implementations\n",
    "        # Stage 1: Task Planning\n",
    "        # Stage 2: Model Selection\n",
    "        # Stage 3: Task Execution (from hugginggpt_execution_with_inference_client)\n",
    "        # Stage 4: Response Generation\n",
    "        self.response_generator = ResponseGenerator(openrouter_api_key)\n",
    "\n",
    "        print(\"‚úÖ Pipeline Ready!\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "    async def process_complete_request(self,\n",
    "                                       user_input: str,\n",
    "                                       tasks: List[Dict],\n",
    "                                       model_assignments: Dict,\n",
    "                                       execution_results: Dict,\n",
    "                                       use_llm_for_response: bool = True) -> ResponseGenerationResult:\n",
    "        \"\"\"\n",
    "        Process complete request through all 4 stages\n",
    "        (Assumes stages 1-3 are already completed)\n",
    "        \"\"\"\n",
    "\n",
    "        # Stage 4: Response Generation\n",
    "        final_response = self.response_generator.generate_final_response(\n",
    "            user_input=user_input,\n",
    "            tasks=tasks,\n",
    "            model_assignments=model_assignments,\n",
    "            execution_results=execution_results,\n",
    "            use_llm=use_llm_for_response\n",
    "        )\n",
    "\n",
    "        # Display formatted response\n",
    "        self.response_generator.display_final_response(final_response)\n",
    "\n",
    "        return final_response\n",
    "\n",
    "# ============================================================================\n",
    "# TEST FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "async def test_response_generation():\n",
    "    \"\"\"Test response generation with sample data\"\"\"\n",
    "\n",
    "    print(\"üß™ TESTING RESPONSE GENERATION\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Sample data (as if from previous stages)\n",
    "    user_input = \"Can you detect objects in my image and tell me what you see?\"\n",
    "\n",
    "    tasks = [\n",
    "        {\"id\": 0, \"task\": \"object-detection\", \"args\": {\"image\": \"test.jpg\"}},\n",
    "        {\"id\": 1, \"task\": \"image-to-text\", \"args\": {\"image\": \"test.jpg\"}}\n",
    "    ]\n",
    "\n",
    "    model_assignments = {\n",
    "        0: {\"model_id\": \"facebook/detr-resnet-50\"},\n",
    "        1: {\"model_id\": \"Salesforce/blip-image-captioning-base\"}\n",
    "    }\n",
    "\n",
    "    # Mock execution results\n",
    "    from dataclasses import dataclass\n",
    "\n",
    "    @dataclass\n",
    "    class MockResult:\n",
    "        task_id: int\n",
    "        task_type: str\n",
    "        status: str\n",
    "        result: Any\n",
    "        inference_time: float\n",
    "        model_used: str\n",
    "        error: Optional[str] = None\n",
    "        resource_path: Optional[str] = None\n",
    "\n",
    "    execution_results = {\n",
    "        0: MockResult(\n",
    "            task_id=0,\n",
    "            task_type=\"object-detection\",\n",
    "            status=\"success\",\n",
    "            result={\n",
    "                \"detections\": [\n",
    "                    {\"label\": \"cat\", \"score\": 0.95},\n",
    "                    {\"label\": \"couch\", \"score\": 0.87}\n",
    "                ],\n",
    "                \"count\": 2\n",
    "            },\n",
    "            inference_time=2.3,\n",
    "            model_used=\"facebook/detr-resnet-50\",\n",
    "            resource_path=\"outputs/task_0_data.json\"\n",
    "        ),\n",
    "        1: MockResult(\n",
    "            task_id=1,\n",
    "            task_type=\"image-to-text\",\n",
    "            status=\"success\",\n",
    "            result=\"a cat sitting on a couch\",\n",
    "            inference_time=1.5,\n",
    "            model_used=\"Salesforce/blip-image-captioning-base\",\n",
    "            resource_path=\"outputs/task_1_text.txt\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Initialize response generator\n",
    "    generator = ResponseGenerator(openrouter_api_key= os.getenv(\"OPENROUNTER_API_KEY\"))  # Use rule-based for demo\n",
    "\n",
    "    # Generate response\n",
    "    final_response = generator.generate_final_response(\n",
    "        user_input=user_input,\n",
    "        tasks=tasks,\n",
    "        model_assignments=model_assignments,\n",
    "        execution_results=execution_results,\n",
    "        use_llm=False\n",
    "    )\n",
    "\n",
    "    # Display\n",
    "    generator.display_final_response(final_response)\n",
    "\n",
    "    return generator, final_response\n",
    "\n",
    "# ============================================================================\n",
    "# JUPYTER STARTUP\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\" or \"ipykernel\" in __import__(\"sys\").modules:\n",
    "    print(\"=\"*70)\n",
    "    print(\"üé® HUGGINGGPT RESPONSE GENERATION - STAGE 4\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nüìã Test response generation:\")\n",
    "    print(\"\\n   generator, response = await test_response_generation()\")\n",
    "    print(\"\\n\" + \"=\"*70)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b18J8IAgdZU",
    "outputId": "76ff8ae4-ad33-4156-f7b7-6c9d1aac8722"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "üé® HUGGINGGPT RESPONSE GENERATION - STAGE 4\n",
      "======================================================================\n",
      "\n",
      "üìã Test response generation:\n",
      "\n",
      "   generator, response = await test_response_generation()\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "generator, response = await test_response_generation()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vu6X4-rUggX8",
    "outputId": "689f9391-2293-4387-8f04-2df600a3eeb4"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üß™ TESTING RESPONSE GENERATION\n",
      "======================================================================\n",
      "üé® Response Generator initialized with LLM\n",
      "\n",
      "======================================================================\n",
      "üé® STAGE 4: RESPONSE GENERATION\n",
      "======================================================================\n",
      "üìù Generating response with rules...\n",
      "\n",
      "‚úÖ Response generated (confidence: high)\n",
      "======================================================================\n",
      "\n",
      "                           üéØ FINAL RESPONSE                           \n",
      "======================================================================\n",
      "\n",
      "üìù User Request:\n",
      "   Can you detect objects in my image and tell me what you see?\n",
      "\n",
      "üí¨ Response:\n",
      "----------------------------------------------------------------------\n",
      "   Based on your request: 'Can you detect objects in my image and tell me what you see?', here's what I found:\n",
      "   \n",
      "   üîç Object Detection: I found 2 objects in the image:\n",
      "      ‚Ä¢ cat (confidence: 95.0%)\n",
      "      ‚Ä¢ couch (confidence: 87.0%)\n",
      "      üìÅ Saved to: outputs/task_0_data.json\n",
      "   \n",
      "   üí¨ Image Caption: a cat sitting on a couch\n",
      "      üìÅ Saved to: outputs/task_1_text.txt\n",
      "   \n",
      "   \n",
      "   ‚úÖ Confidence Level: HIGH\n",
      "      Successfully completed 2/2 tasks\n",
      "   \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üìä Summary:\n",
      "   ‚Ä¢ Planned 2 tasks: object-detection, image-to-text\n",
      "   ‚Ä¢ Used models: facebook/detr-resnet-50, Salesforce/blip-image-captioning-base\n",
      "   ‚Ä¢ Executed 2 tasks (2 successful)\n",
      "   ‚Ä¢ Confidence: HIGH\n",
      "\n",
      "üìÅ Generated Files:\n",
      "   ‚Ä¢ outputs/task_0_data.json\n",
      "   ‚Ä¢ outputs/task_1_text.txt\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ]
  }
 ]
}
