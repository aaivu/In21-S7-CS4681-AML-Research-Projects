{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# HuggingGPT Task Planning Implementation - Stage 1\n",
    "\n",
    "This notebook implements a simplified version of the task planning stage from the HuggingGPT paper. The code is divided into the following sections:\n",
    "\n",
    "1.  **Setup and Imports**: Install necessary packages and import libraries.\n",
    "2.  **Task Definitions**: Define available task types and the TaskNode data structure.\n",
    "3.  **HuggingGPTTaskPlanner Class**: Implement the core task planning logic, including LLM integrations.\n",
    "4.  **Example Usage and Testing**: Demonstrate how to use the planner with test cases and different LLM providers.\n",
    "5.  **Interactive Mode**: Provide an interactive loop for testing the planner."
   ],
   "id": "e41f66d60d5262df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Setup and Imports\n",
    "# =====================\n",
    "\n",
    "# Install required packages (run this cell first)\n",
    "# !pip install openai>=1.0.0 transformers torch requests json5 langchain-openai\n",
    "\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Optional: For LangChain + OpenRouter integration\n",
    "try:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    HAS_LANGCHAIN = True\n",
    "except ImportError:\n",
    "    HAS_LANGCHAIN = False\n",
    "    print(\"LangChain not installed. Install with: pip install langchain-openai\")\n",
    "\n",
    "# Optional: For OpenAI integration\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    HAS_OPENAI = True\n",
    "except ImportError:\n",
    "    HAS_OPENAI = False\n",
    "    print(\"OpenAI not installed. Install with: pip install openai>=1.0.0\")\n",
    "\n",
    "# Optional: For Hugging Face integration\n",
    "try:\n",
    "    from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "    import torch\n",
    "    HAS_TRANSFORMERS = True\n",
    "except ImportError:\n",
    "    HAS_TRANSFORMERS = False\n",
    "    print(\"Transformers not installed. Install with: pip install transformers torch\")\n",
    "\n",
    "# Optional: For free API alternatives\n",
    "try:\n",
    "    import requests\n",
    "    HAS_REQUESTS = True\n",
    "except ImportError:\n",
    "    HAS_REQUESTS = False"
   ],
   "id": "44477981e8893c00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pip install langchain-openai",
   "id": "af4ffe6b6b9af99b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 2. Task Definitions\n",
    "# ===================\n",
    "\n",
    "# Define available tasks\n",
    "class TaskType(Enum):\n",
    "    OBJECT_DETECTION = \"object-detection\"\n",
    "    IMAGE_TO_TEXT = \"image-to-text\"\n",
    "    IMAGE_CLASSIFICATION = \"image-cls\"\n",
    "    VISUAL_QUESTION_ANSWERING = \"visual-question-answering\"\n",
    "    POSE_DETECTION = \"pose-detection\"\n",
    "    POSE_TEXT_TO_IMAGE = \"pose-text-to-image\"\n",
    "    TEXT_TO_IMAGE = \"text-to-image\"\n",
    "    IMAGE_SEGMENTATION = \"image-segmentation\"\n",
    "    DEPTH_ESTIMATION = \"depth-estimation\"\n",
    "    TEXT_CLASSIFICATION = \"text-classification\"\n",
    "    TEXT_GENERATION = \"text-generation\"\n",
    "    SPEECH_TO_TEXT = \"speech-to-text\"\n",
    "    TEXT_TO_SPEECH = \"text-to-speech\"\n",
    "\n",
    "@dataclass\n",
    "class TaskNode:\n",
    "    task: str\n",
    "    id: int\n",
    "    dep: List[int]\n",
    "    args: Dict[str, Any]\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"task\": self.task,\n",
    "            \"id\": self.id,\n",
    "            \"dep\": self.dep,\n",
    "            \"args\": self.args\n",
    "        }"
   ],
   "id": "7b9f41d6935b538f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3. HuggingGPTTaskPlanner Class\n",
    "# ===============================\n",
    "\n",
    "class HuggingGPTTaskPlanner:\n",
    "    def __init__(self, available_tasks: List[str] = None,\n",
    "                 openai_api_key: str = None,\n",
    "                 openrouter_api_key: str = None,\n",
    "                 llm_provider: str = \"rule_based\"):\n",
    "        \"\"\"\n",
    "        Initialize the Task Planner\n",
    "\n",
    "        Args:\n",
    "            available_tasks: List of available task types\n",
    "            openai_api_key: OpenAI API key for direct OpenAI integration\n",
    "            openrouter_api_key: OpenRouter API key for accessing multiple models\n",
    "            llm_provider: LLM provider (\"openai\", \"openrouter\", \"huggingface_local\", \"huggingface_api\", \"ollama\", \"rule_based\")\n",
    "        \"\"\"\n",
    "        if available_tasks is None:\n",
    "            self.available_tasks = [task.value for task in TaskType]\n",
    "        else:\n",
    "            self.available_tasks = available_tasks\n",
    "\n",
    "        self.llm_provider = llm_provider\n",
    "\n",
    "        # Initialize OpenAI client (direct)\n",
    "        self.openai_client = None\n",
    "        if llm_provider == \"openai\":\n",
    "            if openai_api_key and HAS_OPENAI:\n",
    "                self.openai_client = OpenAI(api_key=openai_api_key)\n",
    "            elif HAS_OPENAI and os.getenv(\"OPENAI_API_KEY\"):\n",
    "                self.openai_client = OpenAI()\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  OpenAI API key not provided. Set OPENAI_API_KEY environment variable or pass api_key parameter.\")\n",
    "\n",
    "        # Initialize LangChain + OpenRouter client\n",
    "        self.langchain_client = None\n",
    "        if llm_provider == \"openrouter\":\n",
    "            if openrouter_api_key and HAS_LANGCHAIN:\n",
    "                self.langchain_client = ChatOpenAI(\n",
    "                    model=\"openai/gpt-4o-mini\",  # Default model\n",
    "                    api_key=openrouter_api_key,\n",
    "                    base_url=\"https://openrouter.ai/api/v1\",\n",
    "                    temperature=0.1,\n",
    "                    max_tokens=1000\n",
    "                )\n",
    "                print(\"‚úÖ OpenRouter + LangChain initialized successfully!\")\n",
    "            elif HAS_LANGCHAIN and os.getenv(\"OPENROUTER_API_KEY\"):\n",
    "                self.langchain_client = ChatOpenAI(\n",
    "                    model=\"openai/gpt-4o-mini\",\n",
    "                    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "                    base_url=\"https://openrouter.ai/api/v1\",\n",
    "                    temperature=0.1,\n",
    "                    max_tokens=1000\n",
    "                )\n",
    "                print(\"‚úÖ OpenRouter + LangChain initialized from environment!\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  OpenRouter API key not provided or LangChain not available.\")\n",
    "\n",
    "        # Initialize Hugging Face model (for local inference)\n",
    "        self.hf_pipeline = None\n",
    "\n",
    "        if llm_provider == \"huggingface_local\" and HAS_TRANSFORMERS:\n",
    "            self.initialize_huggingface_model()\n",
    "        elif llm_provider == \"huggingface_api\":\n",
    "            # Uses Hugging Face Inference API (free tier available)\n",
    "            self.hf_api_token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "            if not self.hf_api_token:\n",
    "                print(\"‚ö†Ô∏è  Set HUGGINGFACE_API_TOKEN environment variable for HF API access\")\n",
    "\n",
    "        # Ollama support (free local LLM)\n",
    "        self.ollama_url = \"http://localhost:11434\"  # Default Ollama URL\n",
    "\n",
    "        # Define demonstrations (examples from the paper)\n",
    "        self.demonstrations = [\n",
    "            {\n",
    "                \"input\": \"Can you tell me how many objects in e1.jpg?\",\n",
    "                \"output\": [{\"task\": \"object-detection\", \"id\": 0, \"dep\": [-1], \"args\": {\"image\": \"e1.jpg\"}}]\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"In e2.jpg, what's the animal and what's it doing?\",\n",
    "                \"output\": [\n",
    "                    {\"task\": \"image-to-text\", \"id\": 0, \"dep\": [-1], \"args\": {\"image\": \"e2.jpg\"}},\n",
    "                    {\"task\": \"image-cls\", \"id\": 1, \"dep\": [-1], \"args\": {\"image\": \"e2.jpg\"}},\n",
    "                    {\"task\": \"object-detection\", \"id\": 2, \"dep\": [-1], \"args\": {\"image\": \"e2.jpg\"}},\n",
    "                    {\"task\": \"visual-question-answering\", \"id\": 3, \"dep\": [-1], \"args\": {\"text\": \"what's the animal doing?\", \"image\": \"e2.jpg\"}}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"First generate a HED image of e3.jpg, then based on the HED image and a text 'a girl reading a book', create a new image as a response.\",\n",
    "                \"output\": [\n",
    "                    {\"task\": \"pose-detection\", \"id\": 0, \"dep\": [-1], \"args\": {\"image\": \"e3.jpg\"}},\n",
    "                    {\"task\": \"pose-text-to-image\", \"id\": 1, \"dep\": [0], \"args\": {\"text\": \"a girl reading a book\", \"image\": \"<resource>-0\"}}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        self.chat_logs = []  # Store conversation history\n",
    "\n",
    "    def initialize_huggingface_model(self, model_name: str = \"microsoft/DialoGPT-small\"):\n",
    "        \"\"\"Initialize Hugging Face model for local inference\"\"\"\n",
    "        try:\n",
    "            print(f\"üì• Loading Hugging Face model: {model_name}\")\n",
    "            print(\"‚è≥ This may take a few minutes for first-time download...\")\n",
    "\n",
    "            # Use a smaller, faster model for task planning\n",
    "            self.hf_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=model_name,\n",
    "                tokenizer=model_name,\n",
    "                max_length=512,\n",
    "                do_sample=True,\n",
    "                temperature=0.1\n",
    "            )\n",
    "            print(\"‚úÖ Hugging Face model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to load Hugging Face model: {e}\")\n",
    "            print(\"üí° Trying with a simpler model...\")\n",
    "            try:\n",
    "                # Fallback to a very small model\n",
    "                self.hf_pipeline = pipeline(\n",
    "                    \"text-generation\",\n",
    "                    model=\"distilgpt2\",\n",
    "                    max_length=256,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.1\n",
    "                )\n",
    "                print(\"‚úÖ Fallback model loaded successfully!\")\n",
    "            except Exception as e2:\n",
    "                print(f\"‚ùå Fallback model also failed: {e2}\")\n",
    "                self.hf_pipeline = None\n",
    "\n",
    "    def create_task_planning_prompt(self, user_input: str, chat_logs: List[str] = None) -> str:\n",
    "        \"\"\"Create the task planning prompt based on HuggingGPT format\"\"\"\n",
    "        available_tasks_str = \", \".join(self.available_tasks)\n",
    "\n",
    "        # Format demonstrations\n",
    "        demo_str = \"\"\n",
    "        for demo in self.demonstrations:\n",
    "            demo_str += f\"{demo['input']}\\n{json.dumps(demo['output'])}\\n\\n\"\n",
    "\n",
    "        # Format chat logs if available\n",
    "        chat_logs_str = \"\"\n",
    "        if chat_logs:\n",
    "            chat_logs_str = \"\\n\".join(chat_logs[-5:])  # Last 5 entries\n",
    "\n",
    "        prompt = f\"\"\"#1 Task Planning Stage - Parse user input into structured tasks.\n",
    "\n",
    "Format: [{{\"task\": task_name, \"id\": task_id, \"dep\": [dependency_ids], \"args\": {{\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}}}]\n",
    "\n",
    "Available tasks: {available_tasks_str}\n",
    "\n",
    "Dependencies:\n",
    "- \"dep\": [-1] for no dependencies\n",
    "- \"dep\": [0,1] for dependencies on tasks 0 and 1\n",
    "- Use \"<resource>-task_id\" to reference outputs from previous tasks\n",
    "\n",
    "Examples:\n",
    "\n",
    "{demo_str}\n",
    "\n",
    "Chat history: {chat_logs_str}\n",
    "\n",
    "Parse this user input into JSON task array:\n",
    "\"{user_input}\"\n",
    "\n",
    "Response (JSON array only):\"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def call_huggingface_local(self, prompt: str) -> str:\n",
    "        \"\"\"Call local Hugging Face model for task planning\"\"\"\n",
    "        if not self.hf_pipeline:\n",
    "            raise Exception(\"Hugging Face model not initialized\")\n",
    "\n",
    "        try:\n",
    "            # Simplified prompt for smaller models\n",
    "            simple_prompt = f\"Task planning for: {prompt}\\nJSON output:\"\n",
    "\n",
    "            outputs = self.hf_pipeline(\n",
    "                simple_prompt,\n",
    "                max_length=len(simple_prompt.split()) + 100,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=50256  # GPT-2 EOS token\n",
    "            )\n",
    "\n",
    "            response = outputs[0]['generated_text']\n",
    "            # Remove the input prompt from response\n",
    "            response = response[len(simple_prompt):].strip()\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Hugging Face model error: {e}\")\n",
    "            return \"[]\"\n",
    "\n",
    "    def call_huggingface_api(self, prompt: str, model: str = \"gpt2\") -> str:\n",
    "        \"\"\"Call Hugging Face Inference API (free tier available)\"\"\"\n",
    "        if not HAS_REQUESTS:\n",
    "            raise Exception(\"Requests library not available\")\n",
    "\n",
    "        url = f\"https://api-inference.huggingface.co/models/{model}\"\n",
    "        headers = {\"Authorization\": f\"Bearer {self.hf_api_token}\"}\n",
    "\n",
    "        payload = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 200,\n",
    "                \"temperature\": 0.1,\n",
    "                \"do_sample\": True\n",
    "            }\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=payload, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                if isinstance(result, list) and len(result) > 0:\n",
    "                    return result[0].get('generated_text', '[]')\n",
    "            return \"[]\"\n",
    "        except Exception as e:\n",
    "            print(f\"Hugging Face API error: {e}\")\n",
    "            return \"[]\"\n",
    "\n",
    "    def call_ollama(self, prompt: str, model: str = \"llama2\") -> str:\n",
    "        \"\"\"Call Ollama API for local LLM inference\"\"\"\n",
    "        if not HAS_REQUESTS:\n",
    "            raise Exception(\"Requests library not available\")\n",
    "\n",
    "        url = f\"{self.ollama_url}/api/generate\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": 0.1,\n",
    "                \"num_predict\": 200\n",
    "            }\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(url, json=payload, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                return result.get('response', '[]')\n",
    "            return \"[]\"\n",
    "        except Exception as e:\n",
    "            print(f\"Ollama API error: {e}\")\n",
    "            return \"[]\"\n",
    "\n",
    "    def call_llm(self, prompt: str) -> str:\n",
    "        \"\"\"Call the configured LLM provider\"\"\"\n",
    "        if self.llm_provider == \"openai\" and self.openai_client:\n",
    "            return self.call_openai_llm(prompt)\n",
    "        elif self.llm_provider == \"openrouter\" and self.langchain_client:\n",
    "            return self.call_openrouter_llm(prompt)\n",
    "        elif self.llm_provider == \"huggingface_local\" and self.hf_pipeline:\n",
    "            return self.call_huggingface_local(prompt)\n",
    "        elif self.llm_provider == \"huggingface_api\" and self.hf_api_token:\n",
    "            return self.call_huggingface_api(prompt)\n",
    "        elif self.llm_provider == \"ollama\":\n",
    "            return self.call_ollama(prompt)\n",
    "        else:\n",
    "            return \"[]\" # Fallback if no LLM is configured/available\n",
    "\n",
    "    def call_openrouter_llm(self, prompt: str) -> str:\n",
    "        \"\"\"Call OpenRouter via LangChain for task planning\"\"\"\n",
    "        if not self.langchain_client:\n",
    "            raise Exception(\"LangChain + OpenRouter client not initialized\")\n",
    "\n",
    "        try:\n",
    "            response = self.langchain_client.invoke(prompt)\n",
    "            return response.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"OpenRouter API error: {e}\")\n",
    "            return \"[]\"\n",
    "\n",
    "    def call_openai_llm(self, prompt: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "        \"\"\"Call OpenAI API for task planning\"\"\"\n",
    "        if not self.openai_client:\n",
    "            raise Exception(\"OpenAI client not initialized. Provide API key or set OPENAI_API_KEY environment variable.\")\n",
    "\n",
    "        try:\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"OpenAI API error: {e}\")\n",
    "            return \"[]\"\n",
    "\n",
    "    def parse_llm_response(self, response: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Parse LLM response to extract JSON tasks\"\"\"\n",
    "        try:\n",
    "            # Extract JSON array from response\n",
    "            json_match = re.search(r'\\[.*\\]', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group(0)\n",
    "                tasks = json.loads(json_str)\n",
    "                return tasks\n",
    "            else:\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing LLM response: {e}\")\n",
    "            return []\n",
    "\n",
    "    def fallback_parse_user_request(self, user_input: str) -> List[TaskNode]:\n",
    "        \"\"\"Fallback rule-based parsing when LLM is not available\"\"\"\n",
    "        tasks = []\n",
    "        task_id = 0\n",
    "\n",
    "        user_input_lower = user_input.lower()\n",
    "        image_files = re.findall(r'\\b\\w+\\.(jpg|jpeg|png|gif|bmp)\\b', user_input, re.IGNORECASE)\n",
    "\n",
    "        if \"how many objects\" in user_input_lower and image_files:\n",
    "            tasks.append(TaskNode(\n",
    "                task=\"object-detection\",\n",
    "                id=task_id,\n",
    "                dep=[-1],\n",
    "                args={\"image\": image_files[0]}\n",
    "            ))\n",
    "\n",
    "        elif \"what\" in user_input_lower and (\"animal\" in user_input_lower or \"doing\" in user_input_lower) and image_files:\n",
    "            tasks.extend([\n",
    "                TaskNode(task=\"image-to-text\", id=task_id, dep=[-1], args={\"image\": image_files[0]}),\n",
    "                TaskNode(task=\"image-cls\", id=task_id+1, dep=[-1], args={\"image\": image_files[0]}),\n",
    "                TaskNode(task=\"object-detection\", id=task_id+2, dep=[-1], args={\"image\": image_files[0]}),\n",
    "                TaskNode(task=\"visual-question-answering\", id=task_id+3, dep=[-1],\n",
    "                        args={\"text\": user_input, \"image\": image_files[0]})\n",
    "            ])\n",
    "\n",
    "        elif \"generate\" in user_input_lower and \"image\" in user_input_lower:\n",
    "            if \"pose\" in user_input_lower or \"hed\" in user_input_lower:\n",
    "                if image_files:\n",
    "                    tasks.append(TaskNode(task=\"pose-detection\", id=task_id, dep=[-1], args={\"image\": image_files[0]}))\n",
    "                    task_id += 1\n",
    "\n",
    "                text_match = re.search(r'\"([^\"]*)\"', user_input)\n",
    "                if text_match:\n",
    "                    text_content = text_match.group(1)\n",
    "                    tasks.append(TaskNode(\n",
    "                        task=\"pose-text-to-image\",\n",
    "                        id=task_id,\n",
    "                        dep=[task_id-1],\n",
    "                        args={\"text\": text_content, \"image\": f\"<resource>-{task_id-1}\"}\n",
    "                    ))\n",
    "\n",
    "        return tasks\n",
    "\n",
    "    def plan_tasks(self, user_input: str, use_llm: bool = True) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Main task planning function\"\"\"\n",
    "\n",
    "        if use_llm:\n",
    "            try:\n",
    "                provider_available = False\n",
    "\n",
    "                if self.llm_provider == \"openai\" and self.openai_client:\n",
    "                    print(\"ü§ñ Using OpenAI for task planning...\")\n",
    "                    provider_available = True\n",
    "                elif self.llm_provider == \"openrouter\" and self.langchain_client:\n",
    "                    print(\"üåê Using OpenRouter (GPT-4o-mini) for task planning...\")\n",
    "                    provider_available = True\n",
    "                elif self.llm_provider == \"huggingface_local\" and self.hf_pipeline:\n",
    "                    print(\"ü§ó Using local Hugging Face model for task planning...\")\n",
    "                    provider_available = True\n",
    "                elif self.llm_provider == \"huggingface_api\" and self.hf_api_token:\n",
    "                    print(\"üåê Using Hugging Face API for task planning...\")\n",
    "                    provider_available = True\n",
    "                elif self.llm_provider == \"ollama\":\n",
    "                    print(\"ü¶ô Using Ollama for task planning...\")\n",
    "                    provider_available = True\n",
    "\n",
    "                if provider_available:\n",
    "                    prompt = self.create_task_planning_prompt(user_input, self.chat_logs)\n",
    "                    print(f\"üìù Sending prompt to {self.llm_provider}...\")\n",
    "                    llm_response = self.call_llm(prompt)\n",
    "                    print(f\"üì® Received response from {self.llm_provider}\")\n",
    "                    tasks = self.parse_llm_response(llm_response)\n",
    "\n",
    "                    if tasks:\n",
    "                        print(f\"‚úÖ Successfully generated {len(tasks)} tasks using LLM\")\n",
    "                        # Print parsed tasks for debugging\n",
    "                        for i, task in enumerate(tasks):\n",
    "                            print(f\"   Task {i}: {task.get('task', 'unknown')} -> {task.get('args', {})}\")\n",
    "                    else:\n",
    "                        print(\"‚ö†Ô∏è  LLM returned no valid tasks, using fallback...\")\n",
    "                        task_nodes = self.fallback_parse_user_request(user_input)\n",
    "                        tasks = [task.to_dict() for task in task_nodes]\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è  LLM provider '{self.llm_provider}' not available, using rule-based parsing...\")\n",
    "                    task_nodes = self.fallback_parse_user_request(user_input)\n",
    "                    tasks = [task.to_dict() for task in task_nodes]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå LLM error: {e}\")\n",
    "                print(\"üîÑ Using fallback parsing...\")\n",
    "                task_nodes = self.fallback_parse_user_request(user_input)\n",
    "                tasks = [task.to_dict() for task in task_nodes]\n",
    "        else:\n",
    "            print(\"üìù Using rule-based parsing...\")\n",
    "            task_nodes = self.fallback_parse_user_request(user_input)\n",
    "            tasks = [task.to_dict() for task in task_nodes]\n",
    "\n",
    "        # Add to chat logs\n",
    "        self.chat_logs.append(f\"User: {user_input}\")\n",
    "        self.chat_logs.append(f\"Tasks: {json.dumps(tasks, indent=2)}\")\n",
    "\n",
    "        return tasks\n",
    "\n",
    "    def visualize_task_graph(self, tasks: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"Create a text-based visualization of task dependencies\"\"\"\n",
    "        if not tasks:\n",
    "            return \"No tasks to visualize\"\n",
    "\n",
    "        graph = \"Task Dependency Graph:\\n\"\n",
    "        graph += \"=\" * 30 + \"\\n\"\n",
    "\n",
    "        for task in tasks:\n",
    "            deps = task['dep']\n",
    "            dep_str = \"None\" if deps == [-1] else f\"Tasks {deps}\"\n",
    "            graph += f\"Task {task['id']}: {task['task']}\\n\"\n",
    "            graph += f\"  Dependencies: {dep_str}\\n\"\n",
    "            graph += f\"  Args: {task['args']}\\n\"\n",
    "            graph += \"-\" * 20 + \"\\n\"\n",
    "\n",
    "        return graph\n",
    "\n",
    "    def get_execution_order(self, tasks: List[Dict[str, Any]]) -> List[List[int]]:\n",
    "        \"\"\"Determine execution order based on dependencies\"\"\"\n",
    "        if not tasks:\n",
    "            return []\n",
    "\n",
    "        # Create dependency graph\n",
    "        task_deps = {task['id']: task['dep'] for task in tasks}\n",
    "        executed = set()\n",
    "        execution_order = []\n",
    "\n",
    "        while len(executed) < len(tasks):\n",
    "            current_batch = []\n",
    "            for task_id, deps in task_deps.items():\n",
    "                if task_id in executed:\n",
    "                    continue\n",
    "                # Check if all dependencies are satisfied\n",
    "                if all(dep == -1 or dep in executed for dep in deps):\n",
    "                    current_batch.append(task_id)\n",
    "\n",
    "            if not current_batch:\n",
    "                break  # Circular dependency or error\n",
    "\n",
    "            execution_order.append(current_batch)\n",
    "            executed.update(current_batch)\n",
    "\n",
    "        return execution_order"
   ],
   "id": "1fd26bbc668aaf4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 4. Example Usage and Testing\n",
    "# ============================\n",
    "\n",
    "# Example usage and configuration\n",
    "print(\"üöÄ HuggingGPT Task Planner - WITH OPENROUTER SUPPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"üí∞ COST-EFFECTIVE LLM Options:\")\n",
    "print(\"1. üåê OpenRouter + LangChain (RECOMMENDED - cheap GPT-4o-mini)\")\n",
    "print(\"2. üìù Rule-based (free, immediate)\")\n",
    "print(\"3. ü§ó Hugging Face Local (free, requires download)\")\n",
    "print(\"4. ü¶ô Ollama (free, requires local installation)\")\n",
    "print(\"5. ü§ñ Direct OpenAI (expensive)\")\n",
    "print()\n",
    "\n",
    "print(\"üéØ RECOMMENDED: OpenRouter with GPT-4o-mini\")\n",
    "print(\"   üí≤ ~$0.15 per 1M tokens (very cheap!)\")\n",
    "print(\"   üöÄ High quality task planning\")\n",
    "print(\"   üîë Just need OpenRouter API key\")\n",
    "print()\n",
    "\n",
    "# Configuration Examples:\n",
    "\n",
    "print(\"üîß Quick Setup with OpenRouter:\")\n",
    "print(\"   1. Get API key from: https://openrouter.ai/\")\n",
    "print(\"   2. Uncomment the OpenRouter setup below\")\n",
    "print()\n",
    "\n",
    "# RECOMMENDED: OpenRouter + LangChain Setup\n",
    "print(\"üéØ Setting up OpenRouter + LangChain (RECOMMENDED)\")\n",
    "\n",
    "# Your OpenRouter API key\n",
    "openrouter_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "try:\n",
    "    planner = HuggingGPTTaskPlanner(\n",
    "        openrouter_api_key=openrouter_key,\n",
    "        llm_provider=\"openrouter\"\n",
    "    )\n",
    "    print(\"‚úÖ OpenRouter + LangChain initialized successfully!\")\n",
    "    print(\"ü§ñ Using GPT-4o-mini for high-quality task planning\")\n",
    "    print(\"üí∞ Cost: ~$0.15 per 1M tokens\")\n",
    "    USE_LLM = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå OpenRouter setup failed: {e}\")\n",
    "    print(\"üîÑ Falling back to rule-based parsing...\")\n",
    "    planner = HuggingGPTTaskPlanner(llm_provider=\"rule_based\")\n",
    "    USE_LLM = False\n",
    "\n",
    "# Alternative configurations (comment out the above and uncomment one of these):\n",
    "\n",
    "# Rule-based only\n",
    "# planner = HuggingGPTTaskPlanner(llm_provider=\"rule_based\")\n",
    "# USE_LLM = False\n",
    "# print(\"üìù Using rule-based parsing (safe fallback)\")\n",
    "\n",
    "# Environment variable approach for OpenRouter\n",
    "# import os\n",
    "# os.environ[\"OPENROUTER_API_KEY\"] = \"sk-or-v1-your-full-key-here\"\n",
    "# planner = HuggingGPTTaskPlanner(llm_provider=\"openrouter\")\n",
    "# USE_LLM = True\n",
    "\n",
    "# Hugging Face Local (FREE)\n",
    "# planner = HuggingGPTTaskPlanner(llm_provider=\"huggingface_local\")\n",
    "# USE_LLM = True\n",
    "\n",
    "print(\"‚úÖ Planner ready!\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    \"Can you tell me how many objects in e1.jpg?\",\n",
    "    \"In e2.jpg, what's the animal and what's it doing?\",\n",
    "    \"First generate a HED image of e3.jpg, then based on the HED image and a text 'a girl reading a book', create a new image as a response.\",\n",
    "    \"Create an image of a sunset over mountains\",\n",
    "    \"What objects are in photo.png and describe the scene?\"\n",
    "]\n",
    "\n",
    "print(\"Running test cases:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    print(f\"\\nüîç Test Case {i}:\")\n",
    "    print(f\"Input: {test_case}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Plan tasks using the configured provider\n",
    "    tasks = planner.plan_tasks(test_case, use_llm=USE_LLM)\n",
    "\n",
    "    if tasks:\n",
    "        print(\"\\nüìã Generated Tasks:\")\n",
    "        print(json.dumps(tasks, indent=2))\n",
    "\n",
    "        print(\"\\nüîó Task Graph:\")\n",
    "        print(planner.visualize_task_graph(tasks))\n",
    "\n",
    "        print(\"‚ö° Execution Order:\")\n",
    "        execution_order = planner.get_execution_order(tasks)\n",
    "        for batch_idx, batch in enumerate(execution_order):\n",
    "            print(f\"  Batch {batch_idx + 1}: Tasks {batch}\")\n",
    "    else:\n",
    "        print(\"‚ùå No tasks generated\")\n",
    "\n",
    "    print(\"=\" * 50)"
   ],
   "id": "fc96f9bc7e28299b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 5. Interactive Mode\n",
    "# ===================\n",
    "\n",
    "print(\"\\nüéØ Interactive Task Planning\")\n",
    "print(\"Enter 'quit' to exit\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nEnter your request: \").strip()\n",
    "    if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "        break\n",
    "\n",
    "    if user_input:\n",
    "        print(f\"\\nüîç Processing: {user_input}\")\n",
    "        tasks = planner.plan_tasks(user_input, use_llm=USE_LLM)\n",
    "\n",
    "        if tasks:\n",
    "            print(\"\\nüìã Generated Tasks:\")\n",
    "            print(json.dumps(tasks, indent=2))\n",
    "            print(\"\\nüîó Task Dependencies:\")\n",
    "            print(planner.visualize_task_graph(tasks))\n",
    "\n",
    "            # Show execution order\n",
    "            execution_order = planner.get_execution_order(tasks)\n",
    "            if execution_order:\n",
    "                print(\"\\n‚ö° Execution Order:\")\n",
    "                for batch_idx, batch in enumerate(execution_order):\n",
    "                    print(f\"  Batch {batch_idx + 1}: Tasks {batch}\")\n",
    "        else:\n",
    "            print(\"‚ùå Could not parse the request into tasks\")\n",
    "\n",
    "print(\"\\n‚úÖ Task Planning Demo Complete!\")"
   ],
   "id": "ac27e214a4849c9b"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
