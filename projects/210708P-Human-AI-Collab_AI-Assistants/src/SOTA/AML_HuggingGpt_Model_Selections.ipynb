{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbbb3b82"
   },
   "source": [
    "# HuggingGPT Model Selection - Stage 2\n",
    "=====================================\n",
    "Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. This stage filters and ranks models, then uses an LLM for final selection."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a322a790",
    "outputId": "43351037-2510-4401-874e-2473c170e5d1"
   },
   "source": [
    "# Data classes\n",
    "import json\n",
    "import os\n",
    "\n",
    "import requests\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Optional: For LangChain + OpenRouter integration\n",
    "try:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    HAS_LANGCHAIN = True\n",
    "    print(\"LangChain + OpenRouter integration available.\")\n",
    "except ImportError:\n",
    "    HAS_LANGCHAIN = False\n",
    "    print(\"LangChain + OpenRouter integration not available.\")\n",
    "\n",
    "@dataclass\n",
    "class ModelInfo:\n",
    "    \"\"\"Information about a Hugging Face model\"\"\"\n",
    "    model_id: str\n",
    "    task_type: str\n",
    "    downloads: int\n",
    "    description: str\n",
    "    pipeline_tag: str\n",
    "    tags: List[str]\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "    def to_candidate_format(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to the format expected by the model selection prompt\"\"\"\n",
    "        return {\n",
    "            \"model_id\": self.model_id,\n",
    "            \"metadata\": {\n",
    "                \"downloads\": self.downloads,\n",
    "                \"pipeline_tag\": self.pipeline_tag,\n",
    "                \"tags\": self.tags\n",
    "            },\n",
    "            \"description\": self.description[:200] + \"...\" if len(self.description) > 200 else self.description\n",
    "        }"
   ],
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LangChain + OpenRouter integration available.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0b6c68fd"
   },
   "source": [
    "# Hugging Face Model Registry\n",
    "class HuggingFaceModelRegistry:\n",
    "    \"\"\"Manages Hugging Face model information and filtering\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.models_cache = {}\n",
    "        self.last_cache_update = 0\n",
    "        self.cache_duration = 3600  # 1 hour\n",
    "\n",
    "        # Task type mapping from HuggingGPT tasks to HF pipeline tags\n",
    "        self.task_mapping = {\n",
    "            \"object-detection\": [\"object-detection\", \"zero-shot-object-detection\"],\n",
    "            \"image-to-text\": [\"image-to-text\", \"image-captioning\"],\n",
    "            \"image-cls\": [\"image-classification\", \"zero-shot-image-classification\"],\n",
    "            \"visual-question-answering\": [\"visual-question-answering\"],\n",
    "            \"pose-detection\": [\"object-detection\", \"keypoint-detection\"],\n",
    "            \"pose-text-to-image\": [\"text-to-image\"],\n",
    "            \"text-to-image\": [\"text-to-image\"],\n",
    "            \"image-segmentation\": [\"image-segmentation\", \"semantic-segmentation\"],\n",
    "            \"depth-estimation\": [\"depth-estimation\"],\n",
    "            \"text-classification\": [\"text-classification\", \"zero-shot-classification\"],\n",
    "            \"text-generation\": [\"text-generation\", \"text2text-generation\"],\n",
    "            \"speech-to-text\": [\"automatic-speech-recognition\"],\n",
    "            \"text-to-speech\": [\"text-to-speech\", \"text-to-audio\"]\n",
    "        }\n",
    "\n",
    "    def get_hf_models_by_task(self, task_type: str, top_k: int = 5) -> List[ModelInfo]:\n",
    "        \"\"\"\n",
    "        Fetch top-k models from Hugging Face for a specific task type\n",
    "        Implements the filtering and ranking strategy from HuggingGPT\n",
    "        \"\"\"\n",
    "        # Check cache first\n",
    "        cache_key = f\"{task_type}_{top_k}\"\n",
    "        current_time = time.time()\n",
    "\n",
    "        if (cache_key in self.models_cache and\n",
    "            current_time - self.last_cache_update < self.cache_duration):\n",
    "            return self.models_cache[cache_key]\n",
    "\n",
    "        # Get pipeline tags for this task\n",
    "        pipeline_tags = self.task_mapping.get(task_type, [task_type])\n",
    "\n",
    "        models = []\n",
    "        for pipeline_tag in pipeline_tags:\n",
    "            try:\n",
    "                # Fetch models from Hugging Face API\n",
    "                url = \"https://huggingface.co/api/models\"\n",
    "                params = {\n",
    "                    \"pipeline_tag\": pipeline_tag,\n",
    "                    \"sort\": \"downloads\",\n",
    "                    \"direction\": -1,  # Descending order\n",
    "                    \"limit\": top_k * 2  # Get more to filter later\n",
    "                }\n",
    "\n",
    "                response = requests.get(url, params=params, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    hf_models = response.json()\n",
    "\n",
    "                    for model_data in hf_models[:top_k]:\n",
    "                        model_info = ModelInfo(\n",
    "                            model_id=model_data.get(\"modelId\", \"\"),\n",
    "                            task_type=task_type,\n",
    "                            downloads=model_data.get(\"downloads\", 0),\n",
    "                            description=self._extract_description(model_data),\n",
    "                            pipeline_tag=model_data.get(\"pipeline_tag\", pipeline_tag),\n",
    "                            tags=model_data.get(\"tags\", []),\n",
    "                            metadata=model_data\n",
    "                        )\n",
    "                        models.append(model_info)\n",
    "\n",
    "                        if len(models) >= top_k:\n",
    "                            break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching models for {pipeline_tag}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Sort by downloads and take top-k\n",
    "        models.sort(key=lambda x: x.downloads, reverse=True)\n",
    "        models = models[:top_k]\n",
    "\n",
    "        # Add fallback models if not enough found\n",
    "        if len(models) < top_k:\n",
    "            fallback_models = self._get_fallback_models(task_type, top_k - len(models))\n",
    "            models.extend(fallback_models)\n",
    "\n",
    "        # Cache results\n",
    "        self.models_cache[cache_key] = models\n",
    "        self.last_cache_update = current_time\n",
    "\n",
    "        return models\n",
    "\n",
    "    def _extract_description(self, model_data: Dict) -> str:\n",
    "        \"\"\"Extract description from model data\"\"\"\n",
    "        # Try to get description from various fields\n",
    "        description = \"\"\n",
    "\n",
    "        if \"description\" in model_data and model_data[\"description\"]:\n",
    "            description = model_data[\"description\"]\n",
    "        elif \"card_data\" in model_data and model_data[\"card_data\"]:\n",
    "            card_data = model_data[\"card_data\"]\n",
    "            if \"short_description\" in card_data:\n",
    "                description = card_data[\"short_description\"]\n",
    "\n",
    "        # If no description, create one from model ID and tags\n",
    "        if not description:\n",
    "            model_id = model_data.get(\"modelId\", \"\")\n",
    "            pipeline_tag = model_data.get(\"pipeline_tag\", \"\")\n",
    "            description = f\"A {pipeline_tag} model: {model_id}\"\n",
    "\n",
    "        return description\n",
    "\n",
    "    def _get_fallback_models(self, task_type: str, count: int) -> List[ModelInfo]:\n",
    "        \"\"\"Get fallback models when not enough models are found\"\"\"\n",
    "        fallback_mapping = {\n",
    "            \"object-detection\": [\n",
    "                \"facebook/detr-resnet-50\",\n",
    "                \"hustvl/yolos-tiny\"\n",
    "            ],\n",
    "            \"image-to-text\": [\n",
    "                \"Salesforce/blip-image-captioning-base\",\n",
    "                \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "            ],\n",
    "            \"image-cls\": [\n",
    "                \"google/vit-base-patch16-224\",\n",
    "                \"microsoft/resnet-50\"\n",
    "            ],\n",
    "            \"text-to-image\": [\n",
    "                \"runwayml/stable-diffusion-v1-5\",\n",
    "                \"CompVis/stable-diffusion-v1-4\"\n",
    "            ],\n",
    "            \"text-generation\": [\n",
    "                \"gpt2\",\n",
    "                \"microsoft/DialoGPT-medium\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        fallback_ids = fallback_mapping.get(task_type, [\"gpt2\"])\n",
    "        fallback_models = []\n",
    "\n",
    "        for i, model_id in enumerate(fallback_ids[:count]):\n",
    "            fallback_models.append(ModelInfo(\n",
    "                model_id=model_id,\n",
    "                task_type=task_type,\n",
    "                downloads=100000 - i * 1000,  # Simulated download count\n",
    "                description=f\"Fallback model for {task_type}\",\n",
    "                pipeline_tag=self.task_mapping.get(task_type, [task_type])[0],\n",
    "                tags=[task_type],\n",
    "                metadata={\"fallback\": True}\n",
    "            ))\n",
    "\n",
    "        return fallback_models"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ea698d1e"
   },
   "source": [
    "# HuggingGPT Model Selector\n",
    "class HuggingGPTModelSelector:\n",
    "    \"\"\"\n",
    "    Model Selection component that matches HuggingGPT's approach:\n",
    "    1. Filter models by task type\n",
    "    2. Rank by downloads\n",
    "    3. Select top-K candidates\n",
    "    4. Use LLM for final selection with in-context learning\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, openrouter_api_key: str = None, top_k_models: int = 5):\n",
    "        self.model_registry = HuggingFaceModelRegistry()\n",
    "        self.top_k_models = top_k_models\n",
    "\n",
    "        # Initialize LangChain client for model selection\n",
    "        self.llm_client = None\n",
    "        if openrouter_api_key and HAS_LANGCHAIN:\n",
    "            self.llm_client = ChatOpenAI(\n",
    "                model=\"openai/gpt-4o-mini\",\n",
    "                api_key=os.getenv(\"OPENROUTER_API_KEY\", openrouter_api_key),\n",
    "                base_url=\"https://openrouter.ai/api/v1\",\n",
    "                temperature=0.1,\n",
    "                max_tokens=200\n",
    "            )\n",
    "        print(\"LLM status: \",self.llm_client,HAS_LANGCHAIN)\n",
    "        # Demonstration examples for model selection (from HuggingGPT paper)\n",
    "        self.demonstrations = [\n",
    "            {\n",
    "                \"task\": {\"task\": \"object-detection\", \"args\": {\"image\": \"image1.jpg\"}},\n",
    "                \"candidates\": [\n",
    "                    {\"model_id\": \"facebook/detr-resnet-50\", \"downloads\": 500000, \"description\": \"End-to-End Object Detection model\"},\n",
    "                    {\"model_id\": \"hustvl/yolos-tiny\", \"downloads\": 200000, \"description\": \"You Only Look at One Sequence: Object Detection\"}\n",
    "                ],\n",
    "                \"selection\": {\"id\": \"facebook/detr-resnet-50\", \"reason\": \"Higher downloads and proven performance for object detection\"}\n",
    "            },\n",
    "            {\n",
    "                \"task\": {\"task\": \"text-to-image\", \"args\": {\"text\": \"a beautiful sunset\"}},\n",
    "                \"candidates\": [\n",
    "                    {\"model_id\": \"runwayml/stable-diffusion-v1-5\", \"downloads\": 1000000, \"description\": \"Stable Diffusion model for text-to-image generation\"},\n",
    "                    {\"model_id\": \"CompVis/stable-diffusion-v1-4\", \"downloads\": 800000, \"description\": \"Earlier version of Stable Diffusion\"}\n",
    "                ],\n",
    "                \"selection\": {\"id\": \"runwayml/stable-diffusion-v1-5\", \"reason\": \"More recent version with better performance\"}\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def create_model_selection_prompt(self, task: Dict[str, Any], candidates: List[ModelInfo]) -> str:\n",
    "        \"\"\"\n",
    "        Create the model selection prompt following HuggingGPT format\n",
    "        \"\"\"\n",
    "        # Format candidates\n",
    "        candidates_str = \"\"\n",
    "        for i, model in enumerate(candidates):\n",
    "            candidate_dict = model.to_candidate_format()\n",
    "            candidates_str += f'{{\"model_id\": \"{candidate_dict[\"model_id\"]}\", \"metadata\": {json.dumps(candidate_dict[\"metadata\"])}, \"description\": \"{candidate_dict[\"description\"]}\"}}\\n'\n",
    "\n",
    "        # Format demonstrations\n",
    "        demo_str = \"\"\n",
    "        for demo in self.demonstrations:\n",
    "            demo_str += f\"Task: {json.dumps(demo['task'])}\\n\"\n",
    "            demo_str += f\"Selection: {json.dumps(demo['selection'])}\\n\\n\"\n",
    "\n",
    "        prompt = f\"\"\"#2 Model Selection Stage - Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: {{\"id\": \"model_id\", \"reason\": \"your detailed reason for the choice\"}}.\n",
    "\n",
    "We have a list of models for you to choose from:\n",
    "{candidates_str}\n",
    "\n",
    "Examples:\n",
    "{demo_str}\n",
    "\n",
    "Current task to assign: {json.dumps(task)}\n",
    "\n",
    "Select the most appropriate model from the candidate list above. Consider factors like:\n",
    "- Task compatibility\n",
    "- Model popularity (downloads)\n",
    "- Model description relevance\n",
    "- Performance indicators\n",
    "\n",
    "Response (JSON format only):\"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def select_model_with_llm(self, task: Dict[str, Any], candidates: List[ModelInfo]) -> Dict[str, Any]:\n",
    "        \"\"\"Use LLM to select the best model from candidates\"\"\"\n",
    "        if not self.llm_client or not candidates:\n",
    "            print(\"LLM not initialized or no candidates, using fallback\")\n",
    "            return self.fallback_model_selection(task, candidates)\n",
    "\n",
    "        try:\n",
    "            prompt = self.create_model_selection_prompt(task, candidates)\n",
    "            response = self.llm_client.invoke(prompt)\n",
    "\n",
    "            # Parse JSON response\n",
    "            response_text = response.content.strip()\n",
    "\n",
    "            # Extract JSON from response\n",
    "            json_match = None\n",
    "            try:\n",
    "                # Try to parse the entire response as JSON\n",
    "                json_match = json.loads(response_text)\n",
    "            except:\n",
    "                # Try to find JSON in the response\n",
    "                import re\n",
    "                json_pattern = r'\\{[^}]*\"id\"[^}]*\\}'\n",
    "                matches = re.findall(json_pattern, response_text)\n",
    "                if matches:\n",
    "                    json_match = json.loads(matches[0])\n",
    "\n",
    "            if json_match and \"id\" in json_match:\n",
    "                # Validate that selected model is in candidates\n",
    "                selected_id = json_match[\"id\"]\n",
    "                for candidate in candidates:\n",
    "                    if candidate.model_id == selected_id:\n",
    "                        print(\"LLM selected model:\", selected_id)\n",
    "                        return {\n",
    "                            \"model_id\": selected_id,\n",
    "                            \"reason\": json_match.get(\"reason\", \"Selected by LLM\"),\n",
    "                            \"model_info\": candidate,\n",
    "                            \"selection_method\": \"llm\"\n",
    "                        }\n",
    "\n",
    "            # Fallback if LLM selection failed\n",
    "            print(\"LLM selection parsing failed, using fallback\")\n",
    "            return self.fallback_model_selection(task, candidates)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"LLM model selection error: {e}\")\n",
    "            return self.fallback_model_selection(task, candidates)\n",
    "\n",
    "    def fallback_model_selection(self, task: Dict[str, Any], candidates: List[ModelInfo]) -> Dict[str, Any]:\n",
    "        \"\"\"Fallback model selection based on downloads and task compatibility\"\"\"\n",
    "        if not candidates:\n",
    "            return {\n",
    "                \"model_id\": \"fallback-model\",\n",
    "                \"reason\": \"No candidates available\",\n",
    "                \"model_info\": None,\n",
    "                \"selection_method\": \"fallback\"\n",
    "            }\n",
    "\n",
    "        # Sort by downloads (already sorted, but just to be sure)\n",
    "        best_candidate = max(candidates, key=lambda x: x.downloads)\n",
    "\n",
    "        return {\n",
    "            \"model_id\": best_candidate.model_id,\n",
    "            \"reason\": f\"Highest downloads ({best_candidate.downloads:,}) for task type\",\n",
    "            \"model_info\": best_candidate,\n",
    "            \"selection_method\": \"downloads\"\n",
    "        }\n",
    "\n",
    "    def select_models_for_tasks(self, tasks: List[Dict[str, Any]]) -> Dict[int, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Main function to select models for all tasks\n",
    "        Implements the complete HuggingGPT model selection pipeline\n",
    "        \"\"\"\n",
    "        model_assignments = {}\n",
    "\n",
    "        print(f\"üîç Starting model selection for {len(tasks)} tasks...\")\n",
    "\n",
    "        for task in tasks:\n",
    "            task_id = task.get(\"id\", 0)\n",
    "            task_type = task.get(\"task\", \"\")\n",
    "\n",
    "            print(f\"\\nüìã Task {task_id}: {task_type}\")\n",
    "\n",
    "            # Step 1: Filter models by task type and get top-K\n",
    "            print(f\"  üîç Fetching top-{self.top_k_models} models for '{task_type}'...\")\n",
    "            candidates = self.model_registry.get_hf_models_by_task(task_type, self.top_k_models)\n",
    "\n",
    "            if candidates:\n",
    "                print(f\"  ‚úÖ Found {len(candidates)} candidate models:\")\n",
    "                for i, candidate in enumerate(candidates[:3]):  # Show top 3\n",
    "                    print(f\"    {i+1}. {candidate.model_id} ({candidate.downloads:,} downloads)\")\n",
    "\n",
    "                # Step 2: Use LLM for in-context task-model assignment\n",
    "                print(f\"  ü§ñ Selecting best model using LLM...\")\n",
    "                selection = self.select_model_with_llm(task, candidates)\n",
    "\n",
    "                print(f\"  ‚úÖ Selected: {selection['model_id']}\")\n",
    "                print(f\"  üí° Reason: {selection['reason']}\")\n",
    "\n",
    "                model_assignments[task_id] = selection\n",
    "            else:\n",
    "                print(f\"  ‚ùå No models found for task type: {task_type}\")\n",
    "                model_assignments[task_id] = {\n",
    "                    \"model_id\": \"no-model-found\",\n",
    "                    \"reason\": f\"No suitable models found for {task_type}\",\n",
    "                    \"model_info\": None,\n",
    "                    \"selection_method\": \"error\"\n",
    "                }\n",
    "\n",
    "        return model_assignments\n",
    "\n",
    "    def get_model_assignment_summary(self, assignments: Dict[int, Dict[str, Any]]) -> str:\n",
    "        \"\"\"Generate a summary of model assignments\"\"\"\n",
    "        summary = \"Model Assignment Summary:\\n\"\n",
    "        summary += \"=\" * 40 + \"\\n\"\n",
    "\n",
    "        for task_id, assignment in assignments.items():\n",
    "            summary += f\"Task {task_id}: {assignment['model_id']}\\n\"\n",
    "            summary += f\"  Method: {assignment['selection_method']}\\n\"\n",
    "            summary += f\"  Reason: {assignment['reason']}\\n\"\n",
    "\n",
    "            if assignment['model_info']:\n",
    "                summary += f\"  Downloads: {assignment['model_info'].downloads:,}\\n\"\n",
    "            summary += \"-\" * 20 + \"\\n\"\n",
    "\n",
    "        return summary"
   ],
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3c6b8f0e",
    "outputId": "278ea251-78bb-40ef-a892-fd328b1e4e85"
   },
   "source": [
    "# Example usage and testing\n",
    "def test_model_selection():\n",
    "    \"\"\"Test the model selection system with example tasks\"\"\"\n",
    "\n",
    "    print(\"üöÄ HuggingGPT Model Selection Demo\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Example tasks from task planning\n",
    "    example_tasks = [\n",
    "        {\"task\": \"object-detection\", \"id\": 0, \"dep\": [-1], \"args\": {\"image\": \"e1.jpg\"}},\n",
    "        {\"task\": \"image-to-text\", \"id\": 1, \"dep\": [-1], \"args\": {\"image\": \"e2.jpg\"}},\n",
    "        {\"task\": \"text-to-image\", \"id\": 2, \"dep\": [-1], \"args\": {\"text\": \"a beautiful sunset\"}}\n",
    "    ]\n",
    "\n",
    "    # Initialize model selector (replace with your OpenRouter key for LLM selection)\n",
    "    selector = HuggingGPTModelSelector(openrouter_api_key=os.getenv(\"OPENROUTER_API_KEY\", None))\n",
    "    # selector = HuggingGPTModelSelector()  # Will use fallback selection\n",
    "\n",
    "    # Select models for tasks\n",
    "    assignments = selector.select_models_for_tasks(example_tasks)\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(selector.get_model_assignment_summary(assignments))\n",
    "\n",
    "    return selector, assignments\n",
    "\n",
    "# Run the test\n",
    "if __name__ == \"__main__\":\n",
    "    selector, assignments = test_model_selection()"
   ],
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "üöÄ HuggingGPT Model Selection Demo\n",
      "==================================================\n",
      "LLM status:  client=<openai.resources.chat.completions.completions.Completions object at 0x7db128d7aed0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7db128d79370> root_client=<openai.OpenAI object at 0x7db1280b2d80> root_async_client=<openai.AsyncOpenAI object at 0x7db128d7b230> model_name='openai/gpt-4o-mini' temperature=0.1 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://openrouter.ai/api/v1' max_tokens=200 True\n",
      "üîç Starting model selection for 3 tasks...\n",
      "\n",
      "üìã Task 0: object-detection\n",
      "  üîç Fetching top-5 models for 'object-detection'...\n",
      "  ‚úÖ Found 5 candidate models:\n",
      "    1. tech4humans/yolov8s-signature-detector (41,151,738 downloads)\n",
      "    2. microsoft/table-transformer-detection (2,119,774 downloads)\n",
      "    3. microsoft/table-transformer-structure-recognition (1,057,417 downloads)\n",
      "  ü§ñ Selecting best model using LLM...\n",
      "LLM selected model: tech4humans/yolov8s-signature-detector\n",
      "  ‚úÖ Selected: tech4humans/yolov8s-signature-detector\n",
      "  üí° Reason: This model is specifically designed for object detection and has the highest number of downloads among the options, indicating strong popularity and likely performance. Its focus on signature detection suggests it may be well-suited for various object detection tasks.\n",
      "\n",
      "üìã Task 1: image-to-text\n",
      "  üîç Fetching top-5 models for 'image-to-text'...\n",
      "  ‚úÖ Found 5 candidate models:\n",
      "    1. Salesforce/blip-image-captioning-base (1,751,558 downloads)\n",
      "    2. Salesforce/blip-image-captioning-large (1,294,457 downloads)\n",
      "    3. breezedeus/pix2text-mfr (534,149 downloads)\n",
      "  ü§ñ Selecting best model using LLM...\n",
      "LLM selected model: Salesforce/blip-image-captioning-large\n",
      "  ‚úÖ Selected: Salesforce/blip-image-captioning-large\n",
      "  üí° Reason: This model is specifically designed for image-to-text tasks, has a high number of downloads indicating popularity and reliability, and is a larger version which typically offers better performance in generating captions compared to smaller models.\n",
      "\n",
      "üìã Task 2: text-to-image\n",
      "  üîç Fetching top-5 models for 'text-to-image'...\n",
      "  ‚úÖ Found 5 candidate models:\n",
      "    1. stable-diffusion-v1-5/stable-diffusion-v1-5 (2,998,117 downloads)\n",
      "    2. stabilityai/stable-diffusion-xl-base-1.0 (2,242,507 downloads)\n",
      "    3. black-forest-labs/FLUX.1-dev (1,734,543 downloads)\n",
      "  ü§ñ Selecting best model using LLM...\n",
      "LLM selected model: stabilityai/stable-diffusion-xl-base-1.0\n",
      "  ‚úÖ Selected: stabilityai/stable-diffusion-xl-base-1.0\n",
      "  üí° Reason: This model is a more recent version of the stable diffusion series, specifically designed for text-to-image tasks, and has a high number of downloads, indicating its popularity and reliability for generating images from text prompts.\n",
      "\n",
      "==================================================\n",
      "Model Assignment Summary:\n",
      "========================================\n",
      "Task 0: tech4humans/yolov8s-signature-detector\n",
      "  Method: llm\n",
      "  Reason: This model is specifically designed for object detection and has the highest number of downloads among the options, indicating strong popularity and likely performance. Its focus on signature detection suggests it may be well-suited for various object detection tasks.\n",
      "  Downloads: 41,151,738\n",
      "--------------------\n",
      "Task 1: Salesforce/blip-image-captioning-large\n",
      "  Method: llm\n",
      "  Reason: This model is specifically designed for image-to-text tasks, has a high number of downloads indicating popularity and reliability, and is a larger version which typically offers better performance in generating captions compared to smaller models.\n",
      "  Downloads: 1,294,457\n",
      "--------------------\n",
      "Task 2: stabilityai/stable-diffusion-xl-base-1.0\n",
      "  Method: llm\n",
      "  Reason: This model is a more recent version of the stable diffusion series, specifically designed for text-to-image tasks, and has a high number of downloads, indicating its popularity and reliability for generating images from text prompts.\n",
      "  Downloads: 2,242,507\n",
      "--------------------\n",
      "\n"
     ]
    }
   ]
  }
 ]
}
