{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport os\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")  # your Hugging Face token stored in Kaggle secrets\nos.environ[\"HF_TOKEN\"] = hf_token\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nrepo_id = \"O1-OPEN/OpenO1-Qwen-7B-v0.1\"  # full repo\nsubfolder = \"checkpoint-1000\"            # the model folder inside repo\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    repo_id,\n    subfolder=subfolder,\n    trust_remote_code=True,\n    token=hf_token\n)\n\n# Model\nmodel = AutoModelForCausalLM.from_pretrained(\n    repo_id,\n    subfolder=subfolder,\n    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n    device_map=\"auto\" if device == \"cuda\" else None,\n    trust_remote_code=True,\n    token=hf_token\n)\n\nmodel.eval()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# COMPREHENSIVE BENCHMARK EVALUATION\n# ==============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"BENCHMARK EVALUATION\")\nprint(\"=\"*80)\nMAX_LENGTH = 512\nMAX_NEW_TOKENS = 256\nimport re\nimport json\nfrom tqdm import tqdm\nfrom typing import Dict, List, Tuple\nimport numpy as np\nfrom datasets import load_dataset\n# Configuration\ntasks_to_run = [\"gsm8k\", \"math\", \"mmlu\", \"hellaswag\", \"arc_challenge\", \"bbh\"]\nMAX_SAMPLES = 50 # Limit samples per task for faster evaluation\nEVAL_BATCH_SIZE = 1  # Process one at a time for generation\n\ndef normalize_answer(text: str) -> str:\n    \"\"\"Normalize answer for comparison\"\"\"\n    text = text.lower().strip()\n    # Remove extra whitespace\n    text = ' '.join(text.split())\n    return text\n\ndef extract_numeric_answer(text: str) -> str:\n    \"\"\"Extract numeric answer from text\"\"\"\n    # Look for patterns like \"####\" followed by number (GSM8K format)\n    match = re.search(r'####\\s*(-?\\d+\\.?\\d*)', text)\n    if match:\n        return match.group(1)\n    \n    # Look for \"the answer is X\"\n    match = re.search(r'(?:answer is|equals?)\\s*[:\\-]?\\s*(-?\\d+\\.?\\d*)', text.lower())\n    if match:\n        return match.group(1)\n    \n    # Extract last number in text\n    numbers = re.findall(r'-?\\d+\\.?\\d*', text)\n    return numbers[-1] if numbers else \"\"\nMAX_LENGTH = 512\nMAX_NEW_TOKENS = 256\ndef extract_letter_answer(text: str) -> str:\n    \"\"\"Extract letter answer (A, B, C, D) from text\"\"\"\n    # Look for explicit answer format\n    match = re.search(r'(?:answer is|answer:|correct answer is)\\s*([A-D])', text, re.IGNORECASE)\n    if match:\n        return match.group(1).upper()\n    \n    # Look for standalone letter in parentheses or brackets\n    match = re.search(r'[\\(\\[]([A-D])[\\)\\]]', text, re.IGNORECASE)\n    if match:\n        return match.group(1).upper()\n    \n    # Last resort: first letter A-D that appears\n    match = re.search(r'\\b([A-D])\\b', text, re.IGNORECASE)\n    if match:\n        return match.group(1).upper()\n    \n    return \"\"\n\n# ==============================================================================\n# GSM8K Evaluation\n# ==============================================================================\ndef evaluate_gsm8k(model, tokenizer, num_samples=MAX_SAMPLES):\n    \"\"\"Evaluate on GSM8K dataset\"\"\"\n    print(\"\\n--- GSM8K Evaluation ---\")\n    ds = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n    ds = ds.select(range(min(num_samples, len(ds))))\n    \n    correct = 0\n    total = 0\n    \n    for item in tqdm(ds, desc=\"GSM8K\"):\n        question = item[\"question\"]\n        answer = item[\"answer\"].split(\"####\")[-1].strip()\n        \n        prompt = f\"Problem: {question}\\n\\nSolution:\"\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, \n                          max_length=MAX_LENGTH).to(model.device)\n        \n        with torch.no_grad():\n            output = model.generate(\n                **inputs,\n                max_new_tokens=MAX_NEW_TOKENS,\n                temperature=0.7,\n                do_sample=True,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n        \n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        predicted = extract_numeric_answer(response)\n        \n        if normalize_answer(predicted) == normalize_answer(answer):\n            correct += 1\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"GSM8K Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy\n\n# ==============================================================================\n# MATH Dataset Evaluation\n# ==============================================================================\ndef evaluate_math(model, tokenizer, num_samples=MAX_SAMPLES):\n    \"\"\"Evaluate on MATH dataset\"\"\"\n    print(\"\\n--- MATH Dataset Evaluation ---\")\n    ds = load_dataset(\"math_dataset\", \"algebra__linear_1d\", split=\"test\")\n    ds = ds.select(range(min(num_samples, len(ds))))\n    \n    correct = 0\n    total = 0\n    \n    for item in tqdm(ds, desc=\"MATH\"):\n        question = item[\"question\"]\n        answer = item[\"answer\"]\n        \n        prompt = f\"Problem: {question}\\n\\nSolution:\"\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n                          max_length=MAX_LENGTH).to(model.device)\n        \n        with torch.no_grad():\n            output = model.generate(\n                **inputs,\n                max_new_tokens=MAX_NEW_TOKENS,\n                temperature=0.7,\n                do_sample=True,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n        \n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        predicted = extract_numeric_answer(response)\n        actual = extract_numeric_answer(answer)\n        \n        if normalize_answer(predicted) == normalize_answer(actual):\n            correct += 1\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"MATH Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy\n\n# ==============================================================================\n# MMLU Evaluation\n# ==============================================================================\ndef evaluate_mmlu(model, tokenizer, num_samples=MAX_SAMPLES):\n    \"\"\"Evaluate on MMLU dataset\"\"\"\n    print(\"\\n--- MMLU Evaluation ---\")\n    ds = load_dataset(\"cais/mmlu\", \"abstract_algebra\", split=\"test\")\n    ds = ds.select(range(min(num_samples, len(ds))))\n    \n    correct = 0\n    total = 0\n    \n    for item in tqdm(ds, desc=\"MMLU\"):\n        question = item[\"question\"]\n        choices = item[\"choices\"]\n        answer_idx = item[\"answer\"]\n        \n        # Format multiple choice\n        choice_text = \"\\n\".join([f\"{chr(65+i)}. {c}\" for i, c in enumerate(choices)])\n        prompt = f\"Question: {question}\\n\\n{choice_text}\\n\\nAnswer:\"\n        \n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n                          max_length=MAX_LENGTH).to(model.device)\n        \n        with torch.no_grad():\n            output = model.generate(\n                **inputs,\n                max_new_tokens=10,\n                temperature=0.1,\n                do_sample=False,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n        \n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        predicted_letter = extract_letter_answer(response)\n        correct_letter = chr(65 + answer_idx)\n        \n        if predicted_letter == correct_letter:\n            correct += 1\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"MMLU Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy\n\n# ==============================================================================\n# ARC Challenge Evaluation\n# ==============================================================================\ndef evaluate_arc_challenge(model, tokenizer, num_samples=MAX_SAMPLES):\n    \"\"\"Evaluate on ARC Challenge dataset\"\"\"\n    print(\"\\n--- ARC Challenge Evaluation ---\")\n    ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"test\")\n    ds = ds.select(range(min(num_samples, len(ds))))\n    \n    correct = 0\n    total = 0\n    \n    for item in tqdm(ds, desc=\"ARC-C\"):\n        question = item[\"question\"]\n        choices = item[\"choices\"][\"text\"]\n        labels = item[\"choices\"][\"label\"]\n        answer = item[\"answerKey\"]\n        \n        # Format multiple choice\n        choice_text = \"\\n\".join([f\"{labels[i]}. {choices[i]}\" for i in range(len(choices))])\n        prompt = f\"Question: {question}\\n\\n{choice_text}\\n\\nAnswer:\"\n        \n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n                          max_length=MAX_LENGTH).to(model.device)\n        \n        with torch.no_grad():\n            output = model.generate(\n                **inputs,\n                max_new_tokens=10,\n                temperature=0.1,\n                do_sample=False,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n        \n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        predicted = extract_letter_answer(response)\n        \n        if predicted.upper() == answer.upper():\n            correct += 1\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"ARC Challenge Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy\n\n# ==============================================================================\n# BBH Evaluation\n# ==============================================================================\ndef evaluate_bbh(model, tokenizer, num_samples=MAX_SAMPLES):\n    \"\"\"Evaluate on BBH (Big Bench Hard) dataset\"\"\"\n    print(\"\\n--- BBH Evaluation ---\")\n    ds = load_dataset(\"lukaemon/bbh\", \"boolean_expressions\", split=\"test\")\n    ds = ds.select(range(min(num_samples, len(ds))))\n    \n    correct = 0\n    total = 0\n    \n    for item in tqdm(ds, desc=\"BBH\"):\n        question = item[\"input\"]\n        answer = item[\"target\"]\n        \n        prompt = f\"Question: {question}\\n\\nAnswer:\"\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n                          max_length=MAX_LENGTH).to(model.device)\n        \n        with torch.no_grad():\n            output = model.generate(\n                **inputs,\n                max_new_tokens=50,\n                temperature=0.1,\n                do_sample=False,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n        \n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        \n        # Check if answer is contained in response\n        if normalize_answer(answer) in normalize_answer(response):\n            correct += 1\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"BBH Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy\n\n# ==============================================================================\n# HellaSwag Evaluation\n# ==============================================================================\ndef evaluate_hellaswag(model, tokenizer, num_samples=MAX_SAMPLES):\n    \"\"\"Evaluate on HellaSwag dataset\"\"\"\n    print(\"\\n--- HellaSwag Evaluation ---\")\n    ds = load_dataset(\"hellaswag\", split=\"validation\")\n    ds = ds.select(range(min(num_samples, len(ds))))\n    \n    correct = 0\n    total = 0\n    \n    for item in tqdm(ds, desc=\"HellaSwag\"):\n        context = item[\"ctx\"]\n        endings = item[\"endings\"]\n        label = int(item[\"label\"])\n        \n        # Score each ending\n        scores = []\n        for ending in endings:\n            full_text = context + \" \" + ending\n            inputs = tokenizer(full_text, return_tensors=\"pt\", truncation=True,\n                             max_length=MAX_LENGTH).to(model.device)\n            \n            with torch.no_grad():\n                outputs = model(**inputs, labels=inputs[\"input_ids\"])\n                # Use negative loss as score (lower loss = better)\n                scores.append(-outputs.loss.item())\n        \n        # Predict the ending with highest score\n        predicted = np.argmax(scores)\n        \n        if predicted == label:\n            correct += 1\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"HellaSwag Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy\n\n# ==============================================================================\n# Run All Evaluations\n# ==============================================================================\ndef run_all_benchmarks(model, tokenizer, tasks=None):\n    \"\"\"Run all specified benchmarks\"\"\"\n    if tasks is None:\n        tasks = tasks_to_run\n    \n    results = {}\n    \n    if \"gsm8k\" in tasks:\n        results[\"gsm8k\"] = evaluate_gsm8k(model, tokenizer)\n    \n    if \"math\" in tasks:\n        results[\"math\"] = evaluate_math(model, tokenizer)\n    \n    if \"mmlu\" in tasks:\n        results[\"mmlu\"] = evaluate_mmlu(model, tokenizer)\n    \n    if \"arc_challenge\" in tasks:\n        results[\"arc_challenge\"] = evaluate_arc_challenge(model, tokenizer)\n    \n    if \"bbh\" in tasks:\n        results[\"bbh\"] = evaluate_bbh(model, tokenizer)\n    \n    if \"hellaswag\" in tasks:\n        results[\"hellaswag\"] = evaluate_hellaswag(model, tokenizer)\n    \n    return results\n\n# ==============================================================================\n# Main Evaluation\n# ==============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"STARTING BENCHMARK EVALUATION\")\nprint(\"=\"*80)\n\n# Load the trained model\nprint(\"Loading model...\")\nmodel.eval()\n\n# Run benchmarks\nresults = run_all_benchmarks(model, tokenizer, tasks_to_run)\n\n# Print summary\nprint(\"\\n\" + \"=\"*80)\nprint(\"BENCHMARK RESULTS SUMMARY\")\nprint(\"=\"*80)\nprint(results)\nfor task, accuracy in results.items():\n    print(f\"{task.upper()}: {accuracy*100:.2f}%\")\n\n# Calculate average\navg_accuracy = np.mean(list(results.values()))\nprint(f\"\\nAVERAGE ACCURACY: {avg_accuracy*100:.2f}%\")\n\n# Save results\nresults_with_avg = {**results, \"average\": avg_accuracy}\nwith open(\"benchmark_results.json\", \"w\") as f:\n    json.dump(results_with_avg, f, indent=2)\n\nprint(\"\\n✓ Results saved to benchmark_results.json\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4 - Small helper functions: generation, normalize answers, scoring\nimport numpy as np\n\ndef generate_text(prompt, max_new_tokens=128, do_sample=False, temperature=0.0):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=do_sample, temperature=temperature)\n    text = tokenizer.decode(out[0], skip_special_tokens=True)\n    # return only the generated suffix (after prompt)\n    if prompt in text:\n        return text.split(prompt, 1)[1].strip()\n    return text.strip()\n\n# math answer normalizer (simple)\ndef normalize_math_answer(text):\n    \n    # remove commas, words, extract first numeric-like token\n    s = text.strip().lower()\n    s = s.replace(\",\", \"\")\n    # common prefixes \"the answer is\", \"a:\"\n    s = re.sub(r\"^(answer[:\\s]*)\", \"\", s)\n    # try to find a number or fraction\n    m = re.search(r\"(-?\\d+(\\.\\d+)?(/\\d+)?)\", s)\n    if m:\n        return m.group(1)\n    # fallback: return cleaned text\n    return re.sub(r\"\\s+\", \" \", s).strip()\n\n# multiple-choice answer normalizer (for MMLU/Hellaswag/ARC etc.)\ndef normalize_choice(text):\n    t = text.strip().upper()\n    # prefer single letter A/B/C/D\n    m = re.search(r\"\\b([A-D])\\b\", t)\n    if m:\n        return m.group(1)\n    # maybe returns option text; we can't map automatically, return full text\n    return t\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_examples = 100  # set to None for full dataset (may be large); use small for quick testing\n\noutput_file = \"eval_results.json\"\ntasks_to_run = [\"gsm8k\", \"math\", \"mmlu\", \"hellaswag\", \"arc_challenge\", \"bbh\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5 - Evaluate GSM8K (exact-match numeric)\nfrom datasets import load_dataset\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nimport os, json, math, re\ndef eval_gsm8k(n=None):\n    ds = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n    if n:\n        ds = ds.select(range(min(n, len(ds))))\n    correct = 0\n    total = 0\n    for ex in tqdm(ds, desc=\"GSM8K\"):\n        prompt = ex[\"question\"] + \"\\n\\nLet's think step by step.\\nAnswer:\"\n        ans = generate_text(prompt, max_new_tokens=128, do_sample=False, temperature=0.0)\n        pred = normalize_math_answer(ans)\n        gold = normalize_math_answer(ex[\"answer\"])\n        total += 1\n        if pred == gold:\n            correct += 1\n    return {\"accuracy\": correct/total if total>0 else 0, \"correct\": correct, \"total\": total}\n\n# Quick run (use max_examples to limit)\ngsm8k_res = eval_gsm8k(n=max_examples)\nprint(\"GSM8K:\", gsm8k_res)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nimport os, json, math, re\n\n# Cell 6 - Evaluate Swallow-MATH (exact match of final answer)\ndef eval_swallow_math(n=None):\n    ds = load_dataset(\"tokyotech-llm/swallow-math\", split=\"test\")\n    if n:\n        ds = ds.select(range(min(n, len(ds))))\n    correct=0; total=0\n    for ex in tqdm(ds, desc=\"Swallow-MATH\"):\n        q = ex.get(\"question\", \"\")\n        a = ex.get(\"answer\", \"\")\n\n        # decode bytes if necessary\n        if isinstance(q, (bytes, bytearray)):\n            q = q.decode(\"utf-8\", errors=\"ignore\")\n        if isinstance(a, (bytes, bytearray)):\n            a = a.decode(\"utf-8\", errors=\"ignore\")\n\n        prompt = q + \"\\n\\nLet's think step by step.\\nAnswer:\"\n        ans = generate_text(prompt, max_new_tokens=256, do_sample=False)\n        pred = normalize_math_answer(ans)\n        gold = normalize_math_answer(a)\n\n        total += 1\n        if pred == gold and gold!=\"\":\n            correct += 1\n    return {\"accuracy\": correct/total if total>0 else 0, \"correct\": correct, \"total\": total}\n\n# May fail if dataset missing/format mismatch. Use max_examples small to debug.\ntry:\n    swallow_math_res = eval_swallow_math(n=max_examples)\n    print(\"Swallow-MATH:\", swallow_math_res)\nexcept Exception as e:\n    print(\"Swallow-MATH eval failed. Error:\", e)\n    swallow_math_res = {\"error\": str(e)}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nimport os, json, math, re\n# Cell 6 - Evaluate MATH (exact match of final boxed answer)\n# MATH dataset is large and has many long problems, use small n for debugging\ndef eval_math(n=None):\n    ds = load_dataset(\"math_dataset\", \"algebra__linear_1d\", split=\"test\")  # dataset id may vary; fallback to 'math'\n    # If the above fails, use datasets 'MATH' / check available config; handle exceptions\n    # For safety below, try a few dataset names\n    if len(ds)==0:\n        ds = load_dataset(\"MATH\", split=\"test\")\n    if n:\n        ds = ds.select(range(min(n, len(ds))))\n    correct=0; total=0\n    for ex in tqdm(ds, desc=\"MATH\"):\n        prompt = ex[\"question\"] + \"\\n\\nLet's think step by step.\\nAnswer:\"\n        ans = generate_text(prompt, max_new_tokens=256, do_sample=False)\n        pred = normalize_math_answer(ans)\n        # gold answer fields vary; try some keys\n        gold = ex.get(\"answer\") or ex.get(\"solution\") or \"\"\n        gold = normalize_math_answer(gold)\n        total += 1\n        if pred == gold and gold!=\"\":\n            correct += 1\n    return {\"accuracy\": correct/total if total>0 else 0, \"correct\": correct, \"total\": total}\n\n# May fail if dataset names differ. Use max_examples small to debug.\ntry:\n    math_res = eval_math(n=max_examples)\n    print(\"MATH:\", math_res)\nexcept Exception as e:\n    print(\"MATH eval failed (dataset name or format mismatch). Error:\", e)\n    math_res = {\"error\": str(e)}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7 - Evaluate multiple-choice tasks (MMLU, Hellaswag, ARC, BBH where applicable)\n# We'll handle MMLU (57 tasks aggregated) using the 'mmlu' dataset from HF; some datasets may require processing.\n# For BBH and others, dataset ids vary; we'll try standard names, otherwise leave as TODO.\n\ndef eval_multiple_choice(dataset_name, split=\"test\", n=None, prompt_template=None, choice_key=\"choices\", question_key=\"question\", answer_key=\"answer\"):\n    ds = load_dataset(dataset_name, split=split)\n    if n:\n        ds = ds.select(range(min(n, len(ds))))\n    correct=0; total=0\n    for ex in tqdm(ds, desc=dataset_name):\n        # Build a zero-shot prompt. Some datasets have different formats.\n        q = ex.get(question_key) or ex.get(\"question\") or ex.get(\"context\") or \"\"\n        choices = ex.get(choice_key) or ex.get(\"choices\") or ex.get(\"options\") or None\n        if choices is None:\n            # try alternative structures\n            if \"choices\" in ex:\n                choices = ex[\"choices\"]\n            else:\n                choices = []\n        # simple prompt\n        prompt = q + \"\\n\\nChoices:\\n\"\n        if isinstance(choices, dict):\n            # sometimes choices is a dict with \"text\" list\n            choice_list = choices.get(\"text\", [])\n        else:\n            choice_list = choices if isinstance(choices, list) else []\n        for i, ch in enumerate(choice_list):\n            prompt += f\"{chr(65+i)}. {ch}\\n\"\n        prompt += \"\\nAnswer (choose letter):\"\n        out = generate_text(prompt, max_new_tokens=30, do_sample=False)\n        pred = normalize_choice(out)\n        gold = ex.get(answer_key)\n        # gold may be index or letter or text - try to normalize\n        if isinstance(gold, int):\n            gold_letter = chr(65+gold)\n        elif isinstance(gold, str) and gold.strip().upper() in [\"A\",\"B\",\"C\",\"D\"]:\n            gold_letter = gold.strip().upper()\n        else:\n            # try to map gold text to a letter\n            gold_letter = None\n            # if gold is text, find matching choice\n            for i,ch in enumerate(choice_list):\n                if isinstance(gold,str) and gold.strip().lower() in str(ch).lower():\n                    gold_letter = chr(65+i)\n                    break\n        total += 1\n        if gold_letter and pred==gold_letter:\n            correct += 1\n    return {\"accuracy\": correct/total if total>0 else 0, \"correct\": correct, \"total\": total}\n\n# Example: Hellaswag\ntry:\n    hellaswag_res = eval_multiple_choice(\"hellaswag\", split=\"validation\", n=max_examples, question_key=\"context\", choice_key=\"endings\", answer_key=\"label\")\n    print(\"Hellaswag:\", hellaswag_res)\nexcept Exception as e:\n    print(\"Hellaswag eval failed:\", e)\n    hellaswag_res = {\"error\": str(e)}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8 - Try MMLU (use 'mmlu' dataset from HF if installed)\ntry:\n    # MMLU has many subject subdatasets; to get full MMLU use the 'mmlu' dataset or process per subject\n    mmlu_ds = load_dataset(\"cais/mmlu\", \"abstract_algebra\", split=\"test\")\n    if max_examples:\n        mmlu_ds = mmlu_ds.select(range(min(max_examples, len(mmlu_ds))))\n    # MMLU examples often have 'question', 'options', 'answer'\n    def run_mmlu(ds):\n        correct=0; total=0\n        for ex in tqdm(ds, desc=\"MMLU\"):\n            q = ex.get(\"input\") or ex.get(\"question\") or ex.get(\"prompt\") or \"\"\n            options = ex.get(\"options\") or ex.get(\"choices\") or ex.get(\"targets\") or []\n            # build prompt\n            prompt = q + \"\\n\\nChoices:\\n\"\n            for i,opt in enumerate(options):\n                prompt += f\"{chr(65+i)}. {opt}\\n\"\n            prompt += \"\\nAnswer (choose letter):\"\n            out = generate_text(prompt, max_new_tokens=20)\n            pred = normalize_choice(out)\n            gold = ex.get(\"output\") or ex.get(\"answer\")\n            gold_letter = None\n            if isinstance(gold, int):\n                gold_letter = chr(65+gold)\n            elif isinstance(gold, str) and gold.strip().upper() in [\"A\",\"B\",\"C\",\"D\"]:\n                gold_letter = gold.strip().upper()\n            else:\n                # attempt to map by text match\n                for i,opt in enumerate(options):\n                    if isinstance(gold,str) and gold.strip().lower() in str(opt).lower():\n                        gold_letter = chr(65+i)\n                        break\n            total += 1\n            if gold_letter and pred==gold_letter:\n                correct += 1\n        return {\"accuracy\": correct/total if total>0 else 0, \"correct\": correct, \"total\": total}\n\n    mmlu_res = run_mmlu(mmlu_ds)\n    print(\"MMLU (sample):\", mmlu_res)\nexcept Exception as e:\n    print(\"MMLU eval failed:\", e)\n    mmlu_res = {\"error\": str(e)}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nfrom tqdm.auto import tqdm\n\n# ---------- ARC-Challenge ----------\ndef eval_arc_challenge(n=None):\n    ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"test\")\n    if n:\n        ds = ds.select(range(min(n, len(ds))))\n    correct, total = 0, 0\n    for ex in tqdm(ds, desc=\"ARC-Challenge\"):\n        q = ex.get(\"question\", \"\")\n        choices = ex.get(\"choices\", {}).get(\"text\", [])\n        ans_key = ex.get(\"answerKey\", \"\")\n\n        # Build multiple-choice style prompt\n        choice_str = \"\\n\".join([f\"{chr(65+i)}. {c}\" for i, c in enumerate(choices)])\n        prompt = q + \"\\n\" + choice_str + \"\\n\\nAnswer:\"\n        \n        model_out = generate_text(prompt, max_new_tokens=64, do_sample=False)\n        pred = model_out.strip().upper()[:1]  # first char like \"A\",\"B\",\"C\"\n        \n        total += 1\n        if pred == ans_key:\n            correct += 1\n    return {\"accuracy\": correct/total if total>0 else 0, \"correct\": correct, \"total\": total}\n\n\n# ---------- BBH (boolean_expressions subset) ----------\ndef eval_bbh_boolean(n=None):\n    ds = load_dataset(\"lukaemon/bbh\", \"boolean_expressions\", split=\"test\")\n    if n:\n        ds = ds.select(range(min(n, len(ds))))\n    correct, total = 0, 0\n    for ex in tqdm(ds, desc=\"BBH-Boolean\"):\n        q = ex.get(\"input\", \"\")\n        gold = ex.get(\"target\", \"\")\n\n        prompt = q + \"\\n\\nAnswer:\"\n        ans = generate_text(prompt, max_new_tokens=128, do_sample=False)\n        pred = ans.strip().lower()\n        gold = gold.strip().lower()\n\n        total += 1\n        if pred == gold:\n            correct += 1\n    return {\"accuracy\": correct/total if total>0 else 0, \"correct\": correct, \"total\": total}\n\n\n# ---------- Run them ----------\ntry:\n    arc_res = eval_arc_challenge(n=max_examples)\n    print(\"ARC-Challenge:\", arc_res)\nexcept Exception as e:\n    print(\"ARC-Challenge eval failed:\", e)\n    arc_res = {\"error\": str(e)}\n\ntry:\n    bbh_res = eval_bbh_boolean(n=max_examples)\n    print(\"BBH-Boolean:\", bbh_res)\nexcept Exception as e:\n    print(\"BBH-Boolean eval failed:\", e)\n    bbh_res = {\"error\": str(e)}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}