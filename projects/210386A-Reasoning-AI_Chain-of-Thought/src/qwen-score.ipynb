{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n# CELL 0: Install/Upgrade Required Packages\n# ==============================================================================\n# Run this first, then restart kernel\n\"\"\"\n!pip uninstall -y transformers accelerate trl peft -y\n!pip install transformers==4.36.2 --no-cache-dir\n!pip install accelerate==0.25.0 --no-cache-dir\n!pip install peft==0.7.1 --no-cache-dir\n!pip install trl==0.7.4 --no-cache-dir\n!pip install bitsandbytes==0.41.3 --no-cache-dir\n!pip install datasets --no-cache-dir\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# RUN THIS CELL FIRST - Install/Upgrade BitsAndBytes\n# ==============================================================================\nimport subprocess\nimport sys\n\nprint(\"=\"*80)\nprint(\"INSTALLING/UPGRADING BITSANDBYTES\")\nprint(\"=\"*80)\n\n# Method 1: Try standard upgrade\ntry:\n    print(\"\\n1. Upgrading bitsandbytes to latest version...\")\n    result = subprocess.run(\n        [sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"bitsandbytes\"],\n        capture_output=True,\n        text=True\n    )\n    print(result.stdout)\n    if result.returncode == 0:\n        print(\"Successfully upgraded bitsandbytes\")\n    else:\n        print(\"Upgrade had some issues, trying alternative method...\")\n        raise Exception(\"Standard install failed\")\nexcept Exception as e:\n    # Method 2: Try with specific version\n    print(\"Trying to install specific version (0.41.0)...\")\n    try:\n        subprocess.check_call(\n            [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"bitsandbytes==0.41.0\"],\n            stdout=subprocess.PIPE\n        )\n        print(\"Installed bitsandbytes 0.41.0\")\n    except:\n        print(\"Could not install specific version\")\n\n# Verify installation\nprint(\"\\n3. Verifying installation...\")\ntry:\n    import bitsandbytes as bnb\n    print(f\"✓ bitsandbytes version: {bnb.__version__}\")\n    print(\"✓ Import successful!\")\nexcept Exception as e:\n    print(f\"✗ Import failed: {e}\")\n    print(\"\\n⚠ IMPORTANT: If bitsandbytes still doesn't work:\")\n    print(\"   - Set USE_QUANTIZATION = False in the config\")\n    print(\"   - The code will automatically fall back to FP16\")\n    print(\"   - You'll need more GPU memory but it will work\")\n\n# Also upgrade related packages\nprint(\"\\n4. Upgrading related packages...\")\ntry:\n    subprocess.check_call(\n        [sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"-q\", \"accelerate\", \"transformers\"],\n    )\n    print(\"Upgraded accelerate and transformers\")\nexcept:\n    print(\"Could not upgrade all packages\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"INSTALLATION COMPLETE \")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T07:04:19.045211Z","iopub.execute_input":"2025-10-04T07:04:19.045481Z","iopub.status.idle":"2025-10-04T07:05:58.714971Z","shell.execute_reply.started":"2025-10-04T07:04:19.045457Z","shell.execute_reply":"2025-10-04T07:05:58.714203Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nINSTALLING/UPGRADING BITSANDBYTES\n================================================================================\n\n1. Upgrading bitsandbytes to latest version...\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.3->bitsandbytes)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.3->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.1/60.1 MB 30.2 MB/s eta 0:00:00\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 4.7 MB/s eta 0:00:00\nDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 94.8 MB/s eta 0:00:00\nDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 71.6 MB/s eta 0:00:00\nDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 54.7 MB/s eta 0:00:00\nDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 2.6 MB/s eta 0:00:00\nDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 5.2 MB/s eta 0:00:00\nDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 31.2 MB/s eta 0:00:00\nDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 13.8 MB/s eta 0:00:00\nDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 1.9 MB/s eta 0:00:00\nDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 74.2 MB/s eta 0:00:00\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed bitsandbytes-0.48.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n\n✓ Successfully upgraded bitsandbytes\n\n3. Verifying installation...\n✓ bitsandbytes version: 0.48.1\n✓ Import successful!\n\n4. Upgrading related packages...\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.4/41.4 kB 1.5 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 374.9/374.9 kB 8.6 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.0/12.0 MB 89.5 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 564.3/564.3 kB 36.2 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 82.4 MB/s eta 0:00:00\n✓ Upgraded accelerate and transformers\n\n================================================================================\nINSTALLATION COMPLETE - Now run the main setup cell\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# # # ==============================================================================\n# # # CELL 1: Install Required Packages\n# # # ==============================================================================\n# # print(\"Installing required packages...\")\n# # import subprocess\n# # import sys\n\n# # # Install bitsandbytes\n# # try:\n# #     import bitsandbytes\n# #     print(\"✓ bitsandbytes already installed\")\n# # except ImportError:\n# #     print(\"Installing bitsandbytes...\")\n# #     subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"bitsandbytes\"])\n# #     print(\"✓ bitsandbytes installed successfully\")\n\n# # ==============================================================================\n# # CELL 2: Imports and Configuration\n# # ==============================================================================\n# import os\n# import torch\n# import torch.nn.functional as F\n# from transformers import (\n#     AutoTokenizer, \n#     AutoModelForCausalLM, \n#     BitsAndBytesConfig,\n# )\n# from peft import (\n#     LoraConfig, \n#     get_peft_model, \n#     prepare_model_for_kbit_training,\n#     PeftModel\n# )\n# from datasets import Dataset, load_dataset\n# import numpy as np\n# from typing import List, Dict, Tuple\n# import re\n# from torch.optim import AdamW\n# from torch.utils.data import DataLoader\n\n# # Try to get HF token\n# try:\n#     from kaggle_secrets import UserSecretsClient\n#     hf = UserSecretsClient()\n#     HF_TOKEN = hf.get_secret(\"HF_TOKEN\")\n# except:\n#     # If not on Kaggle, try environment variable\n#     HF_TOKEN = os.environ.get(\"HF_TOKEN\", None)\n#     if not HF_TOKEN:\n#         print(\"Warning: No HF_TOKEN found. Using public models only.\")\n\n# # Configuration\n# REPO = \"O1-OPEN/OpenO1-Qwen-7B-v0.1\"\n# SUBFOLDER = \"checkpoint-1000\"\n# USE_SUBFOLDER = True\n# USE_QUANTIZATION = True  # Set to False if you have enough GPU memory\n# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# MAX_LENGTH = 512\n# MAX_NEW_TOKENS = 256\n\n# # Training hyperparameters\n# STAGE1_EPOCHS = 3\n# STAGE1_BATCH_SIZE = 2\n# STAGE1_GRAD_ACCUM = 8\n# STAGE1_LR = 2e-4\n# KL_COEF = 0.1\n\n# STAGE2_STEPS = 1000\n# STAGE2_BATCH_SIZE = 2\n# STAGE2_LR = 1e-5\n# CORRECTION_BONUS = 1.0\n\n# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# print(f\"Using device: {DEVICE}\")\n# print(f\"Available GPUs: {torch.cuda.device_count()}\")\n# if torch.cuda.is_available():\n#     print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n\n# # ==============================================================================\n# # CELL 3: Load Tokenizer\n# # ==============================================================================\n# print(\"\\nLoading tokenizer...\")\n# try:\n#     if USE_SUBFOLDER:\n#         tokenizer = AutoTokenizer.from_pretrained(\n#             REPO, \n#             subfolder=SUBFOLDER,\n#             trust_remote_code=True,\n#             token=HF_TOKEN\n#         )\n#     else:\n#         tokenizer = AutoTokenizer.from_pretrained(\n#             REPO,\n#             trust_remote_code=True,\n#             token=HF_TOKEN\n#         )\n# except Exception as e:\n#     print(f\"Error loading from {REPO}: {e}\")\n#     print(\"Falling back to Qwen2.5-1.5B-Instruct as alternative...\")\n#     REPO = \"Qwen/Qwen2.5-1.5B-Instruct\"\n#     USE_SUBFOLDER = False\n#     tokenizer = AutoTokenizer.from_pretrained(REPO, trust_remote_code=True)\n\n# # Set pad token\n# if tokenizer.pad_token is None:\n#     tokenizer.pad_token = tokenizer.eos_token\n#     tokenizer.pad_token_id = tokenizer.eos_token_id\n\n# print(f\"✓ Tokenizer loaded from: {REPO}\")\n\n# # ==============================================================================\n# # CELL 4: Load Models with Quantization\n# # ==============================================================================\n# print(\"\\nLoading models...\")\n\n# # Setup quantization config if enabled\n# if USE_QUANTIZATION and DEVICE == \"cuda\":\n#     print(\"Setting up 4-bit quantization...\")\n#     bnb_config = BitsAndBytesConfig(\n#         load_in_4bit=True,\n#         bnb_4bit_compute_dtype=torch.float16,\n#         bnb_4bit_use_double_quant=True,\n#         bnb_4bit_quant_type=\"nf4\",\n#     )\n#     model_kwargs = {\n#         \"quantization_config\": bnb_config,\n#         \"device_map\": \"auto\",\n#         \"trust_remote_code\": True,\n#         \"token\": HF_TOKEN,\n#     }\n# else:\n#     print(\"Loading model without quantization...\")\n#     model_kwargs = {\n#         \"device_map\": \"auto\",\n#         \"trust_remote_code\": True,\n#         \"token\": HF_TOKEN,\n#         \"torch_dtype\": torch.float16 if DEVICE == \"cuda\" else torch.float32,\n#     }\n\n# if USE_SUBFOLDER and \"Qwen2.5\" not in REPO:\n#     model_kwargs[\"subfolder\"] = SUBFOLDER\n\n# # Load base model\n# print(\"Loading base model (this may take several minutes)...\")\n# try:\n#     base_model = AutoModelForCausalLM.from_pretrained(REPO, **model_kwargs)\n#     base_model.config.use_cache = False\n#     print(f\"✓ Base model loaded\")\n#     if hasattr(base_model, 'hf_device_map'):\n#         print(f\"Device map: {base_model.hf_device_map}\")\n# except Exception as e:\n#     print(f\"Error loading model: {e}\")\n#     raise\n\n# # Load reference model (frozen)\n# print(\"Loading reference model (frozen)...\")\n# ref_model = AutoModelForCausalLM.from_pretrained(REPO, **model_kwargs)\n# ref_model.eval()\n# for p in ref_model.parameters():\n#     p.requires_grad = False\n# print(\"✓ Reference model loaded and frozen\")\n\n# # ==============================================================================\n# # CELL 5: Prepare Model with LoRA\n# # ==============================================================================\n# print(\"\\nPreparing model for training...\")\n# if USE_QUANTIZATION and DEVICE == \"cuda\":\n#     base_model = prepare_model_for_kbit_training(base_model)\n#     print(\"✓ Model prepared for k-bit training\")\n\n# # LoRA configuration - adjust target_modules based on model architecture\n# # Common patterns: \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\" for most models\n# lora_config = LoraConfig(\n#     r=16,\n#     lora_alpha=32,\n#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n#     lora_dropout=0.05,\n#     bias=\"none\",\n#     task_type=\"CAUSAL_LM\"\n# )\n\n# print(\"Attaching LoRA adapters...\")\n# model = get_peft_model(base_model, lora_config)\n# model.print_trainable_parameters()\n\n# # ==============================================================================\n# # CELL 6: Prepare Dataset\n# # ==============================================================================\n# def create_sample_dataset():\n#     \"\"\"Create a small sample dataset for testing\"\"\"\n#     samples = [\n#         {\n#             \"problem\": \"What is 25 + 17?\",\n#             \"first_attempt\": \"Let me calculate: 25 + 17 = 41\",\n#             \"second_attempt\": \"Let me recalculate: 25 + 17 = 42\",\n#             \"answer\": \"42\",\n#             \"is_correct_1\": False,\n#             \"is_correct_2\": True\n#         },\n#         {\n#             \"problem\": \"Solve: 3x + 5 = 14\",\n#             \"first_attempt\": \"3x = 14 - 5 = 9, so x = 4\",\n#             \"second_attempt\": \"3x = 14 - 5 = 9, so x = 3\",\n#             \"answer\": \"3\",\n#             \"is_correct_1\": False,\n#             \"is_correct_2\": True\n#         },\n#         {\n#             \"problem\": \"What is 15 * 8?\",\n#             \"first_attempt\": \"15 * 8 = 110\",\n#             \"second_attempt\": \"Let me recalculate: 15 * 8 = 120\",\n#             \"answer\": \"120\",\n#             \"is_correct_1\": False,\n#             \"is_correct_2\": True\n#         },\n#         {\n#             \"problem\": \"If y - 7 = 12, what is y?\",\n#             \"first_attempt\": \"y = 12 + 7 = 20\",\n#             \"second_attempt\": \"y = 12 + 7 = 19\",\n#             \"answer\": \"19\",\n#             \"is_correct_1\": False,\n#             \"is_correct_2\": True\n#         },\n#     ] * 25  # Repeat to create larger dataset\n#     return Dataset.from_list(samples)\n\n# print(\"\\nCreating/loading dataset...\")\n# train_dataset = create_sample_dataset()\n# print(f\"✓ Dataset size: {len(train_dataset)}\")\n# print(f\"Sample: {train_dataset[0]}\")\n\n# print(\"\\n\" + \"=\"*80)\n# print(\"SETUP COMPLETE! Ready for training.\")\n# print(\"=\"*80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# ==============================================================================\n# CELL 1: Imports and Configuration\n# ==============================================================================\nimport os\nimport torch\nimport torch.nn.functional as F\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    BitsAndBytesConfig,\n)\nfrom peft import (\n    LoraConfig, \n    get_peft_model, \n    prepare_model_for_kbit_training,\n    PeftModel\n)\n# Note: We'll implement a simplified PPO instead of using trl's PPOTrainer to avoid import issues\nfrom datasets import Dataset\nfrom kaggle_secrets import UserSecretsClient\nimport numpy as np\nfrom typing import List, Dict, Tuple\nimport re\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\n\n# Get HF token\nhf = UserSecretsClient()\nHF_TOKEN = hf.get_secret(\"HF_TOKEN\")\n\n# Configuration\nREPO = \"O1-OPEN/OpenO1-Qwen-7B-v0.1\"\nSUBFOLDER = \"checkpoint-1000\"\nUSE_SUBFOLDER = True\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMAX_LENGTH = 512\nMAX_NEW_TOKENS = 256\n\n# Training hyperparameters\nSTAGE1_EPOCHS = 3\nSTAGE1_BATCH_SIZE = 2\nSTAGE1_GRAD_ACCUM = 8\nSTAGE1_LR = 2e-4\nKL_COEF = 0.1  # KL penalty coefficient\n\nSTAGE2_STEPS = 1000\nSTAGE2_BATCH_SIZE = 2\nSTAGE2_LR = 1e-5\nCORRECTION_BONUS = 1.0  # Bonus when second attempt > first\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nprint(f\"Using device: {DEVICE}\")\nprint(f\"Available GPUs: {torch.cuda.device_count()}\")\n\n# ==============================================================================\n# CELL 2: Load Tokenizer and Models (4-bit Quantization)\n# ==============================================================================\nprint(\"Loading tokenizer...\")\nif USE_SUBFOLDER:\n    tokenizer = AutoTokenizer.from_pretrained(\n        REPO, \n        subfolder=SUBFOLDER,\n        trust_remote_code=True,\n        token=HF_TOKEN\n    )\nelse:\n    tokenizer = AutoTokenizer.from_pretrained(\n        REPO,\n        trust_remote_code=True,\n        token=HF_TOKEN\n    )\n\n# Set pad token\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\nprint(\"Setting up 4-bit quantization...\")\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\nprint(\"Loading base model (this may take several minutes)...\")\nmodel_kwargs = {\n    \"quantization_config\": bnb_config,\n    \"device_map\": \"auto\",  # Spreads across both T4s\n    \"trust_remote_code\": True,\n    \"token\": HF_TOKEN,\n}\nif USE_SUBFOLDER:\n    model_kwargs[\"subfolder\"] = SUBFOLDER\n\nbase_model = AutoModelForCausalLM.from_pretrained(REPO, **model_kwargs)\nbase_model.config.use_cache = False  # Required for gradient checkpointing\n\nprint(\"Loading reference model (frozen)...\")\nref_model = AutoModelForCausalLM.from_pretrained(REPO, **model_kwargs)\nref_model.eval()\nfor p in ref_model.parameters():\n    p.requires_grad = False\n\nprint(f\"Base model device map: {base_model.hf_device_map}\")\n\n# ==============================================================================\n# CELL 3: Prepare Model with LoRA\n# ==============================================================================\nprint(\"Preparing model for k-bit training...\")\nbase_model = prepare_model_for_kbit_training(base_model)\n\n# LoRA configuration\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Qwen modules\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nprint(\"Attaching LoRA adapters...\")\nmodel = get_peft_model(base_model, lora_config)\nmodel.print_trainable_parameters()\n\n# ==============================================================================\n# CELL 4: Prepare Dataset (Example Format)\n# ==============================================================================\n# Your dataset should have: problem, first_attempt, second_attempt, correctness\n# Format: {\"problem\": \"...\", \"first_attempt\": \"...\", \"second_attempt\": \"...\", \"answer\": \"...\", \"is_correct_1\": bool, \"is_correct_2\": bool}\ndef create_sample_dataset():\n    \"\"\"Create a small sample dataset for testing\"\"\"\n    samples = [\n        {\n            \"problem\": \"What is 25 + 17?\",\n            \"first_attempt\": \"Let me calculate: 25 + 17 = 41\",\n            \"second_attempt\": \"Let me recalculate: 25 + 17 = 42\",\n            \"answer\": \"42\",\n            \"is_correct_1\": False,\n            \"is_correct_2\": True\n        },\n        {\n            \"problem\": \"Solve: 3x + 5 = 14\",\n            \"first_attempt\": \"3x = 14 - 5 = 9, so x = 4\",\n            \"second_attempt\": \"3x = 14 - 5 = 9, so x = 3\",\n            \"answer\": \"3\",\n            \"is_correct_1\": False,\n            \"is_correct_2\": True\n        },\n        {\n            \"problem\": \"What is 15 * 8?\",\n            \"first_attempt\": \"15 * 8 = 110\",\n            \"second_attempt\": \"Let me recalculate: 15 * 8 = 120\",\n            \"answer\": \"120\",\n            \"is_correct_1\": False,\n            \"is_correct_2\": True\n        },\n        {\n            \"problem\": \"If y - 7 = 12, what is y?\",\n            \"first_attempt\": \"y = 12 + 7 = 20\",\n            \"second_attempt\": \"y = 12 + 7 = 19\",\n            \"answer\": \"19\",\n            \"is_correct_1\": False,\n            \"is_correct_2\": True\n        },\n    ] * 25  # Repeat to create larger dataset\n    return Dataset.from_list(samples)\n\n# def create_sample_dataset():\n#     \"\"\"Create a small sample dataset for testing\"\"\"\n#     samples = [\n#         {\n#             \"problem\": \"What is 25 + 17?\",\n#             \"first_attempt\": \"Let me calculate: 25 + 17 = 41\",\n#             \"second_attempt\": \"Let me recalculate: 25 + 17 = 42\",\n#             \"answer\": \"42\",\n#             \"is_correct_1\": False,\n#             \"is_correct_2\": True\n#         },\n#         {\n#             \"problem\": \"Solve: 3x + 5 = 14\",\n#             \"first_attempt\": \"3x = 14 - 5 = 9, so x = 4\",\n#             \"second_attempt\": \"3x = 14 - 5 = 9, so x = 3\",\n#             \"answer\": \"3\",\n#             \"is_correct_1\": False,\n#             \"is_correct_2\": True\n#         },\n#         # Add more examples...\n#     ]\n#     return Dataset.from_list(samples)\n\n# Load your actual dataset here\nprint(\"Creating/loading dataset...\")\ntrain_dataset = create_sample_dataset()\nprint(f\"Dataset size: {len(train_dataset)}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T07:06:36.354915Z","iopub.execute_input":"2025-10-04T07:06:36.355602Z","iopub.status.idle":"2025-10-04T07:10:07.281798Z","shell.execute_reply.started":"2025-10-04T07:06:36.355578Z","shell.execute_reply":"2025-10-04T07:10:07.281205Z"}},"outputs":[{"name":"stderr","text":"2025-10-04 07:06:41.084717: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759561601.252781      74 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759561601.303558      74 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nAvailable GPUs: 2\nLoading tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c2628b8df0c4ef49529a4e8451d48e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d976b81ce0240c2937eefb40e0fdf60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5644fd3c2d94ac69e72c53a6bb1e704"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81dfe6888ef848f6bd42407ccec49a51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f89d6b9b50014bf6a6cc74538fff81b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b809db1227740b7b97a196fefb75e19"}},"metadata":{}},{"name":"stdout","text":"Setting up 4-bit quantization...\nLoading base model (this may take several minutes)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/730 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21ee4854ba384be79cf76d5b8d302072"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edae6196ebdc483ab5bac6a072dedb7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d99318c8528741b3b79a02c2794ffeb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"checkpoint-1000/model-00004-of-00004.saf(…):   0%|          | 0.00/1.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad0794293dc6455c9196f868f03efd23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"checkpoint-1000/model-00001-of-00004.saf(…):   0%|          | 0.00/4.88G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14c4a6d5f5924753a66111b288c8936b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"checkpoint-1000/model-00002-of-00004.saf(…):   0%|          | 0.00/4.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1f3f6d048cc4fd2b3908f2160a9ae64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"checkpoint-1000/model-00003-of-00004.saf(…):   0%|          | 0.00/4.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bbe2b922c8c4167a679e45ffbf5a7e1"}},"metadata":{}},{"name":"stdout","text":"{\"timestamp\":\"2025-10-04T07:07:57.452672Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 504. Retrying...\",\"request_id\":\"\"},\"filename\":\"/home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n{\"timestamp\":\"2025-10-04T07:07:57.452739Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #0. Sleeping 2.031185106s before the next attempt\"},\"filename\":\"/root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e2c18b7bed84751880fbe1411c65b6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d2cf5bf66a94f5c8295ae3205367c06"}},"metadata":{}},{"name":"stdout","text":"Loading reference model (frozen)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cea507dbff1e447c8ee0b1942a5f421f"}},"metadata":{}},{"name":"stdout","text":"Base model device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\nPreparing model for k-bit training...\nAttaching LoRA adapters...\ntrainable params: 10,092,544 || all params: 7,625,709,056 || trainable%: 0.1323\nCreating/loading dataset...\nDataset size: 100\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# STAGE I - Supervised Fine-tuning with KL Penalty (Fixed)\n# ==============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"STAGE I: Supervised Fine-tuning with KL Penalty (Fixed)\")\nprint(\"=\"*80)\nfrom torch.utils.data import DataLoader\n\n# --- Step 1: Prepare DataLoader ---\ntrain_loader = DataLoader(train_dataset, batch_size=STAGE1_BATCH_SIZE, shuffle=True)\n\n# --- Step 2: Define KL Divergence ---\n# def compute_kl_divergence(logits_policy, logits_ref):\n#     \"\"\"\n#     Compute KL divergence between policy and reference model\n#     \"\"\"\n#     log_probs_policy = F.log_softmax(logits_policy, dim=-1)\n#     probs_ref = F.softmax(logits_ref, dim=-1)\n#     kl = (probs_ref * (probs_ref.log() - log_probs_policy)).sum(dim=-1)\n#     return kl.mean()\nimport torch\nimport torch.nn.functional as F\n\ndef compute_kl_divergence(policy_logits, ref_logits, attention_mask=None, eps=1e-12):\n    \"\"\"\n    Compute KL(P_ref || Q_policy) per token with masking and numeric stability,\n    returning the mean KL per sample.\n\n    Args:\n      policy_logits: Tensor [batch, seq_len, vocab]\n      ref_logits:    Tensor [batch, seq_len, vocab]\n      attention_mask: Optional Tensor [batch, seq_len] with 1 for real tokens, 0 for padding.\n      eps: small value to avoid div/zero (not usually needed with log_softmax but kept for safety).\n\n    Returns:\n      scalar tensor: mean KL across non-padding tokens (averaged over batch)\n    \"\"\"\n    # ensure shapes match\n    assert policy_logits.shape == ref_logits.shape, f\"policy {policy_logits.shape} vs ref {ref_logits.shape}\"\n\n    # stable log-probs\n    log_probs_policy = F.log_softmax(policy_logits, dim=-1)   # log Q\n    log_probs_ref = F.log_softmax(ref_logits, dim=-1)         # log P\n\n    # probs for P (ref) via exp(log_probs_ref) — numerically stable\n    probs_ref = log_probs_ref.exp()\n\n    # per-token KL: sum_vocab P * (log P - log Q)\n    kl_per_token = (probs_ref * (log_probs_ref - log_probs_policy)).sum(dim=-1)  # [batch, seq_len]\n\n    if attention_mask is not None:\n        # cast mask to same dtype\n        mask = attention_mask.to(kl_per_token.dtype)  # [batch, seq_len]\n        # zero out padding tokens, compute per-sample mean over valid tokens\n        valid_tokens_per_sample = mask.sum(dim=1).clamp_min(1.0)  # avoid div by 0\n        kl_per_sample = (kl_per_token * mask).sum(dim=1) / valid_tokens_per_sample\n    else:\n        # mean over seq_len when no mask provided\n        kl_per_sample = kl_per_token.mean(dim=1)\n\n    return kl_per_sample.mean()  # scalar\n\n# --- Step 3: Stage I training step ---\ndef stage1_train_step(batch, model, ref_model, optimizer, tokenizer):\n    model.train()\n    \n    # Prepare prompts and targets\n    prompts = [f\"Problem: {p}\\n\\nFirst attempt: {a1}\\n\\nLet me reconsider:\" \n               for p, a1 in zip(batch[\"problem\"], batch[\"first_attempt\"])]\n    targets = [f\"{t}\" for t in batch[\"second_attempt\"]]\n    \n    # Combine prompts and targets for proper tokenization\n    full_texts = [p + t for p, t in zip(prompts, targets)]\n    \n    # Tokenize the combined text\n    inputs = tokenizer(full_texts, padding='longest', truncation=True,\n                       max_length=MAX_LENGTH, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].to(model.device)\n    attention_mask = inputs[\"attention_mask\"].to(model.device)\n    \n    # Create labels by tokenizing prompts to find where targets start\n    prompt_inputs = tokenizer(prompts, padding='longest', truncation=True,\n                              max_length=MAX_LENGTH, return_tensors=\"pt\")\n    prompt_lengths = (prompt_inputs[\"attention_mask\"].sum(dim=1)).tolist()\n    \n    # Create labels: -100 for prompt tokens, actual tokens for target\n    labels = input_ids.clone()\n    for i, prompt_len in enumerate(prompt_lengths):\n        labels[i, :prompt_len] = -100\n    \n    # Replace padding tokens with -100\n    labels[labels == tokenizer.pad_token_id] = -100\n    \n    # Forward pass\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n    loss_lm = outputs.loss\n    \n    # --- KL penalty on first attempt ---\n    first_prompts = [f\"Problem: {p}\\n\\nSolution:\" for p in batch[\"problem\"]]\n    first_inputs = tokenizer(first_prompts, padding='longest', truncation=True,\n                             max_length=MAX_LENGTH, return_tensors=\"pt\")\n    first_inputs = {k: v.to(model.device) for k, v in first_inputs.items() \n                    if k in [\"input_ids\", \"attention_mask\"]}\n    \n    with torch.no_grad():\n        ref_outputs = ref_model(**first_inputs)\n        ref_logits = ref_outputs.logits\n    \n    policy_outputs = model(**first_inputs)\n    policy_logits = policy_outputs.logits\n    \n    kl_loss = compute_kl_divergence(policy_logits, ref_logits)\n    \n    # --- Total loss ---\n    loss = loss_lm + KL_COEF * kl_loss\n    \n    # Backward and optimizer step\n    optimizer.zero_grad()\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    optimizer.step()\n    \n    return loss.item(), loss_lm.item(), kl_loss.item()\n\n# --- Step 4: Training loop ---\nprint(\"Starting Stage I training...\")\noptimizer = torch.optim.AdamW(model.parameters(), lr=STAGE1_LR)\n\nfor epoch in range(STAGE1_EPOCHS):\n    total_loss = 0\n    total_lm_loss = 0\n    total_kl_loss = 0\n    \n    for step, batch in enumerate(train_loader):\n        loss, lm_loss, kl_loss = stage1_train_step(batch, model, ref_model, optimizer, tokenizer)\n        total_loss += loss\n        total_lm_loss += lm_loss\n        total_kl_loss += kl_loss\n        \n        if step % 10 == 0:\n            print(f\"Epoch {epoch+1}, Step {step}: \"\n                  f\"Loss={loss:.4f}, LM={lm_loss:.4f}, KL={kl_loss:.4f}\")\n    \n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} completed. Avg Loss: {avg_loss:.4f}\")\n\n# --- Step 5: Save checkpoint ---\nprint(\"Stage I completed! Saving checkpoint...\")\nmodel.save_pretrained(\"./stage1_lora\")\ntokenizer.save_pretrained(\"./stage1_lora\")\nprint(\"Stage I checkpoint saved at ./stage1_lora\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T07:22:25.440623Z","iopub.execute_input":"2025-10-04T07:22:25.440940Z","iopub.status.idle":"2025-10-04T07:26:03.458398Z","shell.execute_reply.started":"2025-10-04T07:22:25.440916Z","shell.execute_reply":"2025-10-04T07:26:03.457523Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nSTAGE I: Supervised Fine-tuning with KL Penalty (Fixed)\n================================================================================\nStarting Stage I training...\nEpoch 1, Step 0: Loss=0.0001, LM=0.0000, KL=0.0007\nEpoch 1, Step 10: Loss=0.0043, LM=0.0000, KL=0.0427\nEpoch 1, Step 20: Loss=0.0003, LM=0.0001, KL=0.0023\nEpoch 1, Step 30: Loss=0.0012, LM=0.0000, KL=0.0113\nEpoch 1, Step 40: Loss=0.0003, LM=0.0000, KL=0.0029\nEpoch 1 completed. Avg Loss: 0.0020\nEpoch 2, Step 0: Loss=0.0002, LM=0.0000, KL=0.0016\nEpoch 2, Step 10: Loss=0.0049, LM=0.0000, KL=0.0485\nEpoch 2, Step 20: Loss=0.0006, LM=0.0000, KL=0.0059\nEpoch 2, Step 30: Loss=0.0002, LM=0.0000, KL=0.0021\nEpoch 2, Step 40: Loss=0.0003, LM=0.0000, KL=0.0027\nEpoch 2 completed. Avg Loss: 0.0006\nEpoch 3, Step 0: Loss=0.0000, LM=0.0000, KL=0.0002\nEpoch 3, Step 10: Loss=0.0001, LM=0.0000, KL=0.0003\nEpoch 3, Step 20: Loss=0.0000, LM=0.0000, KL=0.0002\nEpoch 3, Step 30: Loss=0.0001, LM=0.0000, KL=0.0010\nEpoch 3, Step 40: Loss=0.0001, LM=0.0000, KL=0.0011\nEpoch 3 completed. Avg Loss: 0.0002\nStage I completed! Saving checkpoint...\nStage I checkpoint saved at ./stage1_lora\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# CELL 6: Stage II - Simplified REINFORCE with Correction Reward (FIXED)\n# ==============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"STAGE II: REINFORCE Training with Correction Rewards\")\nprint(\"=\"*80)\n\ndef extract_answer(text: str) -> str:\n    \"\"\"Extract numeric answer from text\"\"\"\n    numbers = re.findall(r'-?\\d+\\.?\\d*', text)\n    return numbers[-1] if numbers else \"\"\n\ndef compute_reward(problem: str, first_attempt: str, second_attempt: str, \n                   ground_truth: str) -> float:\n    \"\"\"\n    Compute reward for SCoRe:\n    - Base reward for correctness\n    - Bonus if second attempt is better than first\n    \"\"\"\n    ans1 = extract_answer(first_attempt)\n    ans2 = extract_answer(second_attempt)\n    gt = ground_truth.strip()\n    \n    correct_1 = (ans1 == gt)\n    correct_2 = (ans2 == gt)\n    \n    reward = 1.0 if correct_2 else 0.0\n    \n    if not correct_1 and correct_2:\n        reward += CORRECTION_BONUS\n    \n    if correct_1 and not correct_2:\n        reward -= CORRECTION_BONUS\n    \n    return reward\n\nclass SimpleValueHead(torch.nn.Module):\n    \"\"\"Simple value head for policy gradient\"\"\"\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.value_head = torch.nn.Linear(hidden_size, 1)\n    \n    def forward(self, hidden_states):\n        return self.value_head(hidden_states[:, -1, :]).squeeze(-1)\n\n# Add value head to model\nprint(\"Adding value head to model...\")\nhidden_size = model.config.hidden_size\nvalue_head = SimpleValueHead(hidden_size).to(model.device)\noptimizer_rl = AdamW(\n    list(model.parameters()) + list(value_head.parameters()),\n    lr=STAGE2_LR\n)\n\ndef reinforce_step(model, value_head, ref_model, tokenizer, batch, optimizer):\n    \"\"\"Single REINFORCE training step\"\"\"\n    model.train()\n    value_head.train()\n    \n    problem = batch[\"problem\"]\n    ground_truth = batch[\"answer\"]\n    \n    # Generate first attempt\n    prompt = f\"Problem: {problem}\\n\\nSolution:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True,\n                      truncation=True, max_length=MAX_LENGTH).to(model.device)\n    \n    with torch.no_grad():\n        outputs_first = model.generate(\n            **inputs,\n            max_new_tokens=MAX_NEW_TOKENS,\n            do_sample=True,\n            top_p=0.95,\n            temperature=0.7,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n    \n    first_attempt = tokenizer.decode(outputs_first[0], skip_special_tokens=True)\n    \n    # Generate second attempt\n    correction_prompt = f\"Problem: {problem}\\n\\nSolution:\\n\\nFirst attempt: {first_attempt}\\n\\nLet me reconsider:\"\n    correction_inputs = tokenizer(correction_prompt, return_tensors=\"pt\", \n                                  padding=True, truncation=True, \n                                  max_length=MAX_LENGTH).to(model.device)\n    \n    # Sample from model (with generation tracking)\n    outputs_second = model.generate(\n        **correction_inputs,\n        max_new_tokens=MAX_NEW_TOKENS,\n        do_sample=True,\n        top_p=0.95,\n        temperature=0.7,\n        pad_token_id=tokenizer.pad_token_id,\n    )\n    \n    second_attempt = tokenizer.decode(outputs_second[0], skip_special_tokens=True)\n    \n    # Compute reward\n    reward = compute_reward(problem, first_attempt, second_attempt, ground_truth)\n    \n    # Get generated tokens\n    generated_tokens = outputs_second[0][correction_inputs.input_ids.shape[1]:]\n    \n    # Forward pass with gradients enabled and hidden states output\n    with torch.enable_grad():\n        full_outputs = model(\n            input_ids=outputs_second,\n            attention_mask=torch.ones_like(outputs_second),\n            output_hidden_states=True  # CRITICAL FIX: Enable hidden states\n        )\n        logits = full_outputs.logits\n        \n        # Compute log probs for generated tokens\n        log_probs = F.log_softmax(logits[0, correction_inputs.input_ids.shape[1]-1:-1, :], dim=-1)\n        \n        # Ensure we have enough generated tokens\n        num_gen_tokens = min(len(generated_tokens), log_probs.shape[0])\n        if num_gen_tokens == 0:\n            return 0.0, reward, 0.0, 0.0\n        \n        generated_tokens = generated_tokens[:num_gen_tokens]\n        selected_log_probs = log_probs[:num_gen_tokens, generated_tokens]\n        \n        # Compute value estimate (now hidden_states is available)\n        value_estimate = value_head(full_outputs.hidden_states[-1])\n        \n        # REINFORCE loss\n        advantage = reward - value_estimate.detach()\n        policy_loss = -(selected_log_probs.mean() * advantage)\n        value_loss = F.mse_loss(value_estimate, torch.tensor([reward]).float().to(model.device))\n        \n        # KL penalty with reference model\n        with torch.no_grad():\n            ref_outputs = ref_model(**correction_inputs)\n            ref_logits = ref_outputs.logits\n        \n        policy_outputs_kl = model(**correction_inputs)\n        policy_logits = policy_outputs_kl.logits\n        kl_loss = compute_kl_divergence(policy_logits, ref_logits)\n        \n        # Total loss\n        total_loss = policy_loss + 0.5 * value_loss + 0.01 * kl_loss\n    \n    # Backward pass\n    optimizer.zero_grad()\n    total_loss.backward()\n    torch.nn.utils.clip_grad_norm_(\n        list(model.parameters()) + list(value_head.parameters()), \n        1.0\n    )\n    optimizer.step()\n    \n    return total_loss.item(), reward, policy_loss.item(), value_loss.item()\n\n# Stage II Training Loop\nprint(\"Starting Stage II REINFORCE training...\")\n\nfor step in range(10): #STAGE2_STEPS\n    idx = step % len(train_dataset)\n    batch = train_dataset[idx]\n    \n    try:\n        loss, reward, policy_loss, value_loss = reinforce_step(\n            model, value_head, ref_model, tokenizer, batch, optimizer_rl\n        )\n        \n        if step % 50 == 0:\n            print(f\"Step {step}: Total Loss={loss:.4f}, Reward={reward:.3f}, \"\n                  f\"Policy Loss={policy_loss:.4f}, Value Loss={value_loss:.4f}\")\n        \n        if step % 200 == 0 and step > 0:\n            print(f\"Saving checkpoint at step {step}...\")\n            model.save_pretrained(f\"./stage2_lora_step{step}\")\n            torch.save(value_head.state_dict(), f\"./stage2_lora_step{step}/value_head.pt\")\n    \n    except Exception as e:\n        print(f\"Error at step {step}: {e}\")\n        import traceback\n        traceback.print_exc()\n        continue\n\nprint(\"Stage II completed!\")\nprint(\"Saving final model...\")\nmodel.save_pretrained(\"./stage2_lora_final\")\ntorch.save(value_head.state_dict(), \"./stage2_lora_final/value_head.pt\")\ntokenizer.save_pretrained(\"./stage2_lora_final\")\n\n# ==============================================================================\n# CELL 7: Inference Test\n# ==============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"TESTING TRAINED MODEL\")\nprint(\"=\"*80)\n\ndef test_score_inference(problem: str):\n    \"\"\"Test the trained SCoRe model\"\"\"\n    model.eval()\n    \n    prompt = f\"Problem: {problem}\\n\\nSolution:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        output1 = model.generate(\n            **inputs,\n            max_new_tokens=MAX_NEW_TOKENS,\n            do_sample=True,\n            temperature=0.7,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n    \n    first_attempt = tokenizer.decode(output1[0], skip_special_tokens=True)\n    \n    correction_prompt = f\"{prompt}\\n\\nFirst attempt: {first_attempt}\\n\\nLet me reconsider:\"\n    inputs2 = tokenizer(correction_prompt, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        output2 = model.generate(\n            **inputs2,\n            max_new_tokens=MAX_NEW_TOKENS,\n            do_sample=True,\n            temperature=0.7,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n    \n    second_attempt = tokenizer.decode(output2[0], skip_special_tokens=True)\n    \n    print(f\"Problem: {problem}\")\n    print(f\"\\nFirst Attempt:\\n{first_attempt}\")\n    print(f\"\\nSecond Attempt (Self-Correction):\\n{second_attempt}\")\n    print(\"-\" * 80)\n    \n\ntest_problems = [\n    \"What is 144 + 256?\",\n    \"Solve for x: 2x - 8 = 14\",\n]\n\nfor prob in test_problems:\n    test_score_inference(prob)\n\nprint(\"\\n✓ Training complete! LoRA adapters saved to ./stage2_lora_final\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T07:26:03.459984Z","iopub.execute_input":"2025-10-04T07:26:03.460329Z","iopub.status.idle":"2025-10-04T07:40:05.154192Z","shell.execute_reply.started":"2025-10-04T07:26:03.460301Z","shell.execute_reply":"2025-10-04T07:40:05.153529Z"}},"outputs":[{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\nCaching is incompatible with gradient checkpointing in Qwen2DecoderLayer. Setting `past_key_values=None`.\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nSTAGE II: REINFORCE Training with Correction Rewards\n================================================================================\nAdding value head to model...\nStarting Stage II REINFORCE training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Step 0: Total Loss=1.5820, Reward=0.000, Policy Loss=-2.4214, Value Loss=8.0055\nStage II completed!\nSaving final model...\n\n================================================================================\nTESTING TRAINED MODEL\n================================================================================\nProblem: What is 144 + 256?\n\nFirst Attempt:\nProblem: What is 144 + 256?\n\nSolution: To find the sum of 144 and 256, we can add them step by step. \n\nFirst, let's align the numbers vertically for clarity:\n\n```\n  144\n+ 256\n-------\n```\n\nNow, let's add each column from right to left:\n\n1. **Units Column**: 4 (from 144) + 6 (from 256) = 10. We write down 0 and carry over 1.\n2. **Tens Column**: 4 (from 144) + 5 (from 256) = 9. Adding the carried over 1 makes it 10. We write down 0 and carry over 1 again.\n3. **Hundreds Column**: 1 (from 144) + 2 (from 256) = 3. Adding the carried over 1 makes it 4.\n\nPutting it all together, the sum is:\n\n```\n  144\n+ 256\n-------\n  400\n```\n\nTherefore, 144 + 256 equals 400. This method ensures that each place value is\n\nSecond Attempt (Self-Correction):\nProblem: What is 144 + 256?\n\nSolution:\n\nFirst attempt: Problem: What is 144 + 256?\n\nSolution: To find the sum of 144 and 256, we can add them step by step. \n\nFirst, let's align the numbers vertically for clarity:\n\n```\n  144\n+ 256\n-------\n```\n\nNow, let's add each column from right to left:\n\n1. **Units Column**: 4 (from 144) + 6 (from 256) = 10. We write down 0 and carry over 1.\n2. **Tens Column**: 4 (from 144) + 5 (from 256) = 9. Adding the carried over 1 makes it 10. We write down 0 and carry over 1 again.\n3. **Hundreds Column**: 1 (from 144) + 2 (from 256) = 3. Adding the carried over 1 makes it 4.\n\nPutting it all together, the sum is:\n\n```\n  144\n+ 256\n-------\n  400\n```\n\nTherefore, 144 + 256 equals 400. This method ensures that each place value is\n\nLet me reconsider:Let me recalculate:Problem: What is 144 + 256?\n\nSolution: Let's recalculate the addition to ensure accuracy.\n\nFirst, let's align the numbers vertically for clarity:\n\n```\n  144\n+ 256\n-------\n```\n\nNow, let's add each column from right to left:\n\n1. **Units Column**: 4 (from 144) + 6 (from 256) = 10. We write down 0 and carry over 1.\n2. **Tens Column**: 4 (from 144) + 5 (from 256) = 9. Adding the carried over 1 makes it 10. We write down 0 and carry over 1 again.\n3. **Hundreds Column**: 1 (from 144) + 2 (from 256) = 3. Adding the carried over 1 makes it 4.\n\nPutting it all together, the sum is:\n\n```\n  144\n+ 256\n-------\n  400\n```\n\nTherefore, 144 + 256 equals 400. This\n--------------------------------------------------------------------------------\nProblem: Solve for x: 2x - 8 = 14\n\nFirst Attempt:\nProblem: Solve for x: 2x - 8 = 14\n\nSolution: First, add 8 to both sides of the equation:\n\n2x - 8 + 8 = 14 + 8\n\nSimplify:\n\n2x = 22\n\nNext, divide both sides by 2:\n\n2x / 2 = 22 / 2\n\nSimplify:\n\nx = 11\n\nAnswer: The value of x is 11.\n\nExplanation: To solve for x in the equation 2x - 8 = 14, I first added 8 to both sides to eliminate the constant term on the left side. This simplifies the equation to 2x = 22. Then, I divided both sides by 2 to isolate x, resulting in x = 11. This method ensures that the equation remains balanced and allows for the accurate determination of x's value. It's important to perform the same operation on both sides of the equation to maintain equality throughout the solving process. Additionally, checking the solution by substituting x = 11 back into the original equation confirms its validity: 2(11) - 8 = 22 - 8 = 14, which matches the right side of the equation.\n\n# Answer: The value of x is 1\n\nSecond Attempt (Self-Correction):\nProblem: Solve for x: 2x - 8 = 14\n\nSolution:\n\nFirst attempt: Problem: Solve for x: 2x - 8 = 14\n\nSolution: First, add 8 to both sides of the equation:\n\n2x - 8 + 8 = 14 + 8\n\nSimplify:\n\n2x = 22\n\nNext, divide both sides by 2:\n\n2x / 2 = 22 / 2\n\nSimplify:\n\nx = 11\n\nAnswer: The value of x is 11.\n\nExplanation: To solve for x in the equation 2x - 8 = 14, I first added 8 to both sides to eliminate the constant term on the left side. This simplifies the equation to 2x = 22. Then, I divided both sides by 2 to isolate x, resulting in x = 11. This method ensures that the equation remains balanced and allows for the accurate determination of x's value. It's important to perform the same operation on both sides of the equation to maintain equality throughout the solving process. Additionally, checking the solution by substituting x = 11 back into the original equation confirms its validity: 2(11) - 8 = 22 - 8 = 14, which matches the right side of the equation.\n\n# Answer: The value of x is 1\n\nLet me reconsider:Problem: Solve for x: 2x - 8 = 14\n\nSolution: First, add 8 to both sides of the equation:\n\n2x - 8 + 8 = 14 + 8\n\nSimplify:\n\n2x = 22\n\nNext, divide both sides by 2:\n\n2x / 2 = 22 / 2\n\nSimplify:\n\nx = 11\n\nAnswer: The value of x is 11.\n\nExplanation: To solve for x in the equation 2x - 8 = 14, I first added 8 to both sides to eliminate the constant term on the left side. This simplifies the equation to 2x = 22. Then, I divided both sides by 2 to isolate x, resulting in x = 11. This method ensures that the equation remains balanced and allows for the accurate determination of x's value. It's important to perform the same operation on both sides of the equation to maintain equality throughout the solving process. Additionally, checking the solution by substituting x = 11 back into the original equation confirms its validity: 2(11) - 8 = 22 - 8 = 14,\n--------------------------------------------------------------------------------\n\n✓ Training complete! LoRA adapters saved to ./stage2_lora_final\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ==============================================================================\n# COMPREHENSIVE BENCHMARK EVALUATION\n# ==============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"BENCHMARK EVALUATION\")\nprint(\"=\"*80)\n\nimport re\nimport json\nfrom tqdm import tqdm\nfrom typing import Dict, List, Tuple\nimport numpy as np\nfrom datasets import load_dataset\n# Configuration\ntasks_to_run = [\"gsm8k\", \"math\", \"mmlu\", \"hellaswag\", \"arc_challenge\", \"bbh\"]\nMAX_SAMPLES = 50 # Limit samples per task for faster evaluation\nEVAL_BATCH_SIZE = 1  # Process one at a time for generation\n\ndef normalize_answer(text: str) -> str:\n    \"\"\"Normalize answer for comparison\"\"\"\n    text = text.lower().strip()\n    # Remove extra whitespace\n    text = ' '.join(text.split())\n    return text\n\ndef extract_numeric_answer(text: str) -> str:\n    \"\"\"Extract numeric answer from text\"\"\"\n    # Look for patterns like \"####\" followed by number (GSM8K format)\n    match = re.search(r'####\\s*(-?\\d+\\.?\\d*)', text)\n    if match:\n        return match.group(1)\n    \n    # Look for \"the answer is X\"\n    match = re.search(r'(?:answer is|equals?)\\s*[:\\-]?\\s*(-?\\d+\\.?\\d*)', text.lower())\n    if match:\n        return match.group(1)\n    \n    # Extract last number in text\n    numbers = re.findall(r'-?\\d+\\.?\\d*', text)\n    return numbers[-1] if numbers else \"\"\n\ndef extract_letter_answer(text: str) -> str:\n    \"\"\"Extract letter answer (A, B, C, D) from text\"\"\"\n    # Look for explicit answer format\n    match = re.search(r'(?:answer is|answer:|correct answer is)\\s*([A-D])', text, re.IGNORECASE)\n    if match:\n        return match.group(1).upper()\n    \n    # Look for standalone letter in parentheses or brackets\n    match = re.search(r'[\\(\\[]([A-D])[\\)\\]]', text, re.IGNORECASE)\n    if match:\n        return match.group(1).upper()\n    \n    # Last resort: first letter A-D that appears\n    match = re.search(r'\\b([A-D])\\b', text, re.IGNORECASE)\n    if match:\n        return match.group(1).upper()\n    \n    return \"\"\n\n# ==============================================================================\n# GSM8K Evaluation\n# ==============================================================================\ndef evaluate_gsm8k(model, tokenizer, num_samples=MAX_SAMPLES):\n    \"\"\"Evaluate on GSM8K dataset\"\"\"\n    print(\"\\n--- GSM8K Evaluation ---\")\n    ds = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n    ds = ds.select(range(min(num_samples, len(ds))))\n    \n    correct = 0\n    total = 0\n    \n    for item in tqdm(ds, desc=\"GSM8K\"):\n        question = item[\"question\"]\n        answer = item[\"answer\"].split(\"####\")[-1].strip()\n        \n        prompt = f\"Problem: {question}\\n\\nSolution:\"\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, \n                          max_length=MAX_LENGTH).to(model.device)\n        \n        with torch.no_grad():\n            output = model.generate(\n                **inputs,\n                max_new_tokens=MAX_NEW_TOKENS,\n                temperature=0.7,\n                do_sample=True,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n        \n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        predicted = extract_numeric_answer(response)\n        \n        if normalize_answer(predicted) == normalize_answer(answer):\n            correct += 1\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"GSM8K Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy\n\n# ==============================================================================\n# MATH Dataset Evaluation\n# ==============================================================================\ndef evaluate_math(model, tokenizer, num_samples=MAX_SAMPLES):\n    \"\"\"Evaluate on MATH dataset\"\"\"\n    print(\"\\n--- MATH Dataset Evaluation ---\")\n    ds = load_dataset(\"math_dataset\", \"algebra__linear_1d\", split=\"test\")\n    ds = ds.select(range(min(num_samples, len(ds))))\n    \n    correct = 0\n    total = 0\n    \n    for item in tqdm(ds, desc=\"MATH\"):\n        question = item[\"question\"]\n        answer = item[\"answer\"]\n        \n        prompt = f\"Problem: {question}\\n\\nSolution:\"\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n                          max_length=MAX_LENGTH).to(model.device)\n        \n        with torch.no_grad():\n            output = model.generate(\n                **inputs,\n                max_new_tokens=MAX_NEW_TOKENS,\n                temperature=0.7,\n                do_sample=True,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n        \n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        predicted = extract_numeric_answer(response)\n        actual = extract_numeric_answer(answer)\n        \n        if normalize_answer(predicted) == normalize_answer(actual):\n            correct += 1\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"MATH Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy\n\n# ==============================================================================\n# MMLU Evaluation\n# ==============================================================================\ndef evaluate_mmlu(model, tokenizer, num_samples=MAX_SAMPLES):\n    \"\"\"Evaluate on MMLU dataset\"\"\"\n    print(\"\\n--- MMLU Evaluation ---\")\n    ds = load_dataset(\"cais/mmlu\", \"abstract_algebra\", split=\"test\")\n    ds = ds.select(range(min(num_samples, len(ds))))\n    \n    correct = 0\n    total = 0\n    \n    for item in tqdm(ds, desc=\"MMLU\"):\n        question = item[\"question\"]\n        choices = item[\"choices\"]\n        answer_idx = item[\"answer\"]\n        \n        # Format multiple choice\n        choice_text = \"\\n\".join([f\"{chr(65+i)}. {c}\" for i, c in enumerate(choices)])\n        prompt = f\"Question: {question}\\n\\n{choice_text}\\n\\nAnswer:\"\n        \n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n                          max_length=MAX_LENGTH).to(model.device)\n        \n        with torch.no_grad():\n            output = model.generate(\n                **inputs,\n                max_new_tokens=10,\n                temperature=0.1,\n                do_sample=False,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n        \n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        predicted_letter = extract_letter_answer(response)\n        correct_letter = chr(65 + answer_idx)\n        \n        if predicted_letter == correct_letter:\n            correct += 1\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"MMLU Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy\n\n# ==============================================================================\n# ARC Challenge Evaluation\n# ==============================================================================\ndef evaluate_arc_challenge(model, tokenizer, num_samples=MAX_SAMPLES):\n    \"\"\"Evaluate on ARC Challenge dataset\"\"\"\n    print(\"\\n--- ARC Challenge Evaluation ---\")\n    ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"test\")\n    ds = ds.select(range(min(num_samples, len(ds))))\n    \n    correct = 0\n    total = 0\n    \n    for item in tqdm(ds, desc=\"ARC-C\"):\n        question = item[\"question\"]\n        choices = item[\"choices\"][\"text\"]\n        labels = item[\"choices\"][\"label\"]\n        answer = item[\"answerKey\"]\n        \n        # Format multiple choice\n        choice_text = \"\\n\".join([f\"{labels[i]}. {choices[i]}\" for i in range(len(choices))])\n        prompt = f\"Question: {question}\\n\\n{choice_text}\\n\\nAnswer:\"\n        \n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n                          max_length=MAX_LENGTH).to(model.device)\n        \n        with torch.no_grad():\n            output = model.generate(\n                **inputs,\n                max_new_tokens=10,\n                temperature=0.1,\n                do_sample=False,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n        \n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        predicted = extract_letter_answer(response)\n        \n        if predicted.upper() == answer.upper():\n            correct += 1\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"ARC Challenge Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy\n\n# ==============================================================================\n# BBH Evaluation\n# ==============================================================================\ndef evaluate_bbh(model, tokenizer, num_samples=MAX_SAMPLES):\n    \"\"\"Evaluate on BBH (Big Bench Hard) dataset\"\"\"\n    print(\"\\n--- BBH Evaluation ---\")\n    ds = load_dataset(\"lukaemon/bbh\", \"boolean_expressions\", split=\"test\")\n    ds = ds.select(range(min(num_samples, len(ds))))\n    \n    correct = 0\n    total = 0\n    \n    for item in tqdm(ds, desc=\"BBH\"):\n        question = item[\"input\"]\n        answer = item[\"target\"]\n        \n        prompt = f\"Question: {question}\\n\\nAnswer:\"\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n                          max_length=MAX_LENGTH).to(model.device)\n        \n        with torch.no_grad():\n            output = model.generate(\n                **inputs,\n                max_new_tokens=50,\n                temperature=0.1,\n                do_sample=False,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n        \n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        \n        # Check if answer is contained in response\n        if normalize_answer(answer) in normalize_answer(response):\n            correct += 1\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"BBH Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy\n\n# ==============================================================================\n# HellaSwag Evaluation\n# ==============================================================================\ndef evaluate_hellaswag(model, tokenizer, num_samples=MAX_SAMPLES):\n    \"\"\"Evaluate on HellaSwag dataset\"\"\"\n    print(\"\\n--- HellaSwag Evaluation ---\")\n    ds = load_dataset(\"hellaswag\", split=\"validation\")\n    ds = ds.select(range(min(num_samples, len(ds))))\n    \n    correct = 0\n    total = 0\n    \n    for item in tqdm(ds, desc=\"HellaSwag\"):\n        context = item[\"ctx\"]\n        endings = item[\"endings\"]\n        label = int(item[\"label\"])\n        \n        # Score each ending\n        scores = []\n        for ending in endings:\n            full_text = context + \" \" + ending\n            inputs = tokenizer(full_text, return_tensors=\"pt\", truncation=True,\n                             max_length=MAX_LENGTH).to(model.device)\n            \n            with torch.no_grad():\n                outputs = model(**inputs, labels=inputs[\"input_ids\"])\n                # Use negative loss as score (lower loss = better)\n                scores.append(-outputs.loss.item())\n        \n        # Predict the ending with highest score\n        predicted = np.argmax(scores)\n        \n        if predicted == label:\n            correct += 1\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"HellaSwag Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy\n\n# ==============================================================================\n# Run All Evaluations\n# ==============================================================================\ndef run_all_benchmarks(model, tokenizer, tasks=None):\n    \"\"\"Run all specified benchmarks\"\"\"\n    if tasks is None:\n        tasks = tasks_to_run\n    \n    results = {}\n    \n    if \"gsm8k\" in tasks:\n        results[\"gsm8k\"] = evaluate_gsm8k(model, tokenizer)\n    \n    if \"math\" in tasks:\n        results[\"math\"] = evaluate_math(model, tokenizer)\n    \n    if \"mmlu\" in tasks:\n        results[\"mmlu\"] = evaluate_mmlu(model, tokenizer)\n    \n    if \"arc_challenge\" in tasks:\n        results[\"arc_challenge\"] = evaluate_arc_challenge(model, tokenizer)\n    \n    if \"bbh\" in tasks:\n        results[\"bbh\"] = evaluate_bbh(model, tokenizer)\n    \n    if \"hellaswag\" in tasks:\n        results[\"hellaswag\"] = evaluate_hellaswag(model, tokenizer)\n    \n    return results\n\n# ==============================================================================\n# Main Evaluation\n# ==============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"STARTING BENCHMARK EVALUATION\")\nprint(\"=\"*80)\n\n# Load the trained model\nprint(\"Loading Stage 2 model...\")\nmodel.eval()\n\n# Run benchmarks\nresults = run_all_benchmarks(model, tokenizer, tasks_to_run)\n\n# Print summary\nprint(\"\\n\" + \"=\"*80)\nprint(\"BENCHMARK RESULTS SUMMARY\")\nprint(\"=\"*80)\nprint(results)\nfor task, accuracy in results.items():\n    print(f\"{task.upper()}: {accuracy*100:.2f}%\")\n\n# Calculate average\navg_accuracy = np.mean(list(results.values()))\nprint(f\"\\nAVERAGE ACCURACY: {avg_accuracy*100:.2f}%\")\n\n# Save results\nresults_with_avg = {**results, \"average\": avg_accuracy}\nwith open(\"benchmark_results.json\", \"w\") as f:\n    json.dump(results_with_avg, f, indent=2)\n\nprint(\"\\n✓ Results saved to benchmark_results.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T07:40:39.890400Z","iopub.execute_input":"2025-10-04T07:40:39.891133Z","iopub.status.idle":"2025-10-04T08:47:25.983227Z","shell.execute_reply.started":"2025-10-04T07:40:39.891106Z","shell.execute_reply":"2025-10-04T08:47:25.982337Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nBENCHMARK EVALUATION\n================================================================================\n\n================================================================================\nSTARTING BENCHMARK EVALUATION\n================================================================================\nLoading Stage 2 model...\n\n--- GSM8K Evaluation ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5d48a5a3ece4bb888bf142e573c92f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"main/train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b70ecf77c48440fd8a7a93f77f19c479"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"main/test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33eb7280aa3543a1bc0aa0499eb2b9a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e5045e2a2b845f1a317a8c96f4c9a9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c934605a621c4ef4974909e2af35e154"}},"metadata":{}},{"name":"stderr","text":"GSM8K: 100%|██████████| 50/50 [25:48<00:00, 30.98s/it]\n","output_type":"stream"},{"name":"stdout","text":"GSM8K Accuracy: 0.2400 (12/50)\n\n--- MATH Dataset Evaluation ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bde74660ce0d42cda3b0668f9c98b71a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"math_dataset.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebfde610981a465eaef907f37bbabf1a"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for math_dataset contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/math_dataset.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"471f53b0629c4871bf2c0ef5f7be5355"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1999998 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86bd8593a50c4dffb3c32fc44e2d8199"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ba0b405d4fe4589b552b26b3eba82aa"}},"metadata":{}},{"name":"stderr","text":"MATH: 100%|██████████| 50/50 [25:56<00:00, 31.13s/it]","output_type":"stream"},{"name":"stdout","text":"MATH Accuracy: 0.1000 (5/50)\n\n--- MMLU Evaluation ---\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cca8c71de9248ed952a6dcfead2ba3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dataset_infos.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b888a87effe845438f182433fa822576"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"abstract_algebra/test-00000-of-00001.par(…):   0%|          | 0.00/9.96k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ded39b45c9514123bad79af8a3a0ce8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"abstract_algebra/validation-00000-of-000(…):   0%|          | 0.00/3.73k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58b9f52318de4a60be62b534eda8933d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"abstract_algebra/dev-00000-of-00001.parq(…):   0%|          | 0.00/3.45k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebd72864db8a46ce93dbc493d34398de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81a337fca46f461e9d959463fdb62b13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b72c351ce7cb4a759a133d5e4913366c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d0bb6882af5479b9b6926b4172196f0"}},"metadata":{}},{"name":"stderr","text":"MMLU:   0%|          | 0/50 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nMMLU: 100%|██████████| 50/50 [01:05<00:00,  1.31s/it]","output_type":"stream"},{"name":"stdout","text":"MMLU Accuracy: 0.5400 (27/50)\n\n--- ARC Challenge Evaluation ---\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e04fdc747dc54cb58a5b52bb7b8858ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ARC-Challenge/train-00000-of-00001.parqu(…):   0%|          | 0.00/190k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df07cbd9ca894176bd0a36499a039b59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ARC-Challenge/test-00000-of-00001.parque(…):   0%|          | 0.00/204k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be4c629fb2184dc6963d8b5c69438a6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ARC-Challenge/validation-00000-of-00001.(…):   0%|          | 0.00/55.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baa61a33d81c4ed2b48f20dbaba5cc41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1119 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c91733cb38dd4e37a0e4960a5d4d2c14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1172 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9342085132a746de8b17f523a46ea896"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/299 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e79f083603a546c7a03c1b07442844b2"}},"metadata":{}},{"name":"stderr","text":"ARC-C: 100%|██████████| 50/50 [01:05<00:00,  1.31s/it]","output_type":"stream"},{"name":"stdout","text":"ARC Challenge Accuracy: 0.8800 (44/50)\n\n--- BBH Evaluation ---\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ac8800898cc4f5da1adf5cbc3db3f6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"boolean_expressions/test-00000-of-00001.(…):   0%|          | 0.00/4.52k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4599df3d75dc42e7aa3234da6b77baf7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdb5ea94c0af4e08a3ecd8990e02071b"}},"metadata":{}},{"name":"stderr","text":"BBH: 100%|██████████| 50/50 [05:02<00:00,  6.05s/it]","output_type":"stream"},{"name":"stdout","text":"BBH Accuracy: 0.9200 (46/50)\n\n--- HellaSwag Evaluation ---\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c77f0dac4b94b898e513a18e5edaee4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/24.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f5aa36ecc6e478789bd12576a82501c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/6.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9edbab84983446f69f2b0a4be5696ebf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001.parquet:   0%|          | 0.00/6.32M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2d77785b2e94f1fa9705b137f14f49a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/39905 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a8866ad86424ee99b51a78b86bc75ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10003 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a781a2e39be2457ea60aad35bf27afac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10042 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bdf4e5c8c0042a980d5758128898c1e"}},"metadata":{}},{"name":"stderr","text":"HellaSwag: 100%|██████████| 50/50 [00:45<00:00,  1.11it/s]","output_type":"stream"},{"name":"stdout","text":"HellaSwag Accuracy: 0.6000 (30/50)\n\n================================================================================\nBENCHMARK RESULTS SUMMARY\n================================================================================\n{'gsm8k': 0.24, 'math': 0.1, 'mmlu': 0.54, 'arc_challenge': 0.88, 'bbh': 0.92, 'hellaswag': 0.6}\nGSM8K: 24.00%\nMATH: 10.00%\nMMLU: 54.00%\nARC_CHALLENGE: 88.00%\nBBH: 92.00%\nHELLASWAG: 60.00%\n\nAVERAGE ACCURACY: 54.67%\n\n✓ Results saved to benchmark_results.json\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}