{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n# CELL 0: Install/Upgrade Required Packages\n# ==============================================================================\n# Run this first, then restart kernel\n\"\"\"\n!pip uninstall -y transformers accelerate trl peft -y\n!pip install transformers==4.36.2 --no-cache-dir\n!pip install accelerate==0.25.0 --no-cache-dir\n!pip install peft==0.7.1 --no-cache-dir\n!pip install trl==0.7.4 --no-cache-dir\n!pip install bitsandbytes==0.41.3 --no-cache-dir\n!pip install datasets==3.6.0 --no-cache-dir\n\"\"\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# RUN THIS CELL FIRST - Install/Upgrade BitsAndBytes\n# ==============================================================================\nimport subprocess\nimport sys\n\nprint(\"=\"*80)\nprint(\"INSTALLING/UPGRADING BITSANDBYTES\")\nprint(\"=\"*80)\n\n# Method 1: Try standard upgrade\ntry:\n    print(\"\\n1. Upgrading bitsandbytes to latest version...\")\n    result = subprocess.run(\n        [sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"bitsandbytes\"],\n        capture_output=True,\n        text=True\n    )\n    print(result.stdout)\n    if result.returncode == 0:\n        print(\"Successfully upgraded bitsandbytes\")\n    else:\n        print(\"Upgrade had some issues, trying alternative method...\")\n        raise Exception(\"Standard install failed\")\nexcept Exception as e:\n    # Method 2: Try with specific version\n    print(\"Trying to install specific version (0.41.0)...\")\n    try:\n        subprocess.check_call(\n            [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"bitsandbytes==0.41.0\"],\n            stdout=subprocess.PIPE\n        )\n        print(\"Installed bitsandbytes 0.41.0\")\n    except:\n        print(\"Could not install specific version\")\n\n# Verify installation\nprint(\"\\n3. Verifying installation...\")\ntry:\n    import bitsandbytes as bnb\n    print(f\"✓ bitsandbytes version: {bnb.__version__}\")\n    print(\"✓ Import successful!\")\nexcept Exception as e:\n    print(f\"✗ Import failed: {e}\")\n    print(\"\\n⚠ IMPORTANT: If bitsandbytes still doesn't work:\")\n    print(\"   - Set USE_QUANTIZATION = False in the config\")\n    print(\"   - The code will automatically fall back to FP16\")\n    print(\"   - You'll need more GPU memory but it will work\")\n\n# Also upgrade related packages\nprint(\"\\n4. Upgrading related packages...\")\ntry:\n    subprocess.check_call(\n        [sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"-q\", \"accelerate\", \"transformers\"],\n    )\n    print(\"Upgraded accelerate and transformers\")\nexcept:\n    print(\"Could not upgrade all packages\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"INSTALLATION COMPLETE \")\nprint(\"=\"*80)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# ==============================================================================\n# CELL 1: Imports and Configuration\n# ==============================================================================\nimport os\nimport torch\nimport torch.nn.functional as F\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    BitsAndBytesConfig,\n)\nfrom peft import (\n    LoraConfig, \n    get_peft_model, \n    prepare_model_for_kbit_training,\n    PeftModel\n)\n# Note: We'll implement a simplified PPO instead of using trl's PPOTrainer to avoid import issues\nfrom datasets import Dataset\nfrom kaggle_secrets import UserSecretsClient\nimport numpy as np\nfrom typing import List, Dict, Tuple\nimport re\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\n\n# Get HF token\nhf = UserSecretsClient()\nHF_TOKEN = hf.get_secret(\"HF_TOKEN\")\n\n# Configuration\nREPO = \"O1-OPEN/OpenO1-LLama-8B-v0.1\"\n# SUBFOLDER = \"checkpoint-1000\"\nUSE_SUBFOLDER = False\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nMAX_LENGTH = 512\nMAX_NEW_TOKENS = 256\n\n# Training hyperparameters\nSTAGE1_EPOCHS = 3\nSTAGE1_BATCH_SIZE = 2\nSTAGE1_GRAD_ACCUM = 8\nSTAGE1_LR = 2e-4\nKL_COEF = 0.1  # KL penalty coefficient\n\nSTAGE2_STEPS = 1000\nSTAGE2_BATCH_SIZE = 2\nSTAGE2_LR = 1e-5\nCORRECTION_BONUS = 1.0  # Bonus when second attempt > first\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nprint(f\"Using device: {DEVICE}\")\nprint(f\"Available GPUs: {torch.cuda.device_count()}\")\n\n# ==============================================================================\n# CELL 2: Load Tokenizer and Models (4-bit Quantization)\n# ==============================================================================\nprint(\"Loading tokenizer...\")\nif USE_SUBFOLDER:\n    tokenizer = AutoTokenizer.from_pretrained(\n        REPO, \n        subfolder=SUBFOLDER,\n        trust_remote_code=True,\n        token=HF_TOKEN\n    )\nelse:\n    tokenizer = AutoTokenizer.from_pretrained(\n        REPO,\n        trust_remote_code=True,\n        token=HF_TOKEN\n    )\n\n# Set pad token\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\nprint(\"Setting up 4-bit quantization...\")\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\nprint(\"Loading base model (this may take several minutes)...\")\nmodel_kwargs = {\n    \"quantization_config\": bnb_config,\n    \"device_map\": \"auto\",  # Spreads across both T4s\n    \"trust_remote_code\": True,\n    \"token\": HF_TOKEN,\n}\nif USE_SUBFOLDER:\n    model_kwargs[\"subfolder\"] = SUBFOLDER\n\nbase_model = AutoModelForCausalLM.from_pretrained(REPO, **model_kwargs)\nbase_model.config.use_cache = False  # Required for gradient checkpointing\n\nprint(\"Loading reference model (frozen)...\")\nref_model = AutoModelForCausalLM.from_pretrained(REPO, **model_kwargs)\nref_model.eval()\nfor p in ref_model.parameters():\n    p.requires_grad = False\n\nprint(f\"Base model device map: {base_model.hf_device_map}\")\n\n# ==============================================================================\n# CELL 3: Prepare Model with LoRA\n# ==============================================================================\nprint(\"Preparing model for k-bit training...\")\nbase_model = prepare_model_for_kbit_training(base_model)\n\n# LoRA configuration\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Qwen modules\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nprint(\"Attaching LoRA adapters...\")\nmodel = get_peft_model(base_model, lora_config)\nmodel.print_trainable_parameters()\n\n# ==============================================================================\n# CELL 4: Prepare Dataset (Example Format)\n# ==============================================================================\n# Your dataset should have: problem, first_attempt, second_attempt, correctness\n# Format: {\"problem\": \"...\", \"first_attempt\": \"...\", \"second_attempt\": \"...\", \"answer\": \"...\", \"is_correct_1\": bool, \"is_correct_2\": bool}\ndef create_sample_dataset():\n    \"\"\"Create a small sample dataset for testing\"\"\"\n    samples = [\n        {\n            \"problem\": \"What is 25 + 17?\",\n            \"first_attempt\": \"Let me calculate: 25 + 17 = 41\",\n            \"second_attempt\": \"Let me recalculate: 25 + 17 = 42\",\n            \"answer\": \"42\",\n            \"is_correct_1\": False,\n            \"is_correct_2\": True\n        },\n        {\n            \"problem\": \"Solve: 3x + 5 = 14\",\n            \"first_attempt\": \"3x = 14 - 5 = 9, so x = 4\",\n            \"second_attempt\": \"3x = 14 - 5 = 9, so x = 3\",\n            \"answer\": \"3\",\n            \"is_correct_1\": False,\n            \"is_correct_2\": True\n        },\n        {\n            \"problem\": \"What is 15 * 8?\",\n            \"first_attempt\": \"15 * 8 = 110\",\n            \"second_attempt\": \"Let me recalculate: 15 * 8 = 120\",\n            \"answer\": \"120\",\n            \"is_correct_1\": False,\n            \"is_correct_2\": True\n        },\n        {\n            \"problem\": \"If y - 7 = 12, what is y?\",\n            \"first_attempt\": \"y = 12 + 7 = 20\",\n            \"second_attempt\": \"y = 12 + 7 = 19\",\n            \"answer\": \"19\",\n            \"is_correct_1\": False,\n            \"is_correct_2\": True\n        },\n    ] * 25  # Repeat to create larger dataset\n    return Dataset.from_list(samples)\n\n# def create_sample_dataset():\n#     \"\"\"Create a small sample dataset for testing\"\"\"\n#     samples = [\n#         {\n#             \"problem\": \"What is 25 + 17?\",\n#             \"first_attempt\": \"Let me calculate: 25 + 17 = 41\",\n#             \"second_attempt\": \"Let me recalculate: 25 + 17 = 42\",\n#             \"answer\": \"42\",\n#             \"is_correct_1\": False,\n#             \"is_correct_2\": True\n#         },\n#         {\n#             \"problem\": \"Solve: 3x + 5 = 14\",\n#             \"first_attempt\": \"3x = 14 - 5 = 9, so x = 4\",\n#             \"second_attempt\": \"3x = 14 - 5 = 9, so x = 3\",\n#             \"answer\": \"3\",\n#             \"is_correct_1\": False,\n#             \"is_correct_2\": True\n#         },\n#         # Add more examples...\n#     ]\n#     return Dataset.from_list(samples)\n\n# Load your actual dataset here\nprint(\"Creating/loading dataset...\")\ntrain_dataset = create_sample_dataset()\nprint(f\"Dataset size: {len(train_dataset)}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# STAGE I - Supervised Fine-tuning with KL Penalty (Fixed)\n# ==============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"STAGE I: Supervised Fine-tuning with KL Penalty (Fixed)\")\nprint(\"=\"*80)\nfrom torch.utils.data import DataLoader\n\n# --- Step 1: Prepare DataLoader ---\ntrain_loader = DataLoader(train_dataset, batch_size=STAGE1_BATCH_SIZE, shuffle=True)\n\n# --- Step 2: Define KL Divergence ---\n# def compute_kl_divergence(logits_policy, logits_ref):\n#     \"\"\"\n#     Compute KL divergence between policy and reference model\n#     \"\"\"\n#     log_probs_policy = F.log_softmax(logits_policy, dim=-1)\n#     probs_ref = F.softmax(logits_ref, dim=-1)\n#     kl = (probs_ref * (probs_ref.log() - log_probs_policy)).sum(dim=-1)\n#     return kl.mean()\nimport torch\nimport torch.nn.functional as F\n\ndef compute_kl_divergence(policy_logits, ref_logits, attention_mask=None, eps=1e-12):\n    \"\"\"\n    Compute KL(P_ref || Q_policy) per token with masking and numeric stability,\n    returning the mean KL per sample.\n\n    Args:\n      policy_logits: Tensor [batch, seq_len, vocab]\n      ref_logits:    Tensor [batch, seq_len, vocab]\n      attention_mask: Optional Tensor [batch, seq_len] with 1 for real tokens, 0 for padding.\n      eps: small value to avoid div/zero (not usually needed with log_softmax but kept for safety).\n\n    Returns:\n      scalar tensor: mean KL across non-padding tokens (averaged over batch)\n    \"\"\"\n    # ensure shapes match\n    assert policy_logits.shape == ref_logits.shape, f\"policy {policy_logits.shape} vs ref {ref_logits.shape}\"\n\n    # stable log-probs\n    log_probs_policy = F.log_softmax(policy_logits, dim=-1)   # log Q\n    log_probs_ref = F.log_softmax(ref_logits, dim=-1)         # log P\n\n    # probs for P (ref) via exp(log_probs_ref) — numerically stable\n    probs_ref = log_probs_ref.exp()\n\n    # per-token KL: sum_vocab P * (log P - log Q)\n    kl_per_token = (probs_ref * (log_probs_ref - log_probs_policy)).sum(dim=-1)  # [batch, seq_len]\n\n    if attention_mask is not None:\n        # cast mask to same dtype\n        mask = attention_mask.to(kl_per_token.dtype)  # [batch, seq_len]\n        # zero out padding tokens, compute per-sample mean over valid tokens\n        valid_tokens_per_sample = mask.sum(dim=1).clamp_min(1.0)  # avoid div by 0\n        kl_per_sample = (kl_per_token * mask).sum(dim=1) / valid_tokens_per_sample\n    else:\n        # mean over seq_len when no mask provided\n        kl_per_sample = kl_per_token.mean(dim=1)\n\n    return kl_per_sample.mean()  # scalar\n\n# --- Step 3: Stage I training step ---\ndef stage1_train_step(batch, model, ref_model, optimizer, tokenizer):\n    model.train()\n    \n    # Prepare prompts and targets\n    prompts = [f\"Problem: {p}\\n\\nFirst attempt: {a1}\\n\\nLet me reconsider:\" \n               for p, a1 in zip(batch[\"problem\"], batch[\"first_attempt\"])]\n    targets = [f\"{t}\" for t in batch[\"second_attempt\"]]\n    \n    # Combine prompts and targets for proper tokenization\n    full_texts = [p + t for p, t in zip(prompts, targets)]\n    \n    # Tokenize the combined text\n    inputs = tokenizer(full_texts, padding='longest', truncation=True,\n                       max_length=MAX_LENGTH, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].to(model.device)\n    attention_mask = inputs[\"attention_mask\"].to(model.device)\n    \n    # Create labels by tokenizing prompts to find where targets start\n    prompt_inputs = tokenizer(prompts, padding='longest', truncation=True,\n                              max_length=MAX_LENGTH, return_tensors=\"pt\")\n    prompt_lengths = (prompt_inputs[\"attention_mask\"].sum(dim=1)).tolist()\n    \n    # Create labels: -100 for prompt tokens, actual tokens for target\n    labels = input_ids.clone()\n    for i, prompt_len in enumerate(prompt_lengths):\n        labels[i, :prompt_len] = -100\n    \n    # Replace padding tokens with -100\n    labels[labels == tokenizer.pad_token_id] = -100\n    \n    # Forward pass\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n    loss_lm = outputs.loss\n    \n    # --- KL penalty on first attempt ---\n    first_prompts = [f\"Problem: {p}\\n\\nSolution:\" for p in batch[\"problem\"]]\n    first_inputs = tokenizer(first_prompts, padding='longest', truncation=True,\n                             max_length=MAX_LENGTH, return_tensors=\"pt\")\n    first_inputs = {k: v.to(model.device) for k, v in first_inputs.items() \n                    if k in [\"input_ids\", \"attention_mask\"]}\n    \n    with torch.no_grad():\n        ref_outputs = ref_model(**first_inputs)\n        ref_logits = ref_outputs.logits\n    \n    policy_outputs = model(**first_inputs)\n    policy_logits = policy_outputs.logits\n    \n    kl_loss = compute_kl_divergence(policy_logits, ref_logits)\n    \n    # --- Total loss ---\n    loss = loss_lm + KL_COEF * kl_loss\n    \n    # Backward and optimizer step\n    optimizer.zero_grad()\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    optimizer.step()\n    \n    return loss.item(), loss_lm.item(), kl_loss.item()\n\n# --- Step 4: Training loop ---\nprint(\"Starting Stage I training...\")\noptimizer = torch.optim.AdamW(model.parameters(), lr=STAGE1_LR)\n\nfor epoch in range(STAGE1_EPOCHS):\n    total_loss = 0\n    total_lm_loss = 0\n    total_kl_loss = 0\n    \n    for step, batch in enumerate(train_loader):\n        loss, lm_loss, kl_loss = stage1_train_step(batch, model, ref_model, optimizer, tokenizer)\n        total_loss += loss\n        total_lm_loss += lm_loss\n        total_kl_loss += kl_loss\n        \n        if step % 10 == 0:\n            print(f\"Epoch {epoch+1}, Step {step}: \"\n                  f\"Loss={loss:.4f}, LM={lm_loss:.4f}, KL={kl_loss:.4f}\")\n    \n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} completed. Avg Loss: {avg_loss:.4f}\")\n\n# --- Step 5: Save checkpoint ---\nprint(\"Stage I completed! Saving checkpoint...\")\nmodel.save_pretrained(\"./stage1_lora\")\ntokenizer.save_pretrained(\"./stage1_lora\")\nprint(\"Stage I checkpoint saved at ./stage1_lora\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 6: Stage II - Simplified REINFORCE with Correction Reward (FIXED)\n# ==============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"STAGE II: REINFORCE Training with Correction Rewards\")\nprint(\"=\"*80)\n\ndef extract_answer(text: str) -> str:\n    \"\"\"Extract numeric answer from text\"\"\"\n    numbers = re.findall(r'-?\\d+\\.?\\d*', text)\n    return numbers[-1] if numbers else \"\"\n\ndef compute_reward(problem: str, first_attempt: str, second_attempt: str, \n                   ground_truth: str) -> float:\n    \"\"\"\n    Compute reward for SCoRe:\n    - Base reward for correctness\n    - Bonus if second attempt is better than first\n    \"\"\"\n    ans1 = extract_answer(first_attempt)\n    ans2 = extract_answer(second_attempt)\n    gt = ground_truth.strip()\n    \n    correct_1 = (ans1 == gt)\n    correct_2 = (ans2 == gt)\n    \n    reward = 1.0 if correct_2 else 0.0\n    \n    if not correct_1 and correct_2:\n        reward += CORRECTION_BONUS\n    \n    if correct_1 and not correct_2:\n        reward -= CORRECTION_BONUS\n    \n    return reward\n\nclass SimpleValueHead(torch.nn.Module):\n    \"\"\"Simple value head for policy gradient\"\"\"\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.value_head = torch.nn.Linear(hidden_size, 1)\n    \n    def forward(self, hidden_states):\n        return self.value_head(hidden_states[:, -1, :]).squeeze(-1)\n\n# Add value head to model\nprint(\"Adding value head to model...\")\nhidden_size = model.config.hidden_size\nvalue_head = SimpleValueHead(hidden_size).to(model.device)\noptimizer_rl = AdamW(\n    list(model.parameters()) + list(value_head.parameters()),\n    lr=STAGE2_LR\n)\n\ndef reinforce_step(model, value_head, ref_model, tokenizer, batch, optimizer):\n    \"\"\"Single REINFORCE training step\"\"\"\n    model.train()\n    value_head.train()\n    \n    problem = batch[\"problem\"]\n    ground_truth = batch[\"answer\"]\n    \n    # Generate first attempt\n    prompt = f\"Problem: {problem}\\n\\nSolution:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True,\n                      truncation=True, max_length=MAX_LENGTH).to(model.device)\n    \n    with torch.no_grad():\n        outputs_first = model.generate(\n            **inputs,\n            max_new_tokens=MAX_NEW_TOKENS,\n            do_sample=True,\n            top_p=0.95,\n            temperature=0.7,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n    \n    first_attempt = tokenizer.decode(outputs_first[0], skip_special_tokens=True)\n    \n    # Generate second attempt\n    correction_prompt = f\"Problem: {problem}\\n\\nSolution:\\n\\nFirst attempt: {first_attempt}\\n\\nLet me reconsider:\"\n    correction_inputs = tokenizer(correction_prompt, return_tensors=\"pt\", \n                                  padding=True, truncation=True, \n                                  max_length=MAX_LENGTH).to(model.device)\n    \n    # Sample from model (with generation tracking)\n    outputs_second = model.generate(\n        **correction_inputs,\n        max_new_tokens=MAX_NEW_TOKENS,\n        do_sample=True,\n        top_p=0.95,\n        temperature=0.7,\n        pad_token_id=tokenizer.pad_token_id,\n    )\n    \n    second_attempt = tokenizer.decode(outputs_second[0], skip_special_tokens=True)\n    \n    # Compute reward\n    reward = compute_reward(problem, first_attempt, second_attempt, ground_truth)\n    \n    # Get generated tokens\n    generated_tokens = outputs_second[0][correction_inputs.input_ids.shape[1]:]\n    \n    # Forward pass with gradients enabled and hidden states output\n    with torch.enable_grad():\n        full_outputs = model(\n            input_ids=outputs_second,\n            attention_mask=torch.ones_like(outputs_second),\n            output_hidden_states=True  # CRITICAL FIX: Enable hidden states\n        )\n        logits = full_outputs.logits\n        \n        # Compute log probs for generated tokens\n        log_probs = F.log_softmax(logits[0, correction_inputs.input_ids.shape[1]-1:-1, :], dim=-1)\n        \n        # Ensure we have enough generated tokens\n        num_gen_tokens = min(len(generated_tokens), log_probs.shape[0])\n        if num_gen_tokens == 0:\n            return 0.0, reward, 0.0, 0.0\n        \n        generated_tokens = generated_tokens[:num_gen_tokens]\n        selected_log_probs = log_probs[:num_gen_tokens, generated_tokens]\n        \n        # Compute value estimate (now hidden_states is available)\n        value_estimate = value_head(full_outputs.hidden_states[-1])\n        \n        # REINFORCE loss\n        advantage = reward - value_estimate.detach()\n        policy_loss = -(selected_log_probs.mean() * advantage)\n        value_loss = F.mse_loss(value_estimate, torch.tensor([reward]).float().to(model.device))\n        \n        # KL penalty with reference model\n        with torch.no_grad():\n            ref_outputs = ref_model(**correction_inputs)\n            ref_logits = ref_outputs.logits\n        \n        policy_outputs_kl = model(**correction_inputs)\n        policy_logits = policy_outputs_kl.logits\n        kl_loss = compute_kl_divergence(policy_logits, ref_logits)\n        \n        # Total loss\n        total_loss = policy_loss + 0.5 * value_loss + 0.01 * kl_loss\n    \n    # Backward pass\n    optimizer.zero_grad()\n    total_loss.backward()\n    torch.nn.utils.clip_grad_norm_(\n        list(model.parameters()) + list(value_head.parameters()), \n        1.0\n    )\n    optimizer.step()\n    \n    return total_loss.item(), reward, policy_loss.item(), value_loss.item()\n\n# Stage II Training Loop\nprint(\"Starting Stage II REINFORCE training...\")\n\nfor step in range(10): #STAGE2_STEPS\n    idx = step % len(train_dataset)\n    batch = train_dataset[idx]\n    \n    try:\n        loss, reward, policy_loss, value_loss = reinforce_step(\n            model, value_head, ref_model, tokenizer, batch, optimizer_rl\n        )\n        \n        if step % 50 == 0:\n            print(f\"Step {step}: Total Loss={loss:.4f}, Reward={reward:.3f}, \"\n                  f\"Policy Loss={policy_loss:.4f}, Value Loss={value_loss:.4f}\")\n        \n        if step % 200 == 0 and step > 0:\n            print(f\"Saving checkpoint at step {step}...\")\n            model.save_pretrained(f\"./stage2_lora_step{step}\")\n            torch.save(value_head.state_dict(), f\"./stage2_lora_step{step}/value_head.pt\")\n    \n    except Exception as e:\n        print(f\"Error at step {step}: {e}\")\n        import traceback\n        traceback.print_exc()\n        continue\n\nprint(\"Stage II completed!\")\nprint(\"Saving final model...\")\nmodel.save_pretrained(\"./stage2_lora_final\")\ntorch.save(value_head.state_dict(), \"./stage2_lora_final/value_head.pt\")\ntokenizer.save_pretrained(\"./stage2_lora_final\")\n\n# ==============================================================================\n# CELL 7: Inference Test\n# ==============================================================================\n# print(\"\\n\" + \"=\"*80)\n# print(\"TESTING TRAINED MODEL\")\n# print(\"=\"*80)\n\n# def test_score_inference(problem: str):\n#     \"\"\"Test the trained SCoRe model\"\"\"\n#     model.eval()\n    \n#     prompt = f\"Problem: {problem}\\n\\nSolution:\"\n#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n#     with torch.no_grad():\n#         output1 = model.generate(\n#             **inputs,\n#             max_new_tokens=MAX_NEW_TOKENS,\n#             do_sample=True,\n#             temperature=0.7,\n#             pad_token_id=tokenizer.pad_token_id,\n#         )\n    \n#     first_attempt = tokenizer.decode(output1[0], skip_special_tokens=True)\n    \n#     correction_prompt = f\"{prompt}\\n\\nFirst attempt: {first_attempt}\\n\\nLet me reconsider:\"\n#     inputs2 = tokenizer(correction_prompt, return_tensors=\"pt\").to(model.device)\n    \n#     with torch.no_grad():\n#         output2 = model.generate(\n#             **inputs2,\n#             max_new_tokens=MAX_NEW_TOKENS,\n#             do_sample=True,\n#             temperature=0.7,\n#             pad_token_id=tokenizer.pad_token_id,\n#         )\n    \n#     second_attempt = tokenizer.decode(output2[0], skip_special_tokens=True)\n    \n#     print(f\"Problem: {problem}\")\n#     print(f\"\\nFirst Attempt:\\n{first_attempt}\")\n#     print(f\"\\nSecond Attempt (Self-Correction):\\n{second_attempt}\")\n#     print(\"-\" * 80)\n    \n\n# test_problems = [\n#     \"What is 144 + 256?\",\n#     \"Solve for x: 2x - 8 = 14\",\n# ]\n\n# for prob in test_problems:\n#     test_score_inference(prob)\n\n# print(\"\\n✓ Training complete! LoRA adapters saved to ./stage2_lora_final\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Verify the installation\nimport datasets\nprint(f\"Successfully installed datasets version: {datasets.__version__}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# COMPREHENSIVE BENCHMARK EVALUATION\n# ==============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"BENCHMARK EVALUATION\")\nprint(\"=\"*80)\n\nimport re\nimport json\nfrom tqdm import tqdm\nfrom typing import Dict, List, Tuple\nimport numpy as np\nimport datasets\nprint(\"version : \",datasets.__version__)\nfrom datasets import load_dataset\n# Configuration\ntasks_to_run = [\"gsm8k\", \"math\", \"mmlu\", \"hellaswag\", \"arc_challenge\", \"bbh\"]\nMAX_SAMPLES = 50 # Limit samples per task for faster evaluation\nEVAL_BATCH_SIZE = 1  # Process one at a time for generation\n\ndef normalize_answer(text: str) -> str:\n    \"\"\"Normalize answer for comparison\"\"\"\n    text = text.lower().strip()\n    # Remove extra whitespace\n    text = ' '.join(text.split())\n    return text\n\ndef extract_numeric_answer(text: str) -> str:\n    \"\"\"Extract numeric answer from text\"\"\"\n    # Look for patterns like \"####\" followed by number (GSM8K format)\n    match = re.search(r'####\\s*(-?\\d+\\.?\\d*)', text)\n    if match:\n        return match.group(1)\n    \n    # Look for \"the answer is X\"\n    match = re.search(r'(?:answer is|equals?)\\s*[:\\-]?\\s*(-?\\d+\\.?\\d*)', text.lower())\n    if match:\n        return match.group(1)\n    \n    # Extract last number in text\n    numbers = re.findall(r'-?\\d+\\.?\\d*', text)\n    return numbers[-1] if numbers else \"\"\n\ndef extract_letter_answer(text: str) -> str:\n    \"\"\"Extract letter answer (A, B, C, D) from text\"\"\"\n    # Look for explicit answer format\n    match = re.search(r'(?:answer is|answer:|correct answer is)\\s*([A-D])', text, re.IGNORECASE)\n    if match:\n        return match.group(1).upper()\n    \n    # Look for standalone letter in parentheses or brackets\n    match = re.search(r'[\\(\\[]([A-D])[\\)\\]]', text, re.IGNORECASE)\n    if match:\n        return match.group(1).upper()\n    \n    # Last resort: first letter A-D that appears\n    match = re.search(r'\\b([A-D])\\b', text, re.IGNORECASE)\n    if match:\n        return match.group(1).upper()\n    \n    return \"\"\n\n# ==============================================================================\n# GSM8K Evaluation\n# ==============================================================================\ndef evaluate_gsm8k(model, tokenizer, num_samples=MAX_SAMPLES):\n    \"\"\"Evaluate on GSM8K dataset\"\"\"\n    print(\"\\n--- GSM8K Evaluation ---\")\n    ds = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n    ds = ds.select(range(min(num_samples, len(ds))))\n    \n    correct = 0\n    total = 0\n    \n    for item in tqdm(ds, desc=\"GSM8K\"):\n        question = item[\"question\"]\n        answer = item[\"answer\"].split(\"####\")[-1].strip()\n        \n        prompt = f\"Problem: {question}\\n\\nSolution:\"\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, \n                          max_length=MAX_LENGTH).to(model.device)\n        \n        with torch.no_grad():\n            output = model.generate(\n                **inputs,\n                max_new_tokens=MAX_NEW_TOKENS,\n                temperature=0.7,\n                do_sample=True,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n        \n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        predicted = extract_numeric_answer(response)\n        \n        if normalize_answer(predicted) == normalize_answer(answer):\n            correct += 1\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"GSM8K Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy\n\n# ==============================================================================\n# MATH Dataset Evaluation\n# ==============================================================================\n# def evaluate_math(model, tokenizer, num_samples=MAX_SAMPLES):\n#     \"\"\"Evaluate on MATH dataset\"\"\"\n#     print(\"\\n--- MATH Dataset Evaluation ---\")\n#     ds = load_dataset(\"math_dataset\", \"algebra__linear_1d\", split=\"test\")\n#     ds = ds.select(range(min(num_samples, len(ds))))\n    \n#     correct = 0\n#     total = 0\n    \n#     for item in tqdm(ds, desc=\"MATH\"):\n#         question = item[\"question\"]\n#         answer = item[\"answer\"]\n        \n#         prompt = f\"Problem: {question}\\n\\nSolution:\"\n#         inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n#                           max_length=MAX_LENGTH).to(model.device)\n        \n#         with torch.no_grad():\n#             output = model.generate(\n#                 **inputs,\n#                 max_new_tokens=MAX_NEW_TOKENS,\n#                 temperature=0.7,\n#                 do_sample=True,\n#                 pad_token_id=tokenizer.pad_token_id,\n#             )\n        \n#         response = tokenizer.decode(output[0], skip_special_tokens=True)\n#         predicted = extract_numeric_answer(response)\n#         actual = extract_numeric_answer(answer)\n        \n#         if normalize_answer(predicted) == normalize_answer(actual):\n#             correct += 1\n#         total += 1\n    \n#     accuracy = correct / total if total > 0 else 0\n#     print(f\"MATH Accuracy: {accuracy:.4f} ({correct}/{total})\")\n#     return accuracy\ndef evaluate_math(model, tokenizer, num_samples=MAX_SAMPLES):\n    \"\"\"Evaluate on MATH dataset\"\"\"\n    print(\"\\n--- MATH Dataset Evaluation ---\")\n    # Add trust_remote_code=True to allow custom dataset code\n    ds = load_dataset(\"math_dataset\", \"algebra__linear_1d\", split=\"test\", trust_remote_code=True)\n    ds = ds.select(range(min(num_samples, len(ds))))\n    \n    correct = 0\n    total = 0\n    \n    for item in tqdm(ds, desc=\"MATH\"):\n        question = item[\"question\"]\n        answer = item[\"answer\"]\n        \n        prompt = f\"Problem: {question}\\n\\nSolution:\"\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n                          max_length=MAX_LENGTH).to(model.device)\n        \n        with torch.no_grad():\n            output = model.generate(\n                **inputs,\n                max_new_tokens=MAX_NEW_TOKENS,\n                temperature=0.7,\n                do_sample=True,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n        \n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        predicted = extract_numeric_answer(response)\n        actual = extract_numeric_answer(answer)\n        \n        if normalize_answer(predicted) == normalize_answer(actual):\n            correct += 1\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"MATH Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy\n# ==============================================================================\n# MMLU Evaluation\n# ==============================================================================\ndef evaluate_mmlu(model, tokenizer, num_samples=MAX_SAMPLES):\n    \"\"\"Evaluate on MMLU dataset\"\"\"\n    print(\"\\n--- MMLU Evaluation ---\")\n    ds = load_dataset(\"cais/mmlu\", \"abstract_algebra\", split=\"test\")\n    ds = ds.select(range(min(num_samples, len(ds))))\n    \n    correct = 0\n    total = 0\n    \n    for item in tqdm(ds, desc=\"MMLU\"):\n        question = item[\"question\"]\n        choices = item[\"choices\"]\n        answer_idx = item[\"answer\"]\n        \n        # Format multiple choice\n        choice_text = \"\\n\".join([f\"{chr(65+i)}. {c}\" for i, c in enumerate(choices)])\n        prompt = f\"Question: {question}\\n\\n{choice_text}\\n\\nAnswer:\"\n        \n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n                          max_length=MAX_LENGTH).to(model.device)\n        \n        with torch.no_grad():\n            output = model.generate(\n                **inputs,\n                max_new_tokens=10,\n                temperature=0.1,\n                do_sample=False,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n        \n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        predicted_letter = extract_letter_answer(response)\n        correct_letter = chr(65 + answer_idx)\n        \n        if predicted_letter == correct_letter:\n            correct += 1\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"MMLU Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy\n\n# ==============================================================================\n# ARC Challenge Evaluation\n# ==============================================================================\ndef evaluate_arc_challenge(model, tokenizer, num_samples=MAX_SAMPLES):\n    \"\"\"Evaluate on ARC Challenge dataset\"\"\"\n    print(\"\\n--- ARC Challenge Evaluation ---\")\n    ds = load_dataset(\"ai2_arc\", \"ARC-Challenge\", split=\"test\")\n    ds = ds.select(range(min(num_samples, len(ds))))\n    \n    correct = 0\n    total = 0\n    \n    for item in tqdm(ds, desc=\"ARC-C\"):\n        question = item[\"question\"]\n        choices = item[\"choices\"][\"text\"]\n        labels = item[\"choices\"][\"label\"]\n        answer = item[\"answerKey\"]\n        \n        # Format multiple choice\n        choice_text = \"\\n\".join([f\"{labels[i]}. {choices[i]}\" for i in range(len(choices))])\n        prompt = f\"Question: {question}\\n\\n{choice_text}\\n\\nAnswer:\"\n        \n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n                          max_length=MAX_LENGTH).to(model.device)\n        \n        with torch.no_grad():\n            output = model.generate(\n                **inputs,\n                max_new_tokens=10,\n                temperature=0.1,\n                do_sample=False,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n        \n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        predicted = extract_letter_answer(response)\n        \n        if predicted.upper() == answer.upper():\n            correct += 1\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"ARC Challenge Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy\n\n# ==============================================================================\n# BBH Evaluation\n# ==============================================================================\ndef evaluate_bbh(model, tokenizer, num_samples=MAX_SAMPLES):\n    \"\"\"Evaluate on BBH (Big Bench Hard) dataset\"\"\"\n    print(\"\\n--- BBH Evaluation ---\")\n    ds = load_dataset(\"lukaemon/bbh\", \"boolean_expressions\", split=\"test\")\n    ds = ds.select(range(min(num_samples, len(ds))))\n    \n    correct = 0\n    total = 0\n    \n    for item in tqdm(ds, desc=\"BBH\"):\n        question = item[\"input\"]\n        answer = item[\"target\"]\n        \n        prompt = f\"Question: {question}\\n\\nAnswer:\"\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n                          max_length=MAX_LENGTH).to(model.device)\n        \n        with torch.no_grad():\n            output = model.generate(\n                **inputs,\n                max_new_tokens=50,\n                temperature=0.1,\n                do_sample=False,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n        \n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        \n        # Check if answer is contained in response\n        if normalize_answer(answer) in normalize_answer(response):\n            correct += 1\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"BBH Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy\n\n# ==============================================================================\n# HellaSwag Evaluation\n# ==============================================================================\ndef evaluate_hellaswag(model, tokenizer, num_samples=MAX_SAMPLES):\n    \"\"\"Evaluate on HellaSwag dataset\"\"\"\n    print(\"\\n--- HellaSwag Evaluation ---\")\n    ds = load_dataset(\"hellaswag\", split=\"validation\")\n    ds = ds.select(range(min(num_samples, len(ds))))\n    \n    correct = 0\n    total = 0\n    \n    for item in tqdm(ds, desc=\"HellaSwag\"):\n        context = item[\"ctx\"]\n        endings = item[\"endings\"]\n        label = int(item[\"label\"])\n        \n        # Score each ending\n        scores = []\n        for ending in endings:\n            full_text = context + \" \" + ending\n            inputs = tokenizer(full_text, return_tensors=\"pt\", truncation=True,\n                             max_length=MAX_LENGTH).to(model.device)\n            \n            with torch.no_grad():\n                outputs = model(**inputs, labels=inputs[\"input_ids\"])\n                # Use negative loss as score (lower loss = better)\n                scores.append(-outputs.loss.item())\n        \n        # Predict the ending with highest score\n        predicted = np.argmax(scores)\n        \n        if predicted == label:\n            correct += 1\n        total += 1\n    \n    accuracy = correct / total if total > 0 else 0\n    print(f\"HellaSwag Accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy\n\n# ==============================================================================\n# Run All Evaluations\n# ==============================================================================\ndef run_all_benchmarks(model, tokenizer, tasks=None):\n    \"\"\"Run all specified benchmarks\"\"\"\n    if tasks is None:\n        tasks = tasks_to_run\n    \n    results = {}\n    \n    # if \"gsm8k\" in tasks:\n    #     results[\"gsm8k\"] = evaluate_gsm8k(model, tokenizer)\n    \n    \n    \n    # if \"mmlu\" in tasks:\n    #     results[\"mmlu\"] = evaluate_mmlu(model, tokenizer)\n    \n    # if \"arc_challenge\" in tasks:\n    #     results[\"arc_challenge\"] = evaluate_arc_challenge(model, tokenizer)\n    \n    # if \"bbh\" in tasks:\n    #     results[\"bbh\"] = evaluate_bbh(model, tokenizer)\n    \n    # if \"hellaswag\" in tasks:\n    #     results[\"hellaswag\"] = evaluate_hellaswag(model, tokenizer)\n    if \"math\" in tasks:\n        results[\"math\"] = evaluate_math(model, tokenizer)\n    \n    return results\n\n# ==============================================================================\n# Main Evaluation\n# ==============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"STARTING BENCHMARK EVALUATION\")\nprint(\"=\"*80)\n\n# Load the trained model\n\nprint(\"Loading Stage 2 model...\")\nmodel.eval()\n\n# Run benchmarks\nresults = run_all_benchmarks(model, tokenizer, tasks_to_run)\n\n# Print summary\nprint(\"\\n\" + \"=\"*80)\nprint(\"BENCHMARK RESULTS SUMMARY\")\nprint(\"=\"*80)\nprint(results)\nfor task, accuracy in results.items():\n    print(f\"{task.upper()}: {accuracy*100:.2f}%\")\n\n# Calculate average\navg_accuracy = np.mean(list(results.values()))\nprint(f\"\\nAVERAGE ACCURACY: {avg_accuracy*100:.2f}%\")\n\n# Save results\nresults_with_avg = {**results, \"average\": avg_accuracy}\nwith open(\"benchmark_results.json\", \"w\") as f:\n    json.dump(results_with_avg, f, indent=2)\n\nprint(\"\\n✓ Results saved to benchmark_results.json\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}