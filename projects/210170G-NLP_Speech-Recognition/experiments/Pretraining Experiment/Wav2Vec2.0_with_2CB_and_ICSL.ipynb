{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9syxLSLYKoJB",
        "outputId": "f57d2a66-390c-41b0-a2fa-7470596d8a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchcodec in /usr/local/lib/python3.12/dist-packages (0.7.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchcodec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9q6bS9TCJJM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from datasets import load_from_disk, DatasetDict\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Union\n",
        "import logging\n",
        "import datasets\n",
        "import math\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    SchedulerType,\n",
        "    Wav2Vec2Config,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    Wav2Vec2ForPreTraining,\n",
        "    get_scheduler,\n",
        "    is_wandb_available,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF2AZoeCuCfu",
        "outputId": "fd1b6537-4000-4b48-bbe7-7f8f66ce2fb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufpGv_z4HTEB"
      },
      "outputs": [],
      "source": [
        "MAX_DURATION = 10.0\n",
        "MIN_DURATION = 5.0\n",
        "GRADIENT_CHECKPOINTING = True\n",
        "MASK_TIME_PROB = None\n",
        "MASK_TIME_LENGTH = None\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VAL_BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-4 #5e-5\n",
        "ADAM_BETA1 = 0.9\n",
        "ADAM_BETA2 = 0.98\n",
        "ADAM_EPSILON = 1e-8\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "MAX_TRAINING_STEPS = None\n",
        "NUM_TRAIN_EPOCHS = 50\n",
        "LR_SCHEDULER_TYPE = \"linear\"\n",
        "NUM_WARMUP_STEPS = 100 #0\n",
        "MAX_GUMBEL_TEMPERATURE = 2.0\n",
        "MIN_GUMBEL_TEMPERATURE = 1.0 #0.5\n",
        "GUMBEL_TEMPERATURE = 0.999999 #0.999995\n",
        "LOGGING_STEPS = 10\n",
        "SAVING_STEPS = 500\n",
        "OUTPUT_DIR = \"/outputs\"\n",
        "INTER_CB_SIMILARITY_WEIGHT = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiOzLEWPXJ_D"
      },
      "outputs": [],
      "source": [
        "logger = get_logger(__name__)\n",
        "logging.basicConfig(level=logging.INFO, force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iiq41dHFD3MH"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataCollatorForWav2Vec2Pretraining:\n",
        "  model: Wav2Vec2ForPreTraining\n",
        "  feature_extractor: Wav2Vec2FeatureExtractor\n",
        "  padding: str = \"longest\"\n",
        "  pad_to_multiple_of: int = None\n",
        "  mask_time_prob: float = 0.65\n",
        "  mask_time_length: int= 10\n",
        "\n",
        "  def __call__(self, features: list[dict[str, Union[list[int], torch.tensor]]]) -> dict[str, torch.Tensor]:\n",
        "    batch = self.feature_extractor.pad(\n",
        "        features,\n",
        "        padding=self.padding,\n",
        "        pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    device = batch[\"input_values\"].device\n",
        "    batch_size = batch['input_values'].shape[0]\n",
        "\n",
        "    mask_indices_seq_length = self.model._get_feat_extract_output_lengths(batch[\"input_values\"].shape[-1])\n",
        "    mask_indices_seq_length = int(mask_indices_seq_length)\n",
        "\n",
        "    if batch.get(\"attention_mask\") is not None:\n",
        "      batch[\"sub_attention_mask\"] = self.model._get_feature_vector_attention_mask(\n",
        "          mask_indices_seq_length, batch[\"attention_mask\"]\n",
        "      )\n",
        "\n",
        "    features_shape = (batch_size, mask_indices_seq_length)\n",
        "\n",
        "    # Sample randomly maksed indices\n",
        "    mask_time_indices = _compute_mask_indices(\n",
        "        features_shape,\n",
        "        self.mask_time_prob,\n",
        "        self.mask_time_length,\n",
        "        attention_mask=batch.get(\"sub_attention_mask\"),\n",
        "    )\n",
        "\n",
        "    # Sample negative indices\n",
        "    sampled_negative_indices = _sample_negative_indices(\n",
        "        features_shape,\n",
        "        self.model.config.num_negatives,\n",
        "        mask_time_indices=mask_time_indices,\n",
        "    )\n",
        "\n",
        "    batch[\"mask_time_indices\"] = torch.tensor(mask_time_indices, dtype=torch.long, device=device)\n",
        "    batch[\"sampled_negative_indices\"] = torch.tensor(sampled_negative_indices, dtype=torch.long, device=device)\n",
        "\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQPNj65jQ61v"
      },
      "outputs": [],
      "source": [
        "def multiply_grads(params, c):\n",
        "  \"\"\"Multiply grad by a constant c\"\"\"\n",
        "  for p in params:\n",
        "    if p.grad is not None:\n",
        "      if torch.is_tensor(c):\n",
        "        c = c.to(p.grad.device)\n",
        "      p.grad.data.mul_(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEeNfAImVHG3"
      },
      "outputs": [],
      "source": [
        "def get_grad_norm(params, scale=1):\n",
        "  \"\"\"Compute grad norm given a gradient scale\"\"\"\n",
        "  total_norm = 0.0\n",
        "  for p in params:\n",
        "    if p.grad is not None:\n",
        "      param_norm = (p.grad.detach().data / scale).norm(2)\n",
        "      total_norm += param_norm.item() ** 2\n",
        "  total_norm = total_norm ** 0.5\n",
        "  return total_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "XtBjp1aRWUBh",
        "outputId": "64760f0c-24f7-4259-d2dc-8a6f911bf44c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Distributed environment: DistributedType.NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wandb have installed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrasarathathsarana63\u001b[0m (\u001b[33mrasarathathsarana63-university-of-moratuwa\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250926_101415-5np2uz9e</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rasarathathsarana63-university-of-moratuwa/wav2vec2-fromscratch/runs/5np2uz9e' target=\"_blank\">cool-wildflower-11</a></strong> to <a href='https://wandb.ai/rasarathathsarana63-university-of-moratuwa/wav2vec2-fromscratch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/rasarathathsarana63-university-of-moratuwa/wav2vec2-fromscratch' target=\"_blank\">https://wandb.ai/rasarathathsarana63-university-of-moratuwa/wav2vec2-fromscratch</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/rasarathathsarana63-university-of-moratuwa/wav2vec2-fromscratch/runs/5np2uz9e' target=\"_blank\">https://wandb.ai/rasarathathsarana63-university-of-moratuwa/wav2vec2-fromscratch/runs/5np2uz9e</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "accelerator = Accelerator()\n",
        "logger.info(accelerator.state, main_process_only=False)\n",
        "\n",
        "if accelerator.is_local_main_process:\n",
        "  datasets.utils.logging.set_verbosity_warning()\n",
        "  transformers.utils.logging.set_verbosity_info()\n",
        "\n",
        "  if is_wandb_available():\n",
        "    print(\"wandb have installed\")\n",
        "    import wandb\n",
        "    wandb.init(project=\"wav2vec2-fromscratch\")\n",
        "\n",
        "else:\n",
        "  datasets.utils.logging.set_verbosity_error()\n",
        "  transformers.utils.logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9zgC005YJP8"
      },
      "outputs": [],
      "source": [
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6725-xCrb_dU"
      },
      "outputs": [],
      "source": [
        "accelerator.wait_for_everyone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_7qKyEfcItx"
      },
      "outputs": [],
      "source": [
        "raw_datasets = DatasetDict()\n",
        "\n",
        "raw_datasets['train'] = load_from_disk(\"/content/drive/MyDrive/SP/SP/librispeech_datasets/dataset_10h\")\n",
        "raw_datasets['val'] = load_from_disk(\"/content/drive/MyDrive/SP/SP/librispeech_datasets/dataset_val_clean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFu1WsBDNRfH"
      },
      "outputs": [],
      "source": [
        "raw_datasets[\"train\"] = raw_datasets[\"train\"].remove_columns(\"duration\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364,
          "referenced_widgets": [
            "43143b40b5e84c3e8702fc8ccebdc776",
            "baafa043633e42179db2731628e80fe1",
            "268577248d3f4c098b596ddae880700d",
            "9156c02c0ab34d69a52a12b41eeb4c8d",
            "820909e43e824fa58eb674987d187ac7",
            "65b133273a604ae6b2a98dbd8a64df0e",
            "f9a95dfed0194f438a5c5a4f81235624",
            "7d7fc311fef6417cac7a9865e7912c05",
            "11d329fed5d8472ebf00b3591173829c",
            "3c5161517e5e426a8b0b3d94d4faf34a",
            "e9e886663f9e4c44b7557d32bd2546c1"
          ]
        },
        "id": "K7Bl4MtnsaqA",
        "outputId": "28842dc0-2a42-4ecd-96cd-ab85ddac6b92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43143b40b5e84c3e8702fc8ccebdc776"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--facebook--wav2vec2-base/snapshots/0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8/preprocessor_config.json\n",
            "Feature extractor Wav2Vec2FeatureExtractor {\n",
            "  \"do_normalize\": true,\n",
            "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
            "  \"feature_size\": 1,\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0.0,\n",
            "  \"return_attention_mask\": true,\n",
            "  \"sampling_rate\": 16000\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\",\n",
        "    return_attention_mask = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AR6KPkzu5Ga"
      },
      "outputs": [],
      "source": [
        "raw_datasets = raw_datasets.cast_column(\n",
        "    'audio', datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epFODlkP--Xg"
      },
      "outputs": [],
      "source": [
        "# only normalized-inputs-training is supported\n",
        "if not feature_extractor.do_normalize:\n",
        "  raise ValueError(\n",
        "      \"Training is only supported for normalized inputs. Make sure ``feature_extractor.do_normalize == True``\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6_eEqKe_JMF"
      },
      "outputs": [],
      "source": [
        "# Set max & min audio length in number of samples\n",
        "max_length = int(MAX_DURATION * feature_extractor.sampling_rate)\n",
        "min_length = int(MIN_DURATION * feature_extractor.sampling_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sJix4Fm_JIu"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "  sample = batch[\"audio\"]\n",
        "  inputs = feature_extractor(\n",
        "      sample[\"array\"],\n",
        "      sampling_rate=sample[\"sampling_rate\"],\n",
        "      max_length=max_length,\n",
        "      truncation=True\n",
        "  )\n",
        "  batch[\"input_values\"] = inputs.input_values[0]\n",
        "  batch[\"input_length\"] = len(inputs.input_values[0])\n",
        "\n",
        "  return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YekAifm2_JGI"
      },
      "outputs": [],
      "source": [
        "# load audio files into numpy arrays\n",
        "with accelerator.main_process_first():\n",
        "  vectorized_datasets = raw_datasets.map(\n",
        "      prepare_dataset,\n",
        "      num_proc=None,\n",
        "      remove_columns=raw_datasets[\"train\"].column_names,\n",
        "  )\n",
        "\n",
        "  if min_length > 0.0:\n",
        "    vectorized_datasets = vectorized_datasets.filter(\n",
        "        lambda x: x > min_length,\n",
        "        num_proc=None,\n",
        "        input_columns=[\"input_length\"]\n",
        "    )\n",
        "\n",
        "  vectorized_datasets = vectorized_datasets.remove_columns(\"input_length\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zz_AgQ8s_JDe",
        "outputId": "f99aa3d1-a15b-497c-f93a-12ffc378062e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--wav2vec2-base/snapshots/0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8/config.json\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Model config Wav2Vec2Config {\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"adapter_attn_dim\": null,\n",
            "  \"adapter_kernel_size\": 3,\n",
            "  \"adapter_stride\": 2,\n",
            "  \"add_adapter\": false,\n",
            "  \"apply_spec_augment\": true,\n",
            "  \"architectures\": [\n",
            "    \"Wav2Vec2ForPreTraining\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"codevector_dim\": 256,\n",
            "  \"contrastive_logits_temperature\": 0.1,\n",
            "  \"conv_bias\": false,\n",
            "  \"conv_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"conv_kernel\": [\n",
            "    10,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"conv_stride\": [\n",
            "    5,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"ctc_loss_reduction\": \"sum\",\n",
            "  \"ctc_zero_infinity\": false,\n",
            "  \"diversity_loss_weight\": 0.1,\n",
            "  \"do_stable_layer_norm\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"feat_extract_activation\": \"gelu\",\n",
            "  \"feat_extract_norm\": \"group\",\n",
            "  \"feat_proj_dropout\": 0.1,\n",
            "  \"feat_quantizer_dropout\": 0.0,\n",
            "  \"final_dropout\": 0.0,\n",
            "  \"freeze_feat_extract_train\": true,\n",
            "  \"gradient_checkpointing\": true,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"mask_channel_length\": 10,\n",
            "  \"mask_channel_min_space\": 1,\n",
            "  \"mask_channel_other\": 0.0,\n",
            "  \"mask_channel_prob\": 0.0,\n",
            "  \"mask_channel_selection\": \"static\",\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_min_space\": 1,\n",
            "  \"mask_time_other\": 0.0,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"mask_time_selection\": \"static\",\n",
            "  \"model_type\": \"wav2vec2\",\n",
            "  \"no_mask_channel_overlap\": false,\n",
            "  \"no_mask_time_overlap\": false,\n",
            "  \"num_adapter_layers\": 3,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_codevector_groups\": 2,\n",
            "  \"num_codevectors_per_group\": 320,\n",
            "  \"num_conv_pos_embedding_groups\": 16,\n",
            "  \"num_conv_pos_embeddings\": 128,\n",
            "  \"num_feat_extract_layers\": 7,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_negatives\": 100,\n",
            "  \"output_hidden_size\": 768,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"proj_codevector_dim\": 256,\n",
            "  \"tdnn_dilation\": [\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"tdnn_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    1500\n",
            "  ],\n",
            "  \"tdnn_kernel\": [\n",
            "    5,\n",
            "    3,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"transformers_version\": \"4.56.1\",\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 32,\n",
            "  \"xvector_output_dim\": 512\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "config = Wav2Vec2Config.from_pretrained(\"facebook/wav2vec2-base\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def inter_codebook_similarity_loss(codebook_vectors):\n",
        "    # codebook_vectors: [G, V, D]\n",
        "    codebook_vectors = codebook_vectors.reshape(config.num_codevector_groups, config.num_codevectors_per_group, -1)\n",
        "    G, V, D = codebook_vectors.shape\n",
        "    losses = []\n",
        "    for i in range(G):\n",
        "        for j in range(i + 1, G):\n",
        "            # Flatten groups into [V, D]\n",
        "            e_i = codebook_vectors[i]  # [V, D]\n",
        "            e_j = codebook_vectors[j]  # [V, D]\n",
        "\n",
        "            # Normalize\n",
        "            e_i = F.normalize(e_i, dim=-1)\n",
        "            e_j = F.normalize(e_j, dim=-1)\n",
        "\n",
        "            # Pairwise cosine similarity: [V, V]\n",
        "            sim = torch.matmul(e_i, e_j.T)\n",
        "\n",
        "            # Mean similarity\n",
        "            losses.append(sim.mean())\n",
        "\n",
        "    return torch.stack(losses).mean() if losses else torch.tensor(0.0, device=codebook_vectors.device)\n"
      ],
      "metadata": {
        "id": "NhIb1i2ssAHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAIPxk3sN5-B"
      },
      "outputs": [],
      "source": [
        "# model = Wav2Vec2ForPreTraining.from_pretrained(\n",
        "#     \"facebook/wav2vec2-base\",\n",
        "#     config=config\n",
        "# )\n",
        "\n",
        "model = Wav2Vec2ForPreTraining(\n",
        "    config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHVKPXFMMy4O"
      },
      "outputs": [],
      "source": [
        "# Activate gradient checkpointing\n",
        "if GRADIENT_CHECKPOINTING:\n",
        "  model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL7wWlNIP6sc"
      },
      "outputs": [],
      "source": [
        "mask_time_prob = config.mask_time_prob if MASK_TIME_PROB is None else MASK_TIME_PROB\n",
        "mask_time_length = config.mask_time_length if MASK_TIME_LENGTH is None else MASK_TIME_LENGTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0yToSoEQxMS"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForWav2Vec2Pretraining(\n",
        "    model=model,\n",
        "    feature_extractor=feature_extractor,\n",
        "    mask_time_prob=mask_time_prob,\n",
        "    mask_time_length=mask_time_length\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFVFHUSYRiR-"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(\n",
        "    vectorized_datasets[\"train\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtvbQfKXTBuo"
      },
      "outputs": [],
      "source": [
        "val_dataloader = DataLoader(\n",
        "    vectorized_datasets[\"val\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=VAL_BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MFTyy1pTMgP"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    list(model.parameters()),\n",
        "    lr=LEARNING_RATE,\n",
        "    betas=[ADAM_BETA1, ADAM_BETA2],\n",
        "    eps=ADAM_EPSILON\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAVnpWMpUeox"
      },
      "outputs": [],
      "source": [
        "model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, val_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68f5gYeaUkXQ"
      },
      "outputs": [],
      "source": [
        "num_update_steps_per_epcoh = math.ceil(len(train_dataloader) / GRADIENT_ACCUMULATION_STEPS)\n",
        "\n",
        "if MAX_TRAINING_STEPS is None:\n",
        "  max_train_steps = num_update_steps_per_epcoh * NUM_TRAIN_EPOCHS\n",
        "else:\n",
        "  max_train_steps = MAX_TRAINING_STEPS\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=LR_SCHEDULER_TYPE,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=NUM_WARMUP_STEPS,\n",
        "    num_training_steps=max_train_steps,\n",
        ")\n",
        "\n",
        "NUM_TRAIN_EPOCHS = math.ceil(max_train_steps / num_update_steps_per_epcoh)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params = [name for name, param in model.named_parameters() if param.requires_grad]\n",
        "print(\"Trainable parameters:\", list(trainable_params))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE3aojSzWwhj",
        "outputId": "75822e6a-1372-44f9-9575-50d307ad1dfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: ['wav2vec2.masked_spec_embed', 'wav2vec2.feature_extractor.conv_layers.0.conv.weight', 'wav2vec2.feature_extractor.conv_layers.0.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.0.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.1.conv.weight', 'wav2vec2.feature_extractor.conv_layers.2.conv.weight', 'wav2vec2.feature_extractor.conv_layers.3.conv.weight', 'wav2vec2.feature_extractor.conv_layers.4.conv.weight', 'wav2vec2.feature_extractor.conv_layers.5.conv.weight', 'wav2vec2.feature_extractor.conv_layers.6.conv.weight', 'wav2vec2.feature_projection.layer_norm.weight', 'wav2vec2.feature_projection.layer_norm.bias', 'wav2vec2.feature_projection.projection.weight', 'wav2vec2.feature_projection.projection.bias', 'wav2vec2.encoder.pos_conv_embed.conv.bias', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.layer_norm.weight', 'wav2vec2.encoder.layer_norm.bias', 'wav2vec2.encoder.layers.0.attention.k_proj.weight', 'wav2vec2.encoder.layers.0.attention.k_proj.bias', 'wav2vec2.encoder.layers.0.attention.v_proj.weight', 'wav2vec2.encoder.layers.0.attention.v_proj.bias', 'wav2vec2.encoder.layers.0.attention.q_proj.weight', 'wav2vec2.encoder.layers.0.attention.q_proj.bias', 'wav2vec2.encoder.layers.0.attention.out_proj.weight', 'wav2vec2.encoder.layers.0.attention.out_proj.bias', 'wav2vec2.encoder.layers.0.layer_norm.weight', 'wav2vec2.encoder.layers.0.layer_norm.bias', 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.0.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.0.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.0.final_layer_norm.weight', 'wav2vec2.encoder.layers.0.final_layer_norm.bias', 'wav2vec2.encoder.layers.1.attention.k_proj.weight', 'wav2vec2.encoder.layers.1.attention.k_proj.bias', 'wav2vec2.encoder.layers.1.attention.v_proj.weight', 'wav2vec2.encoder.layers.1.attention.v_proj.bias', 'wav2vec2.encoder.layers.1.attention.q_proj.weight', 'wav2vec2.encoder.layers.1.attention.q_proj.bias', 'wav2vec2.encoder.layers.1.attention.out_proj.weight', 'wav2vec2.encoder.layers.1.attention.out_proj.bias', 'wav2vec2.encoder.layers.1.layer_norm.weight', 'wav2vec2.encoder.layers.1.layer_norm.bias', 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.1.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.1.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.1.final_layer_norm.weight', 'wav2vec2.encoder.layers.1.final_layer_norm.bias', 'wav2vec2.encoder.layers.2.attention.k_proj.weight', 'wav2vec2.encoder.layers.2.attention.k_proj.bias', 'wav2vec2.encoder.layers.2.attention.v_proj.weight', 'wav2vec2.encoder.layers.2.attention.v_proj.bias', 'wav2vec2.encoder.layers.2.attention.q_proj.weight', 'wav2vec2.encoder.layers.2.attention.q_proj.bias', 'wav2vec2.encoder.layers.2.attention.out_proj.weight', 'wav2vec2.encoder.layers.2.attention.out_proj.bias', 'wav2vec2.encoder.layers.2.layer_norm.weight', 'wav2vec2.encoder.layers.2.layer_norm.bias', 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.2.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.2.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.2.final_layer_norm.weight', 'wav2vec2.encoder.layers.2.final_layer_norm.bias', 'wav2vec2.encoder.layers.3.attention.k_proj.weight', 'wav2vec2.encoder.layers.3.attention.k_proj.bias', 'wav2vec2.encoder.layers.3.attention.v_proj.weight', 'wav2vec2.encoder.layers.3.attention.v_proj.bias', 'wav2vec2.encoder.layers.3.attention.q_proj.weight', 'wav2vec2.encoder.layers.3.attention.q_proj.bias', 'wav2vec2.encoder.layers.3.attention.out_proj.weight', 'wav2vec2.encoder.layers.3.attention.out_proj.bias', 'wav2vec2.encoder.layers.3.layer_norm.weight', 'wav2vec2.encoder.layers.3.layer_norm.bias', 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.3.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.3.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.3.final_layer_norm.weight', 'wav2vec2.encoder.layers.3.final_layer_norm.bias', 'wav2vec2.encoder.layers.4.attention.k_proj.weight', 'wav2vec2.encoder.layers.4.attention.k_proj.bias', 'wav2vec2.encoder.layers.4.attention.v_proj.weight', 'wav2vec2.encoder.layers.4.attention.v_proj.bias', 'wav2vec2.encoder.layers.4.attention.q_proj.weight', 'wav2vec2.encoder.layers.4.attention.q_proj.bias', 'wav2vec2.encoder.layers.4.attention.out_proj.weight', 'wav2vec2.encoder.layers.4.attention.out_proj.bias', 'wav2vec2.encoder.layers.4.layer_norm.weight', 'wav2vec2.encoder.layers.4.layer_norm.bias', 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.4.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.4.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.4.final_layer_norm.weight', 'wav2vec2.encoder.layers.4.final_layer_norm.bias', 'wav2vec2.encoder.layers.5.attention.k_proj.weight', 'wav2vec2.encoder.layers.5.attention.k_proj.bias', 'wav2vec2.encoder.layers.5.attention.v_proj.weight', 'wav2vec2.encoder.layers.5.attention.v_proj.bias', 'wav2vec2.encoder.layers.5.attention.q_proj.weight', 'wav2vec2.encoder.layers.5.attention.q_proj.bias', 'wav2vec2.encoder.layers.5.attention.out_proj.weight', 'wav2vec2.encoder.layers.5.attention.out_proj.bias', 'wav2vec2.encoder.layers.5.layer_norm.weight', 'wav2vec2.encoder.layers.5.layer_norm.bias', 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.5.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.5.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.5.final_layer_norm.weight', 'wav2vec2.encoder.layers.5.final_layer_norm.bias', 'wav2vec2.encoder.layers.6.attention.k_proj.weight', 'wav2vec2.encoder.layers.6.attention.k_proj.bias', 'wav2vec2.encoder.layers.6.attention.v_proj.weight', 'wav2vec2.encoder.layers.6.attention.v_proj.bias', 'wav2vec2.encoder.layers.6.attention.q_proj.weight', 'wav2vec2.encoder.layers.6.attention.q_proj.bias', 'wav2vec2.encoder.layers.6.attention.out_proj.weight', 'wav2vec2.encoder.layers.6.attention.out_proj.bias', 'wav2vec2.encoder.layers.6.layer_norm.weight', 'wav2vec2.encoder.layers.6.layer_norm.bias', 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.6.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.6.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.6.final_layer_norm.weight', 'wav2vec2.encoder.layers.6.final_layer_norm.bias', 'wav2vec2.encoder.layers.7.attention.k_proj.weight', 'wav2vec2.encoder.layers.7.attention.k_proj.bias', 'wav2vec2.encoder.layers.7.attention.v_proj.weight', 'wav2vec2.encoder.layers.7.attention.v_proj.bias', 'wav2vec2.encoder.layers.7.attention.q_proj.weight', 'wav2vec2.encoder.layers.7.attention.q_proj.bias', 'wav2vec2.encoder.layers.7.attention.out_proj.weight', 'wav2vec2.encoder.layers.7.attention.out_proj.bias', 'wav2vec2.encoder.layers.7.layer_norm.weight', 'wav2vec2.encoder.layers.7.layer_norm.bias', 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.7.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.7.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.7.final_layer_norm.weight', 'wav2vec2.encoder.layers.7.final_layer_norm.bias', 'wav2vec2.encoder.layers.8.attention.k_proj.weight', 'wav2vec2.encoder.layers.8.attention.k_proj.bias', 'wav2vec2.encoder.layers.8.attention.v_proj.weight', 'wav2vec2.encoder.layers.8.attention.v_proj.bias', 'wav2vec2.encoder.layers.8.attention.q_proj.weight', 'wav2vec2.encoder.layers.8.attention.q_proj.bias', 'wav2vec2.encoder.layers.8.attention.out_proj.weight', 'wav2vec2.encoder.layers.8.attention.out_proj.bias', 'wav2vec2.encoder.layers.8.layer_norm.weight', 'wav2vec2.encoder.layers.8.layer_norm.bias', 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.8.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.8.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.8.final_layer_norm.weight', 'wav2vec2.encoder.layers.8.final_layer_norm.bias', 'wav2vec2.encoder.layers.9.attention.k_proj.weight', 'wav2vec2.encoder.layers.9.attention.k_proj.bias', 'wav2vec2.encoder.layers.9.attention.v_proj.weight', 'wav2vec2.encoder.layers.9.attention.v_proj.bias', 'wav2vec2.encoder.layers.9.attention.q_proj.weight', 'wav2vec2.encoder.layers.9.attention.q_proj.bias', 'wav2vec2.encoder.layers.9.attention.out_proj.weight', 'wav2vec2.encoder.layers.9.attention.out_proj.bias', 'wav2vec2.encoder.layers.9.layer_norm.weight', 'wav2vec2.encoder.layers.9.layer_norm.bias', 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.9.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.9.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.9.final_layer_norm.weight', 'wav2vec2.encoder.layers.9.final_layer_norm.bias', 'wav2vec2.encoder.layers.10.attention.k_proj.weight', 'wav2vec2.encoder.layers.10.attention.k_proj.bias', 'wav2vec2.encoder.layers.10.attention.v_proj.weight', 'wav2vec2.encoder.layers.10.attention.v_proj.bias', 'wav2vec2.encoder.layers.10.attention.q_proj.weight', 'wav2vec2.encoder.layers.10.attention.q_proj.bias', 'wav2vec2.encoder.layers.10.attention.out_proj.weight', 'wav2vec2.encoder.layers.10.attention.out_proj.bias', 'wav2vec2.encoder.layers.10.layer_norm.weight', 'wav2vec2.encoder.layers.10.layer_norm.bias', 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.10.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.10.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.10.final_layer_norm.weight', 'wav2vec2.encoder.layers.10.final_layer_norm.bias', 'wav2vec2.encoder.layers.11.attention.k_proj.weight', 'wav2vec2.encoder.layers.11.attention.k_proj.bias', 'wav2vec2.encoder.layers.11.attention.v_proj.weight', 'wav2vec2.encoder.layers.11.attention.v_proj.bias', 'wav2vec2.encoder.layers.11.attention.q_proj.weight', 'wav2vec2.encoder.layers.11.attention.q_proj.bias', 'wav2vec2.encoder.layers.11.attention.out_proj.weight', 'wav2vec2.encoder.layers.11.attention.out_proj.bias', 'wav2vec2.encoder.layers.11.layer_norm.weight', 'wav2vec2.encoder.layers.11.layer_norm.bias', 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.11.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.11.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.11.final_layer_norm.weight', 'wav2vec2.encoder.layers.11.final_layer_norm.bias', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.weight', 'project_hid.bias', 'project_q.weight', 'project_q.bias']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3qlqJKPWxYd",
        "outputId": "1b98ff4a-ab5b-4853-9e77-a192fbd7bb79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wav2vec2.masked_spec_embed',\n",
              " 'wav2vec2.feature_extractor.conv_layers.0.conv.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.0.layer_norm.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.0.layer_norm.bias',\n",
              " 'wav2vec2.feature_extractor.conv_layers.1.conv.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.2.conv.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.3.conv.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.4.conv.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.5.conv.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.6.conv.weight',\n",
              " 'wav2vec2.feature_projection.layer_norm.weight',\n",
              " 'wav2vec2.feature_projection.layer_norm.bias',\n",
              " 'wav2vec2.feature_projection.projection.weight',\n",
              " 'wav2vec2.feature_projection.projection.bias',\n",
              " 'wav2vec2.encoder.pos_conv_embed.conv.bias',\n",
              " 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0',\n",
              " 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1',\n",
              " 'wav2vec2.encoder.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.0.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.0.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.0.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.0.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.0.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.0.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.0.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.0.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.0.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.0.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.0.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.0.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.0.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.0.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.1.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.1.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.1.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.1.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.1.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.1.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.1.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.1.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.1.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.1.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.1.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.1.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.1.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.1.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.2.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.2.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.2.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.2.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.2.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.2.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.2.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.2.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.2.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.2.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.2.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.2.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.2.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.2.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.3.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.3.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.3.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.3.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.3.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.3.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.3.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.3.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.3.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.3.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.3.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.3.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.3.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.3.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.4.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.4.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.4.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.4.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.4.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.4.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.4.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.4.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.4.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.4.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.4.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.4.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.4.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.4.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.5.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.5.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.5.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.5.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.5.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.5.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.5.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.5.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.5.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.5.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.5.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.5.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.5.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.5.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.6.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.6.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.6.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.6.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.6.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.6.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.6.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.6.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.6.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.6.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.6.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.6.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.6.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.6.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.7.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.7.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.7.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.7.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.7.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.7.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.7.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.7.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.7.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.7.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.7.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.7.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.7.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.7.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.8.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.8.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.8.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.8.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.8.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.8.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.8.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.8.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.8.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.8.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.8.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.8.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.8.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.8.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.9.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.9.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.9.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.9.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.9.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.9.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.9.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.9.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.9.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.9.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.9.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.9.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.9.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.9.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.10.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.10.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.10.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.10.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.10.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.10.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.10.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.10.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.10.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.10.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.10.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.10.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.10.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.10.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.11.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.11.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.11.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.11.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.11.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.11.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.11.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.11.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.11.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.11.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.11.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.11.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.11.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.11.final_layer_norm.bias',\n",
              " 'quantizer.codevectors',\n",
              " 'quantizer.weight_proj.weight',\n",
              " 'quantizer.weight_proj.bias',\n",
              " 'project_hid.weight',\n",
              " 'project_hid.bias',\n",
              " 'project_q.weight',\n",
              " 'project_q.bias']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b9c5302fc5054a748992c1a79f59938b",
            "ec7d78fe76d74844bb9f32fed2e7f848",
            "3056c28bde6f49749d69c31878caeb8c",
            "4f26062f0f7540e7aba9d0b979849238",
            "230462c4ecea47bc82f999f36010374a",
            "9e0853ed33da4761ba0118d949069050",
            "cd2de418c8b5491aa6436a53fc243c7c",
            "860772e4d1bb451eb5d2de6fc318ba90",
            "a7b5e6e4a4fb4705bf5f0d73522c4b11",
            "6195cfdf498e4c8e9bd89152eef175d0",
            "b65c1bea95444e63a75e6cfe4b1b3878"
          ]
        },
        "id": "2vlMVnu1WhOB",
        "outputId": "9527cb88-0846-4c5b-bd48-b3a614dd032f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:***** Runing training *****\n",
            "INFO:__main__: Num examples = 2641\n",
            "INFO:__main__: Num Epochs = 50\n",
            "INFO:__main__:  Instantaneous batch size per device = 16\n",
            "INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "INFO:__main__:  Gradient Accumulation steps = 4\n",
            "INFO:__main__:  Total optimization steps = None\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9c5302fc5054a748992c1a79f59938b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 5.360e+00| contrast_loss: 4.623e+00| div_loss: 5.945e-01| inter_sim_loss: 7.514e-01| %_mask_idx: 5.841e-02| ppl: 2.595e+02| lr: 1.000e-05| temp: 2.000e+00| grad_norm: 5.486e-01\n",
            "| loss: 5.369e+00| contrast_loss: 4.633e+00| div_loss: 4.992e-01| inter_sim_loss: 7.513e-01| %_mask_idx: 5.688e-02| ppl: 3.205e+02| lr: 2.000e-05| temp: 2.000e+00| grad_norm: 3.650e-01\n",
            "| loss: 5.352e+00| contrast_loss: 4.624e+00| div_loss: 5.217e-01| inter_sim_loss: 7.513e-01| %_mask_idx: 5.745e-02| ppl: 3.061e+02| lr: 3.000e-05| temp: 2.000e+00| grad_norm: 2.548e-01\n",
            "| loss: 5.640e+00| contrast_loss: 4.618e+00| div_loss: 4.981e-01| inter_sim_loss: 7.511e-01| %_mask_idx: 3.870e-02| ppl: 3.212e+02| lr: 4.000e-05| temp: 2.000e+00| grad_norm: 2.645e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.675e+00| val_contrastive_loss: 4.617e+00| val_diversity_loss: 5.789e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 5.317e+00| contrast_loss: 4.616e+00| div_loss: 4.111e-01| inter_sim_loss: 7.508e-01| %_mask_idx: 5.824e-02| ppl: 3.769e+02| lr: 5.200e-05| temp: 2.000e+00| grad_norm: 1.960e-01\n",
            "| loss: 5.332e+00| contrast_loss: 4.616e+00| div_loss: 3.826e-01| inter_sim_loss: 7.505e-01| %_mask_idx: 6.008e-02| ppl: 3.952e+02| lr: 6.200e-05| temp: 2.000e+00| grad_norm: 2.087e-01\n",
            "| loss: 5.601e+00| contrast_loss: 4.617e+00| div_loss: 4.596e-01| inter_sim_loss: 7.502e-01| %_mask_idx: 4.008e-02| ppl: 3.459e+02| lr: 7.200e-05| temp: 2.000e+00| grad_norm: 2.241e-01\n",
            "| loss: 5.312e+00| contrast_loss: 4.619e+00| div_loss: 3.486e-01| inter_sim_loss: 7.498e-01| %_mask_idx: 5.984e-02| ppl: 4.169e+02| lr: 8.200e-05| temp: 2.000e+00| grad_norm: 2.188e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.667e+00| val_contrastive_loss: 4.615e+00| val_diversity_loss: 5.204e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 5.664e+00| contrast_loss: 4.618e+00| div_loss: 4.681e-01| inter_sim_loss: 7.491e-01| %_mask_idx: 3.962e-02| ppl: 3.404e+02| lr: 9.400e-05| temp: 2.000e+00| grad_norm: 2.274e-01\n",
            "| loss: 5.280e+00| contrast_loss: 4.615e+00| div_loss: 3.811e-01| inter_sim_loss: 7.486e-01| %_mask_idx: 6.020e-02| ppl: 3.961e+02| lr: 9.980e-05| temp: 2.000e+00| grad_norm: 1.795e-01\n",
            "| loss: 5.620e+00| contrast_loss: 4.616e+00| div_loss: 3.843e-01| inter_sim_loss: 7.482e-01| %_mask_idx: 4.151e-02| ppl: 3.941e+02| lr: 9.930e-05| temp: 2.000e+00| grad_norm: 2.000e-01\n",
            "| loss: 5.297e+00| contrast_loss: 4.614e+00| div_loss: 3.793e-01| inter_sim_loss: 7.478e-01| %_mask_idx: 5.812e-02| ppl: 3.972e+02| lr: 9.880e-05| temp: 2.000e+00| grad_norm: 1.768e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.666e+00| val_contrastive_loss: 4.615e+00| val_diversity_loss: 5.160e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 5.589e+00| contrast_loss: 4.617e+00| div_loss: 3.846e-01| inter_sim_loss: 7.470e-01| %_mask_idx: 4.045e-02| ppl: 3.939e+02| lr: 9.820e-05| temp: 2.000e+00| grad_norm: 2.239e-01\n",
            "| loss: 5.402e+00| contrast_loss: 4.613e+00| div_loss: 4.023e-01| inter_sim_loss: 7.465e-01| %_mask_idx: 5.283e-02| ppl: 3.826e+02| lr: 9.770e-05| temp: 2.000e+00| grad_norm: 1.797e-01\n",
            "| loss: 5.621e+00| contrast_loss: 4.617e+00| div_loss: 4.154e-01| inter_sim_loss: 7.460e-01| %_mask_idx: 4.097e-02| ppl: 3.741e+02| lr: 9.720e-05| temp: 2.000e+00| grad_norm: 2.032e-01\n",
            "| loss: 5.296e+00| contrast_loss: 4.616e+00| div_loss: 3.348e-01| inter_sim_loss: 7.455e-01| %_mask_idx: 5.844e-02| ppl: 4.257e+02| lr: 9.670e-05| temp: 2.000e+00| grad_norm: 1.759e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.666e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 5.215e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 5.637e+00| contrast_loss: 4.616e+00| div_loss: 4.785e-01| inter_sim_loss: 7.447e-01| %_mask_idx: 3.924e-02| ppl: 3.338e+02| lr: 9.610e-05| temp: 2.000e+00| grad_norm: 1.986e-01\n",
            "| loss: 5.347e+00| contrast_loss: 4.617e+00| div_loss: 3.904e-01| inter_sim_loss: 7.442e-01| %_mask_idx: 5.767e-02| ppl: 3.901e+02| lr: 9.560e-05| temp: 2.000e+00| grad_norm: 1.724e-01\n",
            "| loss: 5.586e+00| contrast_loss: 4.614e+00| div_loss: 4.222e-01| inter_sim_loss: 7.437e-01| %_mask_idx: 4.084e-02| ppl: 3.698e+02| lr: 9.510e-05| temp: 2.000e+00| grad_norm: 1.831e-01\n",
            "| loss: 5.667e+00| contrast_loss: 4.610e+00| div_loss: 4.671e-01| inter_sim_loss: 7.432e-01| %_mask_idx: 3.806e-02| ppl: 3.411e+02| lr: 9.460e-05| temp: 2.000e+00| grad_norm: 1.940e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.666e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 5.187e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 5.285e+00| contrast_loss: 4.614e+00| div_loss: 3.719e-01| inter_sim_loss: 7.424e-01| %_mask_idx: 5.964e-02| ppl: 4.020e+02| lr: 9.400e-05| temp: 2.000e+00| grad_norm: 1.601e-01\n",
            "| loss: 5.316e+00| contrast_loss: 4.615e+00| div_loss: 3.336e-01| inter_sim_loss: 7.419e-01| %_mask_idx: 5.965e-02| ppl: 4.265e+02| lr: 9.350e-05| temp: 2.000e+00| grad_norm: 1.684e-01\n",
            "| loss: 5.308e+00| contrast_loss: 4.613e+00| div_loss: 3.619e-01| inter_sim_loss: 7.414e-01| %_mask_idx: 5.965e-02| ppl: 4.084e+02| lr: 9.300e-05| temp: 2.000e+00| grad_norm: 1.657e-01\n",
            "| loss: 5.285e+00| contrast_loss: 4.618e+00| div_loss: 3.109e-01| inter_sim_loss: 7.410e-01| %_mask_idx: 6.020e-02| ppl: 4.410e+02| lr: 9.250e-05| temp: 2.000e+00| grad_norm: 1.567e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.665e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 5.075e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 5.652e+00| contrast_loss: 4.619e+00| div_loss: 4.607e-01| inter_sim_loss: 7.402e-01| %_mask_idx: 4.021e-02| ppl: 3.452e+02| lr: 9.190e-05| temp: 1.999e+00| grad_norm: 1.875e-01\n",
            "| loss: 5.584e+00| contrast_loss: 4.616e+00| div_loss: 4.300e-01| inter_sim_loss: 7.397e-01| %_mask_idx: 4.008e-02| ppl: 3.648e+02| lr: 9.140e-05| temp: 1.999e+00| grad_norm: 1.908e-01\n",
            "| loss: 5.298e+00| contrast_loss: 4.618e+00| div_loss: 3.101e-01| inter_sim_loss: 7.392e-01| %_mask_idx: 5.804e-02| ppl: 4.415e+02| lr: 9.090e-05| temp: 1.999e+00| grad_norm: 1.529e-01\n",
            "| loss: 5.609e+00| contrast_loss: 4.616e+00| div_loss: 3.949e-01| inter_sim_loss: 7.387e-01| %_mask_idx: 4.215e-02| ppl: 3.872e+02| lr: 9.040e-05| temp: 1.999e+00| grad_norm: 1.678e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.665e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 5.146e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 5.289e+00| contrast_loss: 4.617e+00| div_loss: 3.110e-01| inter_sim_loss: 7.379e-01| %_mask_idx: 5.821e-02| ppl: 4.410e+02| lr: 8.980e-05| temp: 1.999e+00| grad_norm: 1.626e-01\n",
            "| loss: 5.280e+00| contrast_loss: 4.615e+00| div_loss: 3.123e-01| inter_sim_loss: 7.374e-01| %_mask_idx: 5.967e-02| ppl: 4.402e+02| lr: 8.930e-05| temp: 1.999e+00| grad_norm: 1.474e-01\n",
            "| loss: 5.709e+00| contrast_loss: 4.615e+00| div_loss: 4.174e-01| inter_sim_loss: 7.369e-01| %_mask_idx: 3.820e-02| ppl: 3.729e+02| lr: 8.880e-05| temp: 1.999e+00| grad_norm: 1.890e-01\n",
            "| loss: 5.604e+00| contrast_loss: 4.614e+00| div_loss: 3.968e-01| inter_sim_loss: 7.364e-01| %_mask_idx: 3.988e-02| ppl: 3.860e+02| lr: 8.830e-05| temp: 1.999e+00| grad_norm: 1.804e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.664e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 4.995e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 5.271e+00| contrast_loss: 4.616e+00| div_loss: 3.162e-01| inter_sim_loss: 7.356e-01| %_mask_idx: 5.912e-02| ppl: 4.376e+02| lr: 8.770e-05| temp: 1.999e+00| grad_norm: 1.573e-01\n",
            "| loss: 5.605e+00| contrast_loss: 4.614e+00| div_loss: 4.182e-01| inter_sim_loss: 7.350e-01| %_mask_idx: 4.002e-02| ppl: 3.724e+02| lr: 8.720e-05| temp: 1.999e+00| grad_norm: 1.737e-01\n",
            "| loss: 5.579e+00| contrast_loss: 4.617e+00| div_loss: 4.435e-01| inter_sim_loss: 7.345e-01| %_mask_idx: 4.044e-02| ppl: 3.562e+02| lr: 8.670e-05| temp: 1.999e+00| grad_norm: 1.809e-01\n",
            "| loss: 5.626e+00| contrast_loss: 4.615e+00| div_loss: 4.204e-01| inter_sim_loss: 7.340e-01| %_mask_idx: 3.907e-02| ppl: 3.709e+02| lr: 8.620e-05| temp: 1.999e+00| grad_norm: 1.775e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.663e+00| val_contrastive_loss: 4.612e+00| val_diversity_loss: 5.051e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 5.297e+00| contrast_loss: 4.615e+00| div_loss: 3.178e-01| inter_sim_loss: 7.332e-01| %_mask_idx: 5.776e-02| ppl: 4.366e+02| lr: 8.560e-05| temp: 1.999e+00| grad_norm: 1.572e-01\n",
            "| loss: 5.637e+00| contrast_loss: 4.612e+00| div_loss: 4.223e-01| inter_sim_loss: 7.326e-01| %_mask_idx: 3.911e-02| ppl: 3.697e+02| lr: 8.510e-05| temp: 1.999e+00| grad_norm: 1.634e-01\n",
            "| loss: 5.605e+00| contrast_loss: 4.611e+00| div_loss: 4.594e-01| inter_sim_loss: 7.321e-01| %_mask_idx: 3.973e-02| ppl: 3.460e+02| lr: 8.460e-05| temp: 1.999e+00| grad_norm: 1.798e-01\n",
            "| loss: 5.272e+00| contrast_loss: 4.617e+00| div_loss: 3.093e-01| inter_sim_loss: 7.316e-01| %_mask_idx: 5.888e-02| ppl: 4.421e+02| lr: 8.410e-05| temp: 1.999e+00| grad_norm: 1.513e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.663e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 4.981e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 5.604e+00| contrast_loss: 4.615e+00| div_loss: 4.678e-01| inter_sim_loss: 7.308e-01| %_mask_idx: 4.111e-02| ppl: 3.406e+02| lr: 8.350e-05| temp: 1.999e+00| grad_norm: 1.791e-01\n",
            "| loss: 5.565e+00| contrast_loss: 4.613e+00| div_loss: 3.914e-01| inter_sim_loss: 7.303e-01| %_mask_idx: 4.008e-02| ppl: 3.895e+02| lr: 8.300e-05| temp: 1.999e+00| grad_norm: 1.873e-01\n",
            "| loss: 5.609e+00| contrast_loss: 4.616e+00| div_loss: 4.458e-01| inter_sim_loss: 7.299e-01| %_mask_idx: 4.017e-02| ppl: 3.547e+02| lr: 8.250e-05| temp: 1.999e+00| grad_norm: 1.761e-01\n",
            "| loss: 5.288e+00| contrast_loss: 4.615e+00| div_loss: 3.581e-01| inter_sim_loss: 7.294e-01| %_mask_idx: 5.906e-02| ppl: 4.108e+02| lr: 8.200e-05| temp: 1.999e+00| grad_norm: 1.402e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.664e+00| val_contrastive_loss: 4.613e+00| val_diversity_loss: 5.121e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 5.608e+00| contrast_loss: 4.618e+00| div_loss: 4.048e-01| inter_sim_loss: 7.287e-01| %_mask_idx: 3.878e-02| ppl: 3.809e+02| lr: 8.140e-05| temp: 1.999e+00| grad_norm: 1.867e-01\n",
            "| loss: 5.597e+00| contrast_loss: 4.616e+00| div_loss: 4.146e-01| inter_sim_loss: 7.282e-01| %_mask_idx: 4.003e-02| ppl: 3.747e+02| lr: 8.090e-05| temp: 1.999e+00| grad_norm: 1.622e-01\n",
            "| loss: 5.591e+00| contrast_loss: 4.614e+00| div_loss: 4.068e-01| inter_sim_loss: 7.278e-01| %_mask_idx: 3.918e-02| ppl: 3.796e+02| lr: 8.040e-05| temp: 1.999e+00| grad_norm: 1.775e-01\n",
            "| loss: 5.280e+00| contrast_loss: 4.615e+00| div_loss: 3.287e-01| inter_sim_loss: 7.274e-01| %_mask_idx: 6.008e-02| ppl: 4.296e+02| lr: 7.990e-05| temp: 1.999e+00| grad_norm: 1.536e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.663e+00| val_contrastive_loss: 4.613e+00| val_diversity_loss: 4.988e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 5.337e+00| contrast_loss: 4.615e+00| div_loss: 3.305e-01| inter_sim_loss: 7.267e-01| %_mask_idx: 5.686e-02| ppl: 4.285e+02| lr: 7.930e-05| temp: 1.999e+00| grad_norm: 1.544e-01\n",
            "| loss: 5.274e+00| contrast_loss: 4.618e+00| div_loss: 2.847e-01| inter_sim_loss: 7.263e-01| %_mask_idx: 5.885e-02| ppl: 4.578e+02| lr: 7.880e-05| temp: 1.999e+00| grad_norm: 1.411e-01\n",
            "| loss: 5.570e+00| contrast_loss: 4.616e+00| div_loss: 4.059e-01| inter_sim_loss: 7.259e-01| %_mask_idx: 4.073e-02| ppl: 3.802e+02| lr: 7.830e-05| temp: 1.999e+00| grad_norm: 1.643e-01\n",
            "| loss: 5.297e+00| contrast_loss: 4.616e+00| div_loss: 3.100e-01| inter_sim_loss: 7.254e-01| %_mask_idx: 5.725e-02| ppl: 4.416e+02| lr: 7.780e-05| temp: 1.999e+00| grad_norm: 1.431e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.664e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 5.049e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 5.592e+00| contrast_loss: 4.617e+00| div_loss: 4.077e-01| inter_sim_loss: 7.247e-01| %_mask_idx: 3.996e-02| ppl: 3.791e+02| lr: 7.720e-05| temp: 1.999e+00| grad_norm: 1.644e-01\n",
            "| loss: 5.590e+00| contrast_loss: 4.614e+00| div_loss: 4.205e-01| inter_sim_loss: 7.242e-01| %_mask_idx: 3.990e-02| ppl: 3.709e+02| lr: 7.670e-05| temp: 1.999e+00| grad_norm: 1.739e-01\n",
            "| loss: 5.322e+00| contrast_loss: 4.615e+00| div_loss: 3.310e-01| inter_sim_loss: 7.238e-01| %_mask_idx: 5.684e-02| ppl: 4.282e+02| lr: 7.620e-05| temp: 1.999e+00| grad_norm: 1.577e-01\n",
            "| loss: 5.604e+00| contrast_loss: 4.617e+00| div_loss: 4.175e-01| inter_sim_loss: 7.234e-01| %_mask_idx: 3.917e-02| ppl: 3.728e+02| lr: 7.570e-05| temp: 1.999e+00| grad_norm: 1.508e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.663e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 4.944e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 5.275e+00| contrast_loss: 4.614e+00| div_loss: 3.294e-01| inter_sim_loss: 7.227e-01| %_mask_idx: 5.942e-02| ppl: 4.292e+02| lr: 7.510e-05| temp: 1.999e+00| grad_norm: 1.607e-01\n",
            "| loss: 5.248e+00| contrast_loss: 4.616e+00| div_loss: 3.059e-01| inter_sim_loss: 7.223e-01| %_mask_idx: 6.012e-02| ppl: 4.442e+02| lr: 7.460e-05| temp: 1.999e+00| grad_norm: 1.460e-01\n",
            "| loss: 5.590e+00| contrast_loss: 4.616e+00| div_loss: 4.227e-01| inter_sim_loss: 7.220e-01| %_mask_idx: 4.034e-02| ppl: 3.695e+02| lr: 7.410e-05| temp: 1.999e+00| grad_norm: 1.811e-01\n",
            "| loss: 5.587e+00| contrast_loss: 4.615e+00| div_loss: 4.111e-01| inter_sim_loss: 7.216e-01| %_mask_idx: 3.961e-02| ppl: 3.769e+02| lr: 7.360e-05| temp: 1.999e+00| grad_norm: 1.683e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.662e+00| val_contrastive_loss: 4.613e+00| val_diversity_loss: 4.976e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| loss: 5.584e+00| contrast_loss: 4.614e+00| div_loss: 3.958e-01| inter_sim_loss: 7.210e-01| %_mask_idx: 4.007e-02| ppl: 3.867e+02| lr: 7.300e-05| temp: 1.999e+00| grad_norm: 1.642e-01\n",
            "| loss: 5.320e+00| contrast_loss: 4.617e+00| div_loss: 3.060e-01| inter_sim_loss: 7.205e-01| %_mask_idx: 5.628e-02| ppl: 4.441e+02| lr: 7.250e-05| temp: 1.999e+00| grad_norm: 1.504e-01\n",
            "| loss: 5.551e+00| contrast_loss: 4.613e+00| div_loss: 3.793e-01| inter_sim_loss: 7.201e-01| %_mask_idx: 4.008e-02| ppl: 3.973e+02| lr: 7.200e-05| temp: 1.999e+00| grad_norm: 1.579e-01\n",
            "| loss: 5.326e+00| contrast_loss: 4.617e+00| div_loss: 3.543e-01| inter_sim_loss: 7.197e-01| %_mask_idx: 5.741e-02| ppl: 4.133e+02| lr: 7.150e-05| temp: 1.999e+00| grad_norm: 1.442e-01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| val_loss: 4.662e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 4.841e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        }
      ],
      "source": [
        "total_batch_size = TRAIN_BATCH_SIZE * accelerator.num_processes * GRADIENT_ACCUMULATION_STEPS\n",
        "logger.info(\"***** Runing training *****\")\n",
        "logger.info(f\" Num examples = {len(vectorized_datasets['train'])}\")\n",
        "logger.info(f\" Num Epochs = {NUM_TRAIN_EPOCHS}\")\n",
        "logger.info(f\"  Instantaneous batch size per device = {TRAIN_BATCH_SIZE}\")\n",
        "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "logger.info(f\"  Gradient Accumulation steps = {GRADIENT_ACCUMULATION_STEPS}\")\n",
        "logger.info(f\"  Total optimization steps = {MAX_TRAINING_STEPS}\")\n",
        "\n",
        "completed_steps = 0\n",
        "starting_epoch = 0\n",
        "\n",
        "progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "completed_steps = 0\n",
        "starting_epoch = 0\n",
        "\n",
        "for epoch in range(starting_epoch, NUM_TRAIN_EPOCHS):\n",
        "  model.train()\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    num_losses = batch[\"mask_time_indices\"].sum()\n",
        "    sub_attention_mask = batch.pop(\"sub_attention_mask\", None)\n",
        "    sub_attention_mask = (\n",
        "        sub_attention_mask if sub_attention_mask is not None else torch.ones_like(batch[\"mask_time_indices\"])\n",
        "    )\n",
        "    precent_masked = num_losses / sub_attention_mask.sum()\n",
        "\n",
        "    outputs = model(**batch)\n",
        "\n",
        "    inter_sim_loss = inter_codebook_similarity_loss(model.quantizer.codevectors)\n",
        "    # total_loss = outputs.loss + INTER_CB_SIMILARITY_WEIGHT * inter_sim_loss\n",
        "\n",
        "    loss = (outputs.loss / GRADIENT_ACCUMULATION_STEPS) + INTER_CB_SIMILARITY_WEIGHT * inter_sim_loss\n",
        "    accelerator.backward(loss)\n",
        "\n",
        "    if accelerator.state.num_processes > 1:\n",
        "      num_losses = accelerator.gather_for_metrics(num_losses).sum()\n",
        "      gradient_multiplier = accelerator.state.num_processes / num_losses\n",
        "      multiply_grads(model.parameters(), gradient_multiplier)\n",
        "    else:\n",
        "      multiply_grads(model.parameters(), 1 / num_losses)\n",
        "\n",
        "    # Update step\n",
        "    if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0 or step == len(train_dataloader) - 1:\n",
        "      scale = (\n",
        "          accelerator.scaler._scale.item()\n",
        "          if hasattr(accelerator, \"scaler\") and accelerator.scaler is not None\n",
        "          else 1.0\n",
        "      )\n",
        "      if accelerator.state.num_processes > 1:\n",
        "        grad_norm = get_grad_norm(model.module.parameters(), scale)\n",
        "      else:\n",
        "        grad_norm = get_grad_norm(model.parameters(), scale)\n",
        "\n",
        "      accelerator.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      if not accelerator.optimizer_step_was_skipped:\n",
        "        lr_scheduler.step()\n",
        "      elif accelerator.is_local_main_process:\n",
        "        progress_bar.write(\n",
        "          f\"Gradients have overflown - skipping update step... Updating gradient scale to {scale}...\"\n",
        "        )\n",
        "\n",
        "      # update gumbel temperature\n",
        "      gumble_temperature = max(\n",
        "          MAX_GUMBEL_TEMPERATURE * GUMBEL_TEMPERATURE**completed_steps,\n",
        "          MIN_GUMBEL_TEMPERATURE,\n",
        "      )\n",
        "\n",
        "      if hasattr(model, \"module\"):\n",
        "        model.module.set_gumbel_temperature(gumble_temperature)\n",
        "      else:\n",
        "        model.set_gumbel_temperature(gumble_temperature)\n",
        "\n",
        "      progress_bar.update(1)\n",
        "      completed_steps += 1\n",
        "\n",
        "      # Log all results\n",
        "      if (step + 1) % (GRADIENT_ACCUMULATION_STEPS * LOGGING_STEPS) == 0:\n",
        "        loss.detach()\n",
        "        outputs.contrastive_loss.detach()\n",
        "        outputs.diversity_loss.detach()\n",
        "\n",
        "        if accelerator.state.num_processes > 1:\n",
        "          loss = accelerator.gather_for_metrics(loss).sum()\n",
        "          outputs.contrastive_loss = accelerator.gather_for_metrics(outputs.contrastive_loss).sum()\n",
        "          outputs.diversity_loss = accelerator.gather_for_metrics(outputs.diversity_loss).sum()\n",
        "          percent_masked = accelerator.gather_for_metrics(precent_masked).sum()\n",
        "\n",
        "        train_logs = {\n",
        "            \"loss\": (loss * GRADIENT_ACCUMULATION_STEPS) / num_losses,\n",
        "            \"contrast_loss\": outputs.contrastive_loss / num_losses,\n",
        "            \"div_loss\": outputs.diversity_loss / num_losses,\n",
        "            \"inter_sim_loss\": inter_sim_loss,\n",
        "            \"%_mask_idx\": precent_masked / accelerator.num_processes,\n",
        "            \"ppl\": outputs.codevector_perplexity,\n",
        "            \"lr\": torch.tensor(optimizer.param_groups[0][\"lr\"]),\n",
        "            \"temp\": torch.tensor(gumble_temperature),\n",
        "            \"grad_norm\": torch.tensor(grad_norm),\n",
        "        }\n",
        "\n",
        "\n",
        "        log_str = \"\"\n",
        "        for k, v in train_logs.items():\n",
        "          log_str += f\"| {k}: {v.item():.3e}\"\n",
        "\n",
        "        if accelerator.is_local_main_process:\n",
        "          progress_bar.write(log_str)\n",
        "          if is_wandb_available():\n",
        "            wandb.log(train_logs)\n",
        "\n",
        "      # save model\n",
        "      if (step + 1) % (GRADIENT_ACCUMULATION_STEPS * SAVING_STEPS) == 0:\n",
        "        if OUTPUT_DIR is not None:\n",
        "            accelerator.wait_for_everyone()\n",
        "            unwrapped_model = accelerator.unwrap_model(model)\n",
        "            unwrapped_model.save_pretrained(\n",
        "                OUTPUT_DIR,\n",
        "                is_main_process=accelerator.is_main_process,\n",
        "                save_function=accelerator.save\n",
        "            )\n",
        "\n",
        "      if completed_steps >= max_train_steps:\n",
        "        break\n",
        "  model.eval()\n",
        "\n",
        "  val_logs = {\n",
        "      \"val_loss\": 0,\n",
        "      \"val_contrastive_loss\": 0,\n",
        "      \"val_diversity_loss\": 0,\n",
        "      \"val_num_losses\": 0,\n",
        "  }\n",
        "\n",
        "  for step, batch in enumerate(val_dataloader):\n",
        "    with torch.no_grad():\n",
        "        batch.pop(\"sub_attention_mask\", None)\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    val_logs[\"val_loss\"] += outputs.loss\n",
        "    val_logs[\"val_contrastive_loss\"] += outputs.contrastive_loss\n",
        "    val_logs[\"val_diversity_loss\"] += outputs.diversity_loss\n",
        "    val_logs[\"val_num_losses\"] += batch[\"mask_time_indices\"].sum()\n",
        "\n",
        "  if accelerator.num_processes > 1:\n",
        "      val_logs = {k: accelerator.gather_for_metrics(v).sum() for k, v in val_logs.items()}\n",
        "\n",
        "  val_logs = {k: v / val_logs[\"val_num_losses\"] for k, v in val_logs.items()}\n",
        "\n",
        "  log_str = \"\"\n",
        "  for k, v in val_logs.items():\n",
        "      log_str += f\"| {k}: {v.item():.3e}\"\n",
        "\n",
        "  if accelerator.is_local_main_process:\n",
        "      progress_bar.write(log_str)\n",
        "      if is_wandb_available():\n",
        "          wandb.log(val_logs)\n",
        "\n",
        "  if OUTPUT_DIR is not None:\n",
        "      accelerator.wait_for_everyone()\n",
        "      unwrapped_model = accelerator.unwrap_model(model)\n",
        "      unwrapped_model.save_pretrained(\n",
        "          OUTPUT_DIR, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G17806dWGy8W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "43143b40b5e84c3e8702fc8ccebdc776": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_baafa043633e42179db2731628e80fe1",
              "IPY_MODEL_268577248d3f4c098b596ddae880700d",
              "IPY_MODEL_9156c02c0ab34d69a52a12b41eeb4c8d"
            ],
            "layout": "IPY_MODEL_820909e43e824fa58eb674987d187ac7"
          }
        },
        "baafa043633e42179db2731628e80fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65b133273a604ae6b2a98dbd8a64df0e",
            "placeholder": "",
            "style": "IPY_MODEL_f9a95dfed0194f438a5c5a4f81235624",
            "value": "Fetching1files:100%"
          }
        },
        "268577248d3f4c098b596ddae880700d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d7fc311fef6417cac7a9865e7912c05",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11d329fed5d8472ebf00b3591173829c",
            "value": 1
          }
        },
        "9156c02c0ab34d69a52a12b41eeb4c8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c5161517e5e426a8b0b3d94d4faf34a",
            "placeholder": "",
            "style": "IPY_MODEL_e9e886663f9e4c44b7557d32bd2546c1",
            "value": "1/1[00:00&lt;00:00,31.28it/s]"
          }
        },
        "820909e43e824fa58eb674987d187ac7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65b133273a604ae6b2a98dbd8a64df0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9a95dfed0194f438a5c5a4f81235624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d7fc311fef6417cac7a9865e7912c05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11d329fed5d8472ebf00b3591173829c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c5161517e5e426a8b0b3d94d4faf34a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9e886663f9e4c44b7557d32bd2546c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9c5302fc5054a748992c1a79f59938b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec7d78fe76d74844bb9f32fed2e7f848",
              "IPY_MODEL_3056c28bde6f49749d69c31878caeb8c",
              "IPY_MODEL_4f26062f0f7540e7aba9d0b979849238"
            ],
            "layout": "IPY_MODEL_230462c4ecea47bc82f999f36010374a"
          }
        },
        "ec7d78fe76d74844bb9f32fed2e7f848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e0853ed33da4761ba0118d949069050",
            "placeholder": "",
            "style": "IPY_MODEL_cd2de418c8b5491aa6436a53fc243c7c",
            "value": "32%"
          }
        },
        "3056c28bde6f49749d69c31878caeb8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_860772e4d1bb451eb5d2de6fc318ba90",
            "max": 2100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7b5e6e4a4fb4705bf5f0d73522c4b11",
            "value": 672
          }
        },
        "4f26062f0f7540e7aba9d0b979849238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6195cfdf498e4c8e9bd89152eef175d0",
            "placeholder": "",
            "style": "IPY_MODEL_b65c1bea95444e63a75e6cfe4b1b3878",
            "value": "672/2100[4:33:13&lt;6:00:14,15.14s/it]"
          }
        },
        "230462c4ecea47bc82f999f36010374a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e0853ed33da4761ba0118d949069050": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd2de418c8b5491aa6436a53fc243c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "860772e4d1bb451eb5d2de6fc318ba90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7b5e6e4a4fb4705bf5f0d73522c4b11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6195cfdf498e4c8e9bd89152eef175d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b65c1bea95444e63a75e6cfe4b1b3878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}