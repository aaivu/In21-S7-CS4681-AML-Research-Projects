{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9syxLSLYKoJB",
        "outputId": "2e517a04-69bd-4685-f598-d5115dc72409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchcodec\n",
            "  Downloading torchcodec-0.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.4 kB)\n",
            "Downloading torchcodec-0.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchcodec\n",
            "Successfully installed torchcodec-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchcodec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9q6bS9TCJJM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from datasets import load_from_disk, DatasetDict\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Union\n",
        "import logging\n",
        "import datasets\n",
        "import math\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    SchedulerType,\n",
        "    Wav2Vec2Config,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    Wav2Vec2ForPreTraining,\n",
        "    get_scheduler,\n",
        "    is_wandb_available,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF2AZoeCuCfu",
        "outputId": "2748eb8d-30c5-4bb9-80c8-084f0f6efe6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufpGv_z4HTEB"
      },
      "outputs": [],
      "source": [
        "# MAX_DURATION = 10.0\n",
        "# MIN_DURATION = 5.0\n",
        "# GRADIENT_CHECKPOINTING = True\n",
        "# MASK_TIME_PROB = None\n",
        "# MASK_TIME_LENGTH = None\n",
        "# TRAIN_BATCH_SIZE = 16\n",
        "# VAL_BATCH_SIZE = 16\n",
        "# LEARNING_RATE = 5e-5\n",
        "# ADAM_BETA1 = 0.9\n",
        "# ADAM_BETA2 = 0.999\n",
        "# ADAM_EPSILON = 1e-8\n",
        "# GRADIENT_ACCUMULATION_STEPS = 1\n",
        "# MAX_TRAINING_STEPS = None\n",
        "# NUM_TRAIN_EPOCHS = 3\n",
        "# LR_SCHEDULER_TYPE = \"linear\"\n",
        "# NUM_WARMUP_STEPS = 0\n",
        "# MAX_GUMBEL_TEMPERATURE = 2.0\n",
        "# MIN_GUMBEL_TEMPERATURE = 0.5\n",
        "# GUMBEL_TEMPERATURE = 0.999995\n",
        "# LOGGING_STEPS = 10\n",
        "# SAVING_STEPS = 500\n",
        "# OUTPUT_DIR = \"/outputs\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_DURATION = 10.0\n",
        "MIN_DURATION = 5.0\n",
        "GRADIENT_CHECKPOINTING = True\n",
        "MASK_TIME_PROB = None\n",
        "MASK_TIME_LENGTH = None\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VAL_BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-4 #5e-5\n",
        "ADAM_BETA1 = 0.9\n",
        "ADAM_BETA2 = 0.98\n",
        "ADAM_EPSILON = 1e-8\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "MAX_TRAINING_STEPS = None\n",
        "NUM_TRAIN_EPOCHS = 50\n",
        "LR_SCHEDULER_TYPE = \"linear\"\n",
        "NUM_WARMUP_STEPS = 100 #0\n",
        "MAX_GUMBEL_TEMPERATURE = 2.0\n",
        "MIN_GUMBEL_TEMPERATURE = 1.0 #0.5\n",
        "GUMBEL_TEMPERATURE = 0.999999 #0.999995\n",
        "LOGGING_STEPS = 10\n",
        "SAVING_STEPS = 500\n",
        "OUTPUT_DIR = \"/outputs\"\n",
        "INTER_CB_SIMILARITY_WEIGHT = 0.1"
      ],
      "metadata": {
        "id": "YwRN7PWCfezc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiOzLEWPXJ_D"
      },
      "outputs": [],
      "source": [
        "logger = get_logger(__name__)\n",
        "logging.basicConfig(level=logging.INFO, force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iiq41dHFD3MH"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataCollatorForWav2Vec2Pretraining:\n",
        "  model: Wav2Vec2ForPreTraining\n",
        "  feature_extractor: Wav2Vec2FeatureExtractor\n",
        "  padding: str = \"longest\"\n",
        "  pad_to_multiple_of: int = None\n",
        "  mask_time_prob: float = 0.65\n",
        "  mask_time_length: int= 10\n",
        "\n",
        "  def __call__(self, features: list[dict[str, Union[list[int], torch.tensor]]]) -> dict[str, torch.Tensor]:\n",
        "    batch = self.feature_extractor.pad(\n",
        "        features,\n",
        "        padding=self.padding,\n",
        "        pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    device = batch[\"input_values\"].device\n",
        "    batch_size = batch['input_values'].shape[0]\n",
        "\n",
        "    mask_indices_seq_length = self.model._get_feat_extract_output_lengths(batch[\"input_values\"].shape[-1])\n",
        "    mask_indices_seq_length = int(mask_indices_seq_length)\n",
        "\n",
        "    if batch.get(\"attention_mask\") is not None:\n",
        "      batch[\"sub_attention_mask\"] = self.model._get_feature_vector_attention_mask(\n",
        "          mask_indices_seq_length, batch[\"attention_mask\"]\n",
        "      )\n",
        "\n",
        "    features_shape = (batch_size, mask_indices_seq_length)\n",
        "\n",
        "    # Sample randomly maksed indices\n",
        "    mask_time_indices = _compute_mask_indices(\n",
        "        features_shape,\n",
        "        self.mask_time_prob,\n",
        "        self.mask_time_length,\n",
        "        attention_mask=batch.get(\"sub_attention_mask\"),\n",
        "    )\n",
        "\n",
        "    # Sample negative indices\n",
        "    sampled_negative_indices = _sample_negative_indices(\n",
        "        features_shape,\n",
        "        self.model.config.num_negatives,\n",
        "        mask_time_indices=mask_time_indices,\n",
        "    )\n",
        "\n",
        "    batch[\"mask_time_indices\"] = torch.tensor(mask_time_indices, dtype=torch.long, device=device)\n",
        "    batch[\"sampled_negative_indices\"] = torch.tensor(sampled_negative_indices, dtype=torch.long, device=device)\n",
        "\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQPNj65jQ61v"
      },
      "outputs": [],
      "source": [
        "def multiply_grads(params, c):\n",
        "  \"\"\"Multiply grad by a constant c\"\"\"\n",
        "  for p in params:\n",
        "    if p.grad is not None:\n",
        "      if torch.is_tensor(c):\n",
        "        c = c.to(p.grad.device)\n",
        "      p.grad.data.mul_(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEeNfAImVHG3"
      },
      "outputs": [],
      "source": [
        "def get_grad_norm(params, scale=1):\n",
        "  \"\"\"Compute grad norm given a gradient scale\"\"\"\n",
        "  total_norm = 0.0\n",
        "  for p in params:\n",
        "    if p.grad is not None:\n",
        "      param_norm = (p.grad.detach().data / scale).norm(2)\n",
        "      total_norm += param_norm.item() ** 2\n",
        "  total_norm = total_norm ** 0.5\n",
        "  return total_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "XtBjp1aRWUBh",
        "outputId": "07631571-c626-410c-87cb-f0e5b3450f42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Distributed environment: DistributedType.NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wandb have installed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrasarathathsarana63\u001b[0m (\u001b[33mrasarathathsarana63-university-of-moratuwa\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250926_102805-8722uq3v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rasarathathsarana63-university-of-moratuwa/wav2vec2-fromscratch/runs/8722uq3v' target=\"_blank\">dandy-leaf-12</a></strong> to <a href='https://wandb.ai/rasarathathsarana63-university-of-moratuwa/wav2vec2-fromscratch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/rasarathathsarana63-university-of-moratuwa/wav2vec2-fromscratch' target=\"_blank\">https://wandb.ai/rasarathathsarana63-university-of-moratuwa/wav2vec2-fromscratch</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/rasarathathsarana63-university-of-moratuwa/wav2vec2-fromscratch/runs/8722uq3v' target=\"_blank\">https://wandb.ai/rasarathathsarana63-university-of-moratuwa/wav2vec2-fromscratch/runs/8722uq3v</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "accelerator = Accelerator()\n",
        "logger.info(accelerator.state, main_process_only=False)\n",
        "\n",
        "if accelerator.is_local_main_process:\n",
        "  datasets.utils.logging.set_verbosity_warning()\n",
        "  transformers.utils.logging.set_verbosity_info()\n",
        "\n",
        "  if is_wandb_available():\n",
        "    print(\"wandb have installed\")\n",
        "    import wandb\n",
        "    wandb.init(project=\"wav2vec2-fromscratch\")\n",
        "\n",
        "else:\n",
        "  datasets.utils.logging.set_verbosity_error()\n",
        "  transformers.utils.logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9zgC005YJP8"
      },
      "outputs": [],
      "source": [
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6725-xCrb_dU"
      },
      "outputs": [],
      "source": [
        "accelerator.wait_for_everyone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_7qKyEfcItx"
      },
      "outputs": [],
      "source": [
        "raw_datasets = DatasetDict()\n",
        "\n",
        "raw_datasets['train'] = load_from_disk(\"/content/drive/MyDrive/SP/librispeech_datasets/dataset_10h\")\n",
        "raw_datasets['val'] = load_from_disk(\"/content/drive/MyDrive/SP/librispeech_datasets/dataset_val_clean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFu1WsBDNRfH"
      },
      "outputs": [],
      "source": [
        "raw_datasets[\"train\"] = raw_datasets[\"train\"].remove_columns(\"duration\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396,
          "referenced_widgets": [
            "d2750510dd6d4cd99a5362379143140c",
            "78f52f50e4ce4e69a7aae21e1659cc49",
            "90136bda08d44f47a99e6c1fe13d8e81",
            "6de61b9befdd446d8712cfe19e61b810",
            "3f9f19156abf4b85a30557b2f8101d05",
            "3002788c30024de7acd749ab1444b50b",
            "18c08bbfc8964b8cbe530a1a1b782bcc",
            "4ca98d4d32814dd99cec01280505783d",
            "f6622ddf18774677a6fc41c026e46bf8",
            "b2cf57b104494cd2a8b925df0f188a03",
            "d8fd2f2bf7894cb99be219f6922d524b",
            "85c70f120de449e2ae531c29a5f70c83",
            "4fe6e261ccdf40fbaa65459a2cf9ecf2",
            "83f30717aeb140c9a61a3c86258cccd8",
            "8f6031c5d9ae4ec9b1b8fcaa05780539",
            "2439ddb36139424491f4b1939faa6e69",
            "39e4feec328642259d0f61af1d35757d",
            "d976b818d9b34d44939406e9a05ea0a0",
            "51b16415094e46a48b6e8d6cb4eec869",
            "a4938e0732c64f84961f8ff19fa4c8b7",
            "0b7191fea2c74e349c38529339a09437",
            "3f0c89b9ee1e46a88391491d0191ec0e"
          ]
        },
        "id": "K7Bl4MtnsaqA",
        "outputId": "a890c2b0-b465-4397-f55f-54ce46500743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2750510dd6d4cd99a5362379143140c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85c70f120de449e2ae531c29a5f70c83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--facebook--wav2vec2-base/snapshots/0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8/preprocessor_config.json\n",
            "Feature extractor Wav2Vec2FeatureExtractor {\n",
            "  \"do_normalize\": true,\n",
            "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
            "  \"feature_size\": 1,\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0.0,\n",
            "  \"return_attention_mask\": true,\n",
            "  \"sampling_rate\": 16000\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\",\n",
        "    return_attention_mask = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AR6KPkzu5Ga"
      },
      "outputs": [],
      "source": [
        "raw_datasets = raw_datasets.cast_column(\n",
        "    'audio', datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epFODlkP--Xg"
      },
      "outputs": [],
      "source": [
        "# only normalized-inputs-training is supported\n",
        "if not feature_extractor.do_normalize:\n",
        "  raise ValueError(\n",
        "      \"Training is only supported for normalized inputs. Make sure ``feature_extractor.do_normalize == True``\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6_eEqKe_JMF"
      },
      "outputs": [],
      "source": [
        "# Set max & min audio length in number of samples\n",
        "max_length = int(MAX_DURATION * feature_extractor.sampling_rate)\n",
        "min_length = int(MIN_DURATION * feature_extractor.sampling_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sJix4Fm_JIu"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "  sample = batch[\"audio\"]\n",
        "  inputs = feature_extractor(\n",
        "      sample[\"array\"],\n",
        "      sampling_rate=sample[\"sampling_rate\"],\n",
        "      max_length=max_length,\n",
        "      truncation=True\n",
        "  )\n",
        "  batch[\"input_values\"] = inputs.input_values[0]\n",
        "  batch[\"input_length\"] = len(inputs.input_values[0])\n",
        "\n",
        "  return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YekAifm2_JGI"
      },
      "outputs": [],
      "source": [
        "# load audio files into numpy arrays\n",
        "with accelerator.main_process_first():\n",
        "  vectorized_datasets = raw_datasets.map(\n",
        "      prepare_dataset,\n",
        "      num_proc=None,\n",
        "      remove_columns=raw_datasets[\"train\"].column_names,\n",
        "  )\n",
        "\n",
        "  if min_length > 0.0:\n",
        "    vectorized_datasets = vectorized_datasets.filter(\n",
        "        lambda x: x > min_length,\n",
        "        num_proc=None,\n",
        "        input_columns=[\"input_length\"]\n",
        "    )\n",
        "\n",
        "  vectorized_datasets = vectorized_datasets.remove_columns(\"input_length\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "535a0ddec14948a29782e34314d4ad03",
            "bb1ccf60e060451e872c96fc3ed58199",
            "73eb5966cc864cbfadba6e1df28a2fb7",
            "d2c68c2d477d4afdbcd4e94c642b44a1",
            "8f1e21afbf1c463bba76884646b4c2b1",
            "9c1ab7743826452391c4013f6773036d",
            "345cdd815720491eaaeb7ffa474dcc21",
            "af553d32a0bb4c45bcf4a8623d170be1",
            "9612cee1bbdf4a45b785b7ec1c48fd74",
            "00e1ec8580d54c88870aeb906204db0d",
            "8b2e554eeae2439fa677333c9778525d"
          ]
        },
        "id": "Zz_AgQ8s_JDe",
        "outputId": "c59e89a0-acf3-4031-d1ed-fbed994449cb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "535a0ddec14948a29782e34314d4ad03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--wav2vec2-base/snapshots/0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8/config.json\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Model config Wav2Vec2Config {\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"adapter_attn_dim\": null,\n",
            "  \"adapter_kernel_size\": 3,\n",
            "  \"adapter_stride\": 2,\n",
            "  \"add_adapter\": false,\n",
            "  \"apply_spec_augment\": true,\n",
            "  \"architectures\": [\n",
            "    \"Wav2Vec2ForPreTraining\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"codevector_dim\": 256,\n",
            "  \"contrastive_logits_temperature\": 0.1,\n",
            "  \"conv_bias\": false,\n",
            "  \"conv_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"conv_kernel\": [\n",
            "    10,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"conv_stride\": [\n",
            "    5,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"ctc_loss_reduction\": \"sum\",\n",
            "  \"ctc_zero_infinity\": false,\n",
            "  \"diversity_loss_weight\": 0.1,\n",
            "  \"do_stable_layer_norm\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"feat_extract_activation\": \"gelu\",\n",
            "  \"feat_extract_norm\": \"group\",\n",
            "  \"feat_proj_dropout\": 0.1,\n",
            "  \"feat_quantizer_dropout\": 0.0,\n",
            "  \"final_dropout\": 0.0,\n",
            "  \"freeze_feat_extract_train\": true,\n",
            "  \"gradient_checkpointing\": true,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"mask_channel_length\": 10,\n",
            "  \"mask_channel_min_space\": 1,\n",
            "  \"mask_channel_other\": 0.0,\n",
            "  \"mask_channel_prob\": 0.0,\n",
            "  \"mask_channel_selection\": \"static\",\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_min_space\": 1,\n",
            "  \"mask_time_other\": 0.0,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"mask_time_selection\": \"static\",\n",
            "  \"model_type\": \"wav2vec2\",\n",
            "  \"no_mask_channel_overlap\": false,\n",
            "  \"no_mask_time_overlap\": false,\n",
            "  \"num_adapter_layers\": 3,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_codevector_groups\": 2,\n",
            "  \"num_codevectors_per_group\": 320,\n",
            "  \"num_conv_pos_embedding_groups\": 16,\n",
            "  \"num_conv_pos_embeddings\": 128,\n",
            "  \"num_feat_extract_layers\": 7,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_negatives\": 100,\n",
            "  \"output_hidden_size\": 768,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"proj_codevector_dim\": 256,\n",
            "  \"tdnn_dilation\": [\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"tdnn_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    1500\n",
            "  ],\n",
            "  \"tdnn_kernel\": [\n",
            "    5,\n",
            "    3,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"transformers_version\": \"4.56.1\",\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 32,\n",
            "  \"xvector_output_dim\": 512\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "config = Wav2Vec2Config.from_pretrained(\"facebook/wav2vec2-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAIPxk3sN5-B"
      },
      "outputs": [],
      "source": [
        "# model = Wav2Vec2ForPreTraining.from_pretrained(\n",
        "#     \"facebook/wav2vec2-base\",\n",
        "#     config=config\n",
        "# )\n",
        "\n",
        "model = Wav2Vec2ForPreTraining(\n",
        "    config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHVKPXFMMy4O"
      },
      "outputs": [],
      "source": [
        "# Activate gradient checkpointing\n",
        "if GRADIENT_CHECKPOINTING:\n",
        "  model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL7wWlNIP6sc"
      },
      "outputs": [],
      "source": [
        "mask_time_prob = config.mask_time_prob if MASK_TIME_PROB is None else MASK_TIME_PROB\n",
        "mask_time_length = config.mask_time_length if MASK_TIME_LENGTH is None else MASK_TIME_LENGTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0yToSoEQxMS"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForWav2Vec2Pretraining(\n",
        "    model=model,\n",
        "    feature_extractor=feature_extractor,\n",
        "    mask_time_prob=mask_time_prob,\n",
        "    mask_time_length=mask_time_length\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFVFHUSYRiR-"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(\n",
        "    vectorized_datasets[\"train\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtvbQfKXTBuo"
      },
      "outputs": [],
      "source": [
        "val_dataloader = DataLoader(\n",
        "    vectorized_datasets[\"val\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=VAL_BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MFTyy1pTMgP"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    list(model.parameters()),\n",
        "    lr=LEARNING_RATE,\n",
        "    betas=[ADAM_BETA1, ADAM_BETA2],\n",
        "    eps=ADAM_EPSILON\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAVnpWMpUeox"
      },
      "outputs": [],
      "source": [
        "model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, val_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68f5gYeaUkXQ"
      },
      "outputs": [],
      "source": [
        "num_update_steps_per_epcoh = math.ceil(len(train_dataloader) / GRADIENT_ACCUMULATION_STEPS)\n",
        "\n",
        "if MAX_TRAINING_STEPS is None:\n",
        "  max_train_steps = num_update_steps_per_epcoh * NUM_TRAIN_EPOCHS\n",
        "else:\n",
        "  max_train_steps = MAX_TRAINING_STEPS\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=LR_SCHEDULER_TYPE,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=NUM_WARMUP_STEPS,\n",
        "    num_training_steps=max_train_steps,\n",
        ")\n",
        "\n",
        "NUM_TRAIN_EPOCHS = math.ceil(max_train_steps / num_update_steps_per_epcoh)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params = [name for name, param in model.named_parameters() if param.requires_grad]\n",
        "print(\"Trainable parameters:\", list(trainable_params))"
      ],
      "metadata": {
        "id": "G17806dWGy8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c416d39-f68d-49a5-e219-68cf696ee1c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: ['wav2vec2.masked_spec_embed', 'wav2vec2.feature_extractor.conv_layers.0.conv.weight', 'wav2vec2.feature_extractor.conv_layers.0.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.0.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.1.conv.weight', 'wav2vec2.feature_extractor.conv_layers.2.conv.weight', 'wav2vec2.feature_extractor.conv_layers.3.conv.weight', 'wav2vec2.feature_extractor.conv_layers.4.conv.weight', 'wav2vec2.feature_extractor.conv_layers.5.conv.weight', 'wav2vec2.feature_extractor.conv_layers.6.conv.weight', 'wav2vec2.feature_projection.layer_norm.weight', 'wav2vec2.feature_projection.layer_norm.bias', 'wav2vec2.feature_projection.projection.weight', 'wav2vec2.feature_projection.projection.bias', 'wav2vec2.encoder.pos_conv_embed.conv.bias', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.layer_norm.weight', 'wav2vec2.encoder.layer_norm.bias', 'wav2vec2.encoder.layers.0.attention.k_proj.weight', 'wav2vec2.encoder.layers.0.attention.k_proj.bias', 'wav2vec2.encoder.layers.0.attention.v_proj.weight', 'wav2vec2.encoder.layers.0.attention.v_proj.bias', 'wav2vec2.encoder.layers.0.attention.q_proj.weight', 'wav2vec2.encoder.layers.0.attention.q_proj.bias', 'wav2vec2.encoder.layers.0.attention.out_proj.weight', 'wav2vec2.encoder.layers.0.attention.out_proj.bias', 'wav2vec2.encoder.layers.0.layer_norm.weight', 'wav2vec2.encoder.layers.0.layer_norm.bias', 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.0.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.0.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.0.final_layer_norm.weight', 'wav2vec2.encoder.layers.0.final_layer_norm.bias', 'wav2vec2.encoder.layers.1.attention.k_proj.weight', 'wav2vec2.encoder.layers.1.attention.k_proj.bias', 'wav2vec2.encoder.layers.1.attention.v_proj.weight', 'wav2vec2.encoder.layers.1.attention.v_proj.bias', 'wav2vec2.encoder.layers.1.attention.q_proj.weight', 'wav2vec2.encoder.layers.1.attention.q_proj.bias', 'wav2vec2.encoder.layers.1.attention.out_proj.weight', 'wav2vec2.encoder.layers.1.attention.out_proj.bias', 'wav2vec2.encoder.layers.1.layer_norm.weight', 'wav2vec2.encoder.layers.1.layer_norm.bias', 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.1.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.1.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.1.final_layer_norm.weight', 'wav2vec2.encoder.layers.1.final_layer_norm.bias', 'wav2vec2.encoder.layers.2.attention.k_proj.weight', 'wav2vec2.encoder.layers.2.attention.k_proj.bias', 'wav2vec2.encoder.layers.2.attention.v_proj.weight', 'wav2vec2.encoder.layers.2.attention.v_proj.bias', 'wav2vec2.encoder.layers.2.attention.q_proj.weight', 'wav2vec2.encoder.layers.2.attention.q_proj.bias', 'wav2vec2.encoder.layers.2.attention.out_proj.weight', 'wav2vec2.encoder.layers.2.attention.out_proj.bias', 'wav2vec2.encoder.layers.2.layer_norm.weight', 'wav2vec2.encoder.layers.2.layer_norm.bias', 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.2.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.2.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.2.final_layer_norm.weight', 'wav2vec2.encoder.layers.2.final_layer_norm.bias', 'wav2vec2.encoder.layers.3.attention.k_proj.weight', 'wav2vec2.encoder.layers.3.attention.k_proj.bias', 'wav2vec2.encoder.layers.3.attention.v_proj.weight', 'wav2vec2.encoder.layers.3.attention.v_proj.bias', 'wav2vec2.encoder.layers.3.attention.q_proj.weight', 'wav2vec2.encoder.layers.3.attention.q_proj.bias', 'wav2vec2.encoder.layers.3.attention.out_proj.weight', 'wav2vec2.encoder.layers.3.attention.out_proj.bias', 'wav2vec2.encoder.layers.3.layer_norm.weight', 'wav2vec2.encoder.layers.3.layer_norm.bias', 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.3.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.3.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.3.final_layer_norm.weight', 'wav2vec2.encoder.layers.3.final_layer_norm.bias', 'wav2vec2.encoder.layers.4.attention.k_proj.weight', 'wav2vec2.encoder.layers.4.attention.k_proj.bias', 'wav2vec2.encoder.layers.4.attention.v_proj.weight', 'wav2vec2.encoder.layers.4.attention.v_proj.bias', 'wav2vec2.encoder.layers.4.attention.q_proj.weight', 'wav2vec2.encoder.layers.4.attention.q_proj.bias', 'wav2vec2.encoder.layers.4.attention.out_proj.weight', 'wav2vec2.encoder.layers.4.attention.out_proj.bias', 'wav2vec2.encoder.layers.4.layer_norm.weight', 'wav2vec2.encoder.layers.4.layer_norm.bias', 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.4.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.4.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.4.final_layer_norm.weight', 'wav2vec2.encoder.layers.4.final_layer_norm.bias', 'wav2vec2.encoder.layers.5.attention.k_proj.weight', 'wav2vec2.encoder.layers.5.attention.k_proj.bias', 'wav2vec2.encoder.layers.5.attention.v_proj.weight', 'wav2vec2.encoder.layers.5.attention.v_proj.bias', 'wav2vec2.encoder.layers.5.attention.q_proj.weight', 'wav2vec2.encoder.layers.5.attention.q_proj.bias', 'wav2vec2.encoder.layers.5.attention.out_proj.weight', 'wav2vec2.encoder.layers.5.attention.out_proj.bias', 'wav2vec2.encoder.layers.5.layer_norm.weight', 'wav2vec2.encoder.layers.5.layer_norm.bias', 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.5.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.5.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.5.final_layer_norm.weight', 'wav2vec2.encoder.layers.5.final_layer_norm.bias', 'wav2vec2.encoder.layers.6.attention.k_proj.weight', 'wav2vec2.encoder.layers.6.attention.k_proj.bias', 'wav2vec2.encoder.layers.6.attention.v_proj.weight', 'wav2vec2.encoder.layers.6.attention.v_proj.bias', 'wav2vec2.encoder.layers.6.attention.q_proj.weight', 'wav2vec2.encoder.layers.6.attention.q_proj.bias', 'wav2vec2.encoder.layers.6.attention.out_proj.weight', 'wav2vec2.encoder.layers.6.attention.out_proj.bias', 'wav2vec2.encoder.layers.6.layer_norm.weight', 'wav2vec2.encoder.layers.6.layer_norm.bias', 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.6.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.6.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.6.final_layer_norm.weight', 'wav2vec2.encoder.layers.6.final_layer_norm.bias', 'wav2vec2.encoder.layers.7.attention.k_proj.weight', 'wav2vec2.encoder.layers.7.attention.k_proj.bias', 'wav2vec2.encoder.layers.7.attention.v_proj.weight', 'wav2vec2.encoder.layers.7.attention.v_proj.bias', 'wav2vec2.encoder.layers.7.attention.q_proj.weight', 'wav2vec2.encoder.layers.7.attention.q_proj.bias', 'wav2vec2.encoder.layers.7.attention.out_proj.weight', 'wav2vec2.encoder.layers.7.attention.out_proj.bias', 'wav2vec2.encoder.layers.7.layer_norm.weight', 'wav2vec2.encoder.layers.7.layer_norm.bias', 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.7.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.7.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.7.final_layer_norm.weight', 'wav2vec2.encoder.layers.7.final_layer_norm.bias', 'wav2vec2.encoder.layers.8.attention.k_proj.weight', 'wav2vec2.encoder.layers.8.attention.k_proj.bias', 'wav2vec2.encoder.layers.8.attention.v_proj.weight', 'wav2vec2.encoder.layers.8.attention.v_proj.bias', 'wav2vec2.encoder.layers.8.attention.q_proj.weight', 'wav2vec2.encoder.layers.8.attention.q_proj.bias', 'wav2vec2.encoder.layers.8.attention.out_proj.weight', 'wav2vec2.encoder.layers.8.attention.out_proj.bias', 'wav2vec2.encoder.layers.8.layer_norm.weight', 'wav2vec2.encoder.layers.8.layer_norm.bias', 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.8.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.8.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.8.final_layer_norm.weight', 'wav2vec2.encoder.layers.8.final_layer_norm.bias', 'wav2vec2.encoder.layers.9.attention.k_proj.weight', 'wav2vec2.encoder.layers.9.attention.k_proj.bias', 'wav2vec2.encoder.layers.9.attention.v_proj.weight', 'wav2vec2.encoder.layers.9.attention.v_proj.bias', 'wav2vec2.encoder.layers.9.attention.q_proj.weight', 'wav2vec2.encoder.layers.9.attention.q_proj.bias', 'wav2vec2.encoder.layers.9.attention.out_proj.weight', 'wav2vec2.encoder.layers.9.attention.out_proj.bias', 'wav2vec2.encoder.layers.9.layer_norm.weight', 'wav2vec2.encoder.layers.9.layer_norm.bias', 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.9.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.9.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.9.final_layer_norm.weight', 'wav2vec2.encoder.layers.9.final_layer_norm.bias', 'wav2vec2.encoder.layers.10.attention.k_proj.weight', 'wav2vec2.encoder.layers.10.attention.k_proj.bias', 'wav2vec2.encoder.layers.10.attention.v_proj.weight', 'wav2vec2.encoder.layers.10.attention.v_proj.bias', 'wav2vec2.encoder.layers.10.attention.q_proj.weight', 'wav2vec2.encoder.layers.10.attention.q_proj.bias', 'wav2vec2.encoder.layers.10.attention.out_proj.weight', 'wav2vec2.encoder.layers.10.attention.out_proj.bias', 'wav2vec2.encoder.layers.10.layer_norm.weight', 'wav2vec2.encoder.layers.10.layer_norm.bias', 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.10.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.10.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.10.final_layer_norm.weight', 'wav2vec2.encoder.layers.10.final_layer_norm.bias', 'wav2vec2.encoder.layers.11.attention.k_proj.weight', 'wav2vec2.encoder.layers.11.attention.k_proj.bias', 'wav2vec2.encoder.layers.11.attention.v_proj.weight', 'wav2vec2.encoder.layers.11.attention.v_proj.bias', 'wav2vec2.encoder.layers.11.attention.q_proj.weight', 'wav2vec2.encoder.layers.11.attention.q_proj.bias', 'wav2vec2.encoder.layers.11.attention.out_proj.weight', 'wav2vec2.encoder.layers.11.attention.out_proj.bias', 'wav2vec2.encoder.layers.11.layer_norm.weight', 'wav2vec2.encoder.layers.11.layer_norm.bias', 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.11.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.11.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.11.final_layer_norm.weight', 'wav2vec2.encoder.layers.11.final_layer_norm.bias', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.weight', 'project_hid.bias', 'project_q.weight', 'project_q.bias']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRzBPNoRCkRg",
        "outputId": "b775e0ad-3700-4c00-a1ba-d1438abc11fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wav2vec2.masked_spec_embed',\n",
              " 'wav2vec2.feature_extractor.conv_layers.0.conv.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.0.layer_norm.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.0.layer_norm.bias',\n",
              " 'wav2vec2.feature_extractor.conv_layers.1.conv.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.2.conv.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.3.conv.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.4.conv.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.5.conv.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.6.conv.weight',\n",
              " 'wav2vec2.feature_projection.layer_norm.weight',\n",
              " 'wav2vec2.feature_projection.layer_norm.bias',\n",
              " 'wav2vec2.feature_projection.projection.weight',\n",
              " 'wav2vec2.feature_projection.projection.bias',\n",
              " 'wav2vec2.encoder.pos_conv_embed.conv.bias',\n",
              " 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0',\n",
              " 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1',\n",
              " 'wav2vec2.encoder.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.0.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.0.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.0.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.0.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.0.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.0.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.0.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.0.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.0.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.0.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.0.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.0.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.0.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.0.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.1.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.1.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.1.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.1.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.1.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.1.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.1.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.1.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.1.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.1.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.1.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.1.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.1.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.1.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.2.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.2.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.2.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.2.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.2.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.2.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.2.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.2.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.2.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.2.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.2.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.2.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.2.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.2.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.3.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.3.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.3.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.3.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.3.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.3.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.3.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.3.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.3.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.3.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.3.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.3.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.3.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.3.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.4.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.4.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.4.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.4.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.4.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.4.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.4.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.4.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.4.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.4.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.4.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.4.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.4.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.4.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.5.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.5.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.5.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.5.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.5.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.5.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.5.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.5.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.5.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.5.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.5.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.5.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.5.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.5.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.6.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.6.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.6.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.6.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.6.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.6.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.6.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.6.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.6.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.6.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.6.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.6.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.6.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.6.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.7.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.7.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.7.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.7.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.7.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.7.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.7.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.7.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.7.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.7.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.7.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.7.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.7.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.7.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.8.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.8.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.8.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.8.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.8.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.8.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.8.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.8.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.8.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.8.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.8.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.8.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.8.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.8.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.9.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.9.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.9.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.9.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.9.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.9.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.9.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.9.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.9.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.9.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.9.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.9.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.9.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.9.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.10.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.10.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.10.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.10.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.10.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.10.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.10.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.10.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.10.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.10.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.10.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.10.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.10.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.10.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.11.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.11.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.11.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.11.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.11.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.11.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.11.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.11.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.11.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.11.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.11.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.11.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.11.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.11.final_layer_norm.bias',\n",
              " 'quantizer.codevectors',\n",
              " 'quantizer.weight_proj.weight',\n",
              " 'quantizer.weight_proj.bias',\n",
              " 'project_hid.weight',\n",
              " 'project_hid.bias',\n",
              " 'project_q.weight',\n",
              " 'project_q.bias']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8ea690f808464877963189bd206a9d2e",
            "70cac061391545c6b2f40b7a1e9db274",
            "f4250de969f64556a4a9292245f85af9",
            "4d5d93a41703476b9a5659f8c111606c",
            "99e9984a4e2e4ee3b1942d7bf7ab692b",
            "734e9cf62ded49ec89898bc90f533e3f",
            "8a67e6a2836a467e85294b2b1db5b912",
            "44463b62e935441e87857173802329b3",
            "cd44897d59544a21bc309fe07ba810d5",
            "3b27c1e579764edcabfc70ebbd9388b9",
            "75422607561946ff96a7a166cd6f1644"
          ]
        },
        "id": "2vlMVnu1WhOB",
        "outputId": "b6a1f377-d86b-4100-f2b3-58cdb5c9e4a1"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:***** Runing training *****\n",
            "INFO:__main__: Num examples = 2641\n",
            "INFO:__main__: Num Epochs = 50\n",
            "INFO:__main__:  Instantaneous batch size per device = 16\n",
            "INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "INFO:__main__:  Gradient Accumulation steps = 4\n",
            "INFO:__main__:  Total optimization steps = None\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ea690f808464877963189bd206a9d2e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 4.683e+00| contrast_loss: 4.623e+00| div_loss: 5.945e-01| %_mask_idx: 5.841e-02| ppl: 2.595e+02| lr: 1.000e-05| temp: 2.000e+00| grad_norm: 5.486e-01\n",
            "| loss: 4.683e+00| contrast_loss: 4.633e+00| div_loss: 4.992e-01| %_mask_idx: 5.688e-02| ppl: 3.205e+02| lr: 2.000e-05| temp: 2.000e+00| grad_norm: 3.650e-01\n",
            "| loss: 4.676e+00| contrast_loss: 4.624e+00| div_loss: 5.217e-01| %_mask_idx: 5.745e-02| ppl: 3.061e+02| lr: 3.000e-05| temp: 2.000e+00| grad_norm: 2.547e-01\n",
            "| loss: 4.668e+00| contrast_loss: 4.618e+00| div_loss: 4.981e-01| %_mask_idx: 3.870e-02| ppl: 3.212e+02| lr: 4.000e-05| temp: 2.000e+00| grad_norm: 2.646e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.675e+00| val_contrastive_loss: 4.617e+00| val_diversity_loss: 5.791e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 4.657e+00| contrast_loss: 4.616e+00| div_loss: 4.111e-01| %_mask_idx: 5.824e-02| ppl: 3.769e+02| lr: 5.200e-05| temp: 2.000e+00| grad_norm: 1.971e-01\n",
            "| loss: 4.655e+00| contrast_loss: 4.616e+00| div_loss: 3.816e-01| %_mask_idx: 6.008e-02| ppl: 3.958e+02| lr: 6.200e-05| temp: 2.000e+00| grad_norm: 2.078e-01\n",
            "| loss: 4.664e+00| contrast_loss: 4.618e+00| div_loss: 4.566e-01| %_mask_idx: 4.008e-02| ppl: 3.477e+02| lr: 7.200e-05| temp: 2.000e+00| grad_norm: 2.255e-01\n",
            "| loss: 4.652e+00| contrast_loss: 4.617e+00| div_loss: 3.503e-01| %_mask_idx: 5.984e-02| ppl: 4.158e+02| lr: 8.200e-05| temp: 2.000e+00| grad_norm: 2.136e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.667e+00| val_contrastive_loss: 4.615e+00| val_diversity_loss: 5.230e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 4.664e+00| contrast_loss: 4.617e+00| div_loss: 4.636e-01| %_mask_idx: 3.962e-02| ppl: 3.433e+02| lr: 9.400e-05| temp: 2.000e+00| grad_norm: 2.326e-01\n",
            "| loss: 4.649e+00| contrast_loss: 4.613e+00| div_loss: 3.612e-01| %_mask_idx: 6.020e-02| ppl: 4.088e+02| lr: 9.980e-05| temp: 2.000e+00| grad_norm: 1.908e-01\n",
            "| loss: 4.656e+00| contrast_loss: 4.617e+00| div_loss: 3.854e-01| %_mask_idx: 4.151e-02| ppl: 3.934e+02| lr: 9.930e-05| temp: 2.000e+00| grad_norm: 2.246e-01\n",
            "| loss: 4.653e+00| contrast_loss: 4.617e+00| div_loss: 3.575e-01| %_mask_idx: 5.812e-02| ppl: 4.112e+02| lr: 9.880e-05| temp: 2.000e+00| grad_norm: 1.739e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.666e+00| val_contrastive_loss: 4.615e+00| val_diversity_loss: 5.070e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 4.657e+00| contrast_loss: 4.617e+00| div_loss: 3.956e-01| %_mask_idx: 4.045e-02| ppl: 3.868e+02| lr: 9.820e-05| temp: 2.000e+00| grad_norm: 2.297e-01\n",
            "| loss: 4.656e+00| contrast_loss: 4.615e+00| div_loss: 4.137e-01| %_mask_idx: 5.283e-02| ppl: 3.752e+02| lr: 9.770e-05| temp: 2.000e+00| grad_norm: 1.899e-01\n",
            "| loss: 4.656e+00| contrast_loss: 4.617e+00| div_loss: 3.871e-01| %_mask_idx: 4.097e-02| ppl: 3.923e+02| lr: 9.720e-05| temp: 2.000e+00| grad_norm: 2.040e-01\n",
            "| loss: 4.653e+00| contrast_loss: 4.618e+00| div_loss: 3.500e-01| %_mask_idx: 5.844e-02| ppl: 4.160e+02| lr: 9.670e-05| temp: 2.000e+00| grad_norm: 1.747e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.666e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 5.199e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 4.660e+00| contrast_loss: 4.614e+00| div_loss: 4.562e-01| %_mask_idx: 3.924e-02| ppl: 3.480e+02| lr: 9.610e-05| temp: 2.000e+00| grad_norm: 1.966e-01\n",
            "| loss: 4.651e+00| contrast_loss: 4.613e+00| div_loss: 3.804e-01| %_mask_idx: 5.767e-02| ppl: 3.965e+02| lr: 9.560e-05| temp: 2.000e+00| grad_norm: 1.922e-01\n",
            "| loss: 4.652e+00| contrast_loss: 4.614e+00| div_loss: 3.813e-01| %_mask_idx: 4.084e-02| ppl: 3.960e+02| lr: 9.510e-05| temp: 2.000e+00| grad_norm: 1.876e-01\n",
            "| loss: 4.659e+00| contrast_loss: 4.615e+00| div_loss: 4.395e-01| %_mask_idx: 3.806e-02| ppl: 3.587e+02| lr: 9.460e-05| temp: 2.000e+00| grad_norm: 1.993e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.666e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 5.159e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 4.654e+00| contrast_loss: 4.616e+00| div_loss: 3.772e-01| %_mask_idx: 5.964e-02| ppl: 3.986e+02| lr: 9.400e-05| temp: 2.000e+00| grad_norm: 1.893e-01\n",
            "| loss: 4.649e+00| contrast_loss: 4.616e+00| div_loss: 3.374e-01| %_mask_idx: 5.965e-02| ppl: 4.240e+02| lr: 9.350e-05| temp: 2.000e+00| grad_norm: 1.830e-01\n",
            "| loss: 4.648e+00| contrast_loss: 4.615e+00| div_loss: 3.302e-01| %_mask_idx: 5.965e-02| ppl: 4.287e+02| lr: 9.300e-05| temp: 2.000e+00| grad_norm: 1.603e-01\n",
            "| loss: 4.651e+00| contrast_loss: 4.617e+00| div_loss: 3.349e-01| %_mask_idx: 6.020e-02| ppl: 4.256e+02| lr: 9.250e-05| temp: 2.000e+00| grad_norm: 1.524e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.664e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 4.993e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 4.661e+00| contrast_loss: 4.617e+00| div_loss: 4.344e-01| %_mask_idx: 4.021e-02| ppl: 3.620e+02| lr: 9.190e-05| temp: 1.999e+00| grad_norm: 1.847e-01\n",
            "| loss: 4.655e+00| contrast_loss: 4.614e+00| div_loss: 4.098e-01| %_mask_idx: 4.008e-02| ppl: 3.777e+02| lr: 9.140e-05| temp: 1.999e+00| grad_norm: 1.745e-01\n",
            "| loss: 4.646e+00| contrast_loss: 4.615e+00| div_loss: 3.072e-01| %_mask_idx: 5.804e-02| ppl: 4.434e+02| lr: 9.090e-05| temp: 1.999e+00| grad_norm: 1.455e-01\n",
            "| loss: 4.657e+00| contrast_loss: 4.615e+00| div_loss: 4.240e-01| %_mask_idx: 4.215e-02| ppl: 3.686e+02| lr: 9.040e-05| temp: 1.999e+00| grad_norm: 1.766e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.665e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 5.176e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 4.646e+00| contrast_loss: 4.615e+00| div_loss: 3.079e-01| %_mask_idx: 5.821e-02| ppl: 4.430e+02| lr: 8.980e-05| temp: 1.999e+00| grad_norm: 1.532e-01\n",
            "| loss: 4.645e+00| contrast_loss: 4.614e+00| div_loss: 3.085e-01| %_mask_idx: 5.967e-02| ppl: 4.425e+02| lr: 8.930e-05| temp: 1.999e+00| grad_norm: 1.683e-01\n",
            "| loss: 4.660e+00| contrast_loss: 4.616e+00| div_loss: 4.454e-01| %_mask_idx: 3.820e-02| ppl: 3.549e+02| lr: 8.880e-05| temp: 1.999e+00| grad_norm: 1.977e-01\n",
            "| loss: 4.657e+00| contrast_loss: 4.614e+00| div_loss: 4.389e-01| %_mask_idx: 3.988e-02| ppl: 3.591e+02| lr: 8.830e-05| temp: 1.999e+00| grad_norm: 1.651e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.664e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 5.033e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 4.647e+00| contrast_loss: 4.615e+00| div_loss: 3.188e-01| %_mask_idx: 5.912e-02| ppl: 4.360e+02| lr: 8.770e-05| temp: 1.999e+00| grad_norm: 1.666e-01\n",
            "| loss: 4.656e+00| contrast_loss: 4.616e+00| div_loss: 4.047e-01| %_mask_idx: 4.002e-02| ppl: 3.810e+02| lr: 8.720e-05| temp: 1.999e+00| grad_norm: 1.690e-01\n",
            "| loss: 4.659e+00| contrast_loss: 4.616e+00| div_loss: 4.316e-01| %_mask_idx: 4.044e-02| ppl: 3.637e+02| lr: 8.670e-05| temp: 1.999e+00| grad_norm: 1.693e-01\n",
            "| loss: 4.654e+00| contrast_loss: 4.614e+00| div_loss: 3.970e-01| %_mask_idx: 3.907e-02| ppl: 3.859e+02| lr: 8.620e-05| temp: 1.999e+00| grad_norm: 1.778e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.662e+00| val_contrastive_loss: 4.612e+00| val_diversity_loss: 4.989e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 4.646e+00| contrast_loss: 4.614e+00| div_loss: 3.227e-01| %_mask_idx: 5.776e-02| ppl: 4.335e+02| lr: 8.560e-05| temp: 1.999e+00| grad_norm: 1.432e-01\n",
            "| loss: 4.656e+00| contrast_loss: 4.616e+00| div_loss: 4.057e-01| %_mask_idx: 3.911e-02| ppl: 3.804e+02| lr: 8.510e-05| temp: 1.999e+00| grad_norm: 1.877e-01\n",
            "| loss: 4.661e+00| contrast_loss: 4.615e+00| div_loss: 4.606e-01| %_mask_idx: 3.973e-02| ppl: 3.452e+02| lr: 8.460e-05| temp: 1.999e+00| grad_norm: 1.755e-01\n",
            "| loss: 4.650e+00| contrast_loss: 4.617e+00| div_loss: 3.318e-01| %_mask_idx: 5.888e-02| ppl: 4.276e+02| lr: 8.410e-05| temp: 1.999e+00| grad_norm: 1.502e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.663e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 4.931e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 4.662e+00| contrast_loss: 4.615e+00| div_loss: 4.700e-01| %_mask_idx: 4.111e-02| ppl: 3.392e+02| lr: 8.350e-05| temp: 1.999e+00| grad_norm: 1.928e-01\n",
            "| loss: 4.659e+00| contrast_loss: 4.619e+00| div_loss: 4.039e-01| %_mask_idx: 4.008e-02| ppl: 3.815e+02| lr: 8.300e-05| temp: 1.999e+00| grad_norm: 1.735e-01\n",
            "| loss: 4.659e+00| contrast_loss: 4.617e+00| div_loss: 4.228e-01| %_mask_idx: 4.017e-02| ppl: 3.694e+02| lr: 8.250e-05| temp: 1.999e+00| grad_norm: 1.702e-01\n",
            "| loss: 4.650e+00| contrast_loss: 4.617e+00| div_loss: 3.328e-01| %_mask_idx: 5.906e-02| ppl: 4.270e+02| lr: 8.200e-05| temp: 1.999e+00| grad_norm: 1.497e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.664e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 5.068e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 4.657e+00| contrast_loss: 4.616e+00| div_loss: 4.060e-01| %_mask_idx: 3.878e-02| ppl: 3.802e+02| lr: 8.140e-05| temp: 1.999e+00| grad_norm: 1.647e-01\n",
            "| loss: 4.655e+00| contrast_loss: 4.615e+00| div_loss: 4.031e-01| %_mask_idx: 4.003e-02| ppl: 3.820e+02| lr: 8.090e-05| temp: 1.999e+00| grad_norm: 1.758e-01\n",
            "| loss: 4.653e+00| contrast_loss: 4.614e+00| div_loss: 3.825e-01| %_mask_idx: 3.918e-02| ppl: 3.952e+02| lr: 8.040e-05| temp: 1.999e+00| grad_norm: 1.694e-01\n",
            "| loss: 4.648e+00| contrast_loss: 4.615e+00| div_loss: 3.325e-01| %_mask_idx: 6.008e-02| ppl: 4.272e+02| lr: 7.990e-05| temp: 1.999e+00| grad_norm: 1.468e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.663e+00| val_contrastive_loss: 4.613e+00| val_diversity_loss: 4.934e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 4.649e+00| contrast_loss: 4.615e+00| div_loss: 3.342e-01| %_mask_idx: 5.686e-02| ppl: 4.261e+02| lr: 7.930e-05| temp: 1.999e+00| grad_norm: 1.432e-01\n",
            "| loss: 4.646e+00| contrast_loss: 4.617e+00| div_loss: 2.850e-01| %_mask_idx: 5.885e-02| ppl: 4.576e+02| lr: 7.880e-05| temp: 1.999e+00| grad_norm: 1.272e-01\n",
            "| loss: 4.657e+00| contrast_loss: 4.617e+00| div_loss: 4.057e-01| %_mask_idx: 4.073e-02| ppl: 3.804e+02| lr: 7.830e-05| temp: 1.999e+00| grad_norm: 1.592e-01\n",
            "| loss: 4.644e+00| contrast_loss: 4.615e+00| div_loss: 2.939e-01| %_mask_idx: 5.725e-02| ppl: 4.519e+02| lr: 7.780e-05| temp: 1.999e+00| grad_norm: 1.457e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.664e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 5.027e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 4.655e+00| contrast_loss: 4.615e+00| div_loss: 4.014e-01| %_mask_idx: 3.996e-02| ppl: 3.831e+02| lr: 7.720e-05| temp: 1.999e+00| grad_norm: 1.627e-01\n",
            "| loss: 4.653e+00| contrast_loss: 4.613e+00| div_loss: 4.025e-01| %_mask_idx: 3.990e-02| ppl: 3.824e+02| lr: 7.670e-05| temp: 1.999e+00| grad_norm: 1.759e-01\n",
            "| loss: 4.645e+00| contrast_loss: 4.613e+00| div_loss: 3.179e-01| %_mask_idx: 5.684e-02| ppl: 4.365e+02| lr: 7.620e-05| temp: 1.999e+00| grad_norm: 1.570e-01\n",
            "| loss: 4.659e+00| contrast_loss: 4.616e+00| div_loss: 4.355e-01| %_mask_idx: 3.917e-02| ppl: 3.613e+02| lr: 7.570e-05| temp: 1.999e+00| grad_norm: 1.706e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.663e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 4.944e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 4.650e+00| contrast_loss: 4.616e+00| div_loss: 3.376e-01| %_mask_idx: 5.942e-02| ppl: 4.239e+02| lr: 7.510e-05| temp: 1.999e+00| grad_norm: 1.570e-01\n",
            "| loss: 4.650e+00| contrast_loss: 4.617e+00| div_loss: 3.262e-01| %_mask_idx: 6.012e-02| ppl: 4.312e+02| lr: 7.460e-05| temp: 1.999e+00| grad_norm: 1.611e-01\n",
            "| loss: 4.655e+00| contrast_loss: 4.614e+00| div_loss: 4.124e-01| %_mask_idx: 4.034e-02| ppl: 3.760e+02| lr: 7.410e-05| temp: 1.999e+00| grad_norm: 1.848e-01\n",
            "| loss: 4.656e+00| contrast_loss: 4.614e+00| div_loss: 4.179e-01| %_mask_idx: 3.961e-02| ppl: 3.726e+02| lr: 7.360e-05| temp: 1.999e+00| grad_norm: 1.544e-01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| val_loss: 4.662e+00| val_contrastive_loss: 4.613e+00| val_diversity_loss: 4.930e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| loss: 4.654e+00| contrast_loss: 4.615e+00| div_loss: 3.843e-01| %_mask_idx: 4.007e-02| ppl: 3.941e+02| lr: 7.300e-05| temp: 1.999e+00| grad_norm: 1.574e-01\n",
            "| loss: 4.644e+00| contrast_loss: 4.614e+00| div_loss: 3.051e-01| %_mask_idx: 5.628e-02| ppl: 4.447e+02| lr: 7.250e-05| temp: 1.999e+00| grad_norm: 1.487e-01\n",
            "| loss: 4.659e+00| contrast_loss: 4.618e+00| div_loss: 4.118e-01| %_mask_idx: 4.008e-02| ppl: 3.764e+02| lr: 7.200e-05| temp: 1.999e+00| grad_norm: 1.542e-01\n",
            "| loss: 4.651e+00| contrast_loss: 4.616e+00| div_loss: 3.541e-01| %_mask_idx: 5.741e-02| ppl: 4.133e+02| lr: 7.150e-05| temp: 1.999e+00| grad_norm: 1.449e-01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| val_loss: 4.663e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 4.828e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| loss: 4.651e+00| contrast_loss: 4.616e+00| div_loss: 3.492e-01| %_mask_idx: 6.047e-02| ppl: 4.165e+02| lr: 7.090e-05| temp: 1.999e+00| grad_norm: 1.441e-01\n",
            "| loss: 4.645e+00| contrast_loss: 4.613e+00| div_loss: 3.145e-01| %_mask_idx: 5.577e-02| ppl: 4.387e+02| lr: 7.040e-05| temp: 1.999e+00| grad_norm: 1.397e-01\n",
            "| loss: 4.657e+00| contrast_loss: 4.614e+00| div_loss: 4.315e-01| %_mask_idx: 3.929e-02| ppl: 3.638e+02| lr: 6.990e-05| temp: 1.999e+00| grad_norm: 1.580e-01\n",
            "| loss: 4.644e+00| contrast_loss: 4.613e+00| div_loss: 3.057e-01| %_mask_idx: 6.018e-02| ppl: 4.443e+02| lr: 6.940e-05| temp: 1.999e+00| grad_norm: 1.351e-01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| val_loss: 4.663e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 4.867e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| loss: 4.658e+00| contrast_loss: 4.618e+00| div_loss: 4.020e-01| %_mask_idx: 4.059e-02| ppl: 3.827e+02| lr: 6.880e-05| temp: 1.999e+00| grad_norm: 1.715e-01\n"
          ]
        }
      ],
      "source": [
        "total_batch_size = TRAIN_BATCH_SIZE * accelerator.num_processes * GRADIENT_ACCUMULATION_STEPS\n",
        "logger.info(\"***** Runing training *****\")\n",
        "logger.info(f\" Num examples = {len(vectorized_datasets['train'])}\")\n",
        "logger.info(f\" Num Epochs = {NUM_TRAIN_EPOCHS}\")\n",
        "logger.info(f\"  Instantaneous batch size per device = {TRAIN_BATCH_SIZE}\")\n",
        "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "logger.info(f\"  Gradient Accumulation steps = {GRADIENT_ACCUMULATION_STEPS}\")\n",
        "logger.info(f\"  Total optimization steps = {MAX_TRAINING_STEPS}\")\n",
        "\n",
        "completed_steps = 0\n",
        "starting_epoch = 0\n",
        "\n",
        "progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "completed_steps = 0\n",
        "starting_epoch = 0\n",
        "\n",
        "for epoch in range(starting_epoch, NUM_TRAIN_EPOCHS):\n",
        "  model.train()\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    num_losses = batch[\"mask_time_indices\"].sum()\n",
        "    sub_attention_mask = batch.pop(\"sub_attention_mask\", None)\n",
        "    sub_attention_mask = (\n",
        "        sub_attention_mask if sub_attention_mask is not None else torch.ones_like(batch[\"mask_time_indices\"])\n",
        "    )\n",
        "    precent_masked = num_losses / sub_attention_mask.sum()\n",
        "\n",
        "    outputs = model(**batch)\n",
        "\n",
        "    loss = outputs.loss / GRADIENT_ACCUMULATION_STEPS\n",
        "    accelerator.backward(loss)\n",
        "\n",
        "    if accelerator.state.num_processes > 1:\n",
        "      num_losses = accelerator.gather_for_metrics(num_losses).sum()\n",
        "      gradient_multiplier = accelerator.state.num_processes / num_losses\n",
        "      multiply_grads(model.parameters(), gradient_multiplier)\n",
        "    else:\n",
        "      multiply_grads(model.parameters(), 1 / num_losses)\n",
        "\n",
        "    # Update step\n",
        "    if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0 or step == len(train_dataloader) - 1:\n",
        "      scale = (\n",
        "          accelerator.scaler._scale.item()\n",
        "          if hasattr(accelerator, \"scaler\") and accelerator.scaler is not None\n",
        "          else 1.0\n",
        "      )\n",
        "      if accelerator.state.num_processes > 1:\n",
        "        grad_norm = get_grad_norm(model.module.parameters(), scale)\n",
        "      else:\n",
        "        grad_norm = get_grad_norm(model.parameters(), scale)\n",
        "\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      if not accelerator.optimizer_step_was_skipped:\n",
        "        lr_scheduler.step()\n",
        "      elif accelerator.is_local_main_process:\n",
        "        progress_bar.write(\n",
        "          f\"Gradients have overflown - skipping update step... Updating gradient scale to {scale}...\"\n",
        "        )\n",
        "\n",
        "      # update gumbel temperature\n",
        "      gumble_temperature = max(\n",
        "          MAX_GUMBEL_TEMPERATURE * GUMBEL_TEMPERATURE**completed_steps,\n",
        "          MIN_GUMBEL_TEMPERATURE,\n",
        "      )\n",
        "\n",
        "      if hasattr(model, \"module\"):\n",
        "        model.module.set_gumbel_temperature(gumble_temperature)\n",
        "      else:\n",
        "        model.set_gumbel_temperature(gumble_temperature)\n",
        "\n",
        "      progress_bar.update(1)\n",
        "      completed_steps += 1\n",
        "\n",
        "      # Log all results\n",
        "      if (step + 1) % (GRADIENT_ACCUMULATION_STEPS * LOGGING_STEPS) == 0:\n",
        "        loss.detach()\n",
        "        outputs.contrastive_loss.detach()\n",
        "        outputs.diversity_loss.detach()\n",
        "\n",
        "        if accelerator.state.num_processes > 1:\n",
        "          loss = accelerator.gather_for_metrics(loss).sum()\n",
        "          outputs.contrastive_loss = accelerator.gather_for_metrics(outputs.contrastive_loss).sum()\n",
        "          outputs.diversity_loss = accelerator.gather_for_metrics(outputs.diversity_loss).sum()\n",
        "          percent_masked = accelerator.gather_for_metrics(precent_masked).sum()\n",
        "\n",
        "        train_logs = {\n",
        "            \"loss\": (loss * GRADIENT_ACCUMULATION_STEPS) / num_losses,\n",
        "            \"contrast_loss\": outputs.contrastive_loss / num_losses,\n",
        "            \"div_loss\": outputs.diversity_loss / num_losses,\n",
        "            \"%_mask_idx\": precent_masked / accelerator.num_processes,\n",
        "            \"ppl\": outputs.codevector_perplexity,\n",
        "            \"lr\": torch.tensor(optimizer.param_groups[0][\"lr\"]),\n",
        "            \"temp\": torch.tensor(gumble_temperature),\n",
        "            \"grad_norm\": torch.tensor(grad_norm),\n",
        "        }\n",
        "\n",
        "        log_str = \"\"\n",
        "        for k, v in train_logs.items():\n",
        "          log_str += f\"| {k}: {v.item():.3e}\"\n",
        "\n",
        "        if accelerator.is_local_main_process:\n",
        "          progress_bar.write(log_str)\n",
        "          if is_wandb_available():\n",
        "            wandb.log(train_logs)\n",
        "\n",
        "      # save model\n",
        "      if (step + 1) % (GRADIENT_ACCUMULATION_STEPS * SAVING_STEPS) == 0:\n",
        "        if OUTPUT_DIR is not None:\n",
        "            accelerator.wait_for_everyone()\n",
        "            unwrapped_model = accelerator.unwrap_model(model)\n",
        "            unwrapped_model.save_pretrained(\n",
        "                OUTPUT_DIR,\n",
        "                is_main_process=accelerator.is_main_process,\n",
        "                save_function=accelerator.save\n",
        "            )\n",
        "\n",
        "      if completed_steps >= max_train_steps:\n",
        "        break\n",
        "  model.eval()\n",
        "\n",
        "  val_logs = {\n",
        "      \"val_loss\": 0,\n",
        "      \"val_contrastive_loss\": 0,\n",
        "      \"val_diversity_loss\": 0,\n",
        "      \"val_num_losses\": 0,\n",
        "  }\n",
        "\n",
        "  for step, batch in enumerate(val_dataloader):\n",
        "    with torch.no_grad():\n",
        "        batch.pop(\"sub_attention_mask\", None)\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    val_logs[\"val_loss\"] += outputs.loss\n",
        "    val_logs[\"val_contrastive_loss\"] += outputs.contrastive_loss\n",
        "    val_logs[\"val_diversity_loss\"] += outputs.diversity_loss\n",
        "    val_logs[\"val_num_losses\"] += batch[\"mask_time_indices\"].sum()\n",
        "\n",
        "  if accelerator.num_processes > 1:\n",
        "      val_logs = {k: accelerator.gather_for_metrics(v).sum() for k, v in val_logs.items()}\n",
        "\n",
        "  val_logs = {k: v / val_logs[\"val_num_losses\"] for k, v in val_logs.items()}\n",
        "\n",
        "  log_str = \"\"\n",
        "  for k, v in val_logs.items():\n",
        "      log_str += f\"| {k}: {v.item():.3e}\"\n",
        "\n",
        "  if accelerator.is_local_main_process:\n",
        "      progress_bar.write(log_str)\n",
        "      if is_wandb_available():\n",
        "          wandb.log(val_logs)\n",
        "\n",
        "  if OUTPUT_DIR is not None:\n",
        "      accelerator.wait_for_everyone()\n",
        "      unwrapped_model = accelerator.unwrap_model(model)\n",
        "      unwrapped_model.save_pretrained(\n",
        "          OUTPUT_DIR, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OvnO4aKcHz5t"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d2750510dd6d4cd99a5362379143140c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78f52f50e4ce4e69a7aae21e1659cc49",
              "IPY_MODEL_90136bda08d44f47a99e6c1fe13d8e81",
              "IPY_MODEL_6de61b9befdd446d8712cfe19e61b810"
            ],
            "layout": "IPY_MODEL_3f9f19156abf4b85a30557b2f8101d05"
          }
        },
        "78f52f50e4ce4e69a7aae21e1659cc49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3002788c30024de7acd749ab1444b50b",
            "placeholder": "​",
            "style": "IPY_MODEL_18c08bbfc8964b8cbe530a1a1b782bcc",
            "value": "Fetching 1 files: 100%"
          }
        },
        "90136bda08d44f47a99e6c1fe13d8e81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ca98d4d32814dd99cec01280505783d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6622ddf18774677a6fc41c026e46bf8",
            "value": 1
          }
        },
        "6de61b9befdd446d8712cfe19e61b810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2cf57b104494cd2a8b925df0f188a03",
            "placeholder": "​",
            "style": "IPY_MODEL_d8fd2f2bf7894cb99be219f6922d524b",
            "value": " 1/1 [00:00&lt;00:00,  3.45it/s]"
          }
        },
        "3f9f19156abf4b85a30557b2f8101d05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3002788c30024de7acd749ab1444b50b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18c08bbfc8964b8cbe530a1a1b782bcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ca98d4d32814dd99cec01280505783d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6622ddf18774677a6fc41c026e46bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2cf57b104494cd2a8b925df0f188a03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8fd2f2bf7894cb99be219f6922d524b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85c70f120de449e2ae531c29a5f70c83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4fe6e261ccdf40fbaa65459a2cf9ecf2",
              "IPY_MODEL_83f30717aeb140c9a61a3c86258cccd8",
              "IPY_MODEL_8f6031c5d9ae4ec9b1b8fcaa05780539"
            ],
            "layout": "IPY_MODEL_2439ddb36139424491f4b1939faa6e69"
          }
        },
        "4fe6e261ccdf40fbaa65459a2cf9ecf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39e4feec328642259d0f61af1d35757d",
            "placeholder": "​",
            "style": "IPY_MODEL_d976b818d9b34d44939406e9a05ea0a0",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "83f30717aeb140c9a61a3c86258cccd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51b16415094e46a48b6e8d6cb4eec869",
            "max": 159,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4938e0732c64f84961f8ff19fa4c8b7",
            "value": 159
          }
        },
        "8f6031c5d9ae4ec9b1b8fcaa05780539": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b7191fea2c74e349c38529339a09437",
            "placeholder": "​",
            "style": "IPY_MODEL_3f0c89b9ee1e46a88391491d0191ec0e",
            "value": " 159/159 [00:00&lt;00:00, 20.2kB/s]"
          }
        },
        "2439ddb36139424491f4b1939faa6e69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39e4feec328642259d0f61af1d35757d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d976b818d9b34d44939406e9a05ea0a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51b16415094e46a48b6e8d6cb4eec869": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4938e0732c64f84961f8ff19fa4c8b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b7191fea2c74e349c38529339a09437": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f0c89b9ee1e46a88391491d0191ec0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "535a0ddec14948a29782e34314d4ad03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb1ccf60e060451e872c96fc3ed58199",
              "IPY_MODEL_73eb5966cc864cbfadba6e1df28a2fb7",
              "IPY_MODEL_d2c68c2d477d4afdbcd4e94c642b44a1"
            ],
            "layout": "IPY_MODEL_8f1e21afbf1c463bba76884646b4c2b1"
          }
        },
        "bb1ccf60e060451e872c96fc3ed58199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c1ab7743826452391c4013f6773036d",
            "placeholder": "​",
            "style": "IPY_MODEL_345cdd815720491eaaeb7ffa474dcc21",
            "value": "config.json: "
          }
        },
        "73eb5966cc864cbfadba6e1df28a2fb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af553d32a0bb4c45bcf4a8623d170be1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9612cee1bbdf4a45b785b7ec1c48fd74",
            "value": 1
          }
        },
        "d2c68c2d477d4afdbcd4e94c642b44a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00e1ec8580d54c88870aeb906204db0d",
            "placeholder": "​",
            "style": "IPY_MODEL_8b2e554eeae2439fa677333c9778525d",
            "value": " 1.84k/? [00:00&lt;00:00, 155kB/s]"
          }
        },
        "8f1e21afbf1c463bba76884646b4c2b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c1ab7743826452391c4013f6773036d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "345cdd815720491eaaeb7ffa474dcc21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af553d32a0bb4c45bcf4a8623d170be1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "9612cee1bbdf4a45b785b7ec1c48fd74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00e1ec8580d54c88870aeb906204db0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b2e554eeae2439fa677333c9778525d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ea690f808464877963189bd206a9d2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70cac061391545c6b2f40b7a1e9db274",
              "IPY_MODEL_f4250de969f64556a4a9292245f85af9",
              "IPY_MODEL_4d5d93a41703476b9a5659f8c111606c"
            ],
            "layout": "IPY_MODEL_99e9984a4e2e4ee3b1942d7bf7ab692b"
          }
        },
        "70cac061391545c6b2f40b7a1e9db274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_734e9cf62ded49ec89898bc90f533e3f",
            "placeholder": "​",
            "style": "IPY_MODEL_8a67e6a2836a467e85294b2b1db5b912",
            "value": " 35%"
          }
        },
        "f4250de969f64556a4a9292245f85af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44463b62e935441e87857173802329b3",
            "max": 2100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd44897d59544a21bc309fe07ba810d5",
            "value": 725
          }
        },
        "4d5d93a41703476b9a5659f8c111606c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b27c1e579764edcabfc70ebbd9388b9",
            "placeholder": "​",
            "style": "IPY_MODEL_75422607561946ff96a7a166cd6f1644",
            "value": " 725/2100 [4:47:07&lt;7:58:09, 20.86s/it]"
          }
        },
        "99e9984a4e2e4ee3b1942d7bf7ab692b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "734e9cf62ded49ec89898bc90f533e3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a67e6a2836a467e85294b2b1db5b912": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44463b62e935441e87857173802329b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd44897d59544a21bc309fe07ba810d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b27c1e579764edcabfc70ebbd9388b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75422607561946ff96a7a166cd6f1644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}