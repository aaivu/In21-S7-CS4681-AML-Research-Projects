{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9syxLSLYKoJB",
        "outputId": "3d479db2-0d68-42bd-b2ab-1782a1d5ec42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchcodec\n",
            "  Downloading torchcodec-0.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.4 kB)\n",
            "Downloading torchcodec-0.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchcodec\n",
            "Successfully installed torchcodec-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchcodec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9q6bS9TCJJM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from datasets import load_from_disk, DatasetDict\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Union\n",
        "import logging\n",
        "import datasets\n",
        "import math\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    SchedulerType,\n",
        "    Wav2Vec2Config,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    Wav2Vec2ForPreTraining,\n",
        "    get_scheduler,\n",
        "    is_wandb_available,\n",
        "    set_seed,\n",
        ")\n",
        "from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF2AZoeCuCfu",
        "outputId": "35d6a597-00fe-4db1-cd8b-5755adc77043"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufpGv_z4HTEB"
      },
      "outputs": [],
      "source": [
        "MAX_DURATION = 10.0\n",
        "MIN_DURATION = 5.0\n",
        "GRADIENT_CHECKPOINTING = True\n",
        "MASK_TIME_PROB = None\n",
        "MASK_TIME_LENGTH = None\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VAL_BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-4 #5e-5\n",
        "ADAM_BETA1 = 0.9\n",
        "ADAM_BETA2 = 0.98\n",
        "ADAM_EPSILON = 1e-8\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "MAX_TRAINING_STEPS = None\n",
        "NUM_TRAIN_EPOCHS = 50\n",
        "LR_SCHEDULER_TYPE = \"linear\"\n",
        "NUM_WARMUP_STEPS = 100 #0\n",
        "MAX_GUMBEL_TEMPERATURE = 2.0\n",
        "MIN_GUMBEL_TEMPERATURE = 1.0 #0.5\n",
        "GUMBEL_TEMPERATURE = 0.999999 #0.999995\n",
        "LOGGING_STEPS = 10\n",
        "SAVING_STEPS = 500\n",
        "OUTPUT_DIR = \"/outputs\"\n",
        "INTER_CB_SIMILARITY_WEIGHT = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiOzLEWPXJ_D"
      },
      "outputs": [],
      "source": [
        "logger = get_logger(__name__)\n",
        "logging.basicConfig(level=logging.INFO, force=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iiq41dHFD3MH"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataCollatorForWav2Vec2Pretraining:\n",
        "  model: Wav2Vec2ForPreTraining\n",
        "  feature_extractor: Wav2Vec2FeatureExtractor\n",
        "  padding: str = \"longest\"\n",
        "  pad_to_multiple_of: int = None\n",
        "  mask_time_prob: float = 0.65\n",
        "  mask_time_length: int= 10\n",
        "\n",
        "  def __call__(self, features: list[dict[str, Union[list[int], torch.tensor]]]) -> dict[str, torch.Tensor]:\n",
        "    batch = self.feature_extractor.pad(\n",
        "        features,\n",
        "        padding=self.padding,\n",
        "        pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    device = batch[\"input_values\"].device\n",
        "    batch_size = batch['input_values'].shape[0]\n",
        "\n",
        "    mask_indices_seq_length = self.model._get_feat_extract_output_lengths(batch[\"input_values\"].shape[-1])\n",
        "    mask_indices_seq_length = int(mask_indices_seq_length)\n",
        "\n",
        "    if batch.get(\"attention_mask\") is not None:\n",
        "      batch[\"sub_attention_mask\"] = self.model._get_feature_vector_attention_mask(\n",
        "          mask_indices_seq_length, batch[\"attention_mask\"]\n",
        "      )\n",
        "\n",
        "    features_shape = (batch_size, mask_indices_seq_length)\n",
        "\n",
        "    # Sample randomly maksed indices\n",
        "    mask_time_indices = _compute_mask_indices(\n",
        "        features_shape,\n",
        "        self.mask_time_prob,\n",
        "        self.mask_time_length,\n",
        "        attention_mask=batch.get(\"sub_attention_mask\"),\n",
        "    )\n",
        "\n",
        "    # Sample negative indices\n",
        "    sampled_negative_indices = _sample_negative_indices(\n",
        "        features_shape,\n",
        "        self.model.config.num_negatives,\n",
        "        mask_time_indices=mask_time_indices,\n",
        "    )\n",
        "\n",
        "    batch[\"mask_time_indices\"] = torch.tensor(mask_time_indices, dtype=torch.long, device=device)\n",
        "    batch[\"sampled_negative_indices\"] = torch.tensor(sampled_negative_indices, dtype=torch.long, device=device)\n",
        "\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQPNj65jQ61v"
      },
      "outputs": [],
      "source": [
        "def multiply_grads(params, c):\n",
        "  \"\"\"Multiply grad by a constant c\"\"\"\n",
        "  for p in params:\n",
        "    if p.grad is not None:\n",
        "      if torch.is_tensor(c):\n",
        "        c = c.to(p.grad.device)\n",
        "      p.grad.data.mul_(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEeNfAImVHG3"
      },
      "outputs": [],
      "source": [
        "def get_grad_norm(params, scale=1):\n",
        "  \"\"\"Compute grad norm given a gradient scale\"\"\"\n",
        "  total_norm = 0.0\n",
        "  for p in params:\n",
        "    if p.grad is not None:\n",
        "      param_norm = (p.grad.detach().data / scale).norm(2)\n",
        "      total_norm += param_norm.item() ** 2\n",
        "  total_norm = total_norm ** 0.5\n",
        "  return total_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "XtBjp1aRWUBh",
        "outputId": "10c47528-5a45-40ad-e595-ff4ae21074e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Distributed environment: DistributedType.NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: no\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wandb have installed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrasarathathsarana63\u001b[0m (\u001b[33mrasarathathsarana63-university-of-moratuwa\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250926_112244-o8uf27h9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rasarathathsarana63-university-of-moratuwa/wav2vec2-fromscratch/runs/o8uf27h9' target=\"_blank\">icy-plasma-13</a></strong> to <a href='https://wandb.ai/rasarathathsarana63-university-of-moratuwa/wav2vec2-fromscratch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/rasarathathsarana63-university-of-moratuwa/wav2vec2-fromscratch' target=\"_blank\">https://wandb.ai/rasarathathsarana63-university-of-moratuwa/wav2vec2-fromscratch</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/rasarathathsarana63-university-of-moratuwa/wav2vec2-fromscratch/runs/o8uf27h9' target=\"_blank\">https://wandb.ai/rasarathathsarana63-university-of-moratuwa/wav2vec2-fromscratch/runs/o8uf27h9</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "accelerator = Accelerator()\n",
        "logger.info(accelerator.state, main_process_only=False)\n",
        "\n",
        "if accelerator.is_local_main_process:\n",
        "  datasets.utils.logging.set_verbosity_warning()\n",
        "  transformers.utils.logging.set_verbosity_info()\n",
        "\n",
        "  if is_wandb_available():\n",
        "    print(\"wandb have installed\")\n",
        "    import wandb\n",
        "    wandb.init(project=\"wav2vec2-fromscratch\")\n",
        "\n",
        "else:\n",
        "  datasets.utils.logging.set_verbosity_error()\n",
        "  transformers.utils.logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9zgC005YJP8"
      },
      "outputs": [],
      "source": [
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6725-xCrb_dU"
      },
      "outputs": [],
      "source": [
        "accelerator.wait_for_everyone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_7qKyEfcItx"
      },
      "outputs": [],
      "source": [
        "raw_datasets = DatasetDict()\n",
        "\n",
        "raw_datasets['train'] = load_from_disk(\"/content/drive/MyDrive/SP/SP/librispeech_datasets/dataset_10h\")\n",
        "raw_datasets['val'] = load_from_disk(\"/content/drive/MyDrive/SP/SP/librispeech_datasets/dataset_val_clean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFu1WsBDNRfH"
      },
      "outputs": [],
      "source": [
        "raw_datasets[\"train\"] = raw_datasets[\"train\"].remove_columns(\"duration\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396,
          "referenced_widgets": [
            "a30d6d963d2c4b4ab3b3b52527c61e24",
            "affede35b3d44edc82c5ebd2a8417c68",
            "34bdd10c1e3b4b99a30fd56f197696dd",
            "52c8539f82704408850c44a72bd1522d",
            "6cf3cf8c0ff14d5998e2bace0f119ed9",
            "e20224a92eaa44c3a089f6f0d9ea8804",
            "e2f5a0b2abcc45938b8dfe1b87f798a6",
            "38b15b98547843958626ce0c91df374e",
            "dcd076acc889455ea927873749106acf",
            "481fd8c9baae4a24814c68fb7fc7cac4",
            "4578845ad26847a48703ca567fc433fb",
            "5aaef1e5c67c4beaa61efd41113dc2cf",
            "e0996d860aa648f8af39abe4ecebb44a",
            "ed9464b794ef4d1c84ef3f4b84203861",
            "96a1bc3a09304fc6af998f5cd2129f39",
            "68421fb636fa44b5aade237be1ae4203",
            "eb81b4f8672d4fbc97b43539200b5405",
            "24b544a5922e4847978367ebf52ce291",
            "8ff7ebe28c28453f8a3e9226eb4f65ed",
            "901a2c73d4c7439fa1fbd86e2cf7cd6e",
            "99529c0333824dbd99365b4539bf28e3",
            "0bd4782e2e584420897ec63774031620"
          ]
        },
        "id": "K7Bl4MtnsaqA",
        "outputId": "191d6f3b-5058-471d-8824-b2c8962cc1e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a30d6d963d2c4b4ab3b3b52527c61e24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5aaef1e5c67c4beaa61efd41113dc2cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--facebook--wav2vec2-base/snapshots/0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8/preprocessor_config.json\n",
            "Feature extractor Wav2Vec2FeatureExtractor {\n",
            "  \"do_normalize\": true,\n",
            "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
            "  \"feature_size\": 1,\n",
            "  \"padding_side\": \"right\",\n",
            "  \"padding_value\": 0.0,\n",
            "  \"return_attention_mask\": true,\n",
            "  \"sampling_rate\": 16000\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
        "    \"facebook/wav2vec2-base\",\n",
        "    return_attention_mask = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AR6KPkzu5Ga"
      },
      "outputs": [],
      "source": [
        "raw_datasets = raw_datasets.cast_column(\n",
        "    'audio', datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epFODlkP--Xg"
      },
      "outputs": [],
      "source": [
        "# only normalized-inputs-training is supported\n",
        "if not feature_extractor.do_normalize:\n",
        "  raise ValueError(\n",
        "      \"Training is only supported for normalized inputs. Make sure ``feature_extractor.do_normalize == True``\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6_eEqKe_JMF"
      },
      "outputs": [],
      "source": [
        "# Set max & min audio length in number of samples\n",
        "max_length = int(MAX_DURATION * feature_extractor.sampling_rate)\n",
        "min_length = int(MIN_DURATION * feature_extractor.sampling_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sJix4Fm_JIu"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "  sample = batch[\"audio\"]\n",
        "  inputs = feature_extractor(\n",
        "      sample[\"array\"],\n",
        "      sampling_rate=sample[\"sampling_rate\"],\n",
        "      max_length=max_length,\n",
        "      truncation=True\n",
        "  )\n",
        "  batch[\"input_values\"] = inputs.input_values[0]\n",
        "  batch[\"input_length\"] = len(inputs.input_values[0])\n",
        "\n",
        "  return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YekAifm2_JGI"
      },
      "outputs": [],
      "source": [
        "# load audio files into numpy arrays\n",
        "with accelerator.main_process_first():\n",
        "  vectorized_datasets = raw_datasets.map(\n",
        "      prepare_dataset,\n",
        "      num_proc=None,\n",
        "      remove_columns=raw_datasets[\"train\"].column_names,\n",
        "  )\n",
        "\n",
        "  if min_length > 0.0:\n",
        "    vectorized_datasets = vectorized_datasets.filter(\n",
        "        lambda x: x > min_length,\n",
        "        num_proc=None,\n",
        "        input_columns=[\"input_length\"]\n",
        "    )\n",
        "\n",
        "  vectorized_datasets = vectorized_datasets.remove_columns(\"input_length\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b508c5753e40476085dc75d5839b7618",
            "0ce47bba283a4c7eac50285779158b93",
            "ca27e8a36abe4903962a688859caacb9",
            "f5b8fdfe2f694aa8952339c823cc2a76",
            "c17f7ebd3d354d8e9fd66fefbc4237a3",
            "0fdf61ed48c74443a560239560c9279f",
            "a49e7a668b0b4be2b93cb27735c19e60",
            "93b8f93a91c74669bd37a08281489225",
            "fe887ed8cdd74f8dba3b90b000e11b5f",
            "935e4c44dea143018ef5e75b087a46fd",
            "bba10094b3ea4c86b02fcce4dc5a7572"
          ]
        },
        "id": "Zz_AgQ8s_JDe",
        "outputId": "92fa98cf-3665-49c7-a8ad-90ca32529d1b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b508c5753e40476085dc75d5839b7618"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--facebook--wav2vec2-base/snapshots/0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8/config.json\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Model config Wav2Vec2Config {\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"adapter_attn_dim\": null,\n",
            "  \"adapter_kernel_size\": 3,\n",
            "  \"adapter_stride\": 2,\n",
            "  \"add_adapter\": false,\n",
            "  \"apply_spec_augment\": true,\n",
            "  \"architectures\": [\n",
            "    \"Wav2Vec2ForPreTraining\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"classifier_proj_size\": 256,\n",
            "  \"codevector_dim\": 256,\n",
            "  \"contrastive_logits_temperature\": 0.1,\n",
            "  \"conv_bias\": false,\n",
            "  \"conv_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512\n",
            "  ],\n",
            "  \"conv_kernel\": [\n",
            "    10,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    3,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"conv_stride\": [\n",
            "    5,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2,\n",
            "    2\n",
            "  ],\n",
            "  \"ctc_loss_reduction\": \"sum\",\n",
            "  \"ctc_zero_infinity\": false,\n",
            "  \"diversity_loss_weight\": 0.1,\n",
            "  \"do_stable_layer_norm\": false,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"feat_extract_activation\": \"gelu\",\n",
            "  \"feat_extract_norm\": \"group\",\n",
            "  \"feat_proj_dropout\": 0.1,\n",
            "  \"feat_quantizer_dropout\": 0.0,\n",
            "  \"final_dropout\": 0.0,\n",
            "  \"freeze_feat_extract_train\": true,\n",
            "  \"gradient_checkpointing\": true,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"layerdrop\": 0.0,\n",
            "  \"mask_channel_length\": 10,\n",
            "  \"mask_channel_min_space\": 1,\n",
            "  \"mask_channel_other\": 0.0,\n",
            "  \"mask_channel_prob\": 0.0,\n",
            "  \"mask_channel_selection\": \"static\",\n",
            "  \"mask_feature_length\": 10,\n",
            "  \"mask_feature_min_masks\": 0,\n",
            "  \"mask_feature_prob\": 0.0,\n",
            "  \"mask_time_length\": 10,\n",
            "  \"mask_time_min_masks\": 2,\n",
            "  \"mask_time_min_space\": 1,\n",
            "  \"mask_time_other\": 0.0,\n",
            "  \"mask_time_prob\": 0.05,\n",
            "  \"mask_time_selection\": \"static\",\n",
            "  \"model_type\": \"wav2vec2\",\n",
            "  \"no_mask_channel_overlap\": false,\n",
            "  \"no_mask_time_overlap\": false,\n",
            "  \"num_adapter_layers\": 3,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_codevector_groups\": 2,\n",
            "  \"num_codevectors_per_group\": 320,\n",
            "  \"num_conv_pos_embedding_groups\": 16,\n",
            "  \"num_conv_pos_embeddings\": 128,\n",
            "  \"num_feat_extract_layers\": 7,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_negatives\": 100,\n",
            "  \"output_hidden_size\": 768,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"proj_codevector_dim\": 256,\n",
            "  \"tdnn_dilation\": [\n",
            "    1,\n",
            "    2,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"tdnn_dim\": [\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    512,\n",
            "    1500\n",
            "  ],\n",
            "  \"tdnn_kernel\": [\n",
            "    5,\n",
            "    3,\n",
            "    3,\n",
            "    1,\n",
            "    1\n",
            "  ],\n",
            "  \"transformers_version\": \"4.56.1\",\n",
            "  \"use_weighted_layer_sum\": false,\n",
            "  \"vocab_size\": 32,\n",
            "  \"xvector_output_dim\": 512\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "config = Wav2Vec2Config.from_pretrained(\"facebook/wav2vec2-base\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config.num_codevector_groups = 8"
      ],
      "metadata": {
        "id": "nH22JNLMv7Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def inter_codebook_similarity_loss(codebook_vectors):\n",
        "    # codebook_vectors: [G, V, D]\n",
        "    codebook_vectors = codebook_vectors.reshape(config.num_codevector_groups, config.num_codevectors_per_group, -1)\n",
        "    G, V, D = codebook_vectors.shape\n",
        "    losses = []\n",
        "    for i in range(G):\n",
        "        for j in range(i + 1, G):\n",
        "            # Flatten groups into [V, D]\n",
        "            e_i = codebook_vectors[i]  # [V, D]\n",
        "            e_j = codebook_vectors[j]  # [V, D]\n",
        "\n",
        "            # Normalize\n",
        "            e_i = F.normalize(e_i, dim=-1)\n",
        "            e_j = F.normalize(e_j, dim=-1)\n",
        "\n",
        "            # Pairwise cosine similarity: [V, V]\n",
        "            sim = torch.matmul(e_i, e_j.T)\n",
        "\n",
        "            # Mean similarity\n",
        "            losses.append(sim.mean())\n",
        "\n",
        "    return torch.stack(losses).mean() if losses else torch.tensor(0.0, device=codebook_vectors.device)\n"
      ],
      "metadata": {
        "id": "NhIb1i2ssAHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAIPxk3sN5-B"
      },
      "outputs": [],
      "source": [
        "# model = Wav2Vec2ForPreTraining.from_pretrained(\n",
        "#     \"facebook/wav2vec2-base\",\n",
        "#     config=config\n",
        "# )\n",
        "\n",
        "model = Wav2Vec2ForPreTraining(\n",
        "    config=config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHVKPXFMMy4O"
      },
      "outputs": [],
      "source": [
        "# Activate gradient checkpointing\n",
        "if GRADIENT_CHECKPOINTING:\n",
        "  model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL7wWlNIP6sc"
      },
      "outputs": [],
      "source": [
        "mask_time_prob = config.mask_time_prob if MASK_TIME_PROB is None else MASK_TIME_PROB\n",
        "mask_time_length = config.mask_time_length if MASK_TIME_LENGTH is None else MASK_TIME_LENGTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0yToSoEQxMS"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForWav2Vec2Pretraining(\n",
        "    model=model,\n",
        "    feature_extractor=feature_extractor,\n",
        "    mask_time_prob=mask_time_prob,\n",
        "    mask_time_length=mask_time_length\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFVFHUSYRiR-"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(\n",
        "    vectorized_datasets[\"train\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtvbQfKXTBuo"
      },
      "outputs": [],
      "source": [
        "val_dataloader = DataLoader(\n",
        "    vectorized_datasets[\"val\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=VAL_BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MFTyy1pTMgP"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    list(model.parameters()),\n",
        "    lr=LEARNING_RATE,\n",
        "    betas=[ADAM_BETA1, ADAM_BETA2],\n",
        "    eps=ADAM_EPSILON\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAVnpWMpUeox"
      },
      "outputs": [],
      "source": [
        "model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, val_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68f5gYeaUkXQ"
      },
      "outputs": [],
      "source": [
        "num_update_steps_per_epcoh = math.ceil(len(train_dataloader) / GRADIENT_ACCUMULATION_STEPS)\n",
        "\n",
        "if MAX_TRAINING_STEPS is None:\n",
        "  max_train_steps = num_update_steps_per_epcoh * NUM_TRAIN_EPOCHS\n",
        "else:\n",
        "  max_train_steps = MAX_TRAINING_STEPS\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=LR_SCHEDULER_TYPE,\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=NUM_WARMUP_STEPS,\n",
        "    num_training_steps=max_train_steps,\n",
        ")\n",
        "\n",
        "NUM_TRAIN_EPOCHS = math.ceil(max_train_steps / num_update_steps_per_epcoh)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params = [name for name, param in model.named_parameters() if param.requires_grad]\n",
        "print(\"Trainable parameters:\", list(trainable_params))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE3aojSzWwhj",
        "outputId": "e8fdbfd4-d9d5-4034-ddde-1ece560e6ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: ['wav2vec2.masked_spec_embed', 'wav2vec2.feature_extractor.conv_layers.0.conv.weight', 'wav2vec2.feature_extractor.conv_layers.0.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.0.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.1.conv.weight', 'wav2vec2.feature_extractor.conv_layers.2.conv.weight', 'wav2vec2.feature_extractor.conv_layers.3.conv.weight', 'wav2vec2.feature_extractor.conv_layers.4.conv.weight', 'wav2vec2.feature_extractor.conv_layers.5.conv.weight', 'wav2vec2.feature_extractor.conv_layers.6.conv.weight', 'wav2vec2.feature_projection.layer_norm.weight', 'wav2vec2.feature_projection.layer_norm.bias', 'wav2vec2.feature_projection.projection.weight', 'wav2vec2.feature_projection.projection.bias', 'wav2vec2.encoder.pos_conv_embed.conv.bias', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.layer_norm.weight', 'wav2vec2.encoder.layer_norm.bias', 'wav2vec2.encoder.layers.0.attention.k_proj.weight', 'wav2vec2.encoder.layers.0.attention.k_proj.bias', 'wav2vec2.encoder.layers.0.attention.v_proj.weight', 'wav2vec2.encoder.layers.0.attention.v_proj.bias', 'wav2vec2.encoder.layers.0.attention.q_proj.weight', 'wav2vec2.encoder.layers.0.attention.q_proj.bias', 'wav2vec2.encoder.layers.0.attention.out_proj.weight', 'wav2vec2.encoder.layers.0.attention.out_proj.bias', 'wav2vec2.encoder.layers.0.layer_norm.weight', 'wav2vec2.encoder.layers.0.layer_norm.bias', 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.0.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.0.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.0.final_layer_norm.weight', 'wav2vec2.encoder.layers.0.final_layer_norm.bias', 'wav2vec2.encoder.layers.1.attention.k_proj.weight', 'wav2vec2.encoder.layers.1.attention.k_proj.bias', 'wav2vec2.encoder.layers.1.attention.v_proj.weight', 'wav2vec2.encoder.layers.1.attention.v_proj.bias', 'wav2vec2.encoder.layers.1.attention.q_proj.weight', 'wav2vec2.encoder.layers.1.attention.q_proj.bias', 'wav2vec2.encoder.layers.1.attention.out_proj.weight', 'wav2vec2.encoder.layers.1.attention.out_proj.bias', 'wav2vec2.encoder.layers.1.layer_norm.weight', 'wav2vec2.encoder.layers.1.layer_norm.bias', 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.1.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.1.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.1.final_layer_norm.weight', 'wav2vec2.encoder.layers.1.final_layer_norm.bias', 'wav2vec2.encoder.layers.2.attention.k_proj.weight', 'wav2vec2.encoder.layers.2.attention.k_proj.bias', 'wav2vec2.encoder.layers.2.attention.v_proj.weight', 'wav2vec2.encoder.layers.2.attention.v_proj.bias', 'wav2vec2.encoder.layers.2.attention.q_proj.weight', 'wav2vec2.encoder.layers.2.attention.q_proj.bias', 'wav2vec2.encoder.layers.2.attention.out_proj.weight', 'wav2vec2.encoder.layers.2.attention.out_proj.bias', 'wav2vec2.encoder.layers.2.layer_norm.weight', 'wav2vec2.encoder.layers.2.layer_norm.bias', 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.2.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.2.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.2.final_layer_norm.weight', 'wav2vec2.encoder.layers.2.final_layer_norm.bias', 'wav2vec2.encoder.layers.3.attention.k_proj.weight', 'wav2vec2.encoder.layers.3.attention.k_proj.bias', 'wav2vec2.encoder.layers.3.attention.v_proj.weight', 'wav2vec2.encoder.layers.3.attention.v_proj.bias', 'wav2vec2.encoder.layers.3.attention.q_proj.weight', 'wav2vec2.encoder.layers.3.attention.q_proj.bias', 'wav2vec2.encoder.layers.3.attention.out_proj.weight', 'wav2vec2.encoder.layers.3.attention.out_proj.bias', 'wav2vec2.encoder.layers.3.layer_norm.weight', 'wav2vec2.encoder.layers.3.layer_norm.bias', 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.3.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.3.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.3.final_layer_norm.weight', 'wav2vec2.encoder.layers.3.final_layer_norm.bias', 'wav2vec2.encoder.layers.4.attention.k_proj.weight', 'wav2vec2.encoder.layers.4.attention.k_proj.bias', 'wav2vec2.encoder.layers.4.attention.v_proj.weight', 'wav2vec2.encoder.layers.4.attention.v_proj.bias', 'wav2vec2.encoder.layers.4.attention.q_proj.weight', 'wav2vec2.encoder.layers.4.attention.q_proj.bias', 'wav2vec2.encoder.layers.4.attention.out_proj.weight', 'wav2vec2.encoder.layers.4.attention.out_proj.bias', 'wav2vec2.encoder.layers.4.layer_norm.weight', 'wav2vec2.encoder.layers.4.layer_norm.bias', 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.4.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.4.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.4.final_layer_norm.weight', 'wav2vec2.encoder.layers.4.final_layer_norm.bias', 'wav2vec2.encoder.layers.5.attention.k_proj.weight', 'wav2vec2.encoder.layers.5.attention.k_proj.bias', 'wav2vec2.encoder.layers.5.attention.v_proj.weight', 'wav2vec2.encoder.layers.5.attention.v_proj.bias', 'wav2vec2.encoder.layers.5.attention.q_proj.weight', 'wav2vec2.encoder.layers.5.attention.q_proj.bias', 'wav2vec2.encoder.layers.5.attention.out_proj.weight', 'wav2vec2.encoder.layers.5.attention.out_proj.bias', 'wav2vec2.encoder.layers.5.layer_norm.weight', 'wav2vec2.encoder.layers.5.layer_norm.bias', 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.5.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.5.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.5.final_layer_norm.weight', 'wav2vec2.encoder.layers.5.final_layer_norm.bias', 'wav2vec2.encoder.layers.6.attention.k_proj.weight', 'wav2vec2.encoder.layers.6.attention.k_proj.bias', 'wav2vec2.encoder.layers.6.attention.v_proj.weight', 'wav2vec2.encoder.layers.6.attention.v_proj.bias', 'wav2vec2.encoder.layers.6.attention.q_proj.weight', 'wav2vec2.encoder.layers.6.attention.q_proj.bias', 'wav2vec2.encoder.layers.6.attention.out_proj.weight', 'wav2vec2.encoder.layers.6.attention.out_proj.bias', 'wav2vec2.encoder.layers.6.layer_norm.weight', 'wav2vec2.encoder.layers.6.layer_norm.bias', 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.6.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.6.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.6.final_layer_norm.weight', 'wav2vec2.encoder.layers.6.final_layer_norm.bias', 'wav2vec2.encoder.layers.7.attention.k_proj.weight', 'wav2vec2.encoder.layers.7.attention.k_proj.bias', 'wav2vec2.encoder.layers.7.attention.v_proj.weight', 'wav2vec2.encoder.layers.7.attention.v_proj.bias', 'wav2vec2.encoder.layers.7.attention.q_proj.weight', 'wav2vec2.encoder.layers.7.attention.q_proj.bias', 'wav2vec2.encoder.layers.7.attention.out_proj.weight', 'wav2vec2.encoder.layers.7.attention.out_proj.bias', 'wav2vec2.encoder.layers.7.layer_norm.weight', 'wav2vec2.encoder.layers.7.layer_norm.bias', 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.7.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.7.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.7.final_layer_norm.weight', 'wav2vec2.encoder.layers.7.final_layer_norm.bias', 'wav2vec2.encoder.layers.8.attention.k_proj.weight', 'wav2vec2.encoder.layers.8.attention.k_proj.bias', 'wav2vec2.encoder.layers.8.attention.v_proj.weight', 'wav2vec2.encoder.layers.8.attention.v_proj.bias', 'wav2vec2.encoder.layers.8.attention.q_proj.weight', 'wav2vec2.encoder.layers.8.attention.q_proj.bias', 'wav2vec2.encoder.layers.8.attention.out_proj.weight', 'wav2vec2.encoder.layers.8.attention.out_proj.bias', 'wav2vec2.encoder.layers.8.layer_norm.weight', 'wav2vec2.encoder.layers.8.layer_norm.bias', 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.8.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.8.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.8.final_layer_norm.weight', 'wav2vec2.encoder.layers.8.final_layer_norm.bias', 'wav2vec2.encoder.layers.9.attention.k_proj.weight', 'wav2vec2.encoder.layers.9.attention.k_proj.bias', 'wav2vec2.encoder.layers.9.attention.v_proj.weight', 'wav2vec2.encoder.layers.9.attention.v_proj.bias', 'wav2vec2.encoder.layers.9.attention.q_proj.weight', 'wav2vec2.encoder.layers.9.attention.q_proj.bias', 'wav2vec2.encoder.layers.9.attention.out_proj.weight', 'wav2vec2.encoder.layers.9.attention.out_proj.bias', 'wav2vec2.encoder.layers.9.layer_norm.weight', 'wav2vec2.encoder.layers.9.layer_norm.bias', 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.9.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.9.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.9.final_layer_norm.weight', 'wav2vec2.encoder.layers.9.final_layer_norm.bias', 'wav2vec2.encoder.layers.10.attention.k_proj.weight', 'wav2vec2.encoder.layers.10.attention.k_proj.bias', 'wav2vec2.encoder.layers.10.attention.v_proj.weight', 'wav2vec2.encoder.layers.10.attention.v_proj.bias', 'wav2vec2.encoder.layers.10.attention.q_proj.weight', 'wav2vec2.encoder.layers.10.attention.q_proj.bias', 'wav2vec2.encoder.layers.10.attention.out_proj.weight', 'wav2vec2.encoder.layers.10.attention.out_proj.bias', 'wav2vec2.encoder.layers.10.layer_norm.weight', 'wav2vec2.encoder.layers.10.layer_norm.bias', 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.10.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.10.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.10.final_layer_norm.weight', 'wav2vec2.encoder.layers.10.final_layer_norm.bias', 'wav2vec2.encoder.layers.11.attention.k_proj.weight', 'wav2vec2.encoder.layers.11.attention.k_proj.bias', 'wav2vec2.encoder.layers.11.attention.v_proj.weight', 'wav2vec2.encoder.layers.11.attention.v_proj.bias', 'wav2vec2.encoder.layers.11.attention.q_proj.weight', 'wav2vec2.encoder.layers.11.attention.q_proj.bias', 'wav2vec2.encoder.layers.11.attention.out_proj.weight', 'wav2vec2.encoder.layers.11.attention.out_proj.bias', 'wav2vec2.encoder.layers.11.layer_norm.weight', 'wav2vec2.encoder.layers.11.layer_norm.bias', 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.11.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.11.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.11.final_layer_norm.weight', 'wav2vec2.encoder.layers.11.final_layer_norm.bias', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.weight', 'project_hid.bias', 'project_q.weight', 'project_q.bias']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3qlqJKPWxYd",
        "outputId": "ccd0b1a1-61cd-40af-e5c7-312f3b7a530e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wav2vec2.masked_spec_embed',\n",
              " 'wav2vec2.feature_extractor.conv_layers.0.conv.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.0.layer_norm.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.0.layer_norm.bias',\n",
              " 'wav2vec2.feature_extractor.conv_layers.1.conv.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.2.conv.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.3.conv.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.4.conv.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.5.conv.weight',\n",
              " 'wav2vec2.feature_extractor.conv_layers.6.conv.weight',\n",
              " 'wav2vec2.feature_projection.layer_norm.weight',\n",
              " 'wav2vec2.feature_projection.layer_norm.bias',\n",
              " 'wav2vec2.feature_projection.projection.weight',\n",
              " 'wav2vec2.feature_projection.projection.bias',\n",
              " 'wav2vec2.encoder.pos_conv_embed.conv.bias',\n",
              " 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0',\n",
              " 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1',\n",
              " 'wav2vec2.encoder.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.0.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.0.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.0.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.0.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.0.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.0.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.0.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.0.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.0.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.0.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.0.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.0.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.0.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.0.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.1.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.1.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.1.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.1.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.1.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.1.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.1.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.1.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.1.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.1.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.1.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.1.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.1.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.1.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.2.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.2.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.2.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.2.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.2.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.2.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.2.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.2.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.2.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.2.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.2.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.2.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.2.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.2.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.3.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.3.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.3.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.3.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.3.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.3.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.3.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.3.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.3.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.3.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.3.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.3.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.3.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.3.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.4.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.4.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.4.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.4.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.4.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.4.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.4.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.4.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.4.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.4.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.4.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.4.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.4.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.4.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.5.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.5.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.5.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.5.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.5.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.5.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.5.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.5.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.5.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.5.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.5.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.5.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.5.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.5.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.6.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.6.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.6.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.6.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.6.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.6.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.6.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.6.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.6.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.6.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.6.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.6.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.6.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.6.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.7.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.7.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.7.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.7.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.7.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.7.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.7.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.7.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.7.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.7.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.7.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.7.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.7.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.7.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.8.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.8.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.8.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.8.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.8.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.8.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.8.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.8.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.8.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.8.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.8.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.8.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.8.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.8.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.9.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.9.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.9.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.9.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.9.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.9.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.9.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.9.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.9.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.9.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.9.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.9.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.9.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.9.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.10.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.10.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.10.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.10.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.10.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.10.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.10.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.10.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.10.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.10.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.10.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.10.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.10.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.10.final_layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.11.attention.k_proj.weight',\n",
              " 'wav2vec2.encoder.layers.11.attention.k_proj.bias',\n",
              " 'wav2vec2.encoder.layers.11.attention.v_proj.weight',\n",
              " 'wav2vec2.encoder.layers.11.attention.v_proj.bias',\n",
              " 'wav2vec2.encoder.layers.11.attention.q_proj.weight',\n",
              " 'wav2vec2.encoder.layers.11.attention.q_proj.bias',\n",
              " 'wav2vec2.encoder.layers.11.attention.out_proj.weight',\n",
              " 'wav2vec2.encoder.layers.11.attention.out_proj.bias',\n",
              " 'wav2vec2.encoder.layers.11.layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.11.layer_norm.bias',\n",
              " 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.weight',\n",
              " 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.bias',\n",
              " 'wav2vec2.encoder.layers.11.feed_forward.output_dense.weight',\n",
              " 'wav2vec2.encoder.layers.11.feed_forward.output_dense.bias',\n",
              " 'wav2vec2.encoder.layers.11.final_layer_norm.weight',\n",
              " 'wav2vec2.encoder.layers.11.final_layer_norm.bias',\n",
              " 'quantizer.codevectors',\n",
              " 'quantizer.weight_proj.weight',\n",
              " 'quantizer.weight_proj.bias',\n",
              " 'project_hid.weight',\n",
              " 'project_hid.bias',\n",
              " 'project_q.weight',\n",
              " 'project_q.bias']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954,
          "referenced_widgets": [
            "b927a77bc45b499ba62afa6af0009b19",
            "1a5a5bd491164d3fb199c22381d01edb",
            "297b98ea28f9459cbb9ca0b53e0a1765",
            "d22cbfdc29254cc9b058d4a2d5c2b3b7",
            "aee4602aae564f48b70944b5756041ed",
            "5afb16953a1547baac8454bb2869b18f",
            "5478d74b4f0b45418e563322cd0b007f",
            "3d56c8685abc47eb86f3090caac981c4",
            "9ba381b3269c4e5fb6fd61888765d539",
            "8d7662ec2ae44eac80f31e0e0e05b106",
            "24101c9fb9e54f7f80d26838447a4020"
          ]
        },
        "id": "2vlMVnu1WhOB",
        "outputId": "b006f33f-4695-482d-fee0-ed9bd35ee570"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:***** Runing training *****\n",
            "INFO:__main__: Num examples = 2641\n",
            "INFO:__main__: Num Epochs = 50\n",
            "INFO:__main__:  Instantaneous batch size per device = 16\n",
            "INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "INFO:__main__:  Gradient Accumulation steps = 4\n",
            "INFO:__main__:  Total optimization steps = None\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b927a77bc45b499ba62afa6af0009b19",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| loss: 4.713e+00| contrast_loss: 4.648e+00| div_loss: 6.516e-01| inter_sim_loss: 7.515e-01| %_mask_idx: 3.997e-02| ppl: 8.919e+02| lr: 1.000e-05| temp: 2.000e+00| grad_norm: 6.941e-01\n",
            "| loss: 4.701e+00| contrast_loss: 4.640e+00| div_loss: 6.029e-01| inter_sim_loss: 7.515e-01| %_mask_idx: 3.981e-02| ppl: 1.017e+03| lr: 2.000e-05| temp: 2.000e+00| grad_norm: 4.037e-01\n",
            "| loss: 4.671e+00| contrast_loss: 4.621e+00| div_loss: 5.034e-01| inter_sim_loss: 7.515e-01| %_mask_idx: 5.989e-02| ppl: 1.271e+03| lr: 3.000e-05| temp: 2.000e+00| grad_norm: 2.207e-01\n",
            "| loss: 4.675e+00| contrast_loss: 4.623e+00| div_loss: 5.234e-01| inter_sim_loss: 7.515e-01| %_mask_idx: 4.068e-02| ppl: 1.220e+03| lr: 4.000e-05| temp: 2.000e+00| grad_norm: 2.226e-01\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| val_loss: 4.678e+00| val_contrastive_loss: 4.615e+00| val_diversity_loss: 6.253e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| loss: 4.669e+00| contrast_loss: 4.618e+00| div_loss: 5.122e-01| inter_sim_loss: 7.515e-01| %_mask_idx: 4.015e-02| ppl: 1.249e+03| lr: 5.200e-05| temp: 2.000e+00| grad_norm: 2.056e-01\n",
            "| loss: 4.664e+00| contrast_loss: 4.622e+00| div_loss: 4.206e-01| inter_sim_loss: 7.515e-01| %_mask_idx: 5.987e-02| ppl: 1.483e+03| lr: 6.200e-05| temp: 2.000e+00| grad_norm: 1.570e-01\n",
            "| loss: 4.666e+00| contrast_loss: 4.617e+00| div_loss: 4.855e-01| inter_sim_loss: 7.515e-01| %_mask_idx: 3.970e-02| ppl: 1.317e+03| lr: 7.200e-05| temp: 2.000e+00| grad_norm: 1.950e-01\n",
            "| loss: 4.662e+00| contrast_loss: 4.617e+00| div_loss: 4.472e-01| inter_sim_loss: 7.515e-01| %_mask_idx: 3.885e-02| ppl: 1.415e+03| lr: 8.200e-05| temp: 2.000e+00| grad_norm: 1.804e-01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| val_loss: 4.669e+00| val_contrastive_loss: 4.615e+00| val_diversity_loss: 5.423e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| loss: 4.667e+00| contrast_loss: 4.619e+00| div_loss: 4.760e-01| inter_sim_loss: 7.515e-01| %_mask_idx: 3.842e-02| ppl: 1.341e+03| lr: 9.400e-05| temp: 2.000e+00| grad_norm: 1.861e-01\n",
            "| loss: 4.656e+00| contrast_loss: 4.621e+00| div_loss: 3.509e-01| inter_sim_loss: 7.515e-01| %_mask_idx: 5.763e-02| ppl: 1.662e+03| lr: 9.980e-05| temp: 2.000e+00| grad_norm: 1.441e-01\n",
            "| loss: 4.653e+00| contrast_loss: 4.619e+00| div_loss: 3.374e-01| inter_sim_loss: 7.515e-01| %_mask_idx: 5.938e-02| ppl: 1.696e+03| lr: 9.930e-05| temp: 2.000e+00| grad_norm: 1.307e-01\n",
            "| loss: 4.661e+00| contrast_loss: 4.618e+00| div_loss: 4.287e-01| inter_sim_loss: 7.515e-01| %_mask_idx: 4.032e-02| ppl: 1.463e+03| lr: 9.880e-05| temp: 2.000e+00| grad_norm: 1.617e-01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| val_loss: 4.665e+00| val_contrastive_loss: 4.614e+00| val_diversity_loss: 5.144e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| loss: 4.659e+00| contrast_loss: 4.617e+00| div_loss: 4.145e-01| inter_sim_loss: 7.515e-01| %_mask_idx: 3.979e-02| ppl: 1.499e+03| lr: 9.820e-05| temp: 2.000e+00| grad_norm: 1.506e-01\n",
            "| loss: 4.660e+00| contrast_loss: 4.617e+00| div_loss: 4.275e-01| inter_sim_loss: 7.515e-01| %_mask_idx: 3.933e-02| ppl: 1.466e+03| lr: 9.770e-05| temp: 2.000e+00| grad_norm: 1.565e-01\n",
            "| loss: 4.662e+00| contrast_loss: 4.619e+00| div_loss: 4.355e-01| inter_sim_loss: 7.515e-01| %_mask_idx: 4.052e-02| ppl: 1.445e+03| lr: 9.720e-05| temp: 2.000e+00| grad_norm: 1.534e-01\n",
            "| loss: 4.647e+00| contrast_loss: 4.616e+00| div_loss: 3.100e-01| inter_sim_loss: 7.516e-01| %_mask_idx: 5.854e-02| ppl: 1.767e+03| lr: 9.670e-05| temp: 2.000e+00| grad_norm: 1.227e-01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| val_loss: 4.665e+00| val_contrastive_loss: 4.615e+00| val_diversity_loss: 5.014e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| loss: 4.650e+00| contrast_loss: 4.617e+00| div_loss: 3.313e-01| inter_sim_loss: 7.516e-01| %_mask_idx: 5.937e-02| ppl: 1.712e+03| lr: 9.610e-05| temp: 2.000e+00| grad_norm: 1.233e-01\n",
            "| loss: 4.658e+00| contrast_loss: 4.617e+00| div_loss: 3.996e-01| inter_sim_loss: 7.516e-01| %_mask_idx: 4.009e-02| ppl: 1.537e+03| lr: 9.560e-05| temp: 2.000e+00| grad_norm: 1.464e-01\n",
            "| loss: 4.653e+00| contrast_loss: 4.619e+00| div_loss: 3.398e-01| inter_sim_loss: 7.516e-01| %_mask_idx: 5.909e-02| ppl: 1.690e+03| lr: 9.510e-05| temp: 2.000e+00| grad_norm: 1.247e-01\n",
            "| loss: 4.658e+00| contrast_loss: 4.617e+00| div_loss: 4.097e-01| inter_sim_loss: 7.516e-01| %_mask_idx: 4.169e-02| ppl: 1.511e+03| lr: 9.460e-05| temp: 2.000e+00| grad_norm: 1.447e-01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| val_loss: 4.661e+00| val_contrastive_loss: 4.611e+00| val_diversity_loss: 5.025e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| loss: 4.650e+00| contrast_loss: 4.616e+00| div_loss: 3.452e-01| inter_sim_loss: 7.516e-01| %_mask_idx: 5.941e-02| ppl: 1.676e+03| lr: 9.400e-05| temp: 2.000e+00| grad_norm: 1.240e-01\n",
            "| loss: 4.658e+00| contrast_loss: 4.616e+00| div_loss: 4.210e-01| inter_sim_loss: 7.516e-01| %_mask_idx: 3.915e-02| ppl: 1.482e+03| lr: 9.350e-05| temp: 2.000e+00| grad_norm: 1.329e-01\n",
            "| loss: 4.653e+00| contrast_loss: 4.615e+00| div_loss: 3.817e-01| inter_sim_loss: 7.516e-01| %_mask_idx: 3.877e-02| ppl: 1.583e+03| lr: 9.300e-05| temp: 2.000e+00| grad_norm: 1.207e-01\n",
            "| loss: 4.654e+00| contrast_loss: 4.615e+00| div_loss: 3.901e-01| inter_sim_loss: 7.516e-01| %_mask_idx: 4.164e-02| ppl: 1.561e+03| lr: 9.250e-05| temp: 2.000e+00| grad_norm: 1.274e-01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in /outputs/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| val_loss: 4.663e+00| val_contrastive_loss: 4.613e+00| val_diversity_loss: 4.982e-01| val_num_losses: 1.000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in /outputs/model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| loss: 4.655e+00| contrast_loss: 4.613e+00| div_loss: 4.176e-01| inter_sim_loss: 7.516e-01| %_mask_idx: 3.875e-02| ppl: 1.491e+03| lr: 9.190e-05| temp: 1.999e+00| grad_norm: 1.328e-01\n",
            "| loss: 4.647e+00| contrast_loss: 4.614e+00| div_loss: 3.300e-01| inter_sim_loss: 7.516e-01| %_mask_idx: 5.902e-02| ppl: 1.715e+03| lr: 9.140e-05| temp: 1.999e+00| grad_norm: 1.104e-01\n"
          ]
        }
      ],
      "source": [
        "total_batch_size = TRAIN_BATCH_SIZE * accelerator.num_processes * GRADIENT_ACCUMULATION_STEPS\n",
        "logger.info(\"***** Runing training *****\")\n",
        "logger.info(f\" Num examples = {len(vectorized_datasets['train'])}\")\n",
        "logger.info(f\" Num Epochs = {NUM_TRAIN_EPOCHS}\")\n",
        "logger.info(f\"  Instantaneous batch size per device = {TRAIN_BATCH_SIZE}\")\n",
        "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
        "logger.info(f\"  Gradient Accumulation steps = {GRADIENT_ACCUMULATION_STEPS}\")\n",
        "logger.info(f\"  Total optimization steps = {MAX_TRAINING_STEPS}\")\n",
        "\n",
        "completed_steps = 0\n",
        "starting_epoch = 0\n",
        "\n",
        "progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "completed_steps = 0\n",
        "starting_epoch = 0\n",
        "\n",
        "for epoch in range(starting_epoch, NUM_TRAIN_EPOCHS):\n",
        "  model.train()\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    num_losses = batch[\"mask_time_indices\"].sum()\n",
        "    sub_attention_mask = batch.pop(\"sub_attention_mask\", None)\n",
        "    sub_attention_mask = (\n",
        "        sub_attention_mask if sub_attention_mask is not None else torch.ones_like(batch[\"mask_time_indices\"])\n",
        "    )\n",
        "    precent_masked = num_losses / sub_attention_mask.sum()\n",
        "\n",
        "    outputs = model(**batch)\n",
        "\n",
        "    inter_sim_loss = inter_codebook_similarity_loss(model.quantizer.codevectors)\n",
        "    total_loss = outputs.loss + INTER_CB_SIMILARITY_WEIGHT * inter_sim_loss\n",
        "\n",
        "    loss = total_loss / GRADIENT_ACCUMULATION_STEPS\n",
        "    accelerator.backward(loss)\n",
        "\n",
        "    if accelerator.state.num_processes > 1:\n",
        "      num_losses = accelerator.gather_for_metrics(num_losses).sum()\n",
        "      gradient_multiplier = accelerator.state.num_processes / num_losses\n",
        "      multiply_grads(model.parameters(), gradient_multiplier)\n",
        "    else:\n",
        "      multiply_grads(model.parameters(), 1 / num_losses)\n",
        "\n",
        "    # Update step\n",
        "    if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0 or step == len(train_dataloader) - 1:\n",
        "      scale = (\n",
        "          accelerator.scaler._scale.item()\n",
        "          if hasattr(accelerator, \"scaler\") and accelerator.scaler is not None\n",
        "          else 1.0\n",
        "      )\n",
        "      if accelerator.state.num_processes > 1:\n",
        "        grad_norm = get_grad_norm(model.module.parameters(), scale)\n",
        "      else:\n",
        "        grad_norm = get_grad_norm(model.parameters(), scale)\n",
        "\n",
        "      accelerator.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      if not accelerator.optimizer_step_was_skipped:\n",
        "        lr_scheduler.step()\n",
        "      elif accelerator.is_local_main_process:\n",
        "        progress_bar.write(\n",
        "          f\"Gradients have overflown - skipping update step... Updating gradient scale to {scale}...\"\n",
        "        )\n",
        "\n",
        "      # update gumbel temperature\n",
        "      gumble_temperature = max(\n",
        "          MAX_GUMBEL_TEMPERATURE * GUMBEL_TEMPERATURE**completed_steps,\n",
        "          MIN_GUMBEL_TEMPERATURE,\n",
        "      )\n",
        "\n",
        "      if hasattr(model, \"module\"):\n",
        "        model.module.set_gumbel_temperature(gumble_temperature)\n",
        "      else:\n",
        "        model.set_gumbel_temperature(gumble_temperature)\n",
        "\n",
        "      progress_bar.update(1)\n",
        "      completed_steps += 1\n",
        "\n",
        "      # Log all results\n",
        "      if (step + 1) % (GRADIENT_ACCUMULATION_STEPS * LOGGING_STEPS) == 0:\n",
        "        loss.detach()\n",
        "        outputs.contrastive_loss.detach()\n",
        "        outputs.diversity_loss.detach()\n",
        "\n",
        "        if accelerator.state.num_processes > 1:\n",
        "          loss = accelerator.gather_for_metrics(loss).sum()\n",
        "          outputs.contrastive_loss = accelerator.gather_for_metrics(outputs.contrastive_loss).sum()\n",
        "          outputs.diversity_loss = accelerator.gather_for_metrics(outputs.diversity_loss).sum()\n",
        "          percent_masked = accelerator.gather_for_metrics(precent_masked).sum()\n",
        "\n",
        "        train_logs = {\n",
        "            \"loss\": (loss * GRADIENT_ACCUMULATION_STEPS) / num_losses,\n",
        "            \"contrast_loss\": outputs.contrastive_loss / num_losses,\n",
        "            \"div_loss\": outputs.diversity_loss / num_losses,\n",
        "            \"inter_sim_loss\": inter_sim_loss,\n",
        "            \"%_mask_idx\": precent_masked / accelerator.num_processes,\n",
        "            \"ppl\": outputs.codevector_perplexity,\n",
        "            \"lr\": torch.tensor(optimizer.param_groups[0][\"lr\"]),\n",
        "            \"temp\": torch.tensor(gumble_temperature),\n",
        "            \"grad_norm\": torch.tensor(grad_norm),\n",
        "        }\n",
        "\n",
        "\n",
        "        log_str = \"\"\n",
        "        for k, v in train_logs.items():\n",
        "          log_str += f\"| {k}: {v.item():.3e}\"\n",
        "\n",
        "        if accelerator.is_local_main_process:\n",
        "          progress_bar.write(log_str)\n",
        "          if is_wandb_available():\n",
        "            wandb.log(train_logs)\n",
        "\n",
        "      # save model\n",
        "      if (step + 1) % (GRADIENT_ACCUMULATION_STEPS * SAVING_STEPS) == 0:\n",
        "        if OUTPUT_DIR is not None:\n",
        "            accelerator.wait_for_everyone()\n",
        "            unwrapped_model = accelerator.unwrap_model(model)\n",
        "            unwrapped_model.save_pretrained(\n",
        "                OUTPUT_DIR,\n",
        "                is_main_process=accelerator.is_main_process,\n",
        "                save_function=accelerator.save\n",
        "            )\n",
        "\n",
        "      if completed_steps >= max_train_steps:\n",
        "        break\n",
        "  model.eval()\n",
        "\n",
        "  val_logs = {\n",
        "      \"val_loss\": 0,\n",
        "      \"val_contrastive_loss\": 0,\n",
        "      \"val_diversity_loss\": 0,\n",
        "      \"val_num_losses\": 0,\n",
        "  }\n",
        "\n",
        "  for step, batch in enumerate(val_dataloader):\n",
        "    with torch.no_grad():\n",
        "        batch.pop(\"sub_attention_mask\", None)\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    val_logs[\"val_loss\"] += outputs.loss\n",
        "    val_logs[\"val_contrastive_loss\"] += outputs.contrastive_loss\n",
        "    val_logs[\"val_diversity_loss\"] += outputs.diversity_loss\n",
        "    val_logs[\"val_num_losses\"] += batch[\"mask_time_indices\"].sum()\n",
        "\n",
        "  if accelerator.num_processes > 1:\n",
        "      val_logs = {k: accelerator.gather_for_metrics(v).sum() for k, v in val_logs.items()}\n",
        "\n",
        "  val_logs = {k: v / val_logs[\"val_num_losses\"] for k, v in val_logs.items()}\n",
        "\n",
        "  log_str = \"\"\n",
        "  for k, v in val_logs.items():\n",
        "      log_str += f\"| {k}: {v.item():.3e}\"\n",
        "\n",
        "  if accelerator.is_local_main_process:\n",
        "      progress_bar.write(log_str)\n",
        "      if is_wandb_available():\n",
        "          wandb.log(val_logs)\n",
        "\n",
        "  if OUTPUT_DIR is not None:\n",
        "      accelerator.wait_for_everyone()\n",
        "      unwrapped_model = accelerator.unwrap_model(model)\n",
        "      unwrapped_model.save_pretrained(\n",
        "          OUTPUT_DIR, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G17806dWGy8W"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a30d6d963d2c4b4ab3b3b52527c61e24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_affede35b3d44edc82c5ebd2a8417c68",
              "IPY_MODEL_34bdd10c1e3b4b99a30fd56f197696dd",
              "IPY_MODEL_52c8539f82704408850c44a72bd1522d"
            ],
            "layout": "IPY_MODEL_6cf3cf8c0ff14d5998e2bace0f119ed9"
          }
        },
        "affede35b3d44edc82c5ebd2a8417c68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e20224a92eaa44c3a089f6f0d9ea8804",
            "placeholder": "​",
            "style": "IPY_MODEL_e2f5a0b2abcc45938b8dfe1b87f798a6",
            "value": "Fetching 1 files: 100%"
          }
        },
        "34bdd10c1e3b4b99a30fd56f197696dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38b15b98547843958626ce0c91df374e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dcd076acc889455ea927873749106acf",
            "value": 1
          }
        },
        "52c8539f82704408850c44a72bd1522d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_481fd8c9baae4a24814c68fb7fc7cac4",
            "placeholder": "​",
            "style": "IPY_MODEL_4578845ad26847a48703ca567fc433fb",
            "value": " 1/1 [00:00&lt;00:00,  7.00it/s]"
          }
        },
        "6cf3cf8c0ff14d5998e2bace0f119ed9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e20224a92eaa44c3a089f6f0d9ea8804": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2f5a0b2abcc45938b8dfe1b87f798a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38b15b98547843958626ce0c91df374e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcd076acc889455ea927873749106acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "481fd8c9baae4a24814c68fb7fc7cac4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4578845ad26847a48703ca567fc433fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5aaef1e5c67c4beaa61efd41113dc2cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0996d860aa648f8af39abe4ecebb44a",
              "IPY_MODEL_ed9464b794ef4d1c84ef3f4b84203861",
              "IPY_MODEL_96a1bc3a09304fc6af998f5cd2129f39"
            ],
            "layout": "IPY_MODEL_68421fb636fa44b5aade237be1ae4203"
          }
        },
        "e0996d860aa648f8af39abe4ecebb44a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb81b4f8672d4fbc97b43539200b5405",
            "placeholder": "​",
            "style": "IPY_MODEL_24b544a5922e4847978367ebf52ce291",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "ed9464b794ef4d1c84ef3f4b84203861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ff7ebe28c28453f8a3e9226eb4f65ed",
            "max": 159,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_901a2c73d4c7439fa1fbd86e2cf7cd6e",
            "value": 159
          }
        },
        "96a1bc3a09304fc6af998f5cd2129f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99529c0333824dbd99365b4539bf28e3",
            "placeholder": "​",
            "style": "IPY_MODEL_0bd4782e2e584420897ec63774031620",
            "value": " 159/159 [00:00&lt;00:00, 16.8kB/s]"
          }
        },
        "68421fb636fa44b5aade237be1ae4203": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb81b4f8672d4fbc97b43539200b5405": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24b544a5922e4847978367ebf52ce291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ff7ebe28c28453f8a3e9226eb4f65ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "901a2c73d4c7439fa1fbd86e2cf7cd6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99529c0333824dbd99365b4539bf28e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bd4782e2e584420897ec63774031620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b508c5753e40476085dc75d5839b7618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ce47bba283a4c7eac50285779158b93",
              "IPY_MODEL_ca27e8a36abe4903962a688859caacb9",
              "IPY_MODEL_f5b8fdfe2f694aa8952339c823cc2a76"
            ],
            "layout": "IPY_MODEL_c17f7ebd3d354d8e9fd66fefbc4237a3"
          }
        },
        "0ce47bba283a4c7eac50285779158b93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fdf61ed48c74443a560239560c9279f",
            "placeholder": "​",
            "style": "IPY_MODEL_a49e7a668b0b4be2b93cb27735c19e60",
            "value": "config.json: "
          }
        },
        "ca27e8a36abe4903962a688859caacb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93b8f93a91c74669bd37a08281489225",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe887ed8cdd74f8dba3b90b000e11b5f",
            "value": 1
          }
        },
        "f5b8fdfe2f694aa8952339c823cc2a76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_935e4c44dea143018ef5e75b087a46fd",
            "placeholder": "​",
            "style": "IPY_MODEL_bba10094b3ea4c86b02fcce4dc5a7572",
            "value": " 1.84k/? [00:00&lt;00:00, 179kB/s]"
          }
        },
        "c17f7ebd3d354d8e9fd66fefbc4237a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fdf61ed48c74443a560239560c9279f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a49e7a668b0b4be2b93cb27735c19e60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93b8f93a91c74669bd37a08281489225": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "fe887ed8cdd74f8dba3b90b000e11b5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "935e4c44dea143018ef5e75b087a46fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bba10094b3ea4c86b02fcce4dc5a7572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b927a77bc45b499ba62afa6af0009b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a5a5bd491164d3fb199c22381d01edb",
              "IPY_MODEL_297b98ea28f9459cbb9ca0b53e0a1765",
              "IPY_MODEL_d22cbfdc29254cc9b058d4a2d5c2b3b7"
            ],
            "layout": "IPY_MODEL_aee4602aae564f48b70944b5756041ed"
          }
        },
        "1a5a5bd491164d3fb199c22381d01edb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5afb16953a1547baac8454bb2869b18f",
            "placeholder": "​",
            "style": "IPY_MODEL_5478d74b4f0b45418e563322cd0b007f",
            "value": " 13%"
          }
        },
        "297b98ea28f9459cbb9ca0b53e0a1765": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d56c8685abc47eb86f3090caac981c4",
            "max": 2100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ba381b3269c4e5fb6fd61888765d539",
            "value": 274
          }
        },
        "d22cbfdc29254cc9b058d4a2d5c2b3b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d7662ec2ae44eac80f31e0e0e05b106",
            "placeholder": "​",
            "style": "IPY_MODEL_24101c9fb9e54f7f80d26838447a4020",
            "value": " 274/2100 [1:52:57&lt;10:26:36, 20.59s/it]"
          }
        },
        "aee4602aae564f48b70944b5756041ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5afb16953a1547baac8454bb2869b18f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5478d74b4f0b45418e563322cd0b007f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d56c8685abc47eb86f3090caac981c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ba381b3269c4e5fb6fd61888765d539": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d7662ec2ae44eac80f31e0e0e05b106": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24101c9fb9e54f7f80d26838447a4020": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}