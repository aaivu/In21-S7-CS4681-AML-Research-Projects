"""
Test script for TemporalPairBatchSampler validation.

This script tests the TemporalPairBatchSampler implementation to ensure
it correctly inherits from Sampler and handles EgoClip metadata structure.
"""

import torch
import pandas as pd
import numpy as np
from torch.utils.data import Sampler

# Test the import
try:
    from model.temporal_loss import TemporalPairBatchSampler
    print("‚úì TemporalPairBatchSampler import successful")
except ImportError as e:
    print(f"‚úó Import failed: {e}")
    exit(1)


class TestEgoClipDataset:
    """Test dataset that mimics EgoClip metadata structure."""
    
    def __init__(self):
        # Create test metadata with known temporal relationships
        self.metadata = pd.DataFrame([
            # Video 1: Sequential clips with small gaps
            {'video_uid': 'video_001', 'narration_time': 10.0, 'clip_start': 9.5, 'clip_end': 12.0, 'clip_text': 'action 1'},
            {'video_uid': 'video_001', 'narration_time': 12.5, 'clip_start': 12.0, 'clip_end': 14.5, 'clip_text': 'action 2'},
            {'video_uid': 'video_001', 'narration_time': 15.0, 'clip_start': 14.5, 'clip_end': 17.0, 'clip_text': 'action 3'},
            
            # Video 2: Clips with larger gaps
            {'video_uid': 'video_002', 'narration_time': 20.0, 'clip_start': 19.0, 'clip_end': 22.0, 'clip_text': 'action 4'},
            {'video_uid': 'video_002', 'narration_time': 25.0, 'clip_start': 24.0, 'clip_end': 27.0, 'clip_text': 'action 5'},  # 3s gap - too far
            
            # Video 3: Overlapping clips
            {'video_uid': 'video_003', 'narration_time': 30.0, 'clip_start': 29.0, 'clip_end': 32.0, 'clip_text': 'action 6'},
            {'video_uid': 'video_003', 'narration_time': 31.0, 'clip_start': 30.5, 'clip_end': 33.5, 'clip_text': 'action 7'},
            
            # Single clip video
            {'video_uid': 'video_004', 'narration_time': 40.0, 'clip_start': 39.0, 'clip_end': 42.0, 'clip_text': 'action 8'},
        ])
    
    def __len__(self):
        return len(self.metadata)
    
    def __getitem__(self, idx):
        sample = self.metadata.iloc[idx]
        return {
            'video': torch.randn(4, 3, 224, 224),  # Mock video tensor
            'text': sample['clip_text'],
            'meta': sample.to_dict()
        }


def test_sampler_inheritance():
    """Test that TemporalPairBatchSampler correctly inherits from Sampler."""
    print("\nTest 1: Sampler Inheritance")
    print("-" * 30)
    
    dataset = TestEgoClipDataset()
    sampler = TemporalPairBatchSampler(dataset, batch_size=4)
    
    # Check inheritance
    assert isinstance(sampler, Sampler), "Should inherit from torch.utils.data.Sampler"
    print("‚úì Correctly inherits from torch.utils.data.Sampler")
    
    # Check required methods
    assert hasattr(sampler, '__iter__'), "Should have __iter__ method"
    assert hasattr(sampler, '__len__'), "Should have __len__ method"
    print("‚úì Has required __iter__ and __len__ methods")


def test_temporal_pair_detection():
    """Test that temporal pairs are correctly identified."""
    print("\nTest 2: Temporal Pair Detection")
    print("-" * 30)
    
    dataset = TestEgoClipDataset()
    sampler = TemporalPairBatchSampler(dataset, batch_size=4, max_temporal_gap=2.0)
    
    print(f"Dataset size: {len(dataset)}")
    print(f"Videos found: {len(sampler.video_clips)}")
    print(f"Temporal pairs found: {len(sampler.temporal_pairs)}")
    
    # Verify expected temporal pairs
    expected_pairs = []
    
    # Video 1: clips 0-1 and 1-2 should be pairs (gaps ‚â§ 2s)
    expected_pairs.extend([(0, 1), (1, 2)])
    
    # Video 2: clips 3-4 should NOT be a pair (gap = 3s > 2s)
    # (No pairs expected from video 2)
    
    # Video 3: clips 5-6 should be a pair (overlapping)
    expected_pairs.append((5, 6))
    
    # Video 4: only one clip, no pairs
    
    print(f"Expected pairs: {len(expected_pairs)}")
    
    # Check that we found reasonable number of pairs
    assert len(sampler.temporal_pairs) >= len(expected_pairs), f"Should find at least {len(expected_pairs)} pairs"
    
    # Check that pairs are from same video
    for idx1, idx2, distance in sampler.temporal_pairs:
        video1 = dataset.metadata.iloc[idx1]['video_uid']
        video2 = dataset.metadata.iloc[idx2]['video_uid']
        assert video1 == video2, f"Temporal pairs should be from same video: {video1} vs {video2}"
    
    print("‚úì Temporal pairs correctly identified")
    print("‚úì All pairs are from same video")
    
    # Print found pairs for verification
    for i, (idx1, idx2, distance) in enumerate(sampler.temporal_pairs):
        sample1 = dataset.metadata.iloc[idx1]
        sample2 = dataset.metadata.iloc[idx2]
        print(f"  Pair {i+1}: {sample1['video_uid']} - "
              f"times {sample1['narration_time']:.1f}s, {sample2['narration_time']:.1f}s - "
              f"distance {distance:.2f}s")


def test_batch_generation():
    """Test batch generation with temporal pairs."""
    print("\nTest 3: Batch Generation")
    print("-" * 30)
    
    dataset = TestEgoClipDataset()
    batch_size = 4
    temporal_pair_ratio = 0.5
    sampler = TemporalPairBatchSampler(dataset, batch_size=batch_size, temporal_pair_ratio=temporal_pair_ratio)
    
    batches_generated = 0
    total_temporal_pairs = 0
    
    for batch_indices, temporal_pairs_metadata in sampler:
        batches_generated += 1
        
        # Check batch size
        assert len(batch_indices) == batch_size, f"Batch should have {batch_size} samples"
        
        # Check indices are valid
        for idx in batch_indices:
            assert 0 <= idx < len(dataset), f"Invalid index: {idx}"
        
        # Check temporal pairs metadata
        num_pairs = len(temporal_pairs_metadata)
        total_temporal_pairs += num_pairs
        
        print(f"  Batch {batches_generated}: {len(batch_indices)} samples, {num_pairs} temporal pairs")
        
        # Verify temporal pairs metadata format
        for batch_pos1, batch_pos2, distance in temporal_pairs_metadata:
            assert 0 <= batch_pos1 < batch_size, f"Invalid batch position: {batch_pos1}"
            assert 0 <= batch_pos2 < batch_size, f"Invalid batch position: {batch_pos2}"
            assert distance >= 0, f"Distance should be non-negative: {distance}"
            
            # Verify the indices refer to samples from same video
            dataset_idx1 = batch_indices[batch_pos1]
            dataset_idx2 = batch_indices[batch_pos2]
            video1 = dataset.metadata.iloc[dataset_idx1]['video_uid']
            video2 = dataset.metadata.iloc[dataset_idx2]['video_uid']
            assert video1 == video2, f"Temporal pair should be from same video"
        
        if batches_generated >= 3:  # Limit for test
            break
    
    print(f"‚úì Generated {batches_generated} valid batches")
    print(f"‚úì Average temporal pairs per batch: {total_temporal_pairs / batches_generated:.1f}")
    
    # Check that we're getting reasonable number of temporal pairs
    expected_pairs_per_batch = (batch_size * temporal_pair_ratio) // 2
    avg_pairs = total_temporal_pairs / batches_generated
    assert avg_pairs >= 0, "Should have non-negative temporal pairs"
    
    print(f"‚úì Temporal pair ratio working (expected ~{expected_pairs_per_batch}, got {avg_pairs:.1f})")


def test_metadata_format():
    """Test EgoClip metadata format compatibility."""
    print("\nTest 4: EgoClip Metadata Compatibility") 
    print("-" * 30)
    
    dataset = TestEgoClipDataset()
    
    # Verify metadata has expected columns
    required_columns = ['video_uid', 'narration_time', 'clip_start', 'clip_end']
    for col in required_columns:
        assert col in dataset.metadata.columns, f"Missing required column: {col}"
    
    print("‚úì Has required metadata columns: video_uid, narration_time, clip_start, clip_end")
    
    # Test with missing narration_time (fallback scenario)
    test_metadata = dataset.metadata.copy()
    test_metadata = test_metadata.drop('narration_time', axis=1)
    
    class TestDatasetNoNarration:
        def __init__(self):
            self.metadata = test_metadata
        def __len__(self):
            return len(self.metadata)
    
    dataset_no_narr = TestDatasetNoNarration()
    
    try:
        sampler = TemporalPairBatchSampler(dataset_no_narr, batch_size=4)
        print("‚úì Handles missing narration_time gracefully")
    except Exception as e:
        print(f"‚úó Failed to handle missing narration_time: {e}")


def run_all_tests():
    """Run all test functions."""
    print("TemporalPairBatchSampler Test Suite")
    print("=" * 50)
    
    try:
        test_sampler_inheritance()
        test_temporal_pair_detection()
        test_batch_generation()
        test_metadata_format()
        
        print("\n" + "=" * 50)
        print("‚úÖ ALL TESTS PASSED!")
        print("=" * 50)
        
        print("\nTemporalPairBatchSampler Summary:")
        print("‚úì Inherits from torch.utils.data.Sampler")
        print("‚úì Groups clips by video_uid from EgoClip metadata")
        print("‚úì Identifies adjacent clips using narration timestamps")
        print("‚úì Ensures temporal pair ratio in batches")
        print("‚úì Returns indices and metadata for temporal pairs")
        print("‚úì Handles edge cases (missing data, single clips)")
        
    except AssertionError as e:
        print(f"\n‚ùå TEST FAILED: {e}")
    except Exception as e:
        print(f"\nüí• UNEXPECTED ERROR: {e}")


if __name__ == '__main__':
    run_all_tests()