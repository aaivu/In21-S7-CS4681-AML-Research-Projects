# Methodology: CV:Super Resolution

Student: 210572P

Research Area: CV:Super Resolution

Date: 2025-09-01

## 1\. Overview

The methodology introduces ID-CodeFormer, a novel enhancement to the CodeFormer model for Blind Face Restoration (BFR). The core problem being addressed is "identity drift," where the baseline CodeFormer, while robust, fails to preserve unique subject characteristics.

The proposed solution is to integrate an explicit identity-preserving loss function (\$L_{ids}\$) into the CodeFormer training pipeline. This loss is supervised by a pre-trained, frozen ArcFace network, which acts as an expert judge of identity. This new loss term compels the model to generate facial restorations that are not only perceptually realistic but also discriminative of the subject's unique identity, directly minimizing the angular distance between the restored and ground-truth identity embeddings in a high-dimensional feature space.

## 2\. Research Design

The research approach is a comparative experimental design. The methodology involves modifying the training pipeline of the existing state-of-the-art CodeFormer model by introducing the \$L_{ids}\$ loss term.

To validate this approach, the baseline CodeFormer model is re-trained under the exact same conditions (datasets, hyperparameters) as the proposed ID-CodeFormer. The performance of both models is then rigorously evaluated using a suite of quantitative metrics and qualitative visual comparisons. The primary hypothesis is that ID-CodeFormer will demonstrate a statistically significant improvement in identity preservation (measured by IDS) while incurring only a minimal and acceptable trade-off in pixel-level reconstruction quality (measured by PSNR, SSIM).

## 3\. Data Collection

### 3.1 Data Sources

- **Training:** FFHQ dataset \[6\]
- **Evaluation (Synthetic Degradation):** CelebA-Test set (derived from CelebA-HQ)
- **Evaluation (Real-World Degradation):** LFW-Test set

### 3.2 Data Description

- **FFHQ:** A high-quality dataset containing 70,000 face images. Images are resized to 512x512 pixels for training.
- **CelebA-Test:** A benchmark evaluation set containing 3,000 images from CelebA-HQ, which are synthetically degraded to test model performance.
- **LFW-Test:** A standard real-world benchmark for unconstrained face restoration.

### 3.3 Data Preprocessing

- **HQ Images (FFHQ):** Resized to 512x512 pixels for Stage I (VQ-VAE) training.
- **LQ-HQ Pairs:** For Stage II and III training, low-quality (LQ) inputs are generated by applying synthetic degradations (e.g., blur, noise, compression) to the HQ images, following the original CodeFormer protocol.
- **Identity Embeddings:** During training, both the restored output (\$I_{res}\$) and ground-truth (\$I_{gt}\$) images are fed to the pre-trained ArcFace network to extract 512-dimensional feature vectors used for calculating \$L_{ids}\$.

## 4\. Model Architecture

The proposed model, **ID-CodeFormer**, is an enhancement of the baseline CodeFormer architecture. It retains the two primary components:

- **VQ-VAE Backend:** A pre-trained and frozen module (HQ Encoder \$E_H\$, discrete Codebook \$C\$, HQ Decoder \$D_H\$) that stores a "vocabulary" of high-quality facial "visual atoms."
- **Transformer-based Prediction Network:** A trainable module (LQ Encoder \$E_L\$, Transformer \$T\$) that learns to predict the correct sequence of codebook indices from a degraded LQ input.

Novel Contribution (Identity Feedback Loop):

The methodology integrates a new supervisory signal during training.

- A **frozen, pre-trained ArcFace network** is used as a feature extractor.
- During training (Stages II and III), both the restored image (\$I_{res}\$) and the ground-truth image (\$I_{gt}\$) are passed through the ArcFace network to obtain their respective identity embeddings.
- The Identity Loss (\$L_{ids}\$) is calculated as one minus the cosine similarity between these two embeddings:  
    \$L_{ids} = 1 - cos(ArcFace(I_{res}), ArcFace(I_{gt}))\$
- This loss is added to the original CodeFormer loss function (with a weighting hyperparameter \$\\lambda_{ids}\$) and backpropagated to update the weights of the LQ Encoder (\$E_L\$) and the Transformer (\$T\$).

## 5\. Experimental Setup

### 5.1 Evaluation Metrics

A comprehensive suite of metrics is used to evaluate performance:

- **Identity Preservation:**
  - **IDS (Identity Similarity):** The primary metric, calculated as the cosine similarity between the ArcFace embeddings of the restored and ground-truth images. (Higher is better)
- **Reconstruction & Quality:**
  - **PSNR (Peak Signal-to-Noise Ratio):** Measures pixel-wise accuracy. (Higher is better)
  - **SSIM (Structural Similarity Index):** Measures structural similarity. (Higher is better)
- **Perceptual & Realism:**
  - **LPIPS (Learned Perceptual Image Patch Similarity):** Measures perceptual similarity using deep features. (Lower is better)
  - **FID (Fr√©chet Inception Distance):** Measures the realism and diversity of the generated image distribution. (Lower is better)

### 5.2 Baseline Models

The primary baseline for comparison is the **original CodeFormer model** \[1\]. To ensure a fair and direct comparison, the baseline model is re-trained from the official public codebase under the exact same experimental conditions, datasets, and training pipeline as the proposed ID-CodeFormer. The only difference is the exclusion of the \$L_{ids}\$ term.

### 5.3 Hardware/Software Requirements

- **Software:** Python, PyTorch (inferred).
- **Codebase:** The implementation is built upon the official public codebase for CodeFormer \[1\].
- **External Models:** Requires a publicly available, pre-trained ArcFace network \[2\] to serve as the identity feature extractor.
- **Hardware:** (Not specified in the paper, but high-end GPUs are implied for training).

## 6\. Implementation Plan

| **Phase** | **Tasks** | **Duration** | **Deliverables** |
| --- | --- | --- | --- |
| Phase 1 | Data preprocessing & Stage I (Codebook Learning) | 2 weeks | Clean dataset & Frozen VQ-VAE backend |
| --- | --- | --- | --- |
| Phase 2 | Stage II & III (Model implementation & tuning with \$L_{ids}\$) | 3 weeks | Working ID-CodeFormer model |
| --- | --- | --- | --- |
| Phase 3 | Experiments (Run baseline and ID-CodeFormer on benchmarks) | 2 weeks | Quantitative results (PSNR, SSIM, IDS, FID) |
| --- | --- | --- | --- |
| Phase 4 | Analysis & Final Report | 1 week | Final report with quantitative/qualitative analysis |
| --- | --- | --- | --- |

## 7\. Risk Analysis

- **Risk:** **Quality-Fidelity Trade-off.** The primary risk is that optimizing for identity fidelity (via \$L_{ids}\$) will significantly degrade pixel-level reconstruction quality, leading to lower PSNR and SSIM scores.
- **Mitigation:** This trade-off is managed by carefully tuning the loss weight hyperparameter, \$\\lambda_{ids}\$. Based on experiments, a value of 0.1 was found to provide the best balance, offering a tangible improvement in identity (IDS) with only a minimal and acceptable decrease in PSNR/SSIM.
- **Risk:** **Codebook Limitation.** The finite representational capacity of the VQ-VAE codebook may still be insufficient to reconstruct extremely rare or unique facial features, even with the new loss.
- **Mitigation:** This is a fundamental limitation of the CodeFormer architecture. The \$L_{ids}\$ loss mitigates this by forcing the Transformer to find the _best possible combination_ of existing visual atoms to preserve identity, rather than regressing to a "mean" face.

## 8\. Expected Outcomes

- **Expected Results:** The ID-CodeFormer model is expected to show a tangible, quantitative improvement in the **Identity Similarity (IDS) score** compared to the baseline CodeFormer. A slight and acceptable decrease in pixel-level metrics (PSNR, SSIM) is anticipated, confirming a successful navigation of the quality-fidelity trade-off. Qualitative analysis is expected to visually demonstrate that ID-CodeFormer is superior at reconstructing identity-defining details (e.g., eyeglasses, moles) that the baseline model often omits or alters.
- **Contributions:** The primary contribution is a validated methodology for integrating direct identity supervision into discrete latent space restoration models. This work demonstrates that an explicit identity-preserving loss can effectively mitigate the "identity drift" problem inherent in models like CodeFormer, paving the way for more identity-aware generative frameworks.