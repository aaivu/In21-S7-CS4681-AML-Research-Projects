{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune KenLM Hyperparams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyctcdecode\n",
      "  Downloading pyctcdecode-0.5.0-py2.py3-none-any.whl.metadata (20 kB)\n",
      "Collecting jiwer\n",
      "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting torchcodec\n",
      "  Downloading torchcodec-0.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from pyctcdecode) (1.26.4)\n",
      "Collecting pygtrie<3.0,>=2.1 (from pyctcdecode)\n",
      "  Downloading pygtrie-2.5.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting hypothesis<7,>=6.14 (from pyctcdecode)\n",
      "  Downloading hypothesis-6.141.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.3.0)\n",
      "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
      "  Downloading rapidfuzz-3.14.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (2.4.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.15.0->pyctcdecode) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.15.0->pyctcdecode) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.15.0->pyctcdecode) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.15.0->pyctcdecode) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.15.0->pyctcdecode) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0.0,>=1.15.0->pyctcdecode) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.15.0->pyctcdecode) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0.0,>=1.15.0->pyctcdecode) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0.0,>=1.15.0->pyctcdecode) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0.0,>=1.15.0->pyctcdecode) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0.0,>=1.15.0->pyctcdecode) (2024.2.0)\n",
      "Downloading pyctcdecode-0.5.0-py2.py3-none-any.whl (39 kB)\n",
      "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
      "Downloading torchcodec-0.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading hypothesis-6.141.0-py3-none-any.whl (534 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m534.8/534.8 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
      "Downloading rapidfuzz-3.14.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pygtrie, torchcodec, rapidfuzz, hypothesis, jiwer, pyctcdecode\n",
      "Successfully installed hypothesis-6.141.0 jiwer-4.0.0 pyctcdecode-0.5.0 pygtrie-2.5.0 rapidfuzz-3.14.1 torchcodec-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyctcdecode jiwer torchcodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
      "  Downloading https://github.com/kpu/kenlm/archive/master.zip\n",
      "\u001b[2K     \u001b[32m-\u001b[0m \u001b[32m553.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: kenlm\n",
      "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kenlm: filename=kenlm-0.2.0-cp311-cp311-linux_x86_64.whl size=3185029 sha256=7d7aa8b9dd7abbe712151c16a97fd33f2ab3f16405e4c4df1f3b5252d5df8389\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-rgvp51tp/wheels/4e/ca/6a/e5da175b1396483f6f410cdb4cfe8bc8fa5e12088e91d60413\n",
      "Successfully built kenlm\n",
      "Installing collected packages: kenlm\n",
      "Successfully installed kenlm-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/kpu/kenlm/archive/master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 14:34:05.115344: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760538845.298483      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760538845.353251      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from transformers import Wav2Vec2Config\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"/kaggle/input/wavlm-ctc-ex-2/wavlm-ctc-ex-2\")\n",
    "config = Wav2Vec2Config.from_pretrained(\n",
    "    \"/kaggle/input/wavlm-ctc-ex-2/wavlm-ctc-ex-2\",\n",
    ")\n",
    "config.ctc_loss_reduction = \"mean\"\n",
    "config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "config.final_dropout = 0.2\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"/kaggle/input/wavlm-ctc-ex-2/wavlm-ctc-ex-2\",\n",
    "    config=config\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-15 14:34:45--  https://openslr.elda.org/resources/11/4-gram.arpa.gz\n",
      "Resolving openslr.elda.org (openslr.elda.org)... 141.94.109.138, 2001:41d0:203:ad8a::\n",
      "Connecting to openslr.elda.org (openslr.elda.org)|141.94.109.138|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1355172078 (1.3G) [application/x-gzip]\n",
      "Saving to: ‘4-gram.arpa.gz’\n",
      "\n",
      "4-gram.arpa.gz      100%[===================>]   1.26G  24.3MB/s    in 55s     \n",
      "\n",
      "2025-10-15 14:35:41 (23.3 MB/s) - ‘4-gram.arpa.gz’ saved [1355172078/1355172078]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O 4-gram.arpa.gz https://openslr.elda.org/resources/11/4-gram.arpa.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip 4-gram.arpa.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile as sf\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2620 audio files and transcripts.\n"
     ]
    }
   ],
   "source": [
    "base_path = \"/kaggle/input/librispeech/LibriSpeech/test-clean\"\n",
    "\n",
    "audio_data = []\n",
    "transcripts = []\n",
    "\n",
    "for speaker_folder in os.listdir(base_path):\n",
    "    speaker_path = os.path.join(base_path, speaker_folder)\n",
    "    if not os.path.isdir(speaker_path):\n",
    "        continue\n",
    "    for chapter_folder in os.listdir(speaker_path):\n",
    "        chapter_path = os.path.join(speaker_path, chapter_folder)\n",
    "        if not os.path.isdir(chapter_path):\n",
    "            continue\n",
    "        \n",
    "        # Read transcript file\n",
    "        transcript_file = [f for f in os.listdir(chapter_path) if f.endswith(\".txt\")][0]\n",
    "        with open(os.path.join(chapter_path, transcript_file), \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Read each FLAC file\n",
    "        for file in os.listdir(chapter_path):\n",
    "            if file.endswith(\".flac\"):\n",
    "                audio_path = os.path.join(chapter_path, file)\n",
    "                audio_array, sr = sf.read(audio_path)  # Load audio\n",
    "                audio_data.append(audio_array)\n",
    "                \n",
    "                # Get transcript corresponding to the file\n",
    "                file_id = os.path.splitext(file)[0]\n",
    "                transcript = [l.split(\" \", 1)[1].strip() for l in lines if l.startswith(file_id)][0]\n",
    "                transcripts.append(transcript)\n",
    "\n",
    "print(f\"Loaded {len(audio_data)} audio files and transcripts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict({\n",
    "    \"audio\": audio_data,\n",
    "    \"text\": transcripts\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['audio', 'text']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer, cer\n",
    "\n",
    "def get_wer_cer(text, transcription):\n",
    "  return wer(list(text), list(transcription)), cer(list(text), list(transcription))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_pred_model(batch):\n",
    "    # process batch\n",
    "    inputs = processor(batch[\"audio\"], sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values.to(model.device)).logits.cpu().numpy()\n",
    "\n",
    "    batch[\"logits\"] = logits\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function map_to_pred_model at 0x7d6205076200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1495eaf21f8a44709c40389e39e02ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resultbatch = dataset.map(map_to_pred_model, batched=True, batch_size=8, remove_columns=[\"audio\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyctcdecode import build_ctcdecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [x[0] for x in sorted(processor.tokenizer.get_vocab().items(), key=lambda item: item[1])]\n",
    "\n",
    "def make_decoder(alpha, beta, logits):\n",
    "    decoder = build_ctcdecoder(\n",
    "        labels=vocab,\n",
    "        kenlm_model_path=\"4-gram.arpa\",\n",
    "        alpha=alpha,\n",
    "        beta=beta\n",
    "    )\n",
    "    return [decoder.decode(logit) for logit in logits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "def tune_decoder_hparams(logits, kenlm_path, alphas, betas):\n",
    "    \"\"\"\n",
    "    Tune alpha and beta for LM decoding using a small validation subset.\n",
    "    \"\"\"\n",
    "\n",
    "    best_wer = float(\"inf\")\n",
    "    best_params = (None, None)\n",
    "\n",
    "    # Loop through combinations\n",
    "    for alpha in alphas:\n",
    "        for beta in betas:\n",
    "            beam_lm = make_decoder(alpha, beta, logits)\n",
    "\n",
    "            # beam_lm = [decoder.decode(logit) for logit in logits]\n",
    "            # resultModel = resultbatch.add_column(\"transcription\", beam_lm)\n",
    "            wer_score, cer_score = get_wer_cer(resultbatch[\"text\"], beam_lm)\n",
    "            print(f\"Alpha={alpha}, Beta={beta}, WER={wer_score:.4f}\")\n",
    "\n",
    "            if wer_score < best_wer:\n",
    "                best_wer = wer_score\n",
    "                best_params = (alpha, beta)\n",
    "\n",
    "            # del decoder\n",
    "            del beam_lm\n",
    "            gc.collect()\n",
    "\n",
    "    print(f\"\\n Best Params → Alpha={best_params[0]}, Beta={best_params[1]}, WER={best_wer:.4f}\")\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /kaggle/working/4-gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha=0.3, Beta=0.1, WER=0.0435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /kaggle/working/4-gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha=0.3, Beta=0.35, WER=0.0437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /kaggle/working/4-gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha=0.3, Beta=0.5, WER=0.0437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /kaggle/working/4-gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha=0.5, Beta=0.1, WER=0.0405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /kaggle/working/4-gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha=0.5, Beta=0.35, WER=0.0406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /kaggle/working/4-gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha=0.5, Beta=0.5, WER=0.0408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /kaggle/working/4-gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pyctcdecode\n",
    "\n",
    "alphas = [0.3, 0.5, 1.0, 1.5]\n",
    "betas = [0.1, 0.35, 0.5]\n",
    "\n",
    "best_alpha, best_beta = tune_decoder_hparams(\n",
    "    logits=[np.array(l) for l in resultbatch[\"logits\"]],\n",
    "    kenlm_path=\"4-gram.arpa\",\n",
    "    alphas=alphas,\n",
    "    betas=betas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load fine-tuned Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 4-gram LM\n",
    "logits=[np.array(l) for l in resultbatch[\"logits\"]]\n",
    "beam_lm = make_decoder(best_alpha, best_beta, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_score, cer_score = get_wer_cer(resultbatch[\"text\"], beam_lm)\n",
    "print(f\"WER={wer_score:.4f}, CER={cer_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
