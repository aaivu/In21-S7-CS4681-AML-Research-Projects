{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3 – Fine-Tuned WavLM-CTC with KenLM Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyctcdecode in /usr/local/lib/python3.12/dist-packages (0.5.0)\n",
      "Requirement already satisfied: jiwer in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Collecting torchcodec\n",
      "  Downloading torchcodec-0.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from pyctcdecode) (1.26.4)\n",
      "Requirement already satisfied: pygtrie<3.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from pyctcdecode) (2.5.0)\n",
      "Requirement already satisfied: hypothesis<7,>=6.14 in /usr/local/lib/python3.12/dist-packages (from pyctcdecode) (6.140.2)\n",
      "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.2.1)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.12/dist-packages (from jiwer) (3.14.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (2.4.0)\n",
      "Downloading torchcodec-0.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchcodec\n",
      "Successfully installed torchcodec-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyctcdecode jiwer torchcodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
      "  Downloading https://github.com/kpu/kenlm/archive/master.zip\n",
      "\u001b[2K     \u001b[32m-\u001b[0m \u001b[32m553.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: kenlm\n",
      "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kenlm: filename=kenlm-0.2.0-cp312-cp312-linux_x86_64.whl size=3188042 sha256=ee4be9dddde09775d86a5e11bdff79529c3e6f4da2524ff319adb440e9a6e236\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-jv9x6ybu/wheels/92/c8/12/56d187154e078f0eaa74d059017fc1afe1c4d91fbce02ce8d9\n",
      "Successfully built kenlm\n",
      "Installing collected packages: kenlm\n",
      "Successfully installed kenlm-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/kpu/kenlm/archive/master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npjTlWh9MDqU"
   },
   "source": [
    "Load Saved Models in my drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"/content/drive/MyDrive/wavlm-ctc-ex-2\").to(device)\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"/content/drive/MyDrive/wavlm-ctc-ex-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLlp8XAKMI9Q"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install a KenLM Model(4-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-03 15:17:15--  https://openslr.elda.org/resources/11/4-gram.arpa.gz\n",
      "Resolving openslr.elda.org (openslr.elda.org)... 141.94.109.138, 2001:41d0:203:ad8a::\n",
      "Connecting to openslr.elda.org (openslr.elda.org)|141.94.109.138|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1355172078 (1.3G) [application/x-gzip]\n",
      "Saving to: ‘4-gram.arpa.gz’\n",
      "\n",
      "4-gram.arpa.gz      100%[===================>]   1.26G  15.0MB/s    in 91s     \n",
      "\n",
      "2025-10-03 15:18:48 (14.2 MB/s) - ‘4-gram.arpa.gz’ saved [1355172078/1355172078]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O 4-gram.arpa.gz https://openslr.elda.org/resources/11/4-gram.arpa.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip 4-gram.arpa.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucYIqRPrMPLw"
   },
   "source": [
    "Build KenLM Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pyctcdecode.alphabet:Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n"
     ]
    }
   ],
   "source": [
    "import pyctcdecode\n",
    "\n",
    "# Load vocabulary\n",
    "vocab_dict = processor.tokenizer.get_vocab()\n",
    "sorted_vocab = sorted(vocab_dict.items(), key=lambda item: item[1])\n",
    "vocab_list = [x[0] for x in sorted_vocab]\n",
    "\n",
    "# Load 4-gram LM\n",
    "decoder = pyctcdecode.build_ctcdecoder(\n",
    "    labels=vocab_list,\n",
    "    kenlm_model_path=\"4-gram.arpa\"  # or \"4-gram.bin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwwrNbCVMUuI"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def transcribe_with_lm(logits):\n",
    "    # logits: [batch_size, time, vocab_size]\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    log_probs = log_probs.cpu().detach().numpy()\n",
    "    return decoder.decode(log_probs[0])  # single example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9108ca49343646c2ac623b2dbb69390b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441a16b49dc943009c4c353ba7a500ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "librispeech_eval = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer, cer\n",
    "\n",
    "def get_wer_cer(result):\n",
    "  refs = result[\"text\"]\n",
    "  hyps = result[\"transcription\"]\n",
    "\n",
    "  # Convert to plain Python lists\n",
    "  refs = list(refs)\n",
    "  hyps = list(hyps)\n",
    "\n",
    "  print(\"WER:\", wer(refs, hyps))\n",
    "  print(\"CER:\", cer(refs, hyps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icyDIVg5CRDt"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHWdjbvACSjp"
   },
   "source": [
    "### For Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2bbb44f13544b4ab3d355f0bacf41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def map_to_pred_model(batch):\n",
    "    # extract all audio arrays\n",
    "    audio_arrays = [x[\"array\"] for x in batch[\"audio\"]]\n",
    "\n",
    "    # process batch\n",
    "    inputs = processor(audio_arrays, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values.to(model.device)).logits.cpu().numpy()\n",
    "\n",
    "    # decode predictions\n",
    "    beam_lm = [decoder.decode(logit) for logit in logits]\n",
    "\n",
    "    batch[\"transcription\"] = beam_lm\n",
    "    return batch\n",
    "\n",
    "# run evaluation\n",
    "resultModel = librispeech_eval.map(map_to_pred_model, batched=True, batch_size=8, remove_columns=[\"audio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.04161594643944005\n",
      "CER: 0.01348701736937449\n"
     ]
    }
   ],
   "source": [
    "get_wer_cer(resultModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqETPE5rCXyB"
   },
   "source": [
    "### For Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd87c32ab3964ca282492d0bbb777bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2eaedb6e9f4748ba589408085b191c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_dataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5df3c38d0048b2b1a9c847368226d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2703 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resultEval = val_dataset.map(map_to_pred_model, batched=True, batch_size=8, remove_columns=[\"audio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.03941031579721334\n",
      "CER: 0.013371190060182489\n"
     ]
    }
   ],
   "source": [
    "get_wer_cer(resultEval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYZ2IpOqc1U5"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (wavlm)",
   "language": "python",
   "name": "wavlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
