{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T18:20:32.272971Z","iopub.execute_input":"2025-10-13T18:20:32.273179Z","iopub.status.idle":"2025-10-13T18:20:32.548907Z","shell.execute_reply.started":"2025-10-13T18:20:32.273156Z","shell.execute_reply":"2025-10-13T18:20:32.548005Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"! pip install datasets transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T18:20:32.550706Z","iopub.execute_input":"2025-10-13T18:20:32.551106Z","iopub.status.idle":"2025-10-13T18:20:32.890454Z","shell.execute_reply.started":"2025-10-13T18:20:32.551079Z","shell.execute_reply":"2025-10-13T18:20:32.889729Z"}},"outputs":[{"name":"stdout","text":"^C\nTraceback (most recent call last):\n  File \"/usr/local/bin/pip3\", line 4, in <module>\n    from pip._internal.cli.main import main\n  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 11, in <module>\n    from pip._internal.cli.autocompletion import autocomplete\n  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n    from pip._internal.cli.main_parser import create_main_parser\n  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n    from pip._internal.build_env import get_runnable_pip\n  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/build_env.py\", line 19, in <module>\n    from pip._internal.cli.spinners import open_spinner\n  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/spinners.py\", line 9, in <module>\n    from pip._internal.utils.logging import get_indentation\n  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/logging.py\", line 13, in <module>\n    from pip._vendor.rich.console import (\n  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/rich/console.py\", line 47, in <module>\n    from . import errors, themes\n  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/rich/themes.py\", line 1, in <module>\n    from .default_styles import DEFAULT_STYLES\n  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/rich/default_styles.py\", line 3, in <module>\n    from .style import Style\n  File \"/usr/local/lib/python3.11/dist-packages/pip/_vendor/rich/style.py\", line 8, in <module>\n    from .color import Color, ColorParseError, ColorSystem, blend_rgb\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 936, in exec_module\n  File \"<frozen importlib._bootstrap_external>\", line 1032, in get_code\n  File \"<frozen importlib._bootstrap_external>\", line 1131, in get_data\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset, concatenate_datasets\nimport random\nfrom transformers import MT5Tokenizer, MT5ForConditionalGeneration\nfrom transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T18:20:32.893434Z","iopub.execute_input":"2025-10-13T18:20:32.893690Z","execution_failed":"2025-10-13T18:20:55.418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------\n# Load dataset\n# ---------------------------\nlanguage_pair = \"en-es\"\nopus100_dataset = load_dataset(\"opus100\", language_pair)\nprint(opus100_dataset)\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T18:20:55.418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subset_size = 100000\ntrain_subset = opus100_dataset[\"train\"].select(range(subset_size))\nprint(train_subset)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T18:20:55.419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------\n# Noise function\n# ---------------------------\ndef apply_noise(text):\n    words = text.split()\n    noisy_words = []\n    for word in words:\n        if random.random() < 0.1:  # 10% chance to delete a word\n            continue\n        if random.random() < 0.1 and len(word) > 1:  # 10% chance to delete a character\n            char_list = list(word)\n            del char_list[random.randint(0, len(char_list) - 1)]\n            word = \"\".join(char_list)\n        noisy_words.append(word)\n\n    # 5% chance to swap adjacent words\n    if len(noisy_words) > 1 and random.random() < 0.05:\n        swap_index = random.randint(0, len(noisy_words) - 2)\n        noisy_words[swap_index], noisy_words[swap_index + 1] = (\n            noisy_words[swap_index + 1],\n            noisy_words[swap_index],\n        )\n\n    return \" \".join(noisy_words)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T18:20:55.419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------\n# Add noisy fields to training set\n# ---------------------------\ntrain_subset = train_subset.map(\n    lambda example: {\n        \"translation\": {\n            \"en\": example[\"translation\"][\"en\"],\n            \"es\": example[\"translation\"][\"es\"],\n            \"en_noisy\": apply_noise(example[\"translation\"][\"en\"]),\n            \"es_noisy\": apply_noise(example[\"translation\"][\"es\"]),\n        }\n    }\n)\nprint(train_subset[0])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T18:20:55.419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------\n# Tokenizer\n# ---------------------------\ntokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T18:20:55.420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------\n# Tokenization functions (DENOISING!)\n# ---------------------------\ndef tokenize_en_denoise(examples):\n    # Noisy English → Original English\n    en_noisy_batch = [item[\"en_noisy\"] for item in examples[\"translation\"]]\n    en_original_batch = [item[\"en\"] for item in examples[\"translation\"]]\n\n    return tokenizer(\n        en_noisy_batch,\n        max_length=512,\n        truncation=True,\n        text_target=en_original_batch,\n    )","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T18:20:55.420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_fr_denoise(examples):\n    # Noisy French → Original French\n    fr_noisy_batch = [item[\"es_noisy\"] for item in examples[\"translation\"]]\n    fr_original_batch = [item[\"es\"] for item in examples[\"translation\"]]\n\n    return tokenizer(\n        fr_noisy_batch,\n        max_length=512,\n        truncation=True,\n        text_target=fr_original_batch,\n    )","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T18:20:55.420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------\n# Tokenize datasets\n# ---------------------------\ntokenized_en = train_subset.map(tokenize_en_denoise, batched=True)\ntokenized_fr = train_subset.map(tokenize_fr_denoise, batched=True)\n\n# Combine both directions\ntokenized_dataset = concatenate_datasets([tokenized_en, tokenized_fr])\n\n# Remove original column & format\ntokenized_dataset = tokenized_dataset.remove_columns([\"translation\"])\ntokenized_dataset.set_format(\"torch\")\n\nprint(tokenized_dataset[0])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T18:20:55.420Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------\n# Validation set\n# ---------------------------\nvalidation_subset = opus100_dataset[\"validation\"]\n\nvalidation_subset = validation_subset.map(\n    lambda example: {\n        \"translation\": {\n            \"en\": example[\"translation\"][\"en\"],\n            \"es\": example[\"translation\"][\"es\"],\n            \"en_noisy\": apply_noise(example[\"translation\"][\"en\"]),\n            \"es_noisy\": apply_noise(example[\"translation\"][\"es\"]),\n        }\n    }\n)\n\ntokenized_val_en = validation_subset.map(tokenize_en_denoise, batched=True)\ntokenized_val_fr = validation_subset.map(tokenize_fr_denoise, batched=True)\ntokenized_validation_dataset = concatenate_datasets([tokenized_val_en, tokenized_val_fr])\ntokenized_validation_dataset = tokenized_validation_dataset.remove_columns([\"translation\"])\ntokenized_validation_dataset.set_format(\"torch\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T18:20:55.421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------\n# Load model\n# ---------------------------\nmodel = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\nprint(model)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T18:20:55.421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------\n# Training setup\n# ---------------------------\ntraining_args = TrainingArguments(\n    output_dir=\"./mt5-small-denoising\",\n    num_train_epochs=1,\n    per_device_train_batch_size=2,  # reduce if OOM\n    per_device_eval_batch_size=2,\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    seed=42,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    adam_beta1=0.9,\n    adam_beta2=0.999,\n    adam_epsilon=1e-8,\n    max_grad_norm=1.0,\n    dataloader_num_workers=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    gradient_accumulation_steps=4,\n    report_to=\"none\"\n    \n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T18:20:55.421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data collator for dynamic padding\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T18:20:55.421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------\n# Trainer\n# ---------------------------\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    eval_dataset=tokenized_validation_dataset,\n    data_collator=data_collator,\n)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T18:20:55.421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------\n# Train\n# ---------------------------\nprint(\"Start Model Training ------------------------\")\ntrainer.train()\nprint(\"Model trained Successfully --------\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T18:20:55.421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token=\"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\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T18:20:55.422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"repo_name = \"mt5-small-denoising-en-es-correct-deoise\"\n\n# 3. Save trained model & tokenizer locally\nmodel.save_pretrained(repo_name)\ntokenizer.save_pretrained(repo_name)\n\n# 4. Push to Hugging Face Hub\nmodel.push_to_hub(repo_name)\ntokenizer.push_to_hub(repo_name)\n\nprint(f\"✅ Model and tokenizer uploaded successfully to https://huggingface.co/Eshan210352R/{repo_name}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T18:20:55.422Z"}},"outputs":[],"execution_count":null}]}