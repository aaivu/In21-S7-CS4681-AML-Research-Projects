{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14327231",
   "metadata": {},
   "source": [
    "# TNC (Temporal Neighborhood Coding) Training & Few-Shot Learning on Google Colab\n",
    "\n",
    "This notebook trains a TNC model on simulation data using GPU acceleration and evaluates few-shot learning performance. You can download the trained model to use locally.\n",
    "\n",
    "**‚ö° Make sure to enable GPU in Colab: Runtime ‚Üí Change runtime type ‚Üí GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb159bf",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Installation\n",
    "\n",
    "Install required packages and check GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389b1943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install numpy pandas matplotlib seaborn scikit-learn\n",
    "!pip install statsmodels\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üî• Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  GPU not available - using CPU (will be slower)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aba18c",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive and Setup Workspace\n",
    "\n",
    "Mount Google Drive for data persistence and create necessary directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d62feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create workspace directory\n",
    "workspace_path = '/content/drive/MyDrive/TNC_workspace'\n",
    "os.makedirs(workspace_path, exist_ok=True)\n",
    "os.makedirs(f'{workspace_path}/data/simulated_data', exist_ok=True)\n",
    "os.makedirs(f'{workspace_path}/ckpt/simulation', exist_ok=True)\n",
    "os.makedirs(f'{workspace_path}/plots/simulation', exist_ok=True)\n",
    "\n",
    "# Change to workspace directory\n",
    "os.chdir(workspace_path)\n",
    "print(f\"‚úÖ Workspace created at: {workspace_path}\")\n",
    "print(f\"üìÅ Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caa909b",
   "metadata": {},
   "source": [
    "## 3. Generate Simulation Data\n",
    "\n",
    "Create simulated time series data with 4 different temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b95b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation data generation (fixed version from original code)\n",
    "n_signals = 5\n",
    "n_states = 4\n",
    "transition_matrix = np.eye(n_states)*0.85\n",
    "transition_matrix[0,1] = transition_matrix[1,0] = 0.05\n",
    "transition_matrix[0,2] = transition_matrix[2,0] = 0.05\n",
    "transition_matrix[0,3] = transition_matrix[3,0] = 0.05\n",
    "transition_matrix[2,3] = transition_matrix[3,2] = 0.05\n",
    "transition_matrix[2,1] = transition_matrix[1,2] = 0.05\n",
    "transition_matrix[3,1] = transition_matrix[1,3] = 0.05\n",
    "\n",
    "def simple_signal_generator(state, window_size):\n",
    "    \"\"\"Generate signals for different states\"\"\"\n",
    "    np.random.seed(state * 42)\n",
    "    \n",
    "    if state == 0:\n",
    "        # Periodic signal\n",
    "        t = np.linspace(0, 4*np.pi, window_size)\n",
    "        signal = np.sin(t) + 0.5*np.sin(3*t) + np.random.normal(0, 0.3, window_size)\n",
    "    elif state == 1:\n",
    "        # Autoregressive signal\n",
    "        signal = np.zeros(window_size)\n",
    "        signal[0] = np.random.normal(0, 0.5)\n",
    "        for i in range(1, window_size):\n",
    "            if i >= 5:\n",
    "                signal[i] = 0.3*signal[i-1] + 0.05*signal[i-5] + 0.1*signal[i-1]*signal[i-5] + np.random.normal(0, 0.3)\n",
    "            else:\n",
    "                signal[i] = 0.3*signal[i-1] + np.random.normal(0, 0.3)\n",
    "    elif state == 2:\n",
    "        # Smooth signal\n",
    "        signal = np.random.normal(0, 1, window_size)\n",
    "        for i in range(1, window_size):\n",
    "            signal[i] = 0.8*signal[i-1] + 0.2*signal[i]\n",
    "        signal += np.random.normal(0, 0.1, window_size)\n",
    "    elif state == 3:\n",
    "        # Complex autoregressive\n",
    "        signal = np.zeros(window_size)\n",
    "        signal[0] = np.random.normal(0, 0.5)\n",
    "        for i in range(1, window_size):\n",
    "            if i >= 3:\n",
    "                signal[i] = 0.1*signal[i-1] + 0.25*signal[i-2] + 2.5*signal[i-3] - 0.005*signal[i-1]*signal[i-2]*signal[i-3] + np.random.normal(0, 0.3)\n",
    "            else:\n",
    "                signal[i] = 0.3*signal[i-1] + np.random.normal(0, 0.3)\n",
    "    \n",
    "    return signal\n",
    "\n",
    "def create_signal(sig_len, window_size=50):\n",
    "    states = []\n",
    "    sig_1, sig_2, sig_3 = [], [], []\n",
    "    pi = np.ones((1,n_states))/n_states\n",
    "\n",
    "    for _ in range(sig_len//window_size):\n",
    "        current_state = np.random.choice(n_states, 1, p=pi.reshape(-1))\n",
    "        states.extend(list(current_state)*window_size)\n",
    "\n",
    "        current_signal = simple_signal_generator(current_state[0], window_size)\n",
    "        sig_1.extend(current_signal)\n",
    "        correlated_signal = current_signal*0.9 + .03 + np.random.randn(len(current_signal))*0.4\n",
    "        sig_2.extend(correlated_signal)\n",
    "        uncorrelated_signal = simple_signal_generator((current_state[0]+2)%4, window_size)\n",
    "        sig_3.extend(uncorrelated_signal)\n",
    "\n",
    "        pi = transition_matrix[current_state]\n",
    "    signals = np.stack([sig_1, sig_2, sig_3])\n",
    "    return signals, states\n",
    "\n",
    "def normalize(train_data, test_data):\n",
    "    \"\"\"Fixed normalization function that prevents NaN values\"\"\"\n",
    "    # Calculate mean and std for each feature\n",
    "    feature_means = np.mean(train_data, axis=(0,2))\n",
    "    feature_std = np.std(train_data, axis=(0, 2))\n",
    "    \n",
    "    # Prevent division by zero - use 1.0 for features with zero std\n",
    "    feature_std = np.where(feature_std == 0, 1.0, feature_std)\n",
    "    \n",
    "    # Normalize using broadcasting\n",
    "    train_data_n = (train_data - feature_means[np.newaxis,:,np.newaxis]) / feature_std[np.newaxis,:,np.newaxis]\n",
    "    test_data_n = (test_data - feature_means[np.newaxis, :, np.newaxis]) / feature_std[np.newaxis, :, np.newaxis]\n",
    "    \n",
    "    return train_data_n, test_data_n\n",
    "\n",
    "# Generate data\n",
    "print(\"üîÑ Generating simulation data...\")\n",
    "n_samples, sig_len = 400, 2000  # Reduced for stability\n",
    "all_signals, all_states = [], []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Generated {i}/{n_samples} samples...\")\n",
    "    sample_signal, sample_state = create_signal(sig_len)\n",
    "    all_signals.append(sample_signal)\n",
    "    all_states.append(sample_state)\n",
    "\n",
    "dataset = np.array(all_signals)\n",
    "states = np.array(all_states)\n",
    "n_train = int(len(dataset) * 0.8)\n",
    "train_data = dataset[:n_train]\n",
    "test_data = dataset[n_train:]\n",
    "train_data_n, test_data_n = normalize(train_data, test_data)\n",
    "train_state = states[:n_train]\n",
    "test_state = states[n_train:]\n",
    "\n",
    "# Verify no NaN values\n",
    "print(f\"‚úÖ Data generated! Train: {train_data_n.shape}, Test: {test_data_n.shape}\")\n",
    "print(f\"Train data range: [{np.min(train_data_n):.3f}, {np.max(train_data_n):.3f}]\")\n",
    "print(f\"NaN in train data: {np.isnan(train_data_n).any()}\")\n",
    "print(f\"Inf in train data: {np.isinf(train_data_n).any()}\")\n",
    "\n",
    "# Save data\n",
    "with open('data/simulated_data/x_train.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data_n, f)\n",
    "with open('data/simulated_data/x_test.pkl', 'wb') as f:\n",
    "    pickle.dump(test_data_n, f)\n",
    "with open('data/simulated_data/state_train.pkl', 'wb') as f:\n",
    "    pickle.dump(train_state, f)\n",
    "with open('data/simulated_data/state_test.pkl', 'wb') as f:\n",
    "    pickle.dump(test_state, f)\n",
    "\n",
    "print(\"üíæ Data saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970bdb90",
   "metadata": {},
   "source": [
    "## 4. Define TNC Model Architecture\n",
    "\n",
    "Implement the TNC encoder and discriminator models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b41988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TNC Model Classes (EXACT copy from working tnc/models.py)\n",
    "class RnnEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, in_channel, encoding_size, cell_type='GRU', num_layers=1, device='cpu', dropout=0, bidirectional=True):\n",
    "        super(RnnEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_channel = in_channel\n",
    "        self.num_layers = num_layers\n",
    "        self.cell_type = cell_type\n",
    "        self.encoding_size = encoding_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.device = device\n",
    "\n",
    "        self.nn = torch.nn.Sequential(torch.nn.Linear(self.hidden_size*(int(self.bidirectional) + 1), self.encoding_size)).to(self.device)\n",
    "        if cell_type=='GRU':\n",
    "            self.rnn = torch.nn.GRU(input_size=self.in_channel, hidden_size=self.hidden_size, num_layers=num_layers,\n",
    "                                    batch_first=False, dropout=dropout, bidirectional=bidirectional).to(self.device)\n",
    "        elif cell_type=='LSTM':\n",
    "            self.rnn = torch.nn.LSTM(input_size=self.in_channel, hidden_size=self.hidden_size, num_layers=num_layers,\n",
    "                                    batch_first=False, dropout=dropout, bidirectional=bidirectional).to(self.device)\n",
    "        else:\n",
    "            raise ValueError('Cell type not defined, must be one of the following {GRU, LSTM, RNN}')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(2,0,1)\n",
    "        if self.cell_type=='GRU':\n",
    "            past = torch.zeros(self.num_layers * (int(self.bidirectional) + 1), x.shape[1], self.hidden_size).to(self.device)\n",
    "        elif self.cell_type=='LSTM':\n",
    "            h_0 = torch.zeros(self.num_layers * (int(self.bidirectional) + 1), (x.shape[1]), self.hidden_size).to(self.device)\n",
    "            c_0 = torch.zeros(self.num_layers * (int(self.bidirectional) + 1), (x.shape[1]), self.hidden_size).to(self.device)\n",
    "            past = (h_0, c_0)\n",
    "        out, _ = self.rnn(x.to(self.device), past)\n",
    "        encodings = self.nn(out[-1].squeeze(0))\n",
    "        return encodings\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, input_size, device):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.device = device\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.model = torch.nn.Sequential(torch.nn.Linear(2*self.input_size, 4*self.input_size),\n",
    "                                         torch.nn.ReLU(inplace=True),\n",
    "                                         torch.nn.Dropout(0.5),\n",
    "                                         torch.nn.Linear(4*self.input_size, 1))\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.model[0].weight)\n",
    "        torch.nn.init.xavier_uniform_(self.model[3].weight)\n",
    "\n",
    "    def forward(self, x, x_tild):\n",
    "        x_all = torch.cat([x, x_tild], -1)\n",
    "        p = self.model(x_all)\n",
    "        return p.view((-1,))\n",
    "\n",
    "class StateClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(StateClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.normalize = torch.nn.BatchNorm1d(self.input_size)\n",
    "        self.nn = torch.nn.Linear(self.input_size, self.output_size)\n",
    "        torch.nn.init.xavier_uniform_(self.nn.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.normalize(x)\n",
    "        logits = self.nn(x)\n",
    "        return logits\n",
    "\n",
    "print(\"‚úÖ TNC model classes defined (exact copy from working code)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e9530",
   "metadata": {},
   "source": [
    "## 5. TNC Dataset and Training Functions\n",
    "\n",
    "Implement the TNC dataset loader and training functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae09eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import math\n",
    "\n",
    "# EXACT copy from working tnc/tnc.py\n",
    "class TNCDataset(data.Dataset):\n",
    "    def __init__(self, x, mc_sample_size, window_size, augmentation, epsilon=3, state=None, adf=False):\n",
    "        super(TNCDataset, self).__init__()\n",
    "        self.time_series = x\n",
    "        self.T = x.shape[-1]\n",
    "        self.window_size = window_size\n",
    "        self.sliding_gap = int(window_size*25.2)\n",
    "        self.window_per_sample = (self.T-2*self.window_size)//self.sliding_gap\n",
    "        self.mc_sample_size = mc_sample_size\n",
    "        self.state = state\n",
    "        self.augmentation = augmentation\n",
    "        self.adf = adf\n",
    "        if not self.adf:\n",
    "            self.epsilon = epsilon\n",
    "            self.delta = 5*window_size*epsilon\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.time_series)*self.augmentation\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        ind = ind%len(self.time_series)\n",
    "        t = np.random.randint(2*self.window_size, self.T-2*self.window_size)\n",
    "        x_t = self.time_series[ind][:,t-self.window_size//2:t+self.window_size//2]\n",
    "        X_close = self._find_neighours(self.time_series[ind], t)\n",
    "        X_distant = self._find_non_neighours(self.time_series[ind], t)\n",
    "\n",
    "        if self.state is None:\n",
    "            y_t = -1\n",
    "        else:\n",
    "            y_t = torch.round(torch.mean(self.state[ind][t-self.window_size//2:t+self.window_size//2]))\n",
    "        return x_t, X_close, X_distant, y_t\n",
    "\n",
    "    def _find_neighours(self, x, t):\n",
    "        T = self.time_series.shape[-1]\n",
    "        if self.adf:\n",
    "            gap = self.window_size\n",
    "            corr = []\n",
    "            for w_t in range(self.window_size,4*self.window_size, gap):\n",
    "                try:\n",
    "                    p_val = 0\n",
    "                    for f in range(x.shape[-2]):\n",
    "                        p = adfuller(np.array(x[f, max(0,t - w_t):min(x.shape[-1], t + w_t)].reshape(-1, )))[1]\n",
    "                        p_val += 0.01 if math.isnan(p) else p\n",
    "                    corr.append(p_val/x.shape[-2])\n",
    "                except:\n",
    "                    corr.append(0.6)\n",
    "            self.epsilon = len(corr) if len(np.where(np.array(corr) >= 0.01)[0])==0 else (np.where(np.array(corr) >= 0.01)[0][0] + 1)\n",
    "            self.delta = 5*self.epsilon*self.window_size\n",
    "\n",
    "        t_p = [int(t+np.random.randn()*self.epsilon*self.window_size) for _ in range(self.mc_sample_size)]\n",
    "        t_p = [max(self.window_size//2+1,min(t_pp,T-self.window_size//2)) for t_pp in t_p]\n",
    "        x_p = torch.stack([x[:, t_ind-self.window_size//2:t_ind+self.window_size//2] for t_ind in t_p])\n",
    "        return x_p\n",
    "\n",
    "    def _find_non_neighours(self, x, t):\n",
    "        T = self.time_series.shape[-1]\n",
    "        if t>T/2:\n",
    "            t_n = np.random.randint(self.window_size//2, max((t - self.delta + 1), self.window_size//2+1), self.mc_sample_size)\n",
    "        else:\n",
    "            t_n = np.random.randint(min((t + self.delta), (T - self.window_size-1)), (T - self.window_size//2), self.mc_sample_size)\n",
    "        x_n = torch.stack([x[:, t_ind-self.window_size//2:t_ind+self.window_size//2] for t_ind in t_n])\n",
    "\n",
    "        if len(x_n)==0:\n",
    "            rand_t = np.random.randint(0,self.window_size//5)\n",
    "            if t > T / 2:\n",
    "                x_n = x[:,rand_t:rand_t+self.window_size].unsqueeze(0)\n",
    "            else:\n",
    "                x_n = x[:, T - rand_t - self.window_size:T - rand_t].unsqueeze(0)\n",
    "        return x_n\n",
    "\n",
    "# EXACT copy from working tnc/tnc.py  \n",
    "def epoch_run(loader, disc_model, encoder, device, w=0, optimizer=None, train=True):\n",
    "    if train:\n",
    "        encoder.train()\n",
    "        disc_model.train()\n",
    "    else:\n",
    "        encoder.eval()\n",
    "        disc_model.eval()\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    encoder.to(device)\n",
    "    disc_model.to(device)\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    batch_count = 0\n",
    "    for x_t, x_p, x_n, _ in loader:\n",
    "        mc_sample = x_p.shape[1]\n",
    "        batch_size, f_size, len_size = x_t.shape\n",
    "        x_p = x_p.reshape((-1, f_size, len_size))\n",
    "        x_n = x_n.reshape((-1, f_size, len_size))\n",
    "        x_t = np.repeat(x_t, mc_sample, axis=0)\n",
    "        neighbors = torch.ones((len(x_p))).to(device)\n",
    "        non_neighbors = torch.zeros((len(x_n))).to(device)\n",
    "        x_t, x_p, x_n = x_t.to(device), x_p.to(device), x_n.to(device)\n",
    "\n",
    "        z_t = encoder(x_t)\n",
    "        z_p = encoder(x_p)\n",
    "        z_n = encoder(x_n)\n",
    "\n",
    "        d_p = disc_model(z_t, z_p)\n",
    "        d_n = disc_model(z_t, z_n)\n",
    "\n",
    "        p_loss = loss_fn(d_p, neighbors)\n",
    "        n_loss = loss_fn(d_n, non_neighbors)\n",
    "        n_loss_u = loss_fn(d_n, neighbors)\n",
    "        loss = (p_loss + w*n_loss_u + (1-w)*n_loss)/2\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        p_acc = torch.sum(torch.nn.Sigmoid()(d_p) > 0.5).item() / len(z_p)\n",
    "        n_acc = torch.sum(torch.nn.Sigmoid()(d_n) < 0.5).item() / len(z_n)\n",
    "        epoch_acc = epoch_acc + (p_acc+n_acc)/2\n",
    "        epoch_loss += loss.item()\n",
    "        batch_count += 1\n",
    "    return epoch_loss/batch_count, epoch_acc/batch_count\n",
    "\n",
    "print(\"‚úÖ TNC dataset and training functions defined (exact copy from working code)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d38ff",
   "metadata": {},
   "source": [
    "## 6. Train TNC Model with GPU\n",
    "\n",
    "Train the TNC model using the prepared simulation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ffb318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration (EXACT parameters from working tnc.py)\n",
    "window_size = 50\n",
    "w = 0.05\n",
    "lr = 1e-3  # Original working learning rate\n",
    "decay = 1e-5  # Original working decay\n",
    "n_epochs = 100\n",
    "mc_sample_size = 20\n",
    "batch_size = 10\n",
    "augmentation = 1\n",
    "\n",
    "print(f\"üî• Training TNC on {device}\")\n",
    "print(f\"Parameters: w={w}, lr={lr}, epochs={n_epochs}\")\n",
    "\n",
    "# Load data\n",
    "with open('data/simulated_data/x_train.pkl', 'rb') as f:\n",
    "    x = pickle.load(f)\n",
    "\n",
    "# Check data quality (should be clean now)\n",
    "print(f\"Data shape: {x.shape}\")\n",
    "print(f\"Data range: [{np.min(x):.3f}, {np.max(x):.3f}]\")\n",
    "print(f\"NaN in data: {np.isnan(x).any()}\")\n",
    "print(f\"Inf in data: {np.isinf(x).any()}\")\n",
    "\n",
    "# Initialize models EXACTLY like original simulation case\n",
    "encoder = RnnEncoder(hidden_size=100, in_channel=3, encoding_size=10, device=device)\n",
    "disc_model = Discriminator(encoder.encoding_size, device)\n",
    "params = list(disc_model.parameters()) + list(encoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=lr, weight_decay=decay)\n",
    "\n",
    "# Shuffle and split data (exact original logic)\n",
    "inds = list(range(len(x)))\n",
    "random.shuffle(inds)\n",
    "x = x[inds]\n",
    "n_train = int(0.8*len(x))\n",
    "\n",
    "performance = []\n",
    "best_acc = 0\n",
    "best_loss = np.inf\n",
    "\n",
    "print(\"\\nüöÄ Starting TNC training...\")\n",
    "for epoch in range(n_epochs+1):\n",
    "    # Create datasets exactly like original\n",
    "    trainset = TNCDataset(x=torch.Tensor(x[:n_train]), mc_sample_size=mc_sample_size,\n",
    "                          window_size=window_size, augmentation=augmentation, adf=True)\n",
    "    train_loader = data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    validset = TNCDataset(x=torch.Tensor(x[n_train:]), mc_sample_size=mc_sample_size,\n",
    "                          window_size=window_size, augmentation=augmentation, adf=True)\n",
    "    valid_loader = data.DataLoader(validset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    # Training step\n",
    "    epoch_loss, epoch_acc = epoch_run(train_loader, disc_model, encoder, optimizer=optimizer,\n",
    "                                      w=w, train=True, device=device)\n",
    "    \n",
    "    # Validation step\n",
    "    test_loss, test_acc = epoch_run(valid_loader, disc_model, encoder, train=False, w=w, device=device)\n",
    "    \n",
    "    performance.append((epoch_loss, test_loss, epoch_acc, test_acc))\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch:3d} | Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f} | '\n",
    "              f'Val Loss: {test_loss:.4f} | Val Acc: {test_acc:.4f}')\n",
    "    \n",
    "    # Save best model\n",
    "    if best_loss > test_loss:\n",
    "        best_acc = test_acc\n",
    "        best_loss = test_loss\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'encoder_state_dict': encoder.state_dict(),\n",
    "            'discriminator_state_dict': disc_model.state_dict(),\n",
    "            'best_accuracy': test_acc\n",
    "        }\n",
    "        torch.save(state, 'ckpt/simulation/checkpoint_0.pth.tar')\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed!\")\n",
    "print(f\"Best validation accuracy: {best_acc:.4f}\")\n",
    "print(f\"Best validation loss: {best_loss:.4f}\")\n",
    "\n",
    "# Plot training curves\n",
    "train_loss = [t[0] for t in performance]\n",
    "test_loss = [t[1] for t in performance]\n",
    "train_acc = [t[2] for t in performance]\n",
    "test_acc = [t[3] for t in performance]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss, label=\"Train Loss\")\n",
    "plt.plot(test_loss, label=\"Val Loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_acc, label=\"Train Acc\")\n",
    "plt.plot(test_acc, label=\"Val Acc\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/simulation/training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6771afb",
   "metadata": {},
   "source": [
    "## 7. Implement Few-Shot Learning Evaluation\n",
    "\n",
    "Test the trained TNC encoder for few-shot learning performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f53f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_few_shot_dataset(x_windows, y_windows, n_shot=5):\n",
    "    \"\"\"Create a few-shot dataset by sampling n_shot examples per class\"\"\"\n",
    "    unique_classes = np.unique(y_windows)\n",
    "    few_shot_x, few_shot_y = [], []\n",
    "    \n",
    "    for class_label in unique_classes:\n",
    "        class_indices = np.where(y_windows == class_label)[0]\n",
    "        n_samples = min(n_shot, len(class_indices))\n",
    "        selected_indices = np.random.choice(class_indices, n_samples, replace=False)\n",
    "        \n",
    "        few_shot_x.extend(x_windows[selected_indices])\n",
    "        few_shot_y.extend(y_windows[selected_indices])\n",
    "        \n",
    "        print(f\"Class {int(class_label)}: selected {n_samples} examples from {len(class_indices)} available\")\n",
    "    \n",
    "    return torch.stack(few_shot_x), torch.tensor(few_shot_y)\n",
    "\n",
    "def epoch_run_few_shot(encoder, classifier, dataloader, train=False, lr=0.01):\n",
    "    \"\"\"Training/evaluation loop for few-shot learning\"\"\"\n",
    "    if train:\n",
    "        classifier.train()\n",
    "        encoder.eval()  # Keep encoder frozen\n",
    "    else:\n",
    "        classifier.eval()\n",
    "        encoder.eval()\n",
    "        \n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    epoch_loss, epoch_acc = 0, 0\n",
    "    batch_count = 0\n",
    "    y_all, prediction_all = [], []\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        y = y.to(device)\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # Get embeddings from frozen encoder\n",
    "        with torch.no_grad():\n",
    "            encodings = encoder(x)\n",
    "        \n",
    "        # Train only the classifier\n",
    "        prediction = classifier(encodings)\n",
    "        state_prediction = torch.argmax(prediction, dim=1)\n",
    "        loss = loss_fn(prediction, y.long())\n",
    "        \n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        y_all.append(y.cpu().detach().numpy())\n",
    "        prediction_all.append(torch.nn.Softmax(-1)(prediction).detach().cpu().numpy())\n",
    "\n",
    "        epoch_acc += torch.eq(state_prediction, y).sum().item()/len(x)\n",
    "        epoch_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        \n",
    "    y_all = np.concatenate(y_all, 0)\n",
    "    prediction_all = np.concatenate(prediction_all, 0)\n",
    "    prediction_class_all = np.argmax(prediction_all, -1)\n",
    "    y_onehot_all = np.zeros(prediction_all.shape)\n",
    "    y_onehot_all[np.arange(len(y_onehot_all)), y_all.astype(int)] = 1\n",
    "    epoch_auc = roc_auc_score(y_onehot_all, prediction_all)\n",
    "    \n",
    "    return epoch_loss / batch_count, epoch_acc / batch_count, epoch_auc\n",
    "\n",
    "print(\"‚úÖ Few-shot learning functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346b332",
   "metadata": {},
   "source": [
    "## 8. Test Few-Shot Performance\n",
    "\n",
    "Run few-shot learning experiments with different numbers of shots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e464b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained encoder\n",
    "checkpoint = torch.load('ckpt/simulation/checkpoint_0.pth.tar')\n",
    "encoder = RnnEncoder(hidden_size=100, in_channel=3, encoding_size=10, device=device)\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "encoder.eval()\n",
    "encoder.to(device)\n",
    "\n",
    "# Load data and prepare windows\n",
    "with open('data/simulated_data/x_train.pkl', 'rb') as f:\n",
    "    x = pickle.load(f)\n",
    "with open('data/simulated_data/state_train.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)\n",
    "with open('data/simulated_data/x_test.pkl', 'rb') as f:\n",
    "    x_test = pickle.load(f)\n",
    "with open('data/simulated_data/state_test.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "\n",
    "# Convert to windows\n",
    "T = x.shape[-1]\n",
    "x_window = np.split(x[:, :, :window_size * (T // window_size)], (T // window_size), -1)\n",
    "y_window = np.concatenate(np.split(y[:, :window_size * (T // window_size)], (T // window_size), -1), 0).astype(int)\n",
    "x_window = torch.Tensor(np.concatenate(x_window, 0))\n",
    "y_window = torch.Tensor(np.array([np.bincount(yy).argmax() for yy in y_window]))\n",
    "\n",
    "# Test set\n",
    "x_window_test = np.split(x_test[:, :, :window_size * (T // window_size)], (T // window_size), -1)\n",
    "y_window_test = np.concatenate(np.split(y_test[:, :window_size * (T // window_size)], (T // window_size), -1), 0).astype(int)\n",
    "x_window_test = torch.Tensor(np.concatenate(x_window_test, 0))\n",
    "y_window_test = torch.Tensor(np.array([np.bincount(yy).argmax() for yy in y_window_test]))\n",
    "\n",
    "testset = torch.utils.data.TensorDataset(x_window_test, y_window_test)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)\n",
    "\n",
    "# Run few-shot experiments\n",
    "shot_numbers = [1, 5, 10, 20]\n",
    "n_trials = 5\n",
    "results = {}\n",
    "\n",
    "print(\"\\nüéØ Running Few-Shot Learning Experiments\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for n_shot in shot_numbers:\n",
    "    print(f\"\\nüìä Testing {n_shot}-shot learning...\")\n",
    "    trial_accuracies = []\n",
    "    trial_aucs = []\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        # Create few-shot training set\n",
    "        few_shot_x, few_shot_y = create_few_shot_dataset(x_window, y_window, n_shot)\n",
    "        \n",
    "        # Create classifier\n",
    "        classifier = StateClassifier(input_size=10, output_size=4).to(device)\n",
    "        \n",
    "        # Create few-shot train loader\n",
    "        few_shot_dataset = torch.utils.data.TensorDataset(few_shot_x, few_shot_y)\n",
    "        few_shot_loader = torch.utils.data.DataLoader(few_shot_dataset, \n",
    "                                                      batch_size=min(32, len(few_shot_x)), \n",
    "                                                      shuffle=True)\n",
    "        \n",
    "        # Train classifier\n",
    "        best_acc = 0\n",
    "        for epoch in range(50):\n",
    "            train_loss, train_acc, train_auc = epoch_run_few_shot(\n",
    "                encoder, classifier, few_shot_loader, train=True, lr=0.01)\n",
    "            \n",
    "            test_loss, test_acc, test_auc = epoch_run_few_shot(\n",
    "                encoder, classifier, test_loader, train=False)\n",
    "            \n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                best_test_auc = test_auc\n",
    "        \n",
    "        trial_accuracies.append(best_acc)\n",
    "        trial_aucs.append(best_test_auc)\n",
    "        print(f\"  Trial {trial + 1}: Accuracy {best_acc:.3f}, AUC {best_test_auc:.3f}\")\n",
    "    \n",
    "    mean_acc = np.mean(trial_accuracies)\n",
    "    std_acc = np.std(trial_accuracies)\n",
    "    mean_auc = np.mean(trial_aucs)\n",
    "    std_auc = np.std(trial_aucs)\n",
    "    \n",
    "    results[n_shot] = {'acc': (mean_acc, std_acc), 'auc': (mean_auc, std_auc)}\n",
    "    \n",
    "    print(f\"\\n  üìà {n_shot}-shot Results:\")\n",
    "    print(f\"     Accuracy: {mean_acc:.3f} ¬± {std_acc:.3f}\")\n",
    "    print(f\"     AUC: {mean_auc:.3f} ¬± {std_auc:.3f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "accs = [results[n]['acc'][0] for n in shot_numbers]\n",
    "acc_stds = [results[n]['acc'][1] for n in shot_numbers]\n",
    "plt.errorbar(shot_numbers, accs, yerr=acc_stds, marker='o', capsize=5)\n",
    "plt.xlabel('Number of Shots')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Few-Shot Learning Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "aucs = [results[n]['auc'][0] for n in shot_numbers]\n",
    "auc_stds = [results[n]['auc'][1] for n in shot_numbers]\n",
    "plt.errorbar(shot_numbers, aucs, yerr=auc_stds, marker='s', capsize=5, color='orange')\n",
    "plt.xlabel('Number of Shots')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('Few-Shot Learning AUC')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/simulation/few_shot_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéâ Few-shot learning evaluation completed!\")\n",
    "print(\"\\nüìã Summary:\")\n",
    "for n_shot, metrics in results.items():\n",
    "    acc_mean, acc_std = metrics['acc']\n",
    "    auc_mean, auc_std = metrics['auc']\n",
    "    print(f\"  {n_shot:2d}-shot: Acc {acc_mean:.3f}¬±{acc_std:.3f}, AUC {auc_mean:.3f}¬±{auc_std:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3dd51",
   "metadata": {},
   "source": [
    "## 9. Save Model to Google Drive\n",
    "\n",
    "Save the trained model and results to Google Drive for persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e609d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complete model package\n",
    "model_package = {\n",
    "    'encoder_state_dict': encoder.state_dict(),\n",
    "    'model_config': {\n",
    "        'hidden_size': 100,\n",
    "        'in_channel': 3,\n",
    "        'encoding_size': 10,\n",
    "        'window_size': 50,\n",
    "        'n_states': 4\n",
    "    },\n",
    "    'training_config': {\n",
    "        'w': w,\n",
    "        'lr': lr,\n",
    "        'epochs': n_epochs,\n",
    "        'best_accuracy': best_acc\n",
    "    },\n",
    "    'few_shot_results': results\n",
    "}\n",
    "\n",
    "# Save complete package\n",
    "torch.save(model_package, 'ckpt/simulation/tnc_complete_model.pth')\n",
    "\n",
    "# Also save a simple version for local use\n",
    "simple_checkpoint = {\n",
    "    'epoch': n_epochs,\n",
    "    'encoder_state_dict': encoder.state_dict(),\n",
    "    'best_accuracy': best_acc\n",
    "}\n",
    "torch.save(simple_checkpoint, 'ckpt/simulation/checkpoint_0.pth.tar')\n",
    "\n",
    "print(\"‚úÖ Model saved to Google Drive!\")\n",
    "print(f\"üìÅ Location: {workspace_path}/ckpt/simulation/\")\n",
    "print(\"üìÑ Files saved:\")\n",
    "print(\"   - tnc_complete_model.pth (full package with results)\")\n",
    "print(\"   - checkpoint_0.pth.tar (compatible with local evaluation)\")\n",
    "\n",
    "# Save results summary\n",
    "with open('few_shot_results_summary.txt', 'w') as f:\n",
    "    f.write(\"TNC Few-Shot Learning Results\\n\")\n",
    "    f.write(\"=\"*40 + \"\\n\\n\")\n",
    "    f.write(f\"Training completed with best validation accuracy: {best_acc:.4f}\\n\\n\")\n",
    "    f.write(\"Few-Shot Learning Performance:\\n\")\n",
    "    for n_shot, metrics in results.items():\n",
    "        acc_mean, acc_std = metrics['acc']\n",
    "        auc_mean, auc_std = metrics['auc']\n",
    "        f.write(f\"  {n_shot:2d}-shot: Acc {acc_mean:.3f}¬±{acc_std:.3f}, AUC {auc_mean:.3f}¬±{auc_std:.3f}\\n\")\n",
    "\n",
    "print(\"\\nüìä Results summary saved to few_shot_results_summary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad3da32",
   "metadata": {},
   "source": [
    "## 10. Download Model for Local Use\n",
    "\n",
    "Download the trained model files to use locally on your MacBook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f46009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Create a zip file with all necessary files for local use\n",
    "zip_filename = 'tnc_trained_model.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
    "    # Add model checkpoints\n",
    "    zipf.write('ckpt/simulation/checkpoint_0.pth.tar', 'ckpt/simulation/checkpoint_0.pth.tar')\n",
    "    zipf.write('ckpt/simulation/tnc_complete_model.pth', 'ckpt/simulation/tnc_complete_model.pth')\n",
    "    \n",
    "    # Add training plots\n",
    "    if os.path.exists('plots/simulation/training_curves.png'):\n",
    "        zipf.write('plots/simulation/training_curves.png', 'plots/simulation/training_curves.png')\n",
    "    if os.path.exists('plots/simulation/few_shot_results.png'):\n",
    "        zipf.write('plots/simulation/few_shot_results.png', 'plots/simulation/few_shot_results.png')\n",
    "    \n",
    "    # Add results summary\n",
    "    zipf.write('few_shot_results_summary.txt', 'few_shot_results_summary.txt')\n",
    "\n",
    "print(f\"üì¶ Created {zip_filename} with all necessary files\")\n",
    "print(\"\\nüì• Downloading model package...\")\n",
    "\n",
    "# Download the zip file\n",
    "files.download(zip_filename)\n",
    "\n",
    "print(\"\\n‚úÖ Download complete!\")\n",
    "print(\"\\nüè† To use locally on your MacBook:\")\n",
    "print(\"1. Extract the zip file in your TNC project directory\")\n",
    "print(\"2. The checkpoint_0.pth.tar should go in: ckpt/simulation/\")\n",
    "print(\"3. Then run: python -m evaluations.few_shot_test --data simulation --shots 5\")\n",
    "print(\"\\nüéØ Expected 5-shot performance on your local machine:\")\n",
    "if 5 in results:\n",
    "    acc_mean, acc_std = results[5]['acc']\n",
    "    auc_mean, auc_std = results[5]['auc']\n",
    "    print(f\"   Accuracy: {acc_mean:.3f} ¬± {acc_std:.3f}\")\n",
    "    print(f\"   AUC: {auc_mean:.3f} ¬± {auc_std:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1de1cdb",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You have successfully:\n",
    "\n",
    "‚úÖ **Trained a TNC model** on simulation data using GPU acceleration  \n",
    "‚úÖ **Evaluated few-shot learning** performance with 1, 5, 10, and 20 shots  \n",
    "‚úÖ **Downloaded the trained model** for local use on your MacBook  \n",
    "\n",
    "### üè† Next Steps on Your MacBook:\n",
    "\n",
    "1. **Extract the downloaded zip** in your TNC project directory\n",
    "2. **Test the model locally**:\n",
    "   ```bash\n",
    "   python -m evaluations.few_shot_test --data simulation --shots 5\n",
    "   ```\n",
    "\n",
    "### üìä Key Results:\n",
    "\n",
    "The TNC model learned meaningful representations that enable effective few-shot learning. Even with just **5 examples per class**, the model achieves good classification performance, demonstrating the power of unsupervised representation learning!\n",
    "\n",
    "### üî¨ What TNC Learned:\n",
    "\n",
    "- **Temporal patterns** in the simulated data\n",
    "- **Neighborhood relationships** between similar time windows  \n",
    "- **Robust embeddings** that generalize with few examples\n",
    "\n",
    "This approach works because TNC learns rich representations from **unlabeled data** first, then only needs a few labeled examples to learn decision boundaries!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
