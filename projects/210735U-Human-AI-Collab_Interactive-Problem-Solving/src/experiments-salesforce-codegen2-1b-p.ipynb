{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:38:11.559444Z","iopub.execute_input":"2025-09-30T07:38:11.559788Z","iopub.status.idle":"2025-09-30T07:38:15.532498Z","shell.execute_reply.started":"2025-09-30T07:38:11.559758Z","shell.execute_reply":"2025-09-30T07:38:15.531649Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.56.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.35.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Local Inference on GPU \nModel page: https://huggingface.co/Salesforce/codegen2-1B_P\n\nâš ï¸ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/Salesforce/codegen2-1B_P)\n\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) ðŸ™","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"Salesforce/codegen2-1B_P\", trust_remote_code=True)","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:38:35.041159Z","iopub.execute_input":"2025-09-30T07:38:35.041466Z","iopub.status.idle":"2025-09-30T07:38:40.237772Z","shell.execute_reply.started":"2025-09-30T07:38:35.041439Z","shell.execute_reply":"2025-09-30T07:38:40.237169Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nimport subprocess, tempfile, os, sys\nimport pandas as pd","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:38:40.239165Z","iopub.execute_input":"2025-09-30T07:38:40.239467Z","iopub.status.idle":"2025-09-30T07:38:40.243406Z","shell.execute_reply.started":"2025-09-30T07:38:40.239437Z","shell.execute_reply":"2025-09-30T07:38:40.242732Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen2-1B_P\", trust_remote_code=True)\n\n# Ensure model is fully loaded on the chosen device\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Salesforce/codegen2-1B_P\",\n    trust_remote_code=True,\n    torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32\n).to(device)\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:38:40.243969Z","iopub.execute_input":"2025-09-30T07:38:40.244125Z","iopub.status.idle":"2025-09-30T07:38:46.045935Z","shell.execute_reply.started":"2025-09-30T07:38:40.244113Z","shell.execute_reply":"2025-09-30T07:38:46.045163Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"CodeGenForCausalLM(\n  (transformer): CodeGenModel(\n    (wte): Embedding(51200, 2048)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-15): 16 x CodeGenBlock(\n        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n        (attn): CodeGenAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (qkv_proj): Linear(in_features=2048, out_features=6144, bias=False)\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n        )\n        (mlp): CodeGenMLP(\n          (fc_in): Linear(in_features=2048, out_features=8192, bias=True)\n          (fc_out): Linear(in_features=8192, out_features=2048, bias=True)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=2048, out_features=51200, bias=True)\n)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"\ndataset = load_dataset(\"openai/openai_humaneval\")\nproblems = dataset[\"test\"]  # 164 tasks\n\nprint(\"Number of problems:\", len(problems))\nprint(\"Example problem:\\n\", problems[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:39:15.400409Z","iopub.execute_input":"2025-09-30T07:39:15.400969Z","iopub.status.idle":"2025-09-30T07:39:15.962874Z","shell.execute_reply.started":"2025-09-30T07:39:15.400942Z","shell.execute_reply":"2025-09-30T07:39:15.962211Z"}},"outputs":[{"name":"stdout","text":"Number of problems: 164\nExample problem:\n {'task_id': 'HumanEval/0', 'prompt': 'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n', 'canonical_solution': '    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n', 'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\\n\\n\", 'entry_point': 'has_close_elements'}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Prompt Engineering (augmentation at inference time)","metadata":{}},{"cell_type":"code","source":"prompt_format_1= '''\n# Write a correct Python function.\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:39:22.614610Z","iopub.execute_input":"2025-09-30T07:39:22.615343Z","iopub.status.idle":"2025-09-30T07:39:22.619171Z","shell.execute_reply.started":"2025-09-30T07:39:22.615317Z","shell.execute_reply":"2025-09-30T07:39:22.618505Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"prompt_format_2 = '''\nWrite a python function for above prompt. \nFunction should be 100% accurate and should do the task asked in the prompts.\n Follow these rules: \n 1) Include type hints\n 2) Handle edge cases \n 3) Return correct output\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:39:23.293769Z","iopub.execute_input":"2025-09-30T07:39:23.294357Z","iopub.status.idle":"2025-09-30T07:39:23.298681Z","shell.execute_reply.started":"2025-09-30T07:39:23.294319Z","shell.execute_reply":"2025-09-30T07:39:23.297571Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"prompt_format_3 = '''\nTask Description:\n{task_description}\nYour goal is to write a Python function that solves the task accurately.\nFollow these detailed instructions carefully:\n1) Include type hints for function parameters and return type.\n2) Handle edge cases, such as empty inputs, negative numbers, or special characters.\n3) Return the correct output exactly as expected.\n4) Use clear variable names and maintain clean code style.\n5) Include an example showing input and expected output.\n\nExample:\nTask: Write a function add_numbers(a, b) that returns the sum of two numbers.\nInput: a = 3, b = 5\nExpected Output: 8\nFunction:\ndef add_numbers(a: int, b: int) -> int:\n    return a + b\n\nNow, write the solution for the task above:\ndef\n'''\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt_styles = [\n    # 1. Minimal (baseline)\n    lambda task: task,\n    \n    # 2. Instruction style\n    lambda task: f\"# Python 3\\n# Task: {task} {prompt_format_1}\",\n    \n    # 3. Step-by-step / clarify inputs\n    lambda task: f\"# Python 3\\n# Task: {task} {prompt_format_2}\"\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:39:25.372097Z","iopub.execute_input":"2025-09-30T07:39:25.372625Z","iopub.status.idle":"2025-09-30T07:39:25.376511Z","shell.execute_reply.started":"2025-09-30T07:39:25.372600Z","shell.execute_reply":"2025-09-30T07:39:25.375870Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Measure Pass@K","metadata":{}},{"cell_type":"markdown","source":"For each test case, we can generate multiple samples and try pass @1 , pass@2 , pass@10 etc","metadata":{}},{"cell_type":"code","source":"def generate_multiple(prompt, n_samples=5, temperature=0.8, top_p=0.95, max_tokens=256):\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n    outputs = model.generate(\n        input_ids,\n        max_length=input_ids.shape[1] + max_tokens,\n        temperature=temperature,\n        top_p=top_p,\n        do_sample=True,\n        num_return_sequences=n_samples,\n        pad_token_id=tokenizer.eos_token_id,\n    )\n    decoded = [tokenizer.decode(o, skip_special_tokens=True)[len(prompt):].strip()\n               for o in outputs]\n    return decoded\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:39:29.140256Z","iopub.execute_input":"2025-09-30T07:39:29.140530Z","iopub.status.idle":"2025-09-30T07:39:29.145803Z","shell.execute_reply.started":"2025-09-30T07:39:29.140510Z","shell.execute_reply":"2025-09-30T07:39:29.144810Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def run_tests(code, test_code):\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".py\", mode=\"w\") as f:\n        f.write(code + \"\\n\\n\")\n        f.write(test_code + \"\\n\")\n        test_file = f.name\n\n    try:\n        result = subprocess.run([sys.executable, test_file],\n                                capture_output=True, text=True, timeout=5)\n        passed = (result.returncode == 0)\n    except subprocess.TimeoutExpired:\n        passed = False\n    finally:\n        os.remove(test_file)\n\n    return passed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:39:30.732079Z","iopub.execute_input":"2025-09-30T07:39:30.732801Z","iopub.status.idle":"2025-09-30T07:39:30.737647Z","shell.execute_reply.started":"2025-09-30T07:39:30.732774Z","shell.execute_reply":"2025-09-30T07:39:30.736880Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def evaluate_humaneval(n=10, n_samples=5, temps=[0.2,0.5 ,  0.8]):\n    results = []\n    for i in range(n):\n        problem = problems[i]\n        task_id = problem[\"task_id\"]\n        test_code = problem[\"test\"]\n        base_prompt = problem[\"prompt\"]\n\n        for style_idx, style_fn in enumerate(prompt_styles):\n            prompt = style_fn(base_prompt)\n            for temp in temps:\n                codes = generate_multiple(prompt, n_samples=n_samples, temperature=temp)\n                # Compute pass@1, pass@k\n                passes = [run_tests(c, test_code) for c in codes]\n                pass_at_1 = passes[0]\n                pass_at_k = {k: any(passes[:k]) for k in [5, 10]}\n                results.append({\n                    \"task_id\": task_id,\n                    \"prompt_style\": style_idx,\n                    \"temperature\": temp,\n                    \"pass@1\": pass_at_1,\n                    \"pass@5\": pass_at_k[5],\n                    \"pass@10\": pass_at_k[10],\n                    \"codes\": codes[:3]  # preview first 3 samples\n                })\n                print(f\"[{task_id}] style={style_idx} temp={temp} pass@1={pass_at_1} pass@5={pass_at_k[5]} pass@10={pass_at_k[10]}\")\n    return results\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:39:32.723849Z","iopub.execute_input":"2025-09-30T07:39:32.724096Z","iopub.status.idle":"2025-09-30T07:39:32.730360Z","shell.execute_reply.started":"2025-09-30T07:39:32.724079Z","shell.execute_reply":"2025-09-30T07:39:32.729623Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Run evaluation","metadata":{}},{"cell_type":"code","source":"print(\"Evaluation results for Salesforce/codegen2-1B_P after prompt optimization \")\nresults = evaluate_humaneval(n=10, n_samples=5, temps=[0.2, 0.8])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T07:39:37.110724Z","iopub.execute_input":"2025-09-30T07:39:37.111448Z","iopub.status.idle":"2025-09-30T08:15:58.215412Z","shell.execute_reply.started":"2025-09-30T07:39:37.111422Z","shell.execute_reply":"2025-09-30T08:15:58.214628Z"}},"outputs":[{"name":"stdout","text":"Evaluation results for Salesforce/codegen2-1B_P after prompt optimization \n","output_type":"stream"},{"name":"stderr","text":"/root/.cache/huggingface/modules/transformers_modules/Salesforce/codegen2-1B_P/47f05cd3c1b357745fbead74202fc1efe87b6d25/modeling_codegen.py:167: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /pytorch/aten/src/ATen/native/TensorCompare.cpp:529.)\n  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n","output_type":"stream"},{"name":"stdout","text":"[HumanEval/0] style=0 temp=0.2 pass@1=False pass@5=False pass@10=False\n[HumanEval/0] style=0 temp=0.8 pass@1=False pass@5=False pass@10=False\n[HumanEval/0] style=1 temp=0.2 pass@1=True pass@5=True pass@10=True\n[HumanEval/0] style=1 temp=0.8 pass@1=False pass@5=True pass@10=True\n[HumanEval/0] style=2 temp=0.2 pass@1=False pass@5=False pass@10=False\n[HumanEval/0] style=2 temp=0.8 pass@1=False pass@5=False pass@10=False\n[HumanEval/1] style=0 temp=0.2 pass@1=True pass@5=True pass@10=True\n[HumanEval/1] style=0 temp=0.8 pass@1=True pass@5=True pass@10=True\n[HumanEval/1] style=1 temp=0.2 pass@1=False pass@5=True pass@10=True\n[HumanEval/1] style=1 temp=0.8 pass@1=True pass@5=True pass@10=True\n[HumanEval/1] style=2 temp=0.2 pass@1=False pass@5=False pass@10=False\n[HumanEval/1] style=2 temp=0.8 pass@1=False pass@5=False pass@10=False\n[HumanEval/2] style=0 temp=0.2 pass@1=False pass@5=False pass@10=False\n[HumanEval/2] style=0 temp=0.8 pass@1=False pass@5=False pass@10=False\n[HumanEval/2] style=1 temp=0.2 pass@1=True pass@5=True pass@10=True\n[HumanEval/2] style=1 temp=0.8 pass@1=False pass@5=True pass@10=True\n[HumanEval/2] style=2 temp=0.2 pass@1=False pass@5=False pass@10=False\n[HumanEval/2] style=2 temp=0.8 pass@1=False pass@5=False pass@10=False\n[HumanEval/3] style=0 temp=0.2 pass@1=False pass@5=False pass@10=False\n[HumanEval/3] style=0 temp=0.8 pass@1=False pass@5=True pass@10=True\n[HumanEval/3] style=1 temp=0.2 pass@1=True pass@5=True pass@10=True\n[HumanEval/3] style=1 temp=0.8 pass@1=True pass@5=True pass@10=True\n[HumanEval/3] style=2 temp=0.2 pass@1=False pass@5=False pass@10=False\n[HumanEval/3] style=2 temp=0.8 pass@1=False pass@5=False pass@10=False\n[HumanEval/4] style=0 temp=0.2 pass@1=False pass@5=False pass@10=False\n[HumanEval/4] style=0 temp=0.8 pass@1=False pass@5=False pass@10=False\n[HumanEval/4] style=1 temp=0.2 pass@1=True pass@5=True pass@10=True\n[HumanEval/4] style=1 temp=0.8 pass@1=True pass@5=True pass@10=True\n[HumanEval/4] style=2 temp=0.2 pass@1=False pass@5=False pass@10=False\n[HumanEval/4] style=2 temp=0.8 pass@1=False pass@5=False pass@10=False\n[HumanEval/5] style=0 temp=0.2 pass@1=False pass@5=False pass@10=False\n[HumanEval/5] style=0 temp=0.8 pass@1=False pass@5=False pass@10=False\n[HumanEval/5] style=1 temp=0.2 pass@1=True pass@5=True pass@10=True\n[HumanEval/5] style=1 temp=0.8 pass@1=True pass@5=True pass@10=True\n[HumanEval/5] style=2 temp=0.2 pass@1=False pass@5=False pass@10=False\n[HumanEval/5] style=2 temp=0.8 pass@1=False pass@5=False pass@10=False\n[HumanEval/6] style=0 temp=0.2 pass@1=False pass@5=False pass@10=False\n[HumanEval/6] style=0 temp=0.8 pass@1=False pass@5=False pass@10=False\n[HumanEval/6] style=1 temp=0.2 pass@1=False pass@5=True pass@10=True\n[HumanEval/6] style=1 temp=0.8 pass@1=True pass@5=True pass@10=True\n[HumanEval/6] style=2 temp=0.2 pass@1=False pass@5=False pass@10=False\n[HumanEval/6] style=2 temp=0.8 pass@1=False pass@5=False pass@10=False\n[HumanEval/7] style=0 temp=0.2 pass@1=False pass@5=False pass@10=False\n[HumanEval/7] style=0 temp=0.8 pass@1=False pass@5=False pass@10=False\n[HumanEval/7] style=1 temp=0.2 pass@1=True pass@5=True pass@10=True\n[HumanEval/7] style=1 temp=0.8 pass@1=True pass@5=True pass@10=True\n[HumanEval/7] style=2 temp=0.2 pass@1=False pass@5=False pass@10=False\n[HumanEval/7] style=2 temp=0.8 pass@1=False pass@5=False pass@10=False\n[HumanEval/8] style=0 temp=0.2 pass@1=False pass@5=False pass@10=False\n[HumanEval/8] style=0 temp=0.8 pass@1=False pass@5=False pass@10=False\n[HumanEval/8] style=1 temp=0.2 pass@1=True pass@5=True pass@10=True\n[HumanEval/8] style=1 temp=0.8 pass@1=True pass@5=True pass@10=True\n[HumanEval/8] style=2 temp=0.2 pass@1=False pass@5=False pass@10=False\n[HumanEval/8] style=2 temp=0.8 pass@1=False pass@5=False pass@10=False\n[HumanEval/9] style=0 temp=0.2 pass@1=False pass@5=False pass@10=False\n[HumanEval/9] style=0 temp=0.8 pass@1=False pass@5=True pass@10=True\n[HumanEval/9] style=1 temp=0.2 pass@1=True pass@5=True pass@10=True\n[HumanEval/9] style=1 temp=0.8 pass@1=True pass@5=True pass@10=True\n[HumanEval/9] style=2 temp=0.2 pass@1=False pass@5=False pass@10=False\n[HumanEval/9] style=2 temp=0.8 pass@1=False pass@5=False pass@10=False\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# Summarize results","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(results)\nsummary = df.groupby([\"prompt_style\", \"temperature\"])[[\"pass@1\", \"pass@5\", \"pass@10\"]].mean()\nprint(\"\\nSummary table (mean pass@k by style and temperature):\")\nprint(summary)\n\ndf.to_csv(\"humaneval_codegen2_passk_results_with_prompting.csv\", index=False)\nprint(\"\\nSaved detailed results to humaneval_codegen2_passk_results.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Augmentation","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameter Tuning","metadata":{}},{"cell_type":"markdown","source":"**Try systematically:**\n\ntemperature âˆˆ {0.2, 0.6, 0.8, 1.0} \n\ntop_p âˆˆ {0.9, 0.95}\n\nnum_return_sequences âˆˆ {1, 20, 100}","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}