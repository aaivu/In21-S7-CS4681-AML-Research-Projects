{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T04:55:27.172612Z","iopub.execute_input":"2025-09-30T04:55:27.173354Z","iopub.status.idle":"2025-09-30T04:55:31.109843Z","shell.execute_reply.started":"2025-09-30T04:55:27.173329Z","shell.execute_reply":"2025-09-30T04:55:31.108924Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.56.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.35.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# **Salesforce/codegen2-1B_P**","metadata":{}},{"cell_type":"markdown","source":"## Local Inference on GPU \nModel page: https://huggingface.co/Salesforce/codegen2-1B_P\n\nâš ï¸ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/Salesforce/codegen2-1B_P)\n\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) ðŸ™","metadata":{"colab_type":"text"}},{"cell_type":"code","source":"# Use a pipeline as a high-level helper\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=\"Salesforce/codegen2-1B_P\", trust_remote_code=True)","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T04:55:35.938451Z","iopub.execute_input":"2025-09-30T04:55:35.938760Z","iopub.status.idle":"2025-09-30T04:55:41.149333Z","shell.execute_reply.started":"2025-09-30T04:55:35.938729Z","shell.execute_reply":"2025-09-30T04:55:41.148694Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# print(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T04:55:45.637836Z","iopub.execute_input":"2025-09-30T04:55:45.638127Z","iopub.status.idle":"2025-09-30T04:55:45.642470Z","shell.execute_reply.started":"2025-09-30T04:55:45.638108Z","shell.execute_reply":"2025-09-30T04:55:45.641689Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"Load Tokenizer and model\n","metadata":{}},{"cell_type":"code","source":"# # Load model directly\n# from transformers import AutoTokenizer, AutoModelForCausalLM\n\n# tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen2-1B_P\", trust_remote_code=True)\n# model = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen2-1B_P\", trust_remote_code=True)","metadata":{"colab_type":"code","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T04:55:49.442127Z","iopub.execute_input":"2025-09-30T04:55:49.442832Z","iopub.status.idle":"2025-09-30T04:55:53.049803Z","shell.execute_reply.started":"2025-09-30T04:55:49.442806Z","shell.execute_reply":"2025-09-30T04:55:53.049046Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nimport subprocess, tempfile, os, sys\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T04:57:11.545252Z","iopub.execute_input":"2025-09-30T04:57:11.545898Z","iopub.status.idle":"2025-09-30T04:57:11.549275Z","shell.execute_reply.started":"2025-09-30T04:57:11.545873Z","shell.execute_reply":"2025-09-30T04:57:11.548606Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\ntokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen2-1B_P\", trust_remote_code=True)\n\n# Ensure model is fully loaded on the chosen device\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Salesforce/codegen2-1B_P\",\n    trust_remote_code=True,\n    torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32\n).to(device)\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T04:56:55.519385Z","iopub.execute_input":"2025-09-30T04:56:55.519672Z","iopub.status.idle":"2025-09-30T04:57:01.182326Z","shell.execute_reply.started":"2025-09-30T04:56:55.519652Z","shell.execute_reply":"2025-09-30T04:57:01.181569Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"CodeGenForCausalLM(\n  (transformer): CodeGenModel(\n    (wte): Embedding(51200, 2048)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-15): 16 x CodeGenBlock(\n        (ln_1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n        (attn): CodeGenAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (qkv_proj): Linear(in_features=2048, out_features=6144, bias=False)\n          (out_proj): Linear(in_features=2048, out_features=2048, bias=False)\n        )\n        (mlp): CodeGenMLP(\n          (fc_in): Linear(in_features=2048, out_features=8192, bias=True)\n          (fc_out): Linear(in_features=8192, out_features=2048, bias=True)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=2048, out_features=51200, bias=True)\n)"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"# Load Dataset","metadata":{}},{"cell_type":"markdown","source":"**here I am using HumanEval dataset from openai , accessing it via huggingface** ","metadata":{}},{"cell_type":"code","source":"\ndataset = load_dataset(\"openai/openai_humaneval\")\nproblems = dataset[\"test\"]  # 164 tasks\n\nprint(\"Number of problems:\", len(problems))\nprint(\"Example problem:\\n\", problems[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T04:57:13.465867Z","iopub.execute_input":"2025-09-30T04:57:13.466193Z","iopub.status.idle":"2025-09-30T04:57:13.883721Z","shell.execute_reply.started":"2025-09-30T04:57:13.466167Z","shell.execute_reply":"2025-09-30T04:57:13.882995Z"}},"outputs":[{"name":"stdout","text":"Number of problems: 164\nExample problem:\n {'task_id': 'HumanEval/0', 'prompt': 'from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n', 'canonical_solution': '    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n', 'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\\n\\n\", 'entry_point': 'has_close_elements'}\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# **Code generation**","metadata":{}},{"cell_type":"code","source":"def generate_code(prompt, temperature=0.8, top_p=0.95, max_tokens=256):\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n    output = model.generate(\n        input_ids,\n        max_length=input_ids.shape[1] + max_tokens,\n        temperature=temperature,\n        top_p=top_p,\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id,\n    )\n    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n    return decoded[len(prompt):].strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T04:57:24.740847Z","iopub.execute_input":"2025-09-30T04:57:24.741156Z","iopub.status.idle":"2025-09-30T04:57:24.745727Z","shell.execute_reply.started":"2025-09-30T04:57:24.741113Z","shell.execute_reply":"2025-09-30T04:57:24.744910Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# **Run tests**","metadata":{}},{"cell_type":"code","source":"def run_tests(code, test_code):\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".py\", mode=\"w\") as f:\n        f.write(code + \"\\n\\n\")\n        f.write(test_code + \"\\n\")\n        test_file = f.name\n\n    try:\n        result = subprocess.run([sys.executable, test_file],\n                                capture_output=True, text=True, timeout=5)\n        passed = (result.returncode == 0)\n    except subprocess.TimeoutExpired:\n        passed = False\n    finally:\n        os.remove(test_file)\n\n    return passed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T04:57:27.644222Z","iopub.execute_input":"2025-09-30T04:57:27.644487Z","iopub.status.idle":"2025-09-30T04:57:27.649410Z","shell.execute_reply.started":"2025-09-30T04:57:27.644468Z","shell.execute_reply":"2025-09-30T04:57:27.648818Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"# **Evaluate using a subset , here 10 tests**","metadata":{}},{"cell_type":"code","source":"def evaluate_subset(n=10, temps=[0.2, 0.8]):\n    results = []\n    for i in range(n):\n        problem = problems[i]   # <-- access dict row\n        prompt = problem[\"prompt\"]\n        test_code = problem[\"test\"]\n\n        for temp in temps:\n            code = generate_code(prompt, temperature=temp)\n            passed = run_tests(code, test_code)\n            results.append({\n                \"task_id\": problem[\"task_id\"],\n                \"temp\": temp,\n                \"passed\": passed,\n                \"code_snippet\": code[:200]\n            })\n            print(f\"[{problem['task_id']}] temp={temp}, passed={passed}\")\n    return results\n\nprint(\"Baseline performance - Salesforce/codegen2-1B_P\")\nresults = evaluate_subset(n=10, temps=[0.2, 0.8])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T04:57:31.897505Z","iopub.execute_input":"2025-09-30T04:57:31.897767Z","iopub.status.idle":"2025-09-30T05:00:42.127101Z","shell.execute_reply.started":"2025-09-30T04:57:31.897747Z","shell.execute_reply":"2025-09-30T05:00:42.126463Z"}},"outputs":[{"name":"stderr","text":"/root/.cache/huggingface/modules/transformers_modules/Salesforce/codegen2-1B_P/47f05cd3c1b357745fbead74202fc1efe87b6d25/modeling_codegen.py:167: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /pytorch/aten/src/ATen/native/TensorCompare.cpp:529.)\n  attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n","output_type":"stream"},{"name":"stdout","text":"[HumanEval/0] temp=0.2, passed=False\n[HumanEval/0] temp=0.8, passed=False\n[HumanEval/1] temp=0.2, passed=False\n[HumanEval/1] temp=0.8, passed=False\n[HumanEval/2] temp=0.2, passed=False\n[HumanEval/2] temp=0.8, passed=False\n[HumanEval/3] temp=0.2, passed=False\n[HumanEval/3] temp=0.8, passed=False\n[HumanEval/4] temp=0.2, passed=False\n[HumanEval/4] temp=0.8, passed=False\n[HumanEval/5] temp=0.2, passed=False\n[HumanEval/5] temp=0.8, passed=False\n[HumanEval/6] temp=0.2, passed=False\n[HumanEval/6] temp=0.8, passed=False\n[HumanEval/7] temp=0.2, passed=False\n[HumanEval/7] temp=0.8, passed=False\n[HumanEval/8] temp=0.2, passed=False\n[HumanEval/8] temp=0.8, passed=False\n[HumanEval/9] temp=0.2, passed=False\n[HumanEval/9] temp=0.8, passed=False\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"df = pd.DataFrame(results)\nprint(\"\\nSummary (pass rate by temperature):\")\nprint(df.groupby(\"temp\")[\"passed\"].mean())\n\ndf.to_csv(\"humaneval_codegen2_results.csv\", index=False)\nprint(\"\\nSaved results to humaneval_codegen2_results.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}