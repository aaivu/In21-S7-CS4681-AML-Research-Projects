{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e0ed173",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-01T16:40:44.008809Z",
     "iopub.status.busy": "2025-10-01T16:40:44.008575Z",
     "iopub.status.idle": "2025-10-01T16:40:45.434573Z",
     "shell.execute_reply": "2025-10-01T16:40:45.433810Z"
    },
    "papermill": {
     "duration": 1.430194,
     "end_time": "2025-10-01T16:40:45.435763",
     "exception": false,
     "start_time": "2025-10-01T16:40:44.005569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ettsmall/ETTh2.csv\n",
      "/kaggle/input/ettsmall/ETTm2.csv\n",
      "/kaggle/input/ettsmall/ETTm1.csv\n",
      "/kaggle/input/ettsmall/ETTh1.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e058c7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T16:40:45.441159Z",
     "iopub.status.busy": "2025-10-01T16:40:45.440779Z",
     "iopub.status.idle": "2025-10-01T16:40:46.265849Z",
     "shell.execute_reply": "2025-10-01T16:40:46.264688Z"
    },
    "papermill": {
     "duration": 0.82956,
     "end_time": "2025-10-01T16:40:46.267900",
     "exception": false,
     "start_time": "2025-10-01T16:40:45.438340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ts2vec'...\r\n",
      "remote: Enumerating objects: 370, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (161/161), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (102/102), done.\u001b[K\r\n",
      "remote: Total 370 (delta 88), reused 113 (delta 59), pack-reused 209 (from 2)\u001b[K\r\n",
      "Receiving objects: 100% (370/370), 436.45 KiB | 13.64 MiB/s, done.\r\n",
      "Resolving deltas: 100% (198/198), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Niroshan2001/ts2vec.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1792092",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T16:40:46.277848Z",
     "iopub.status.busy": "2025-10-01T16:40:46.276959Z",
     "iopub.status.idle": "2025-10-01T16:40:46.802896Z",
     "shell.execute_reply": "2025-10-01T16:40:46.802125Z"
    },
    "papermill": {
     "duration": 0.531857,
     "end_time": "2025-10-01T16:40:46.804189",
     "exception": false,
     "start_time": "2025-10-01T16:40:46.272332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/ts2vec\n",
      "Branch 'Ridge' set up to track remote branch 'Ridge' from 'origin'.\r\n",
      "Switched to a new branch 'Ridge'\r\n"
     ]
    }
   ],
   "source": [
    "%cd ts2vec\n",
    "!git fetch origin\n",
    "!git checkout Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f5d777f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T16:40:46.809867Z",
     "iopub.status.busy": "2025-10-01T16:40:46.809604Z",
     "iopub.status.idle": "2025-10-01T16:40:56.712932Z",
     "shell.execute_reply": "2025-10-01T16:40:56.711961Z"
    },
    "papermill": {
     "duration": 9.908364,
     "end_time": "2025-10-01T16:40:56.714969",
     "exception": false,
     "start_time": "2025-10-01T16:40:46.806605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bottleneck in /usr/local/lib/python3.11/dist-packages (1.4.2)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bottleneck) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bottleneck) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bottleneck) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bottleneck) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bottleneck) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bottleneck) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bottleneck) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bottleneck) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bottleneck) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bottleneck) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bottleneck) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bottleneck) (2024.2.0)\r\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (0.14.4)\r\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.26.4)\r\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.15.3)\r\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (2.2.3)\r\n",
      "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.0.1)\r\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (25.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.22.3->statsmodels) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.22.3->statsmodels) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.22.3->statsmodels) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.22.3->statsmodels) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.22.3->statsmodels) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3,>=1.22.3->statsmodels) (2.4.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.22.3->statsmodels) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3,>=1.22.3->statsmodels) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.22.3->statsmodels) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3,>=1.22.3->statsmodels) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3,>=1.22.3->statsmodels) (2024.2.0)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bottleneck\n",
    "!pip install statsmodels\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef5737a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T16:40:56.721827Z",
     "iopub.status.busy": "2025-10-01T16:40:56.721310Z",
     "iopub.status.idle": "2025-10-01T16:40:57.019945Z",
     "shell.execute_reply": "2025-10-01T16:40:57.019089Z"
    },
    "papermill": {
     "duration": 0.303351,
     "end_time": "2025-10-01T16:40:57.021215",
     "exception": false,
     "start_time": "2025-10-01T16:40:56.717864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied /kaggle/input/ettsmall/ETTh1.csv -> datasets/ETTh1.csv\n",
      "Copied /kaggle/input/ettsmall/ETTh2.csv -> datasets/ETTh2.csv\n",
      "Copied /kaggle/input/ettsmall/ETTm1.csv -> datasets/ETTm1.csv\n",
      "Copied /kaggle/input/ettsmall/ETTm2.csv -> datasets/ETTm2.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Kaggle path to your CSV\n",
    "source_path = \"/kaggle/input/ettsmall/ETTh1.csv\"  # adjust if needed\n",
    "\n",
    "# Destination path expected by ts2vec\n",
    "dest_folder = \"datasets\"\n",
    "os.makedirs(dest_folder, exist_ok=True)\n",
    "dest_path = os.path.join(dest_folder, \"ETTh1.csv\")\n",
    "\n",
    "# Copy the CSV\n",
    "shutil.copyfile(source_path, dest_path)\n",
    "print(f\"Copied {source_path} -> {dest_path}\")\n",
    "\n",
    "# Kaggle path to your CSV\n",
    "source_path = \"/kaggle/input/ettsmall/ETTh2.csv\"  # adjust if needed\n",
    "\n",
    "# Destination path expected by ts2vec\n",
    "dest_folder = \"datasets\"\n",
    "os.makedirs(dest_folder, exist_ok=True)\n",
    "dest_path = os.path.join(dest_folder, \"ETTh2.csv\")\n",
    "\n",
    "# Copy the CSV\n",
    "shutil.copyfile(source_path, dest_path)\n",
    "print(f\"Copied {source_path} -> {dest_path}\")\n",
    "\n",
    "# Kaggle path to your CSV\n",
    "source_path = \"/kaggle/input/ettsmall/ETTm1.csv\"  # adjust if needed\n",
    "\n",
    "# Destination path expected by ts2vec\n",
    "dest_folder = \"datasets\"\n",
    "os.makedirs(dest_folder, exist_ok=True)\n",
    "dest_path = os.path.join(dest_folder, \"ETTm1.csv\")\n",
    "\n",
    "# Copy the CSV\n",
    "shutil.copyfile(source_path, dest_path)\n",
    "print(f\"Copied {source_path} -> {dest_path}\")\n",
    "\n",
    "\n",
    "# Kaggle path to your CSV\n",
    "source_path = \"/kaggle/input/ettsmall/ETTm2.csv\"  # adjust if needed\n",
    "\n",
    "# Destination path expected by ts2vec\n",
    "dest_folder = \"datasets\"\n",
    "os.makedirs(dest_folder, exist_ok=True)\n",
    "dest_path = os.path.join(dest_folder, \"ETTm2.csv\")\n",
    "\n",
    "# Copy the CSV\n",
    "shutil.copyfile(source_path, dest_path)\n",
    "print(f\"Copied {source_path} -> {dest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06c122db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T16:40:57.027879Z",
     "iopub.status.busy": "2025-10-01T16:40:57.027662Z",
     "iopub.status.idle": "2025-10-01T17:24:45.498749Z",
     "shell.execute_reply": "2025-10-01T17:24:45.497922Z"
    },
    "papermill": {
     "duration": 2628.475879,
     "end_time": "2025-10-01T17:24:45.500236",
     "exception": false,
     "start_time": "2025-10-01T16:40:57.024357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTm1\r\n",
      "Arguments: Namespace(dataset='ETTm1', run_name='forecast_univar', loader='forecast_csv_univar', gpu=0, batch_size=8, lr=0.001, repr_dims=320, max_train_length=3000, iters=None, epochs=None, save_every=None, seed=42, max_threads=8, eval=True, irregular=0)\r\n",
      "Loading data... done\r\n",
      "Epoch #0: loss=9.989359855651855\r\n",
      "Epoch #1: loss=7.78896427154541\r\n",
      "Epoch #2: loss=4.460978984832764\r\n",
      "Epoch #3: loss=3.3511788845062256\r\n",
      "Epoch #4: loss=3.563633680343628\r\n",
      "Epoch #5: loss=3.2374377250671387\r\n",
      "Epoch #6: loss=2.837550163269043\r\n",
      "Epoch #7: loss=2.809091091156006\r\n",
      "Epoch #8: loss=2.8129689693450928\r\n",
      "Epoch #9: loss=2.6590895652770996\r\n",
      "Epoch #10: loss=2.3633980751037598\r\n",
      "Epoch #11: loss=2.559986114501953\r\n",
      "Epoch #12: loss=2.522001266479492\r\n",
      "Epoch #13: loss=2.4188544750213623\r\n",
      "Epoch #14: loss=2.4259088039398193\r\n",
      "Epoch #15: loss=2.3281617164611816\r\n",
      "Epoch #16: loss=2.215205192565918\r\n",
      "Epoch #17: loss=2.1864776611328125\r\n",
      "Epoch #18: loss=2.098573684692383\r\n",
      "Epoch #19: loss=2.119596481323242\r\n",
      "Epoch #20: loss=2.1782233715057373\r\n",
      "Epoch #21: loss=1.995783805847168\r\n",
      "Epoch #22: loss=2.1721279621124268\r\n",
      "Epoch #23: loss=2.0089235305786133\r\n",
      "Epoch #24: loss=2.012910842895508\r\n",
      "Epoch #25: loss=1.8873637914657593\r\n",
      "Epoch #26: loss=1.9768342971801758\r\n",
      "Epoch #27: loss=1.9903315305709839\r\n",
      "Epoch #28: loss=1.847491979598999\r\n",
      "Epoch #29: loss=1.9833472967147827\r\n",
      "Epoch #30: loss=1.9095840454101562\r\n",
      "Epoch #31: loss=1.911112904548645\r\n",
      "Epoch #32: loss=1.7977675199508667\r\n",
      "Epoch #33: loss=1.746093511581421\r\n",
      "Epoch #34: loss=1.9261822700500488\r\n",
      "Epoch #35: loss=1.600064754486084\r\n",
      "Epoch #36: loss=1.7228622436523438\r\n",
      "Epoch #37: loss=1.5807303190231323\r\n",
      "Epoch #38: loss=1.6018866300582886\r\n",
      "Epoch #39: loss=1.53031325340271\r\n",
      "Epoch #40: loss=1.5345358848571777\r\n",
      "Epoch #41: loss=1.7334251403808594\r\n",
      "Epoch #42: loss=1.7676535844802856\r\n",
      "Epoch #43: loss=1.600088357925415\r\n",
      "Epoch #44: loss=1.6270869970321655\r\n",
      "Epoch #45: loss=1.4218440055847168\r\n",
      "Epoch #46: loss=1.4347320795059204\r\n",
      "Epoch #47: loss=1.3384578227996826\r\n",
      "Epoch #48: loss=1.343553900718689\r\n",
      "Epoch #49: loss=1.3531750440597534\r\n",
      "Epoch #50: loss=1.413165807723999\r\n",
      "Epoch #51: loss=1.3329923152923584\r\n",
      "Epoch #52: loss=1.3159961700439453\r\n",
      "Epoch #53: loss=1.3862524032592773\r\n",
      "Epoch #54: loss=1.5701473951339722\r\n",
      "Epoch #55: loss=1.3128291368484497\r\n",
      "Epoch #56: loss=1.2685765027999878\r\n",
      "Epoch #57: loss=1.1933307647705078\r\n",
      "Epoch #58: loss=1.4560850858688354\r\n",
      "Epoch #59: loss=1.2035038471221924\r\n",
      "Epoch #60: loss=1.1509883403778076\r\n",
      "Epoch #61: loss=1.08510422706604\r\n",
      "Epoch #62: loss=1.2037725448608398\r\n",
      "Epoch #63: loss=1.2028086185455322\r\n",
      "Epoch #64: loss=1.0811411142349243\r\n",
      "Epoch #65: loss=1.0962636470794678\r\n",
      "Epoch #66: loss=1.0385713577270508\r\n",
      "Epoch #67: loss=1.1635178327560425\r\n",
      "Epoch #68: loss=1.1371665000915527\r\n",
      "Epoch #69: loss=1.0620636940002441\r\n",
      "Epoch #70: loss=1.3395916223526\r\n",
      "Epoch #71: loss=1.2135769128799438\r\n",
      "Epoch #72: loss=1.341691017150879\r\n",
      "Epoch #73: loss=0.9558789134025574\r\n",
      "Epoch #74: loss=1.4477834701538086\r\n",
      "Epoch #75: loss=1.0775432586669922\r\n",
      "Epoch #76: loss=1.0816576480865479\r\n",
      "Epoch #77: loss=1.0364325046539307\r\n",
      "Epoch #78: loss=0.9832755327224731\r\n",
      "Epoch #79: loss=1.015961766242981\r\n",
      "Epoch #80: loss=0.9615709185600281\r\n",
      "Epoch #81: loss=1.1282048225402832\r\n",
      "Epoch #82: loss=1.1015219688415527\r\n",
      "Epoch #83: loss=1.3092650175094604\r\n",
      "Epoch #84: loss=0.9790540337562561\r\n",
      "Epoch #85: loss=0.9042598009109497\r\n",
      "Epoch #86: loss=1.3966187238693237\r\n",
      "Epoch #87: loss=0.9636083245277405\r\n",
      "Epoch #88: loss=0.9727986454963684\r\n",
      "Epoch #89: loss=1.0120429992675781\r\n",
      "Epoch #90: loss=0.9024974703788757\r\n",
      "Epoch #91: loss=0.9451975226402283\r\n",
      "Epoch #92: loss=1.1032174825668335\r\n",
      "Epoch #93: loss=1.1087539196014404\r\n",
      "Epoch #94: loss=1.7825143337249756\r\n",
      "Epoch #95: loss=1.3082807064056396\r\n",
      "Epoch #96: loss=1.080072283744812\r\n",
      "Epoch #97: loss=0.9658292531967163\r\n",
      "Epoch #98: loss=0.931742787361145\r\n",
      "Epoch #99: loss=1.0563241243362427\r\n",
      "Epoch #100: loss=0.9146449565887451\r\n",
      "Epoch #101: loss=1.0307234525680542\r\n",
      "Epoch #102: loss=1.1304551362991333\r\n",
      "Epoch #103: loss=0.9349724650382996\r\n",
      "Epoch #104: loss=0.9171708226203918\r\n",
      "Epoch #105: loss=1.0254995822906494\r\n",
      "Epoch #106: loss=1.0079916715621948\r\n",
      "Epoch #107: loss=0.8767885565757751\r\n",
      "Epoch #108: loss=1.4220201969146729\r\n",
      "Epoch #109: loss=0.9300587177276611\r\n",
      "Epoch #110: loss=0.9640874266624451\r\n",
      "Epoch #111: loss=1.6435225009918213\r\n",
      "Epoch #112: loss=0.8678208589553833\r\n",
      "Epoch #113: loss=0.7667719721794128\r\n",
      "Epoch #114: loss=1.1110848188400269\r\n",
      "Epoch #115: loss=0.930598258972168\r\n",
      "Epoch #116: loss=0.9037042260169983\r\n",
      "Epoch #117: loss=0.7158550024032593\r\n",
      "Epoch #118: loss=1.0289045572280884\r\n",
      "Epoch #119: loss=0.729461669921875\r\n",
      "Epoch #120: loss=0.7974175214767456\r\n",
      "Epoch #121: loss=0.6999503970146179\r\n",
      "Epoch #122: loss=0.8195237517356873\r\n",
      "Epoch #123: loss=0.779293417930603\r\n",
      "Epoch #124: loss=0.9246505498886108\r\n",
      "Epoch #125: loss=0.9955363273620605\r\n",
      "Epoch #126: loss=1.1080061197280884\r\n",
      "Epoch #127: loss=1.2361350059509277\r\n",
      "Epoch #128: loss=0.6586666107177734\r\n",
      "Epoch #129: loss=0.7759761810302734\r\n",
      "Epoch #130: loss=0.7419954538345337\r\n",
      "Epoch #131: loss=0.6924430727958679\r\n",
      "Epoch #132: loss=0.7231472730636597\r\n",
      "Epoch #133: loss=0.9166280627250671\r\n",
      "Epoch #134: loss=0.7719109654426575\r\n",
      "Epoch #135: loss=1.4347379207611084\r\n",
      "Epoch #136: loss=0.8216921091079712\r\n",
      "Epoch #137: loss=0.7542176246643066\r\n",
      "Epoch #138: loss=0.6616624593734741\r\n",
      "Epoch #139: loss=0.78923100233078\r\n",
      "Epoch #140: loss=0.900143027305603\r\n",
      "Epoch #141: loss=0.8190031051635742\r\n",
      "Epoch #142: loss=0.625327467918396\r\n",
      "Epoch #143: loss=0.689702033996582\r\n",
      "Epoch #144: loss=0.7527655959129333\r\n",
      "Epoch #145: loss=0.9416184425354004\r\n",
      "Epoch #146: loss=0.6697745323181152\r\n",
      "Epoch #147: loss=0.7733764052391052\r\n",
      "Epoch #148: loss=0.751172661781311\r\n",
      "Epoch #149: loss=1.1449496746063232\r\n",
      "Epoch #150: loss=0.6195666193962097\r\n",
      "Epoch #151: loss=0.6576140522956848\r\n",
      "Epoch #152: loss=0.7051981091499329\r\n",
      "Epoch #153: loss=0.7032033801078796\r\n",
      "Epoch #154: loss=0.6297380328178406\r\n",
      "Epoch #155: loss=0.7761441469192505\r\n",
      "Epoch #156: loss=0.9711182713508606\r\n",
      "Epoch #157: loss=0.7805153727531433\r\n",
      "Epoch #158: loss=0.5880745649337769\r\n",
      "Epoch #159: loss=0.7989351749420166\r\n",
      "Epoch #160: loss=0.6040605306625366\r\n",
      "Epoch #161: loss=0.6685425639152527\r\n",
      "Epoch #162: loss=0.7531280517578125\r\n",
      "Epoch #163: loss=0.5923228859901428\r\n",
      "Epoch #164: loss=0.6446442604064941\r\n",
      "Epoch #165: loss=1.0269793272018433\r\n",
      "Epoch #166: loss=0.7395347356796265\r\n",
      "Epoch #167: loss=0.6022858023643494\r\n",
      "Epoch #168: loss=1.0058618783950806\r\n",
      "Epoch #169: loss=0.656110405921936\r\n",
      "Epoch #170: loss=0.8988373875617981\r\n",
      "Epoch #171: loss=0.965528130531311\r\n",
      "Epoch #172: loss=0.6103881597518921\r\n",
      "Epoch #173: loss=0.9002395272254944\r\n",
      "Epoch #174: loss=0.8659075498580933\r\n",
      "Epoch #175: loss=0.6333804130554199\r\n",
      "Epoch #176: loss=0.6150157451629639\r\n",
      "Epoch #177: loss=0.5804818272590637\r\n",
      "Epoch #178: loss=0.5456565618515015\r\n",
      "Epoch #179: loss=0.8782356977462769\r\n",
      "Epoch #180: loss=0.5783449411392212\r\n",
      "Epoch #181: loss=0.723416268825531\r\n",
      "Epoch #182: loss=1.3815163373947144\r\n",
      "Epoch #183: loss=0.6075986623764038\r\n",
      "Epoch #184: loss=0.6702656149864197\r\n",
      "Epoch #185: loss=0.5876385569572449\r\n",
      "Epoch #186: loss=0.6283007860183716\r\n",
      "Epoch #187: loss=0.7133072018623352\r\n",
      "Epoch #188: loss=0.6697210073471069\r\n",
      "Epoch #189: loss=0.6751192808151245\r\n",
      "Epoch #190: loss=0.5467366576194763\r\n",
      "Epoch #191: loss=0.9550003409385681\r\n",
      "Epoch #192: loss=1.3065881729125977\r\n",
      "Epoch #193: loss=0.6914998292922974\r\n",
      "Epoch #194: loss=0.7877891659736633\r\n",
      "Epoch #195: loss=0.5589593052864075\r\n",
      "Epoch #196: loss=0.7294358611106873\r\n",
      "Epoch #197: loss=0.569500744342804\r\n",
      "Epoch #198: loss=0.5436424612998962\r\n",
      "Epoch #199: loss=0.5625726580619812\r\n",
      "Epoch #200: loss=0.6152551770210266\r\n",
      "Epoch #201: loss=1.1778678894042969\r\n",
      "Epoch #202: loss=0.562376856803894\r\n",
      "Epoch #203: loss=0.5173639059066772\r\n",
      "Epoch #204: loss=0.6292310953140259\r\n",
      "Epoch #205: loss=0.8651818633079529\r\n",
      "Epoch #206: loss=0.679650604724884\r\n",
      "Epoch #207: loss=0.5513561964035034\r\n",
      "Epoch #208: loss=0.4901469647884369\r\n",
      "Epoch #209: loss=0.4839281737804413\r\n",
      "Epoch #210: loss=0.9033279418945312\r\n",
      "Epoch #211: loss=0.4447283148765564\r\n",
      "Epoch #212: loss=0.6639991998672485\r\n",
      "Epoch #213: loss=0.4743359088897705\r\n",
      "Epoch #214: loss=0.6797847747802734\r\n",
      "Epoch #215: loss=0.5303672552108765\r\n",
      "Epoch #216: loss=1.293912649154663\r\n",
      "Epoch #217: loss=0.5849770307540894\r\n",
      "Epoch #218: loss=0.5765593647956848\r\n",
      "Epoch #219: loss=0.5470593571662903\r\n",
      "Epoch #220: loss=0.6362044811248779\r\n",
      "Epoch #221: loss=0.5233327150344849\r\n",
      "Epoch #222: loss=0.9341724514961243\r\n",
      "Epoch #223: loss=0.5232124328613281\r\n",
      "Epoch #224: loss=0.6066292524337769\r\n",
      "Epoch #225: loss=0.485956609249115\r\n",
      "Epoch #226: loss=0.5226589441299438\r\n",
      "Epoch #227: loss=0.80645751953125\r\n",
      "Epoch #228: loss=0.7159025073051453\r\n",
      "Epoch #229: loss=0.5232102870941162\r\n",
      "Epoch #230: loss=0.7702761888504028\r\n",
      "Epoch #231: loss=0.47700929641723633\r\n",
      "Epoch #232: loss=0.48240500688552856\r\n",
      "Epoch #233: loss=0.47729286551475525\r\n",
      "Epoch #234: loss=0.5780894756317139\r\n",
      "Epoch #235: loss=0.5059526562690735\r\n",
      "Epoch #236: loss=0.6701772212982178\r\n",
      "Epoch #237: loss=0.5053327679634094\r\n",
      "Epoch #238: loss=0.5103070735931396\r\n",
      "Epoch #239: loss=0.5473924279212952\r\n",
      "Epoch #240: loss=0.5026785135269165\r\n",
      "Epoch #241: loss=0.6318092346191406\r\n",
      "Epoch #242: loss=0.40598827600479126\r\n",
      "Epoch #243: loss=0.9217951893806458\r\n",
      "Epoch #244: loss=0.4446262717247009\r\n",
      "Epoch #245: loss=0.4873124361038208\r\n",
      "Epoch #246: loss=0.6862398982048035\r\n",
      "Epoch #247: loss=0.4658181071281433\r\n",
      "Epoch #248: loss=0.6487042307853699\r\n",
      "Epoch #249: loss=1.3768826723098755\r\n",
      "Epoch #250: loss=0.5198093056678772\r\n",
      "Epoch #251: loss=0.5164721012115479\r\n",
      "Epoch #252: loss=0.5742034316062927\r\n",
      "Epoch #253: loss=0.6985448002815247\r\n",
      "Epoch #254: loss=0.4552012085914612\r\n",
      "Epoch #255: loss=1.0346611738204956\r\n",
      "Epoch #256: loss=0.4927334785461426\r\n",
      "Epoch #257: loss=0.5308170318603516\r\n",
      "Epoch #258: loss=1.1082918643951416\r\n",
      "Epoch #259: loss=0.5656943321228027\r\n",
      "Epoch #260: loss=0.6360695362091064\r\n",
      "Epoch #261: loss=0.5370981693267822\r\n",
      "Epoch #262: loss=0.5416252017021179\r\n",
      "Epoch #263: loss=0.5453941226005554\r\n",
      "Epoch #264: loss=0.6556278467178345\r\n",
      "Epoch #265: loss=0.7409723401069641\r\n",
      "Epoch #266: loss=0.8678785562515259\r\n",
      "Epoch #267: loss=0.6027369499206543\r\n",
      "Epoch #268: loss=0.4627031981945038\r\n",
      "Epoch #269: loss=0.46114581823349\r\n",
      "Epoch #270: loss=0.44049346446990967\r\n",
      "Epoch #271: loss=0.5178865194320679\r\n",
      "Epoch #272: loss=0.45378512144088745\r\n",
      "Epoch #273: loss=0.89799964427948\r\n",
      "Epoch #274: loss=0.4497808814048767\r\n",
      "Epoch #275: loss=0.4765253961086273\r\n",
      "Epoch #276: loss=1.3136574029922485\r\n",
      "Epoch #277: loss=0.510873556137085\r\n",
      "Epoch #278: loss=0.4954977035522461\r\n",
      "Epoch #279: loss=0.4470696449279785\r\n",
      "Epoch #280: loss=0.46862953901290894\r\n",
      "Epoch #281: loss=0.5095586180686951\r\n",
      "Epoch #282: loss=0.7150762677192688\r\n",
      "Epoch #283: loss=0.4669514298439026\r\n",
      "Epoch #284: loss=0.46414297819137573\r\n",
      "Epoch #285: loss=0.45197421312332153\r\n",
      "Epoch #286: loss=0.4228941798210144\r\n",
      "Epoch #287: loss=0.545113742351532\r\n",
      "Epoch #288: loss=0.40462347865104675\r\n",
      "Epoch #289: loss=0.4353902339935303\r\n",
      "Epoch #290: loss=0.5348381996154785\r\n",
      "Epoch #291: loss=0.41907811164855957\r\n",
      "Epoch #292: loss=0.5940715074539185\r\n",
      "Epoch #293: loss=0.50165855884552\r\n",
      "Epoch #294: loss=0.49658867716789246\r\n",
      "Epoch #295: loss=0.4546387493610382\r\n",
      "Epoch #296: loss=0.5126973986625671\r\n",
      "Epoch #297: loss=0.5991630554199219\r\n",
      "Epoch #298: loss=1.3454585075378418\r\n",
      "Epoch #299: loss=0.5646800994873047\r\n",
      "Epoch #300: loss=0.43804794549942017\r\n",
      "Epoch #301: loss=0.416424959897995\r\n",
      "Epoch #302: loss=0.9221091866493225\r\n",
      "Epoch #303: loss=0.4794962406158447\r\n",
      "Epoch #304: loss=0.430366188287735\r\n",
      "Epoch #305: loss=0.39659053087234497\r\n",
      "Epoch #306: loss=0.44383904337882996\r\n",
      "Epoch #307: loss=0.42653393745422363\r\n",
      "Epoch #308: loss=0.35181283950805664\r\n",
      "Epoch #309: loss=0.49163851141929626\r\n",
      "Epoch #310: loss=0.3465888202190399\r\n",
      "Epoch #311: loss=0.4238154888153076\r\n",
      "Epoch #312: loss=0.5582579970359802\r\n",
      "Epoch #313: loss=0.37129294872283936\r\n",
      "Epoch #314: loss=1.7494854927062988\r\n",
      "Epoch #315: loss=0.49811241030693054\r\n",
      "Epoch #316: loss=0.39924588799476624\r\n",
      "Epoch #317: loss=1.054026484489441\r\n",
      "Epoch #318: loss=0.42539161443710327\r\n",
      "Epoch #319: loss=0.4066450595855713\r\n",
      "Epoch #320: loss=0.3900982737541199\r\n",
      "Epoch #321: loss=0.5516205430030823\r\n",
      "Epoch #322: loss=0.5796217918395996\r\n",
      "Epoch #323: loss=0.43109697103500366\r\n",
      "Epoch #324: loss=0.36353713274002075\r\n",
      "Epoch #325: loss=0.44133326411247253\r\n",
      "Epoch #326: loss=0.6165112257003784\r\n",
      "Epoch #327: loss=0.38548868894577026\r\n",
      "Epoch #328: loss=0.8051594495773315\r\n",
      "Epoch #329: loss=0.37758275866508484\r\n",
      "Epoch #330: loss=0.3865169584751129\r\n",
      "Epoch #331: loss=0.4007476568222046\r\n",
      "Epoch #332: loss=0.3696628212928772\r\n",
      "Epoch #333: loss=0.5014266967773438\r\n",
      "Epoch #334: loss=1.4877516031265259\r\n",
      "Epoch #335: loss=1.2564829587936401\r\n",
      "Epoch #336: loss=0.434339314699173\r\n",
      "Epoch #337: loss=0.4735651910305023\r\n",
      "Epoch #338: loss=0.3825855255126953\r\n",
      "Epoch #339: loss=0.5848976373672485\r\n",
      "Epoch #340: loss=0.5928705334663391\r\n",
      "Epoch #341: loss=0.4721496105194092\r\n",
      "Epoch #342: loss=0.41443899273872375\r\n",
      "Epoch #343: loss=0.4244351089000702\r\n",
      "Epoch #344: loss=0.37498939037323\r\n",
      "Epoch #345: loss=0.36652734875679016\r\n",
      "Epoch #346: loss=0.3432728052139282\r\n",
      "Epoch #347: loss=0.34820589423179626\r\n",
      "Epoch #348: loss=0.43838685750961304\r\n",
      "Epoch #349: loss=0.44259172677993774\r\n",
      "Epoch #350: loss=0.33158767223358154\r\n",
      "Epoch #351: loss=0.3975868225097656\r\n",
      "Epoch #352: loss=0.5511448979377747\r\n",
      "Epoch #353: loss=0.5886840224266052\r\n",
      "Epoch #354: loss=0.3860449492931366\r\n",
      "Epoch #355: loss=0.43479982018470764\r\n",
      "Epoch #356: loss=0.3508470058441162\r\n",
      "Epoch #357: loss=0.48424530029296875\r\n",
      "Epoch #358: loss=0.3236677646636963\r\n",
      "Epoch #359: loss=0.30507978796958923\r\n",
      "Epoch #360: loss=0.5883221626281738\r\n",
      "Epoch #361: loss=0.3382546305656433\r\n",
      "Epoch #362: loss=0.3021443784236908\r\n",
      "Epoch #363: loss=0.31187596917152405\r\n",
      "Epoch #364: loss=0.31336236000061035\r\n",
      "Epoch #365: loss=0.3739165663719177\r\n",
      "Epoch #366: loss=0.2612914741039276\r\n",
      "Epoch #367: loss=0.27604949474334717\r\n",
      "Epoch #368: loss=0.26726314425468445\r\n",
      "Epoch #369: loss=0.3081759512424469\r\n",
      "Epoch #370: loss=0.3657524585723877\r\n",
      "Epoch #371: loss=0.26333481073379517\r\n",
      "Epoch #372: loss=0.2956041097640991\r\n",
      "Epoch #373: loss=0.27675238251686096\r\n",
      "Epoch #374: loss=0.23416166007518768\r\n",
      "Epoch #375: loss=0.4076520502567291\r\n",
      "Epoch #376: loss=0.2963859438896179\r\n",
      "Epoch #377: loss=0.28968310356140137\r\n",
      "Epoch #378: loss=0.2764350473880768\r\n",
      "Epoch #379: loss=0.30383220314979553\r\n",
      "Epoch #380: loss=0.30088767409324646\r\n",
      "Epoch #381: loss=0.29687777161598206\r\n",
      "Epoch #382: loss=0.24899563193321228\r\n",
      "Epoch #383: loss=0.252996563911438\r\n",
      "Epoch #384: loss=0.263418972492218\r\n",
      "Epoch #385: loss=2.454521894454956\r\n",
      "Epoch #386: loss=0.4107147753238678\r\n",
      "Epoch #387: loss=0.43651971220970154\r\n",
      "Epoch #388: loss=0.2847096621990204\r\n",
      "Epoch #389: loss=0.3737808167934418\r\n",
      "Epoch #390: loss=0.7295044660568237\r\n",
      "Epoch #391: loss=0.5341477394104004\r\n",
      "Epoch #392: loss=0.4319877624511719\r\n",
      "Epoch #393: loss=0.44289183616638184\r\n",
      "Epoch #394: loss=0.3462773561477661\r\n",
      "Epoch #395: loss=0.36787211894989014\r\n",
      "Epoch #396: loss=0.33724603056907654\r\n",
      "Epoch #397: loss=0.3477914333343506\r\n",
      "Epoch #398: loss=0.33816683292388916\r\n",
      "Epoch #399: loss=0.345064252614975\r\n",
      "Epoch #400: loss=0.3987051546573639\r\n",
      "Epoch #401: loss=0.34161871671676636\r\n",
      "Epoch #402: loss=0.3220207095146179\r\n",
      "Epoch #403: loss=0.37039849162101746\r\n",
      "Epoch #404: loss=0.6200326085090637\r\n",
      "Epoch #405: loss=0.35956722497940063\r\n",
      "Epoch #406: loss=0.3258146047592163\r\n",
      "Epoch #407: loss=0.4268653988838196\r\n",
      "Epoch #408: loss=0.26156526803970337\r\n",
      "Epoch #409: loss=0.3005426228046417\r\n",
      "Epoch #410: loss=0.4150039255619049\r\n",
      "Epoch #411: loss=0.2506188154220581\r\n",
      "Epoch #412: loss=0.6675982475280762\r\n",
      "Epoch #413: loss=0.30554670095443726\r\n",
      "Epoch #414: loss=1.083461046218872\r\n",
      "Epoch #415: loss=0.41710978746414185\r\n",
      "Epoch #416: loss=0.3713914752006531\r\n",
      "Epoch #417: loss=0.2947412133216858\r\n",
      "Epoch #418: loss=0.2787923216819763\r\n",
      "Epoch #419: loss=0.323378324508667\r\n",
      "Epoch #420: loss=0.4573414623737335\r\n",
      "Epoch #421: loss=0.2563897371292114\r\n",
      "Epoch #422: loss=0.38962751626968384\r\n",
      "Epoch #423: loss=0.6147627234458923\r\n",
      "Epoch #424: loss=0.27923551201820374\r\n",
      "Epoch #425: loss=0.5667012929916382\r\n",
      "Epoch #426: loss=0.31219279766082764\r\n",
      "Epoch #427: loss=0.2391485571861267\r\n",
      "Epoch #428: loss=0.6062145829200745\r\n",
      "Epoch #429: loss=0.5510052442550659\r\n",
      "Epoch #430: loss=0.6346138119697571\r\n",
      "Epoch #431: loss=0.3191501498222351\r\n",
      "Epoch #432: loss=0.4162467122077942\r\n",
      "Epoch #433: loss=0.3787112236022949\r\n",
      "Epoch #434: loss=0.3133072853088379\r\n",
      "Epoch #435: loss=0.31938502192497253\r\n",
      "Epoch #436: loss=0.23181948065757751\r\n",
      "Epoch #437: loss=0.5431094169616699\r\n",
      "Epoch #438: loss=0.26490405201911926\r\n",
      "Epoch #439: loss=0.40105900168418884\r\n",
      "Epoch #440: loss=0.6631656289100647\r\n",
      "Epoch #441: loss=0.2619498074054718\r\n",
      "Epoch #442: loss=0.46651437878608704\r\n",
      "Epoch #443: loss=0.24605554342269897\r\n",
      "Epoch #444: loss=0.34742191433906555\r\n",
      "Epoch #445: loss=0.2232149988412857\r\n",
      "Epoch #446: loss=0.2117532640695572\r\n",
      "Epoch #447: loss=0.23986519873142242\r\n",
      "Epoch #448: loss=0.254742830991745\r\n",
      "Epoch #449: loss=0.2665117383003235\r\n",
      "Epoch #450: loss=0.20041660964488983\r\n",
      "Epoch #451: loss=0.4545661211013794\r\n",
      "Epoch #452: loss=0.23898491263389587\r\n",
      "Epoch #453: loss=0.2376168966293335\r\n",
      "Epoch #454: loss=0.32446083426475525\r\n",
      "Epoch #455: loss=0.30590590834617615\r\n",
      "Epoch #456: loss=0.2083531767129898\r\n",
      "Epoch #457: loss=0.19369570910930634\r\n",
      "Epoch #458: loss=0.4322938621044159\r\n",
      "Epoch #459: loss=0.2256937026977539\r\n",
      "Epoch #460: loss=0.1490168273448944\r\n",
      "Epoch #461: loss=0.20872339606285095\r\n",
      "Epoch #462: loss=0.2501921057701111\r\n",
      "Epoch #463: loss=0.20646874606609344\r\n",
      "Epoch #464: loss=0.19510115683078766\r\n",
      "Epoch #465: loss=0.2709656357765198\r\n",
      "Epoch #466: loss=0.16749940812587738\r\n",
      "Epoch #467: loss=0.1441662609577179\r\n",
      "Epoch #468: loss=0.14243675768375397\r\n",
      "Epoch #469: loss=0.15887272357940674\r\n",
      "Epoch #470: loss=0.3258606493473053\r\n",
      "Epoch #471: loss=0.19028262794017792\r\n",
      "Epoch #472: loss=0.22107967734336853\r\n",
      "Epoch #473: loss=0.45020976662635803\r\n",
      "Epoch #474: loss=0.15720537304878235\r\n",
      "Epoch #475: loss=0.1851935237646103\r\n",
      "Epoch #476: loss=0.1459019035100937\r\n",
      "Epoch #477: loss=0.5216160416603088\r\n",
      "Epoch #478: loss=0.38496288657188416\r\n",
      "Epoch #479: loss=0.1629253625869751\r\n",
      "Epoch #480: loss=0.22773882746696472\r\n",
      "Epoch #481: loss=0.5007681250572205\r\n",
      "Epoch #482: loss=0.17939844727516174\r\n",
      "Epoch #483: loss=0.18340037763118744\r\n",
      "Epoch #484: loss=0.30801114439964294\r\n",
      "Epoch #485: loss=0.15255574882030487\r\n",
      "Epoch #486: loss=0.2560362219810486\r\n",
      "Epoch #487: loss=0.27101731300354004\r\n",
      "Epoch #488: loss=0.21501439809799194\r\n",
      "Epoch #489: loss=0.18580909073352814\r\n",
      "Epoch #490: loss=0.16081905364990234\r\n",
      "Epoch #491: loss=0.13745683431625366\r\n",
      "Epoch #492: loss=0.3374500870704651\r\n",
      "Epoch #493: loss=0.15815669298171997\r\n",
      "Epoch #494: loss=0.29228469729423523\r\n",
      "Epoch #495: loss=0.2198878973722458\r\n",
      "Epoch #496: loss=0.21359384059906006\r\n",
      "Epoch #497: loss=0.32189270853996277\r\n",
      "Epoch #498: loss=0.1670306921005249\r\n",
      "Epoch #499: loss=0.23979489505290985\r\n",
      "Epoch #500: loss=0.20060931146144867\r\n",
      "Epoch #501: loss=0.15948307514190674\r\n",
      "Epoch #502: loss=0.13212530314922333\r\n",
      "Epoch #503: loss=0.1327805370092392\r\n",
      "Epoch #504: loss=0.16370752453804016\r\n",
      "Epoch #505: loss=0.3755991756916046\r\n",
      "Epoch #506: loss=0.19992759823799133\r\n",
      "Epoch #507: loss=0.1166800931096077\r\n",
      "Epoch #508: loss=0.12447180598974228\r\n",
      "Epoch #509: loss=0.1254417598247528\r\n",
      "Epoch #510: loss=0.36163705587387085\r\n",
      "Epoch #511: loss=0.3077591061592102\r\n",
      "Epoch #512: loss=0.13520260155200958\r\n",
      "Epoch #513: loss=0.10495162010192871\r\n",
      "Epoch #514: loss=0.1017812043428421\r\n",
      "Epoch #515: loss=0.10449513047933578\r\n",
      "Epoch #516: loss=0.16749022901058197\r\n",
      "Epoch #517: loss=0.16755160689353943\r\n",
      "Epoch #518: loss=0.09921401739120483\r\n",
      "Epoch #519: loss=0.17284135520458221\r\n",
      "Epoch #520: loss=0.18547050654888153\r\n",
      "Epoch #521: loss=0.11520698666572571\r\n",
      "Epoch #522: loss=0.1894567459821701\r\n",
      "Epoch #523: loss=0.3791672885417938\r\n",
      "Epoch #524: loss=0.09065312147140503\r\n",
      "Epoch #525: loss=0.1009075865149498\r\n",
      "Epoch #526: loss=0.20587758719921112\r\n",
      "Epoch #527: loss=0.08825042098760605\r\n",
      "Epoch #528: loss=0.09684303402900696\r\n",
      "Epoch #529: loss=0.1112806648015976\r\n",
      "Epoch #530: loss=0.15215802192687988\r\n",
      "Epoch #531: loss=0.0960041880607605\r\n",
      "Epoch #532: loss=0.12216190993785858\r\n",
      "Epoch #533: loss=0.09871762990951538\r\n",
      "Epoch #534: loss=0.2597416937351227\r\n",
      "Epoch #535: loss=0.13168200850486755\r\n",
      "Epoch #536: loss=0.1545814424753189\r\n",
      "Epoch #537: loss=0.08973819762468338\r\n",
      "Epoch #538: loss=0.15060722827911377\r\n",
      "Epoch #539: loss=0.19249071180820465\r\n",
      "Epoch #540: loss=0.4487779438495636\r\n",
      "Epoch #541: loss=0.17345857620239258\r\n",
      "Epoch #542: loss=0.14327426254749298\r\n",
      "Epoch #543: loss=0.08335068076848984\r\n",
      "Epoch #544: loss=0.3461858332157135\r\n",
      "Epoch #545: loss=0.24827586114406586\r\n",
      "Epoch #546: loss=0.1858055144548416\r\n",
      "Epoch #547: loss=1.4989304542541504\r\n",
      "Epoch #548: loss=0.1559622585773468\r\n",
      "Epoch #549: loss=0.25002607703208923\r\n",
      "Epoch #550: loss=0.17670229077339172\r\n",
      "Epoch #551: loss=0.2449110746383667\r\n",
      "Epoch #552: loss=0.15886273980140686\r\n",
      "Epoch #553: loss=0.22884909808635712\r\n",
      "Epoch #554: loss=0.1617322713136673\r\n",
      "Epoch #555: loss=0.1664573848247528\r\n",
      "Epoch #556: loss=0.15460005402565002\r\n",
      "Epoch #557: loss=0.22650817036628723\r\n",
      "Epoch #558: loss=0.2943287491798401\r\n",
      "Epoch #559: loss=0.22934868931770325\r\n",
      "Epoch #560: loss=0.7711498141288757\r\n",
      "Epoch #561: loss=0.21556894481182098\r\n",
      "Epoch #562: loss=0.2068474143743515\r\n",
      "Epoch #563: loss=0.23932994902133942\r\n",
      "Epoch #564: loss=0.16886620223522186\r\n",
      "Epoch #565: loss=0.38255223631858826\r\n",
      "Epoch #566: loss=0.15305888652801514\r\n",
      "Epoch #567: loss=0.1728370189666748\r\n",
      "Epoch #568: loss=0.23509763181209564\r\n",
      "Epoch #569: loss=0.1281774640083313\r\n",
      "Epoch #570: loss=0.3547130823135376\r\n",
      "Epoch #571: loss=0.15796969830989838\r\n",
      "Epoch #572: loss=0.112906813621521\r\n",
      "Epoch #573: loss=0.13158342242240906\r\n",
      "Epoch #574: loss=0.2208121120929718\r\n",
      "Epoch #575: loss=0.10095588862895966\r\n",
      "Epoch #576: loss=0.1769227385520935\r\n",
      "Epoch #577: loss=0.24398663640022278\r\n",
      "Epoch #578: loss=0.3373810648918152\r\n",
      "Epoch #579: loss=0.20934173464775085\r\n",
      "Epoch #580: loss=0.09497160464525223\r\n",
      "Epoch #581: loss=0.1278550922870636\r\n",
      "Epoch #582: loss=0.3141167461872101\r\n",
      "Epoch #583: loss=0.11343282461166382\r\n",
      "Epoch #584: loss=0.1245574876666069\r\n",
      "Epoch #585: loss=0.15485779941082\r\n",
      "Epoch #586: loss=0.14197325706481934\r\n",
      "Epoch #587: loss=0.21678219735622406\r\n",
      "Epoch #588: loss=0.18123435974121094\r\n",
      "Epoch #589: loss=0.12917295098304749\r\n",
      "Epoch #590: loss=0.12487386912107468\r\n",
      "Epoch #591: loss=0.15196947753429413\r\n",
      "Epoch #592: loss=0.3165040612220764\r\n",
      "Epoch #593: loss=0.33176133036613464\r\n",
      "Epoch #594: loss=0.09092406928539276\r\n",
      "Epoch #595: loss=0.10636697709560394\r\n",
      "Epoch #596: loss=0.17115847766399384\r\n",
      "Epoch #597: loss=0.25086289644241333\r\n",
      "Epoch #598: loss=0.16577166318893433\r\n",
      "Epoch #599: loss=0.08848248422145844\r\n",
      "\r\n",
      "Training time: 0:01:49.026050\r\n",
      "\r\n",
      "Generating hybrid model predictions...\r\n",
      "Training hybrid model for horizon 24...\r\n",
      "Training hybrid model for horizon 48...\r\n",
      "Training hybrid model for horizon 168...\r\n",
      "Training hybrid model for horizon 336...\r\n",
      "Training hybrid model for horizon 720...\r\n",
      "Hybrid model failed for horizon 96: Model not trained for horizon 96\r\n",
      "Hybrid model failed for horizon 288: Model not trained for horizon 288\r\n",
      "Hybrid model failed for horizon 672: Model not trained for horizon 672\r\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.25173e-08): result may not be accurate.\r\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\r\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.25173e-08): result may not be accurate.\r\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\r\n",
      "Shape mismatch for hybrid prediction at horizon 24, falling back to 2-way ensemble\r\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.27373e-08): result may not be accurate.\r\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\r\n",
      "Shape mismatch for hybrid prediction at horizon 48, falling back to 2-way ensemble\r\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.25931e-08): result may not be accurate.\r\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\r\n",
      "Using 2-way ensemble for horizon 96: TS2Vec(0.6), TS2Vec+Time(0.4)\r\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.30512e-08): result may not be accurate.\r\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\r\n",
      "Using 2-way ensemble for horizon 288: TS2Vec(0.5), TS2Vec+Time(0.5)\r\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=3.30747e-08): result may not be accurate.\r\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\r\n",
      "Using 2-way ensemble for horizon 672: TS2Vec(0.5), TS2Vec+Time(0.5)\r\n",
      "Evaluation result: {'ours': {24: {'norm': {'MSE': 0.017006981489436725, 'MAE': 0.09656956891306732}, 'raw': {'MSE': 1.4310452487877985, 'MAE': 0.8858360620257233}}, 48: {'norm': {'MSE': 0.03229994087651013, 'MAE': 0.13479959326681232}, 'raw': {'MSE': 2.7178648342838603, 'MAE': 1.2365214239450144}}, 96: {'norm': {'MSE': 0.044498222289855116, 'MAE': 0.1618418166217626}, 'raw': {'MSE': 3.744284053401973, 'MAE': 1.4845806926648912}}, 288: {'norm': {'MSE': 0.09215441854374688, 'MAE': 0.2323073205760289}, 'raw': {'MSE': 7.754294487457515, 'MAE': 2.130963246031129}}, 672: {'norm': {'MSE': 0.1317313113575303, 'MAE': 0.2809445631490894}, 'raw': {'MSE': 11.08447535806799, 'MAE': 2.5771143877794667}}}, 'ts2vec_infer_time': 6.7566564083099365, 'lr_train_time': {24: 4.467369079589844, 48: 4.700235366821289, 96: 4.9035680294036865, 288: 7.0337042808532715, 672: 11.67248249053955}, 'lr_infer_time': {24: 0.016347169876098633, 48: 0.031125545501708984, 96: 0.04420137405395508, 288: 0.10254287719726562, 672: 0.17451786994934082}}\r\n",
      "Finished.\r\n"
     ]
    }
   ],
   "source": [
    "!python -u train.py ETTm1 forecast_univar --loader forecast_csv_univar --repr-dims 320 --max-threads 8 --seed 42 --eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8912fe3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T17:24:45.555673Z",
     "iopub.status.busy": "2025-10-01T17:24:45.555402Z",
     "iopub.status.idle": "2025-10-01T17:54:55.497365Z",
     "shell.execute_reply": "2025-10-01T17:54:55.496409Z"
    },
    "papermill": {
     "duration": 1809.969291,
     "end_time": "2025-10-01T17:54:55.499004",
     "exception": false,
     "start_time": "2025-10-01T17:24:45.529713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTh1\r\n",
      "Arguments: Namespace(dataset='ETTh1', run_name='forecast_univar', loader='forecast_csv_univar', gpu=0, batch_size=8, lr=0.001, repr_dims=320, max_train_length=3000, iters=None, epochs=None, save_every=None, seed=42, max_threads=8, eval=True, irregular=0)\r\n",
      "Loading data... done\r\n",
      "Epoch #0: loss=6.574136734008789\r\n",
      "Epoch #1: loss=4.3353800773620605\r\n",
      "Epoch #2: loss=3.772355318069458\r\n",
      "Epoch #3: loss=2.7383968830108643\r\n",
      "Epoch #4: loss=2.1007871627807617\r\n",
      "Epoch #5: loss=2.1087467670440674\r\n",
      "Epoch #6: loss=2.1866774559020996\r\n",
      "Epoch #7: loss=1.9104816913604736\r\n",
      "Epoch #8: loss=2.1465914249420166\r\n",
      "Epoch #9: loss=1.8105173110961914\r\n",
      "Epoch #10: loss=2.004054307937622\r\n",
      "Epoch #11: loss=1.9955910444259644\r\n",
      "Epoch #12: loss=2.145476818084717\r\n",
      "Epoch #13: loss=1.9520632028579712\r\n",
      "Epoch #14: loss=1.736497163772583\r\n",
      "Epoch #15: loss=1.8436534404754639\r\n",
      "Epoch #16: loss=1.8428653478622437\r\n",
      "Epoch #17: loss=1.6768500804901123\r\n",
      "Epoch #18: loss=1.7776726484298706\r\n",
      "Epoch #19: loss=1.7381000518798828\r\n",
      "Epoch #20: loss=1.754930019378662\r\n",
      "Epoch #21: loss=1.7470942735671997\r\n",
      "Epoch #22: loss=1.511613130569458\r\n",
      "Epoch #23: loss=1.7258166074752808\r\n",
      "Epoch #24: loss=1.669285774230957\r\n",
      "Epoch #25: loss=1.4829261302947998\r\n",
      "Epoch #26: loss=1.6303147077560425\r\n",
      "Epoch #27: loss=1.5104331970214844\r\n",
      "Epoch #28: loss=1.485397219657898\r\n",
      "Epoch #29: loss=1.3973498344421387\r\n",
      "Epoch #30: loss=1.3441718816757202\r\n",
      "Epoch #31: loss=1.4704641103744507\r\n",
      "Epoch #32: loss=1.4491342306137085\r\n",
      "Epoch #33: loss=1.563313603401184\r\n",
      "Epoch #34: loss=1.3163822889328003\r\n",
      "Epoch #35: loss=1.3848944902420044\r\n",
      "Epoch #36: loss=1.3011571168899536\r\n",
      "Epoch #37: loss=1.3704544305801392\r\n",
      "Epoch #38: loss=1.4231363534927368\r\n",
      "Epoch #39: loss=1.2566592693328857\r\n",
      "Epoch #40: loss=1.3559054136276245\r\n",
      "Epoch #41: loss=1.2900025844573975\r\n",
      "Epoch #42: loss=1.2721765041351318\r\n",
      "Epoch #43: loss=1.302857756614685\r\n",
      "Epoch #44: loss=1.266040325164795\r\n",
      "Epoch #45: loss=1.1446590423583984\r\n",
      "Epoch #46: loss=1.2356956005096436\r\n",
      "Epoch #47: loss=1.2964162826538086\r\n",
      "Epoch #48: loss=0.9890828132629395\r\n",
      "Epoch #49: loss=1.0048778057098389\r\n",
      "Epoch #50: loss=0.9843827486038208\r\n",
      "Epoch #51: loss=1.2183656692504883\r\n",
      "Epoch #52: loss=1.2571616172790527\r\n",
      "Epoch #53: loss=1.0941879749298096\r\n",
      "Epoch #54: loss=1.0665615797042847\r\n",
      "Epoch #55: loss=2.2539191246032715\r\n",
      "Epoch #56: loss=1.3901917934417725\r\n",
      "Epoch #57: loss=1.0617945194244385\r\n",
      "Epoch #58: loss=1.039773941040039\r\n",
      "Epoch #59: loss=1.0789002180099487\r\n",
      "Epoch #60: loss=1.240843415260315\r\n",
      "Epoch #61: loss=1.1885302066802979\r\n",
      "Epoch #62: loss=1.2299245595932007\r\n",
      "Epoch #63: loss=1.1053311824798584\r\n",
      "Epoch #64: loss=0.98392653465271\r\n",
      "Epoch #65: loss=1.1411410570144653\r\n",
      "Epoch #66: loss=1.248295783996582\r\n",
      "Epoch #67: loss=1.3839085102081299\r\n",
      "Epoch #68: loss=1.13595449924469\r\n",
      "Epoch #69: loss=1.0798120498657227\r\n",
      "Epoch #70: loss=1.08742356300354\r\n",
      "Epoch #71: loss=1.2010694742202759\r\n",
      "Epoch #72: loss=1.5142817497253418\r\n",
      "Epoch #73: loss=1.36942720413208\r\n",
      "Epoch #74: loss=1.076561450958252\r\n",
      "Epoch #75: loss=1.2334505319595337\r\n",
      "Epoch #76: loss=1.9695050716400146\r\n",
      "Epoch #77: loss=1.0438486337661743\r\n",
      "Epoch #78: loss=1.0256046056747437\r\n",
      "Epoch #79: loss=1.3148913383483887\r\n",
      "Epoch #80: loss=1.2364873886108398\r\n",
      "Epoch #81: loss=1.118541955947876\r\n",
      "Epoch #82: loss=1.1152251958847046\r\n",
      "Epoch #83: loss=1.0538830757141113\r\n",
      "Epoch #84: loss=1.024424433708191\r\n",
      "Epoch #85: loss=1.0450729131698608\r\n",
      "Epoch #86: loss=0.9294776320457458\r\n",
      "Epoch #87: loss=0.9015794992446899\r\n",
      "Epoch #88: loss=0.9410473704338074\r\n",
      "Epoch #89: loss=0.981245756149292\r\n",
      "Epoch #90: loss=0.9748824834823608\r\n",
      "Epoch #91: loss=0.9379923939704895\r\n",
      "Epoch #92: loss=0.9815959930419922\r\n",
      "Epoch #93: loss=1.2551531791687012\r\n",
      "Epoch #94: loss=0.909565269947052\r\n",
      "Epoch #95: loss=0.7721638679504395\r\n",
      "Epoch #96: loss=0.813596248626709\r\n",
      "Epoch #97: loss=0.842334508895874\r\n",
      "Epoch #98: loss=0.8216285109519958\r\n",
      "Epoch #99: loss=0.8160600662231445\r\n",
      "Epoch #100: loss=0.7971806526184082\r\n",
      "Epoch #101: loss=0.8989998698234558\r\n",
      "Epoch #102: loss=0.7456251978874207\r\n",
      "Epoch #103: loss=0.6750440001487732\r\n",
      "Epoch #104: loss=0.7564975023269653\r\n",
      "Epoch #105: loss=1.40977942943573\r\n",
      "Epoch #106: loss=0.7124804258346558\r\n",
      "Epoch #107: loss=0.6966766715049744\r\n",
      "Epoch #108: loss=0.7479397058486938\r\n",
      "Epoch #109: loss=0.7096601128578186\r\n",
      "Epoch #110: loss=0.750239908695221\r\n",
      "Epoch #111: loss=1.2519495487213135\r\n",
      "Epoch #112: loss=1.405975580215454\r\n",
      "Epoch #113: loss=1.047518253326416\r\n",
      "Epoch #114: loss=0.8900211453437805\r\n",
      "Epoch #115: loss=0.7261326313018799\r\n",
      "Epoch #116: loss=0.8952023386955261\r\n",
      "Epoch #117: loss=0.8241969347000122\r\n",
      "Epoch #118: loss=0.852270781993866\r\n",
      "Epoch #119: loss=0.7994776964187622\r\n",
      "Epoch #120: loss=0.6316738128662109\r\n",
      "Epoch #121: loss=0.6653209328651428\r\n",
      "Epoch #122: loss=0.7150098085403442\r\n",
      "Epoch #123: loss=0.7563095688819885\r\n",
      "Epoch #124: loss=1.1272801160812378\r\n",
      "Epoch #125: loss=0.759096622467041\r\n",
      "Epoch #126: loss=0.6504827737808228\r\n",
      "Epoch #127: loss=0.8607810139656067\r\n",
      "Epoch #128: loss=0.6558936834335327\r\n",
      "Epoch #129: loss=0.6343334317207336\r\n",
      "Epoch #130: loss=0.9687727093696594\r\n",
      "Epoch #131: loss=1.3480570316314697\r\n",
      "Epoch #132: loss=0.6961555480957031\r\n",
      "Epoch #133: loss=0.6074652075767517\r\n",
      "Epoch #134: loss=0.6320125460624695\r\n",
      "Epoch #135: loss=0.6613670587539673\r\n",
      "Epoch #136: loss=0.8902620673179626\r\n",
      "Epoch #137: loss=0.6793019771575928\r\n",
      "Epoch #138: loss=0.8798424601554871\r\n",
      "Epoch #139: loss=0.7609134316444397\r\n",
      "Epoch #140: loss=0.6872704029083252\r\n",
      "Epoch #141: loss=0.9667016863822937\r\n",
      "Epoch #142: loss=0.6736406087875366\r\n",
      "Epoch #143: loss=1.1347126960754395\r\n",
      "Epoch #144: loss=0.6599926948547363\r\n",
      "Epoch #145: loss=0.6313537955284119\r\n",
      "Epoch #146: loss=0.6821715235710144\r\n",
      "Epoch #147: loss=1.020155668258667\r\n",
      "Epoch #148: loss=0.6190508604049683\r\n",
      "Epoch #149: loss=0.6316648125648499\r\n",
      "Epoch #150: loss=0.8625631332397461\r\n",
      "Epoch #151: loss=0.9172177314758301\r\n",
      "Epoch #152: loss=0.7758541107177734\r\n",
      "Epoch #153: loss=0.7607637643814087\r\n",
      "Epoch #154: loss=0.7048874497413635\r\n",
      "Epoch #155: loss=0.6392221450805664\r\n",
      "Epoch #156: loss=0.9553654193878174\r\n",
      "Epoch #157: loss=0.6987410187721252\r\n",
      "Epoch #158: loss=0.8480367064476013\r\n",
      "Epoch #159: loss=0.7717282772064209\r\n",
      "Epoch #160: loss=0.7755694389343262\r\n",
      "Epoch #161: loss=0.9616571068763733\r\n",
      "Epoch #162: loss=0.6036088466644287\r\n",
      "Epoch #163: loss=0.7457327842712402\r\n",
      "Epoch #164: loss=0.8918765187263489\r\n",
      "Epoch #165: loss=0.6044037938117981\r\n",
      "Epoch #166: loss=0.6340610980987549\r\n",
      "Epoch #167: loss=0.5332077145576477\r\n",
      "Epoch #168: loss=0.6521975994110107\r\n",
      "Epoch #169: loss=0.5202752947807312\r\n",
      "Epoch #170: loss=0.47896838188171387\r\n",
      "Epoch #171: loss=0.5418352484703064\r\n",
      "Epoch #172: loss=0.6281659007072449\r\n",
      "Epoch #173: loss=0.5092265605926514\r\n",
      "Epoch #174: loss=0.48788705468177795\r\n",
      "Epoch #175: loss=0.5187448263168335\r\n",
      "Epoch #176: loss=0.4974895119667053\r\n",
      "Epoch #177: loss=0.6459912061691284\r\n",
      "Epoch #178: loss=0.42299413681030273\r\n",
      "Epoch #179: loss=0.5633255839347839\r\n",
      "Epoch #180: loss=0.5307155847549438\r\n",
      "Epoch #181: loss=0.46329769492149353\r\n",
      "Epoch #182: loss=0.44913148880004883\r\n",
      "Epoch #183: loss=0.6727657318115234\r\n",
      "Epoch #184: loss=0.3946346938610077\r\n",
      "Epoch #185: loss=0.5737040638923645\r\n",
      "Epoch #186: loss=0.48362287878990173\r\n",
      "Epoch #187: loss=0.5506259202957153\r\n",
      "Epoch #188: loss=0.4201122522354126\r\n",
      "Epoch #189: loss=0.5670394897460938\r\n",
      "Epoch #190: loss=0.5099174380302429\r\n",
      "Epoch #191: loss=0.41366562247276306\r\n",
      "Epoch #192: loss=0.3585760295391083\r\n",
      "Epoch #193: loss=2.754335403442383\r\n",
      "Epoch #194: loss=0.4188835024833679\r\n",
      "Epoch #195: loss=0.4378211498260498\r\n",
      "Epoch #196: loss=0.4546853303909302\r\n",
      "Epoch #197: loss=0.5480546355247498\r\n",
      "Epoch #198: loss=0.6104449033737183\r\n",
      "Epoch #199: loss=0.4529261291027069\r\n",
      "\r\n",
      "Training time: 0:00:17.662955\r\n",
      "\r\n",
      "Generating hybrid model predictions...\r\n",
      "Training hybrid model for horizon 24...\r\n",
      "Training hybrid model for horizon 48...\r\n",
      "Training hybrid model for horizon 168...\r\n",
      "Training hybrid model for horizon 336...\r\n",
      "Training hybrid model for horizon 720...\r\n",
      "Shape mismatch for hybrid prediction at horizon 24, falling back to 2-way ensemble\r\n",
      "Shape mismatch for hybrid prediction at horizon 48, falling back to 2-way ensemble\r\n",
      "Shape mismatch for hybrid prediction at horizon 168, falling back to 2-way ensemble\r\n",
      "Shape mismatch for hybrid prediction at horizon 336, falling back to 2-way ensemble\r\n",
      "Shape mismatch for hybrid prediction at horizon 720, falling back to 2-way ensemble\r\n",
      "Evaluation result: {'ours': {24: {'norm': {'MSE': 0.03989869190955763, 'MAE': 0.14998093718423766}, 'raw': {'MSE': 3.359788550818608, 'MAE': 1.3762987239838966}}, 48: {'norm': {'MSE': 0.06361785878228536, 'MAE': 0.19147585331458833}, 'raw': {'MSE': 5.357131859080218, 'MAE': 1.7570764494348776}}, 168: {'norm': {'MSE': 0.1235148518864651, 'MAE': 0.270137638977873}, 'raw': {'MSE': 10.400937107534427, 'MAE': 2.4789156195801034}}, 336: {'norm': {'MSE': 0.1442752016924286, 'MAE': 0.2973309962856114}, 'raw': {'MSE': 12.149124385131786, 'MAE': 2.72845521885267}}, 720: {'norm': {'MSE': 0.16425148353756946, 'MAE': 0.3269167303991431}, 'raw': {'MSE': 13.831286877660832, 'MAE': 2.999948442411883}}}, 'ts2vec_infer_time': 1.8457980155944824, 'lr_train_time': {24: 1.548063039779663, 48: 1.4688045978546143, 168: 2.294874906539917, 336: 3.0461134910583496, 720: 4.017616271972656}, 'lr_infer_time': {24: 0.007263898849487305, 48: 0.0080108642578125, 168: 0.0142059326171875, 336: 0.026722192764282227, 720: 0.038835763931274414}}\r\n",
      "Finished.\r\n"
     ]
    }
   ],
   "source": [
    "!python -u train.py ETTh1 forecast_univar --loader forecast_csv_univar --repr-dims 320 --max-threads 8 --seed 42 --eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "157538e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-01T17:54:55.565562Z",
     "iopub.status.busy": "2025-10-01T17:54:55.565315Z",
     "iopub.status.idle": "2025-10-01T18:27:55.775818Z",
     "shell.execute_reply": "2025-10-01T18:27:55.774865Z"
    },
    "papermill": {
     "duration": 1980.245146,
     "end_time": "2025-10-01T18:27:55.777343",
     "exception": false,
     "start_time": "2025-10-01T17:54:55.532197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ETTh2\r\n",
      "Arguments: Namespace(dataset='ETTh2', run_name='forecast_univar', loader='forecast_csv_univar', gpu=0, batch_size=8, lr=0.001, repr_dims=320, max_train_length=3000, iters=None, epochs=None, save_every=None, seed=42, max_threads=8, eval=True, irregular=0)\r\n",
      "Loading data... done\r\n",
      "Epoch #0: loss=6.168617248535156\r\n",
      "Epoch #1: loss=4.044599533081055\r\n",
      "Epoch #2: loss=3.4627490043640137\r\n",
      "Epoch #3: loss=2.6946728229522705\r\n",
      "Epoch #4: loss=2.0270774364471436\r\n",
      "Epoch #5: loss=2.237414598464966\r\n",
      "Epoch #6: loss=2.1579949855804443\r\n",
      "Epoch #7: loss=1.9165842533111572\r\n",
      "Epoch #8: loss=2.145052909851074\r\n",
      "Epoch #9: loss=1.735146164894104\r\n",
      "Epoch #10: loss=1.9368491172790527\r\n",
      "Epoch #11: loss=2.0034806728363037\r\n",
      "Epoch #12: loss=1.9975731372833252\r\n",
      "Epoch #13: loss=1.8884645700454712\r\n",
      "Epoch #14: loss=1.7314738035202026\r\n",
      "Epoch #15: loss=1.837358832359314\r\n",
      "Epoch #16: loss=1.83543062210083\r\n",
      "Epoch #17: loss=1.6370731592178345\r\n",
      "Epoch #18: loss=1.744407057762146\r\n",
      "Epoch #19: loss=1.6893881559371948\r\n",
      "Epoch #20: loss=1.7222189903259277\r\n",
      "Epoch #21: loss=1.703522801399231\r\n",
      "Epoch #22: loss=1.5358915328979492\r\n",
      "Epoch #23: loss=1.682837963104248\r\n",
      "Epoch #24: loss=1.6611063480377197\r\n",
      "Epoch #25: loss=1.4560482501983643\r\n",
      "Epoch #26: loss=1.5888093709945679\r\n",
      "Epoch #27: loss=1.4684133529663086\r\n",
      "Epoch #28: loss=1.4641835689544678\r\n",
      "Epoch #29: loss=1.360331416130066\r\n",
      "Epoch #30: loss=1.3270318508148193\r\n",
      "Epoch #31: loss=1.5059267282485962\r\n",
      "Epoch #32: loss=1.4247925281524658\r\n",
      "Epoch #33: loss=1.5068503618240356\r\n",
      "Epoch #34: loss=1.3026539087295532\r\n",
      "Epoch #35: loss=1.396992802619934\r\n",
      "Epoch #36: loss=1.3236078023910522\r\n",
      "Epoch #37: loss=1.356982707977295\r\n",
      "Epoch #38: loss=1.417854905128479\r\n",
      "Epoch #39: loss=1.1978328227996826\r\n",
      "Epoch #40: loss=1.3393983840942383\r\n",
      "Epoch #41: loss=1.2635700702667236\r\n",
      "Epoch #42: loss=1.2459807395935059\r\n",
      "Epoch #43: loss=1.2977344989776611\r\n",
      "Epoch #44: loss=1.2271323204040527\r\n",
      "Epoch #45: loss=1.1436365842819214\r\n",
      "Epoch #46: loss=1.2001975774765015\r\n",
      "Epoch #47: loss=1.2936699390411377\r\n",
      "Epoch #48: loss=0.9548225402832031\r\n",
      "Epoch #49: loss=0.9725033044815063\r\n",
      "Epoch #50: loss=0.9575824737548828\r\n",
      "Epoch #51: loss=1.227083444595337\r\n",
      "Epoch #52: loss=1.2428492307662964\r\n",
      "Epoch #53: loss=1.0599702596664429\r\n",
      "Epoch #54: loss=1.039817214012146\r\n",
      "Epoch #55: loss=2.1820220947265625\r\n",
      "Epoch #56: loss=1.382569670677185\r\n",
      "Epoch #57: loss=1.0608458518981934\r\n",
      "Epoch #58: loss=1.0328595638275146\r\n",
      "Epoch #59: loss=1.0408809185028076\r\n",
      "Epoch #60: loss=1.2194671630859375\r\n",
      "Epoch #61: loss=1.1510963439941406\r\n",
      "Epoch #62: loss=1.1529340744018555\r\n",
      "Epoch #63: loss=1.1049294471740723\r\n",
      "Epoch #64: loss=0.9805205464363098\r\n",
      "Epoch #65: loss=1.1350460052490234\r\n",
      "Epoch #66: loss=1.1576571464538574\r\n",
      "Epoch #67: loss=1.3322103023529053\r\n",
      "Epoch #68: loss=1.1445244550704956\r\n",
      "Epoch #69: loss=1.0485697984695435\r\n",
      "Epoch #70: loss=1.0315669775009155\r\n",
      "Epoch #71: loss=1.2279043197631836\r\n",
      "Epoch #72: loss=1.3722199201583862\r\n",
      "Epoch #73: loss=1.2017369270324707\r\n",
      "Epoch #74: loss=1.0177594423294067\r\n",
      "Epoch #75: loss=1.2068480253219604\r\n",
      "Epoch #76: loss=1.7890979051589966\r\n",
      "Epoch #77: loss=0.9667552709579468\r\n",
      "Epoch #78: loss=0.9310287237167358\r\n",
      "Epoch #79: loss=1.276185393333435\r\n",
      "Epoch #80: loss=1.1900666952133179\r\n",
      "Epoch #81: loss=1.0733124017715454\r\n",
      "Epoch #82: loss=1.05771005153656\r\n",
      "Epoch #83: loss=0.9041122794151306\r\n",
      "Epoch #84: loss=0.9539413452148438\r\n",
      "Epoch #85: loss=0.9713897109031677\r\n",
      "Epoch #86: loss=0.8969513773918152\r\n",
      "Epoch #87: loss=0.8517240881919861\r\n",
      "Epoch #88: loss=0.8808719515800476\r\n",
      "Epoch #89: loss=0.8665141463279724\r\n",
      "Epoch #90: loss=0.8356314897537231\r\n",
      "Epoch #91: loss=0.9073964357376099\r\n",
      "Epoch #92: loss=0.9527627825737\r\n",
      "Epoch #93: loss=1.3150882720947266\r\n",
      "Epoch #94: loss=0.930424153804779\r\n",
      "Epoch #95: loss=0.7424464225769043\r\n",
      "Epoch #96: loss=0.7897634506225586\r\n",
      "Epoch #97: loss=0.7725496292114258\r\n",
      "Epoch #98: loss=0.7702061533927917\r\n",
      "Epoch #99: loss=0.7965268492698669\r\n",
      "Epoch #100: loss=0.7781855463981628\r\n",
      "Epoch #101: loss=0.847362220287323\r\n",
      "Epoch #102: loss=0.7092375755310059\r\n",
      "Epoch #103: loss=0.6305481195449829\r\n",
      "Epoch #104: loss=0.7180289030075073\r\n",
      "Epoch #105: loss=1.5717209577560425\r\n",
      "Epoch #106: loss=0.6974910497665405\r\n",
      "Epoch #107: loss=0.6539868712425232\r\n",
      "Epoch #108: loss=0.7103175520896912\r\n",
      "Epoch #109: loss=0.7136706709861755\r\n",
      "Epoch #110: loss=0.7434300780296326\r\n",
      "Epoch #111: loss=1.3039156198501587\r\n",
      "Epoch #112: loss=1.3068640232086182\r\n",
      "Epoch #113: loss=1.0036712884902954\r\n",
      "Epoch #114: loss=0.8424060940742493\r\n",
      "Epoch #115: loss=0.6954747438430786\r\n",
      "Epoch #116: loss=0.878915548324585\r\n",
      "Epoch #117: loss=0.7574511766433716\r\n",
      "Epoch #118: loss=0.7761839628219604\r\n",
      "Epoch #119: loss=0.7652482390403748\r\n",
      "Epoch #120: loss=0.6091281175613403\r\n",
      "Epoch #121: loss=0.6576261520385742\r\n",
      "Epoch #122: loss=0.7117013931274414\r\n",
      "Epoch #123: loss=0.7302097678184509\r\n",
      "Epoch #124: loss=1.162268877029419\r\n",
      "Epoch #125: loss=0.8158637285232544\r\n",
      "Epoch #126: loss=0.5835133790969849\r\n",
      "Epoch #127: loss=0.8349689841270447\r\n",
      "Epoch #128: loss=0.6294348835945129\r\n",
      "Epoch #129: loss=0.6051279306411743\r\n",
      "Epoch #130: loss=0.9295322895050049\r\n",
      "Epoch #131: loss=1.4521387815475464\r\n",
      "Epoch #132: loss=0.6892922520637512\r\n",
      "Epoch #133: loss=0.6061618328094482\r\n",
      "Epoch #134: loss=0.6093875169754028\r\n",
      "Epoch #135: loss=0.6447781324386597\r\n",
      "Epoch #136: loss=0.8848415613174438\r\n",
      "Epoch #137: loss=0.6217039823532104\r\n",
      "Epoch #138: loss=0.7776229977607727\r\n",
      "Epoch #139: loss=0.7022820711135864\r\n",
      "Epoch #140: loss=0.6536242961883545\r\n",
      "Epoch #141: loss=0.893511950969696\r\n",
      "Epoch #142: loss=0.617402970790863\r\n",
      "Epoch #143: loss=1.0490412712097168\r\n",
      "Epoch #144: loss=0.6575677990913391\r\n",
      "Epoch #145: loss=0.6916384696960449\r\n",
      "Epoch #146: loss=0.6670091152191162\r\n",
      "Epoch #147: loss=0.7194616794586182\r\n",
      "Epoch #148: loss=0.5622899532318115\r\n",
      "Epoch #149: loss=0.6220782995223999\r\n",
      "Epoch #150: loss=0.8977546095848083\r\n",
      "Epoch #151: loss=0.8598772287368774\r\n",
      "Epoch #152: loss=0.7112445831298828\r\n",
      "Epoch #153: loss=0.605827271938324\r\n",
      "Epoch #154: loss=0.7038586139678955\r\n",
      "Epoch #155: loss=0.5824313163757324\r\n",
      "Epoch #156: loss=0.8746916651725769\r\n",
      "Epoch #157: loss=0.6207519173622131\r\n",
      "Epoch #158: loss=0.8182565569877625\r\n",
      "Epoch #159: loss=0.713206946849823\r\n",
      "Epoch #160: loss=0.6939530968666077\r\n",
      "Epoch #161: loss=0.9094786047935486\r\n",
      "Epoch #162: loss=0.5383219122886658\r\n",
      "Epoch #163: loss=0.7025289535522461\r\n",
      "Epoch #164: loss=0.6757405400276184\r\n",
      "Epoch #165: loss=0.5288639068603516\r\n",
      "Epoch #166: loss=0.588108479976654\r\n",
      "Epoch #167: loss=0.5033743381500244\r\n",
      "Epoch #168: loss=0.5896005630493164\r\n",
      "Epoch #169: loss=0.48580795526504517\r\n",
      "Epoch #170: loss=0.4448239803314209\r\n",
      "Epoch #171: loss=0.4893748164176941\r\n",
      "Epoch #172: loss=0.6195480823516846\r\n",
      "Epoch #173: loss=0.45311105251312256\r\n",
      "Epoch #174: loss=0.4423772692680359\r\n",
      "Epoch #175: loss=0.48225924372673035\r\n",
      "Epoch #176: loss=0.45625367760658264\r\n",
      "Epoch #177: loss=0.594193160533905\r\n",
      "Epoch #178: loss=0.38431400060653687\r\n",
      "Epoch #179: loss=0.5412451028823853\r\n",
      "Epoch #180: loss=0.47796931862831116\r\n",
      "Epoch #181: loss=0.38670679926872253\r\n",
      "Epoch #182: loss=0.4064355492591858\r\n",
      "Epoch #183: loss=0.6168316006660461\r\n",
      "Epoch #184: loss=0.36876070499420166\r\n",
      "Epoch #185: loss=0.5367678999900818\r\n",
      "Epoch #186: loss=0.4335278868675232\r\n",
      "Epoch #187: loss=0.5046036839485168\r\n",
      "Epoch #188: loss=0.3803842067718506\r\n",
      "Epoch #189: loss=0.4722141623497009\r\n",
      "Epoch #190: loss=0.48441362380981445\r\n",
      "Epoch #191: loss=0.38578149676322937\r\n",
      "Epoch #192: loss=0.3466082513332367\r\n",
      "Epoch #193: loss=2.8805086612701416\r\n",
      "Epoch #194: loss=0.3646073341369629\r\n",
      "Epoch #195: loss=0.39968985319137573\r\n",
      "Epoch #196: loss=0.4284377098083496\r\n",
      "Epoch #197: loss=0.5416674613952637\r\n",
      "Epoch #198: loss=0.5877322554588318\r\n",
      "Epoch #199: loss=0.4344017207622528\r\n",
      "\r\n",
      "Training time: 0:00:17.719712\r\n",
      "\r\n",
      "Generating hybrid model predictions...\r\n",
      "Training hybrid model for horizon 24...\r\n",
      "Training hybrid model for horizon 48...\r\n",
      "Training hybrid model for horizon 168...\r\n",
      "Training hybrid model for horizon 336...\r\n",
      "Training hybrid model for horizon 720...\r\n",
      "Shape mismatch for hybrid prediction at horizon 24, falling back to 2-way ensemble\r\n",
      "Shape mismatch for hybrid prediction at horizon 48, falling back to 2-way ensemble\r\n",
      "Shape mismatch for hybrid prediction at horizon 168, falling back to 2-way ensemble\r\n",
      "Shape mismatch for hybrid prediction at horizon 336, falling back to 2-way ensemble\r\n",
      "Shape mismatch for hybrid prediction at horizon 720, falling back to 2-way ensemble\r\n",
      "Evaluation result: {'ours': {24: {'norm': {'MSE': 0.09351163027989123, 'MAE': 0.23374484129161294}, 'raw': {'MSE': 12.549794971977608, 'MAE': 2.70786828616151}}, 48: {'norm': {'MSE': 0.12207809050912169, 'MAE': 0.2714088656565141}, 'raw': {'MSE': 16.38357712163057, 'MAE': 3.1441954219535315}}, 168: {'norm': {'MSE': 0.1822120659411766, 'MAE': 0.3397450965276783}, 'raw': {'MSE': 24.45390014202295, 'MAE': 3.935851448881542}}, 336: {'norm': {'MSE': 0.21348064337905587, 'MAE': 0.3693917398553428}, 'raw': {'MSE': 28.65032185701443, 'MAE': 4.279299478855264}}, 720: {'norm': {'MSE': 0.23254975145290294, 'MAE': 0.3926602611608245}, 'raw': {'MSE': 31.209505093462887, 'MAE': 4.548858757942755}}}, 'ts2vec_infer_time': 1.863586187362671, 'lr_train_time': {24: 1.5358662605285645, 48: 1.8113150596618652, 168: 2.4488561153411865, 336: 3.0646190643310547, 720: 3.9229815006256104}, 'lr_infer_time': {24: 0.007019519805908203, 48: 0.00829458236694336, 168: 0.015466928482055664, 336: 0.026834964752197266, 720: 0.048833608627319336}}\r\n",
      "Finished.\r\n"
     ]
    }
   ],
   "source": [
    "!python -u train.py ETTh2 forecast_univar --loader forecast_csv_univar --repr-dims 320 --max-threads 8 --seed 42 --eval"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2569834,
     "sourceId": 4371501,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6436.246694,
   "end_time": "2025-10-01T18:27:56.140690",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-01T16:40:39.893996",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
