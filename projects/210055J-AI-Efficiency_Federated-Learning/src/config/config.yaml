# Algorithm
#   fedavg: baseline Federated Averaging
#   fedavg_kd: Federated Averaging with knowledge distillation
algorithm: fedavg

# Hyperparameters
lambda_: 0.5
T: 2.0
tau: 0.4

# Dataset
dataset_name: MNIST
batch_size: 50
model_name: mnist_cnn
num_clients: 100
non_iid: true
shards_per_client: 1
seed: 40

# Training parameters
epochs: 500
learning_rate: 0.005
local_epochs: 5          # number of local training passes per client
client_fraction: 0.1     # fraction C of clients sampled each round
