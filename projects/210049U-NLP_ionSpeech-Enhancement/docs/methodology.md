# Methodology: NLP:ionSpeech Enhancement

**Student:** 210049U  
**Research Area:** NLP:ionSpeech Enhancement  
**Date:** 2025-09-01  

---

## 1. Overview

This methodology presents a two-stage approach for improving the perceptual quality of speech separated by transformer-based models. The first stage employs a pre-trained **SepFormer** model for speech separation, while the second stage applies a **lightweight CNN-based denoiser** to suppress residual artifacts and enhance clarity. The proposed method focuses on achieving performance improvements with minimal computational overhead, enabling real-time applicability and scalability.

---

## 2. Research Design

The research adopts an **experimental and comparative design**, structured around evaluating the effectiveness of a post-processing CNN denoiser on outputs generated by a state-of-the-art separation model (SepFormer).  
The workflow includes:
1. **Input:** Mixtures of two speakers from the WSJ0-2mix dataset.  
2. **Stage 1:** Separation using the pre-trained SepFormer model.  
3. **Stage 2:** Post-processing through a lightweight CNN denoiser.  
4. **Evaluation:** Comparison of baseline and enhanced outputs using objective and perceptual metrics (SI-SDR, PESQ, STOI).  

This design allows the analysis of improvements in perceptual speech quality and intelligibility introduced by the denoiser.

---

## 3. Data Collection

### 3.1 Data Sources
- **Dataset:** WSJ0-2mix (Wall Street Journal Corpus, 2-speaker mixtures)  
- **Accessed via:** SpeechBrain toolkit / Hugging Face SepFormer repository  
- **Sampling Rate:** 8 kHz (narrowband)  

### 3.2 Data Description
- **Training Set:** 20,000 utterances from 50 speakers  
- **Validation Set:** 5,000 utterances  
- **Test Set:** 3,000 utterances  
- **Mixtures:** Randomly generated at 0 dB SNR using two clean speech signals  

Each mixture simulates a realistic overlapping conversation, providing a robust testbed for speech separation and enhancement.

### 3.3 Data Preprocessing
- Convert all audio to mono, 8 kHz format  
- Normalize amplitude levels  
- Segment long utterances into manageable chunks  
- Prepare data loaders for separation and enhancement tasks  
- Perform no data augmentation to ensure fair baseline comparison  

---

## 4. Model Architecture

The proposed system is a **two-stage pipeline**:

### Stage 1: SepFormer (Transformer-based Separation)
- **Architecture:** Dual-Path Transformer with encoder, masking network, and decoder  
- **Features:**  
  - Self-attention for long-range dependency modeling  
  - Permutation Invariant Training (PIT)  
  - 8 intra- and inter-transformer blocks with multi-head attention  

### Stage 2: Lightweight CNN Denoiser
- **Structure:** Two 1D convolutional layers with ReLU activation  
- **Parameters:** ~305 total  
- **Kernel Size:** 9 samples (receptive field ≈ 2.1 ms)  
- **Purpose:**  
  - Suppress high-frequency noise and musical artifacts  
  - Preserve intelligibility and naturalness  
- **Advantages:**  
  - Extremely lightweight (real-time feasible)  
  - Model-agnostic — compatible with other separation models  

---

## 5. Experimental Setup

### 5.1 Evaluation Metrics
- **SI-SDR (Scale-Invariant Signal-to-Distortion Ratio):** Measures separation quality  
- **PESQ (Perceptual Evaluation of Speech Quality):** Evaluates perceptual speech quality  
- **STOI (Short-Time Objective Intelligibility):** Assesses speech intelligibility  

### 5.2 Baseline Models
- **Baseline:** SepFormer-only output  
- **Proposed:** SepFormer + CNN Denoiser  
The comparison evaluates whether the proposed post-processing improves both objective and perceptual metrics.

### 5.3 Hardware/Software Requirements
- **Programming Language:** Python 3.10  
- **Frameworks:** PyTorch 2.0, TorchAudio, SpeechBrain  
- **Hardware:**  
  - CPU: 8 GB RAM (minimum)  
  - GPU: NVIDIA with ≥6 GB VRAM (recommended)  
- **Performance:**  
  - SepFormer: 0.42× real-time processing  
  - Proposed system: 0.48× real-time (≈15% overhead increase)  

---

## 6. Implementation Plan

| Phase | Tasks | Duration | Deliverables |
|-------|-------|----------|--------------|
| Phase 1 | Data preprocessing | 2 weeks | Clean, preprocessed WSJ0-2mix dataset |
| Phase 2 | Model implementation | 3 weeks | Integrated SepFormer + CNN denoiser pipeline |
| Phase 3 | Experiments | 2 weeks | Metric results (SI-SDR, PESQ, STOI) |
| Phase 4 | Analysis | 1 week | Comparative analysis and final report |

---

## 7. Risk Analysis

| Risk | Description | Mitigation |
|------|--------------|-------------|
| **Computational Constraints** | GPU memory limitations for SepFormer inference | Use batch-level processing and lightweight model parameters |
| **Overfitting (if trained)** | Denoiser may learn dataset-specific noise | Apply early stopping, regularization |
| **Model Integration Issues** | Compatibility between SepFormer and CNN stages | Standardize data formats and sample rates |
| **Metric Bias** | Objective metrics may not reflect perceptual quality | Combine quantitative (PESQ/STOI) with qualitative spectrogram analysis |

---

## 8. Expected Outcomes

- Improved **speech quality** and **intelligibility** in separated signals  
- Reduction of **residual artifacts** and **musical noise**  
- Validation that lightweight post-processing can enhance transformer-based separation  
- Demonstration of a **modular, low-cost** architecture suitable for real-time applications  
- Establishment of a baseline for future work involving supervised or adaptive denoiser training  

---

**Note:** Update this document as your methodology evolves during implementation.
