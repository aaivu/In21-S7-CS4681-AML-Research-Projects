{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e30aa9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-14T14:36:11.083757Z",
     "iopub.status.busy": "2025-10-14T14:36:11.083559Z",
     "iopub.status.idle": "2025-10-14T14:36:12.550021Z",
     "shell.execute_reply": "2025-10-14T14:36:12.549219Z"
    },
    "papermill": {
     "duration": 1.472361,
     "end_time": "2025-10-14T14:36:12.551287",
     "exception": false,
     "start_time": "2025-10-14T14:36:11.078926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/electricity/electricity.csv\n",
      "/kaggle/input/national-illness/national_illness.csv\n",
      "/kaggle/input/exchange-rate/exchange_rate.csv\n",
      "/kaggle/input/weather/weather.csv\n",
      "/kaggle/input/traffic/traffic.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce18de3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T14:36:12.559116Z",
     "iopub.status.busy": "2025-10-14T14:36:12.558848Z",
     "iopub.status.idle": "2025-10-14T14:36:13.898648Z",
     "shell.execute_reply": "2025-10-14T14:36:13.897749Z"
    },
    "papermill": {
     "duration": 1.345009,
     "end_time": "2025-10-14T14:36:13.900026",
     "exception": false,
     "start_time": "2025-10-14T14:36:12.555017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'PatchTST'...\r\n",
      "remote: Enumerating objects: 419, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (152/152), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (60/60), done.\u001b[K\r\n",
      "remote: Total 419 (delta 102), reused 92 (delta 92), pack-reused 267 (from 3)\u001b[K\r\n",
      "Receiving objects: 100% (419/419), 16.21 MiB | 34.86 MiB/s, done.\r\n",
      "Resolving deltas: 100% (198/198), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/deshithagallage/PatchTST.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "201431af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T14:36:13.909118Z",
     "iopub.status.busy": "2025-10-14T14:36:13.908886Z",
     "iopub.status.idle": "2025-10-14T14:36:13.914930Z",
     "shell.execute_reply": "2025-10-14T14:36:13.914189Z"
    },
    "papermill": {
     "duration": 0.011788,
     "end_time": "2025-10-14T14:36:13.915932",
     "exception": false,
     "start_time": "2025-10-14T14:36:13.904144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/PatchTST\n"
     ]
    }
   ],
   "source": [
    "cd PatchTST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c1b4d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T14:36:13.924171Z",
     "iopub.status.busy": "2025-10-14T14:36:13.923962Z",
     "iopub.status.idle": "2025-10-14T14:36:14.103856Z",
     "shell.execute_reply": "2025-10-14T14:36:14.103183Z"
    },
    "papermill": {
     "duration": 0.185399,
     "end_time": "2025-10-14T14:36:14.105126",
     "exception": false,
     "start_time": "2025-10-14T14:36:13.919727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch 'dev-multi-scale' set up to track remote branch 'dev-multi-scale' from 'origin'.\r\n",
      "Switched to a new branch 'dev-multi-scale'\r\n"
     ]
    }
   ],
   "source": [
    "!git checkout dev-multi-scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a66f7188",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T14:36:14.114224Z",
     "iopub.status.busy": "2025-10-14T14:36:14.113794Z",
     "iopub.status.idle": "2025-10-14T14:36:14.118724Z",
     "shell.execute_reply": "2025-10-14T14:36:14.118103Z"
    },
    "papermill": {
     "duration": 0.010813,
     "end_time": "2025-10-14T14:36:14.119826",
     "exception": false,
     "start_time": "2025-10-14T14:36:14.109013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/PatchTST/PatchTST_supervised\n"
     ]
    }
   ],
   "source": [
    "cd PatchTST_supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17a04493",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T14:36:14.128254Z",
     "iopub.status.busy": "2025-10-14T14:36:14.128082Z",
     "iopub.status.idle": "2025-10-14T14:36:14.253097Z",
     "shell.execute_reply": "2025-10-14T14:36:14.252365Z"
    },
    "papermill": {
     "duration": 0.130491,
     "end_time": "2025-10-14T14:36:14.254180",
     "exception": false,
     "start_time": "2025-10-14T14:36:14.123689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mkdir dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83d1550e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T14:36:14.263177Z",
     "iopub.status.busy": "2025-10-14T14:36:14.262962Z",
     "iopub.status.idle": "2025-10-14T14:36:14.267768Z",
     "shell.execute_reply": "2025-10-14T14:36:14.267201Z"
    },
    "papermill": {
     "duration": 0.01072,
     "end_time": "2025-10-14T14:36:14.268935",
     "exception": false,
     "start_time": "2025-10-14T14:36:14.258215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/PatchTST/PatchTST_supervised/dataset\n"
     ]
    }
   ],
   "source": [
    "cd dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eee7570f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T14:36:14.278217Z",
     "iopub.status.busy": "2025-10-14T14:36:14.278029Z",
     "iopub.status.idle": "2025-10-14T14:36:14.480675Z",
     "shell.execute_reply": "2025-10-14T14:36:14.479697Z"
    },
    "papermill": {
     "duration": 0.209011,
     "end_time": "2025-10-14T14:36:14.481930",
     "exception": false,
     "start_time": "2025-10-14T14:36:14.272919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cp /kaggle/input/weather/weather.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5fb7a19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T14:36:14.490684Z",
     "iopub.status.busy": "2025-10-14T14:36:14.490458Z",
     "iopub.status.idle": "2025-10-14T14:36:15.372546Z",
     "shell.execute_reply": "2025-10-14T14:36:15.371713Z"
    },
    "papermill": {
     "duration": 0.887933,
     "end_time": "2025-10-14T14:36:15.373889",
     "exception": false,
     "start_time": "2025-10-14T14:36:14.485956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cp /kaggle/input/electricity/electricity.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d4d7074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T14:36:15.383173Z",
     "iopub.status.busy": "2025-10-14T14:36:15.382939Z",
     "iopub.status.idle": "2025-10-14T14:36:16.523765Z",
     "shell.execute_reply": "2025-10-14T14:36:16.522852Z"
    },
    "papermill": {
     "duration": 1.146961,
     "end_time": "2025-10-14T14:36:16.525280",
     "exception": false,
     "start_time": "2025-10-14T14:36:15.378319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cp /kaggle/input/traffic/traffic.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bf7ff31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T14:36:16.536333Z",
     "iopub.status.busy": "2025-10-14T14:36:16.536041Z",
     "iopub.status.idle": "2025-10-14T14:36:16.667183Z",
     "shell.execute_reply": "2025-10-14T14:36:16.666412Z"
    },
    "papermill": {
     "duration": 0.137786,
     "end_time": "2025-10-14T14:36:16.668605",
     "exception": false,
     "start_time": "2025-10-14T14:36:16.530819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cp /kaggle/input/exchange-rate/exchange_rate.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c1c1de9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T14:36:16.677606Z",
     "iopub.status.busy": "2025-10-14T14:36:16.677352Z",
     "iopub.status.idle": "2025-10-14T14:36:16.803892Z",
     "shell.execute_reply": "2025-10-14T14:36:16.802933Z"
    },
    "papermill": {
     "duration": 0.132323,
     "end_time": "2025-10-14T14:36:16.805028",
     "exception": false,
     "start_time": "2025-10-14T14:36:16.672705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cp /kaggle/input/national-illness/national_illness.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ccf454a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T14:36:16.813643Z",
     "iopub.status.busy": "2025-10-14T14:36:16.813419Z",
     "iopub.status.idle": "2025-10-14T14:36:16.818242Z",
     "shell.execute_reply": "2025-10-14T14:36:16.817657Z"
    },
    "papermill": {
     "duration": 0.010356,
     "end_time": "2025-10-14T14:36:16.819273",
     "exception": false,
     "start_time": "2025-10-14T14:36:16.808917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/PatchTST/PatchTST_supervised\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e921add1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T14:36:16.827854Z",
     "iopub.status.busy": "2025-10-14T14:36:16.827670Z",
     "iopub.status.idle": "2025-10-14T14:45:49.014840Z",
     "shell.execute_reply": "2025-10-14T14:45:49.014058Z"
    },
    "papermill": {
     "duration": 572.192998,
     "end_time": "2025-10-14T14:45:49.016213",
     "exception": false,
     "start_time": "2025-10-14T14:36:16.823215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\r\n",
      "Namespace(random_seed=2021, is_training=1, model_id='weather_336_96_base_model', model='PatchTST', data='custom', root_path='./dataset/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=96, fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=21, dec_in=7, c_out=7, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=5, batch_size=128, patience=20, learning_rate=0.0001, des='Exp', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, multi_scale=None)\r\n",
      "Use GPU: cuda:0\r\n",
      "PatchTST Single-Scale Mode\r\n",
      ">>>>>>>start training : weather_336_96_base_model_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\r\n",
      "train 36456\r\n",
      "val 5175\r\n",
      "test 10444\r\n",
      "\titers: 100, epoch: 1 | loss: 0.7011203\r\n",
      "\tspeed: 0.3415s/iter; left time: 451.0797s\r\n",
      "\titers: 200, epoch: 1 | loss: 0.7144874\r\n",
      "\tspeed: 0.2937s/iter; left time: 358.6536s\r\n",
      "Epoch: 1 cost time: 84.30096125602722\r\n",
      "Epoch: 1, Steps: 284 | Train Loss: 0.7450223 Vali Loss: 0.5408273 Test Loss: 0.2212430\r\n",
      "Validation loss decreased (inf --> 0.540827).  Saving model ...\r\n",
      "Updating learning rate to 0.0001\r\n",
      "\titers: 100, epoch: 2 | loss: 0.3502237\r\n",
      "\tspeed: 0.7217s/iter; left time: 748.3609s\r\n",
      "\titers: 200, epoch: 2 | loss: 0.4068748\r\n",
      "\tspeed: 0.3446s/iter; left time: 322.8882s\r\n",
      "Epoch: 2 cost time: 94.68525648117065\r\n",
      "Epoch: 2, Steps: 284 | Train Loss: 0.4887680 Vali Loss: 0.4171748 Test Loss: 0.1693872\r\n",
      "Validation loss decreased (0.540827 --> 0.417175).  Saving model ...\r\n",
      "Updating learning rate to 0.0001\r\n",
      "\titers: 100, epoch: 3 | loss: 0.4496108\r\n",
      "\tspeed: 0.7790s/iter; left time: 586.5748s\r\n",
      "\titers: 200, epoch: 3 | loss: 0.4217881\r\n",
      "\tspeed: 0.3382s/iter; left time: 220.8227s\r\n",
      "Epoch: 3 cost time: 95.95711660385132\r\n",
      "Epoch: 3, Steps: 284 | Train Loss: 0.4476539 Vali Loss: 0.4049918 Test Loss: 0.1622655\r\n",
      "Validation loss decreased (0.417175 --> 0.404992).  Saving model ...\r\n",
      "Updating learning rate to 0.0001\r\n",
      "\titers: 100, epoch: 4 | loss: 0.3732247\r\n",
      "\tspeed: 0.7821s/iter; left time: 366.8148s\r\n",
      "\titers: 200, epoch: 4 | loss: 0.6333611\r\n",
      "\tspeed: 0.3372s/iter; left time: 124.4142s\r\n",
      "Epoch: 4 cost time: 96.10520625114441\r\n",
      "Epoch: 4, Steps: 284 | Train Loss: 0.4404233 Vali Loss: 0.3956116 Test Loss: 0.1590451\r\n",
      "Validation loss decreased (0.404992 --> 0.395612).  Saving model ...\r\n",
      "Updating learning rate to 9e-05\r\n",
      "\titers: 100, epoch: 5 | loss: 0.3678261\r\n",
      "\tspeed: 0.7806s/iter; left time: 144.4201s\r\n",
      "\titers: 200, epoch: 5 | loss: 0.8110214\r\n",
      "\tspeed: 0.3396s/iter; left time: 28.8687s\r\n",
      "Epoch: 5 cost time: 96.18678331375122\r\n",
      "Epoch: 5, Steps: 284 | Train Loss: 0.4352635 Vali Loss: 0.3964238 Test Loss: 0.1578531\r\n",
      "EarlyStopping counter: 1 out of 20\r\n",
      "Updating learning rate to 8.1e-05\r\n",
      ">>>>>>>testing : weather_336_96_base_model_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\r\n",
      "test 10444\r\n",
      "mse:0.1590450555086136, mae:0.20734095573425293, rse:0.5253758430480957\r\n"
     ]
    }
   ],
   "source": [
    "!python run_longExp.py \\\n",
    "    --random_seed 2021 \\\n",
    "    --is_training 1 \\\n",
    "    --root_path ./dataset/ \\\n",
    "    --data_path weather.csv \\\n",
    "    --model_id weather_336_96_base_model \\\n",
    "    --model PatchTST \\\n",
    "    --data custom \\\n",
    "    --features M \\\n",
    "    --seq_len 336 \\\n",
    "    --pred_len 96 \\\n",
    "    --enc_in 21 \\\n",
    "    --e_layers 3 \\\n",
    "    --n_heads 16 \\\n",
    "    --d_model 128 \\\n",
    "    --d_ff 256 \\\n",
    "    --dropout 0.2 \\\n",
    "    --fc_dropout 0.2 \\\n",
    "    --head_dropout 0 \\\n",
    "    --patch_len 16 \\\n",
    "    --stride 8 \\\n",
    "    --des 'Exp' \\\n",
    "    --train_epochs 5 \\\n",
    "    --patience 20 \\\n",
    "    --itr 1 \\\n",
    "    --batch_size 128 \\\n",
    "    --learning_rate 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74423350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T14:45:49.027927Z",
     "iopub.status.busy": "2025-10-14T14:45:49.027685Z",
     "iopub.status.idle": "2025-10-14T15:15:49.895576Z",
     "shell.execute_reply": "2025-10-14T15:15:49.894788Z"
    },
    "papermill": {
     "duration": 1800.87542,
     "end_time": "2025-10-14T15:15:49.897083",
     "exception": false,
     "start_time": "2025-10-14T14:45:49.021663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\r\n",
      "Namespace(random_seed=2021, is_training=1, model_id='weather_336_96_multi_scale', model='PatchTST', data='custom', root_path='./dataset/', data_path='weather.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=96, fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=21, dec_in=7, c_out=7, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=5, batch_size=128, patience=20, learning_rate=0.0001, des='Exp', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, multi_scale=['small', 'large'])\r\n",
      "Use GPU: cuda:0\r\n",
      "PatchTST Multi-Scale Configuration: ['small', 'large']\r\n",
      "Using scales: ['small', 'large']\r\n",
      "Patch sizes: [8, 32]\r\n",
      "Strides: [4, 16]\r\n",
      ">>>>>>>start training : weather_336_96_multi_scale_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\r\n",
      "train 36456\r\n",
      "val 5175\r\n",
      "test 10444\r\n",
      "\titers: 100, epoch: 1 | loss: 0.9163474\r\n",
      "\tspeed: 1.0794s/iter; left time: 1425.9040s\r\n",
      "\titers: 200, epoch: 1 | loss: 1.0174698\r\n",
      "\tspeed: 1.0672s/iter; left time: 1303.0051s\r\n",
      "Epoch: 1 cost time: 303.2447474002838\r\n",
      "Epoch: 1, Steps: 284 | Train Loss: 0.9477274 Vali Loss: 0.6390595 Test Loss: 0.5135384\r\n",
      "Validation loss decreased (inf --> 0.639060).  Saving model ...\r\n",
      "Updating learning rate to 0.0001\r\n",
      "\titers: 100, epoch: 2 | loss: 0.6310247\r\n",
      "\tspeed: 2.4507s/iter; left time: 2541.4233s\r\n",
      "\titers: 200, epoch: 2 | loss: 0.4004315\r\n",
      "\tspeed: 1.0690s/iter; left time: 1001.6305s\r\n",
      "Epoch: 2 cost time: 303.41011691093445\r\n",
      "Epoch: 2, Steps: 284 | Train Loss: 0.4858512 Vali Loss: 0.3982258 Test Loss: 0.1624506\r\n",
      "Validation loss decreased (0.639060 --> 0.398226).  Saving model ...\r\n",
      "Updating learning rate to 0.0001\r\n",
      "\titers: 100, epoch: 3 | loss: 0.4735448\r\n",
      "\tspeed: 2.4528s/iter; left time: 1846.9850s\r\n",
      "\titers: 200, epoch: 3 | loss: 0.3824500\r\n",
      "\tspeed: 1.0701s/iter; left time: 698.7718s\r\n",
      "Epoch: 3 cost time: 302.76813077926636\r\n",
      "Epoch: 3, Steps: 284 | Train Loss: 0.4295113 Vali Loss: 0.3902224 Test Loss: 0.1581929\r\n",
      "Validation loss decreased (0.398226 --> 0.390222).  Saving model ...\r\n",
      "Updating learning rate to 0.0001\r\n",
      "\titers: 100, epoch: 4 | loss: 0.4145046\r\n",
      "\tspeed: 2.4452s/iter; left time: 1146.7947s\r\n",
      "\titers: 200, epoch: 4 | loss: 0.3266690\r\n",
      "\tspeed: 1.0701s/iter; left time: 394.8603s\r\n",
      "Epoch: 4 cost time: 303.0756254196167\r\n",
      "Epoch: 4, Steps: 284 | Train Loss: 0.4231337 Vali Loss: 0.3902161 Test Loss: 0.1540431\r\n",
      "Validation loss decreased (0.390222 --> 0.390216).  Saving model ...\r\n",
      "Updating learning rate to 9e-05\r\n",
      "\titers: 100, epoch: 5 | loss: 0.9077821\r\n",
      "\tspeed: 2.4546s/iter; left time: 454.1060s\r\n",
      "\titers: 200, epoch: 5 | loss: 0.4180262\r\n",
      "\tspeed: 1.0706s/iter; left time: 90.9985s\r\n",
      "Epoch: 5 cost time: 303.3582501411438\r\n",
      "Epoch: 5, Steps: 284 | Train Loss: 0.4167900 Vali Loss: 0.3849314 Test Loss: 0.1504630\r\n",
      "Validation loss decreased (0.390216 --> 0.384931).  Saving model ...\r\n",
      "Updating learning rate to 8.1e-05\r\n",
      ">>>>>>>testing : weather_336_96_multi_scale_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\r\n",
      "test 10444\r\n",
      "mse:0.15046298503875732, mae:0.20863749086856842, rse:0.511004626750946\r\n"
     ]
    }
   ],
   "source": [
    "!python run_longExp.py \\\n",
    "    --random_seed 2021 \\\n",
    "    --is_training 1 \\\n",
    "    --root_path ./dataset/ \\\n",
    "    --data_path weather.csv \\\n",
    "    --model_id weather_336_96_multi_scale \\\n",
    "    --model PatchTST \\\n",
    "    --data custom \\\n",
    "    --features M \\\n",
    "    --seq_len 336 \\\n",
    "    --pred_len 96 \\\n",
    "    --enc_in 21 \\\n",
    "    --e_layers 3 \\\n",
    "    --n_heads 16 \\\n",
    "    --d_model 128 \\\n",
    "    --d_ff 256 \\\n",
    "    --dropout 0.2 \\\n",
    "    --fc_dropout 0.2 \\\n",
    "    --head_dropout 0 \\\n",
    "    --patch_len 16 \\\n",
    "    --stride 8 \\\n",
    "    --des 'Exp' \\\n",
    "    --train_epochs 5 \\\n",
    "    --patience 20 \\\n",
    "    --itr 1 \\\n",
    "    --batch_size 128 \\\n",
    "    --learning_rate 0.0001 \\\n",
    "    --multi_scale small large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8fefe11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T15:15:49.911129Z",
     "iopub.status.busy": "2025-10-14T15:15:49.910897Z",
     "iopub.status.idle": "2025-10-14T16:27:14.511564Z",
     "shell.execute_reply": "2025-10-14T16:27:14.510689Z"
    },
    "papermill": {
     "duration": 4284.609337,
     "end_time": "2025-10-14T16:27:14.513084",
     "exception": false,
     "start_time": "2025-10-14T15:15:49.903747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\r\n",
      "Namespace(random_seed=2021, is_training=1, model_id='electricity_336_96_base_model', model='PatchTST', data='custom', root_path='./dataset/', data_path='electricity.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=96, fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=321, dec_in=7, c_out=7, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=5, batch_size=8, patience=10, learning_rate=0.0001, des='Exp', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, multi_scale=None)\r\n",
      "Use GPU: cuda:0\r\n",
      "PatchTST Single-Scale Mode\r\n",
      ">>>>>>>start training : electricity_336_96_base_model_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\r\n",
      "train 17981\r\n",
      "val 2537\r\n",
      "test 5165\r\n",
      "\titers: 100, epoch: 1 | loss: 0.5278893\r\n",
      "\tspeed: 0.3395s/iter; left time: 3780.2073s\r\n",
      "\titers: 200, epoch: 1 | loss: 0.3318447\r\n",
      "\tspeed: 0.3250s/iter; left time: 3586.9667s\r\n",
      "\titers: 300, epoch: 1 | loss: 0.3349011\r\n",
      "\tspeed: 0.3163s/iter; left time: 3459.5160s\r\n",
      "\titers: 400, epoch: 1 | loss: 0.3675778\r\n",
      "\tspeed: 0.3227s/iter; left time: 3496.7821s\r\n",
      "\titers: 500, epoch: 1 | loss: 0.2828482\r\n",
      "\tspeed: 0.3205s/iter; left time: 3440.3792s\r\n",
      "\titers: 600, epoch: 1 | loss: 0.3313993\r\n",
      "\tspeed: 0.3195s/iter; left time: 3397.8902s\r\n",
      "\titers: 700, epoch: 1 | loss: 0.2607889\r\n",
      "\tspeed: 0.3223s/iter; left time: 3395.6852s\r\n",
      "\titers: 800, epoch: 1 | loss: 0.2437205\r\n",
      "\tspeed: 0.3230s/iter; left time: 3371.1280s\r\n",
      "\titers: 900, epoch: 1 | loss: 0.2466685\r\n",
      "\tspeed: 0.3215s/iter; left time: 3322.8377s\r\n",
      "\titers: 1000, epoch: 1 | loss: 0.2478536\r\n",
      "\tspeed: 0.3202s/iter; left time: 3277.5458s\r\n",
      "\titers: 1100, epoch: 1 | loss: 0.2527246\r\n",
      "\tspeed: 0.3188s/iter; left time: 3230.9620s\r\n",
      "\titers: 1200, epoch: 1 | loss: 0.2729540\r\n",
      "\tspeed: 0.3198s/iter; left time: 3209.1691s\r\n",
      "\titers: 1300, epoch: 1 | loss: 0.2061709\r\n",
      "\tspeed: 0.3215s/iter; left time: 3194.6406s\r\n",
      "\titers: 1400, epoch: 1 | loss: 0.2374721\r\n",
      "\tspeed: 0.3229s/iter; left time: 3176.0231s\r\n",
      "\titers: 1500, epoch: 1 | loss: 0.2403047\r\n",
      "\tspeed: 0.3230s/iter; left time: 3144.4593s\r\n",
      "\titers: 1600, epoch: 1 | loss: 0.1834155\r\n",
      "\tspeed: 0.3230s/iter; left time: 3112.0175s\r\n",
      "\titers: 1700, epoch: 1 | loss: 0.2053511\r\n",
      "\tspeed: 0.3230s/iter; left time: 3079.7638s\r\n",
      "\titers: 1800, epoch: 1 | loss: 0.2231662\r\n",
      "\tspeed: 0.3214s/iter; left time: 3032.8350s\r\n",
      "\titers: 1900, epoch: 1 | loss: 0.2196152\r\n",
      "\tspeed: 0.3196s/iter; left time: 2984.2151s\r\n",
      "\titers: 2000, epoch: 1 | loss: 0.1766215\r\n",
      "\tspeed: 0.3189s/iter; left time: 2945.0881s\r\n",
      "\titers: 2100, epoch: 1 | loss: 0.2129649\r\n",
      "\tspeed: 0.3190s/iter; left time: 2914.7145s\r\n",
      "\titers: 2200, epoch: 1 | loss: 0.1785465\r\n",
      "\tspeed: 0.3188s/iter; left time: 2880.8778s\r\n",
      "Epoch: 1 cost time: 721.9212582111359\r\n",
      "Epoch: 1, Steps: 2247 | Train Loss: 0.2756623 Vali Loss: 0.1405221 Test Loss: 0.1613752\r\n",
      "Validation loss decreased (inf --> 0.140522).  Saving model ...\r\n",
      "Updating learning rate to 0.0001\r\n",
      "\titers: 100, epoch: 2 | loss: 0.1948931\r\n",
      "\tspeed: 1.6162s/iter; left time: 14366.1032s\r\n",
      "\titers: 200, epoch: 2 | loss: 0.1753816\r\n",
      "\tspeed: 0.3222s/iter; left time: 2831.6141s\r\n",
      "\titers: 300, epoch: 2 | loss: 0.1939784\r\n",
      "\tspeed: 0.3201s/iter; left time: 2781.0248s\r\n",
      "\titers: 400, epoch: 2 | loss: 0.1671970\r\n",
      "\tspeed: 0.3188s/iter; left time: 2738.2531s\r\n",
      "\titers: 500, epoch: 2 | loss: 0.1581803\r\n",
      "\tspeed: 0.3193s/iter; left time: 2710.2017s\r\n",
      "\titers: 600, epoch: 2 | loss: 0.1583950\r\n",
      "\tspeed: 0.3212s/iter; left time: 2694.7517s\r\n",
      "\titers: 700, epoch: 2 | loss: 0.1652164\r\n",
      "\tspeed: 0.3228s/iter; left time: 2675.9855s\r\n",
      "\titers: 800, epoch: 2 | loss: 0.2160162\r\n",
      "\tspeed: 0.3229s/iter; left time: 2644.1672s\r\n",
      "\titers: 900, epoch: 2 | loss: 0.1444434\r\n",
      "\tspeed: 0.3230s/iter; left time: 2613.0930s\r\n",
      "\titers: 1000, epoch: 2 | loss: 0.1540732\r\n",
      "\tspeed: 0.3229s/iter; left time: 2579.7307s\r\n",
      "\titers: 1100, epoch: 2 | loss: 0.1633892\r\n",
      "\tspeed: 0.3216s/iter; left time: 2537.4577s\r\n",
      "\titers: 1200, epoch: 2 | loss: 0.1993844\r\n",
      "\tspeed: 0.3206s/iter; left time: 2496.9202s\r\n",
      "\titers: 1300, epoch: 2 | loss: 0.1956711\r\n",
      "\tspeed: 0.3189s/iter; left time: 2452.3542s\r\n",
      "\titers: 1400, epoch: 2 | loss: 0.1384152\r\n",
      "\tspeed: 0.3216s/iter; left time: 2440.5839s\r\n",
      "\titers: 1500, epoch: 2 | loss: 0.1383210\r\n",
      "\tspeed: 0.3234s/iter; left time: 2421.5907s\r\n",
      "\titers: 1600, epoch: 2 | loss: 0.2108856\r\n",
      "\tspeed: 0.3219s/iter; left time: 2378.2787s\r\n",
      "\titers: 1700, epoch: 2 | loss: 0.2240162\r\n",
      "\tspeed: 0.3195s/iter; left time: 2328.5769s\r\n",
      "\titers: 1800, epoch: 2 | loss: 0.1374347\r\n",
      "\tspeed: 0.3189s/iter; left time: 2292.3812s\r\n",
      "\titers: 1900, epoch: 2 | loss: 0.1406414\r\n",
      "\tspeed: 0.3192s/iter; left time: 2262.5904s\r\n",
      "\titers: 2000, epoch: 2 | loss: 0.1745583\r\n",
      "\tspeed: 0.3194s/iter; left time: 2232.0506s\r\n",
      "\titers: 2100, epoch: 2 | loss: 0.2165250\r\n",
      "\tspeed: 0.3191s/iter; left time: 2198.2220s\r\n",
      "\titers: 2200, epoch: 2 | loss: 0.1949797\r\n",
      "\tspeed: 0.3188s/iter; left time: 2164.1968s\r\n",
      "Epoch: 2 cost time: 721.1999650001526\r\n",
      "Epoch: 2, Steps: 2247 | Train Loss: 0.1657450 Vali Loss: 0.1245184 Test Loss: 0.1449009\r\n",
      "Validation loss decreased (0.140522 --> 0.124518).  Saving model ...\r\n",
      "Updating learning rate to 0.0001\r\n",
      "\titers: 100, epoch: 3 | loss: 0.1286751\r\n",
      "\tspeed: 1.6283s/iter; left time: 10815.2654s\r\n",
      "\titers: 200, epoch: 3 | loss: 0.1465254\r\n",
      "\tspeed: 0.3231s/iter; left time: 2113.8707s\r\n",
      "\titers: 300, epoch: 3 | loss: 0.1885252\r\n",
      "\tspeed: 0.3219s/iter; left time: 2073.5171s\r\n",
      "\titers: 400, epoch: 3 | loss: 0.1839216\r\n",
      "\tspeed: 0.3199s/iter; left time: 2029.0555s\r\n",
      "\titers: 500, epoch: 3 | loss: 0.2071982\r\n",
      "\tspeed: 0.3203s/iter; left time: 1999.0454s\r\n",
      "\titers: 600, epoch: 3 | loss: 0.1711352\r\n",
      "\tspeed: 0.3230s/iter; left time: 1983.9215s\r\n",
      "\titers: 700, epoch: 3 | loss: 0.1548778\r\n",
      "\tspeed: 0.3195s/iter; left time: 1930.5800s\r\n",
      "\titers: 800, epoch: 3 | loss: 0.2022557\r\n",
      "\tspeed: 0.3188s/iter; left time: 1894.2154s\r\n",
      "\titers: 900, epoch: 3 | loss: 0.2720107\r\n",
      "\tspeed: 0.3195s/iter; left time: 1866.3883s\r\n",
      "\titers: 1000, epoch: 3 | loss: 0.1840166\r\n",
      "\tspeed: 0.3202s/iter; left time: 1838.7681s\r\n",
      "\titers: 1100, epoch: 3 | loss: 0.1387421\r\n",
      "\tspeed: 0.3224s/iter; left time: 1818.7902s\r\n",
      "\titers: 1200, epoch: 3 | loss: 0.1450004\r\n",
      "\tspeed: 0.3229s/iter; left time: 1789.6905s\r\n",
      "\titers: 1300, epoch: 3 | loss: 0.1552057\r\n",
      "\tspeed: 0.3218s/iter; left time: 1751.5021s\r\n",
      "\titers: 1400, epoch: 3 | loss: 0.1210210\r\n",
      "\tspeed: 0.3213s/iter; left time: 1716.2078s\r\n",
      "\titers: 1500, epoch: 3 | loss: 0.1204036\r\n",
      "\tspeed: 0.3228s/iter; left time: 1692.3564s\r\n",
      "\titers: 1600, epoch: 3 | loss: 0.1954177\r\n",
      "\tspeed: 0.3228s/iter; left time: 1659.6463s\r\n",
      "\titers: 1700, epoch: 3 | loss: 0.1673880\r\n",
      "\tspeed: 0.3200s/iter; left time: 1613.2221s\r\n",
      "\titers: 1800, epoch: 3 | loss: 0.1583497\r\n",
      "\tspeed: 0.3181s/iter; left time: 1572.2434s\r\n",
      "\titers: 1900, epoch: 3 | loss: 0.1469561\r\n",
      "\tspeed: 0.3222s/iter; left time: 1560.0700s\r\n",
      "\titers: 2000, epoch: 3 | loss: 0.1459798\r\n",
      "\tspeed: 0.3233s/iter; left time: 1532.9858s\r\n",
      "\titers: 2100, epoch: 3 | loss: 0.1594882\r\n",
      "\tspeed: 0.3208s/iter; left time: 1489.0832s\r\n",
      "\titers: 2200, epoch: 3 | loss: 0.1927959\r\n",
      "\tspeed: 0.3197s/iter; left time: 1452.2149s\r\n",
      "Epoch: 3 cost time: 721.8622193336487\r\n",
      "Epoch: 3, Steps: 2247 | Train Loss: 0.1522552 Vali Loss: 0.1241752 Test Loss: 0.1436533\r\n",
      "Validation loss decreased (0.124518 --> 0.124175).  Saving model ...\r\n",
      "Updating learning rate to 0.0001\r\n",
      "\titers: 100, epoch: 4 | loss: 0.1407925\r\n",
      "\tspeed: 1.6312s/iter; left time: 7169.3136s\r\n",
      "\titers: 200, epoch: 4 | loss: 0.1898603\r\n",
      "\tspeed: 0.3207s/iter; left time: 1377.4948s\r\n",
      "\titers: 300, epoch: 4 | loss: 0.1257039\r\n",
      "\tspeed: 0.3232s/iter; left time: 1355.9347s\r\n",
      "\titers: 400, epoch: 4 | loss: 0.1301913\r\n",
      "\tspeed: 0.3224s/iter; left time: 1320.2145s\r\n",
      "\titers: 500, epoch: 4 | loss: 0.1387844\r\n",
      "\tspeed: 0.3198s/iter; left time: 1277.7412s\r\n",
      "\titers: 600, epoch: 4 | loss: 0.1754128\r\n",
      "\tspeed: 0.3191s/iter; left time: 1242.7621s\r\n",
      "\titers: 700, epoch: 4 | loss: 0.1450814\r\n",
      "\tspeed: 0.3202s/iter; left time: 1215.1490s\r\n",
      "\titers: 800, epoch: 4 | loss: 0.1500617\r\n",
      "\tspeed: 0.3202s/iter; left time: 1182.9854s\r\n",
      "\titers: 900, epoch: 4 | loss: 0.1176744\r\n",
      "\tspeed: 0.3198s/iter; left time: 1149.6286s\r\n",
      "\titers: 1000, epoch: 4 | loss: 0.1344004\r\n",
      "\tspeed: 0.3199s/iter; left time: 1117.9155s\r\n",
      "\titers: 1100, epoch: 4 | loss: 0.1639671\r\n",
      "\tspeed: 0.3203s/iter; left time: 1087.2499s\r\n",
      "\titers: 1200, epoch: 4 | loss: 0.1103050\r\n",
      "\tspeed: 0.3205s/iter; left time: 1055.8996s\r\n",
      "\titers: 1300, epoch: 4 | loss: 0.1208275\r\n",
      "\tspeed: 0.3200s/iter; left time: 1022.4283s\r\n",
      "\titers: 1400, epoch: 4 | loss: 0.1254950\r\n",
      "\tspeed: 0.3209s/iter; left time: 993.2730s\r\n",
      "\titers: 1500, epoch: 4 | loss: 0.1382875\r\n",
      "\tspeed: 0.3206s/iter; left time: 960.0614s\r\n",
      "\titers: 1600, epoch: 4 | loss: 0.1383367\r\n",
      "\tspeed: 0.3210s/iter; left time: 929.1722s\r\n",
      "\titers: 1700, epoch: 4 | loss: 0.1231288\r\n",
      "\tspeed: 0.3203s/iter; left time: 895.2433s\r\n",
      "\titers: 1800, epoch: 4 | loss: 0.1488818\r\n",
      "\tspeed: 0.3203s/iter; left time: 863.1483s\r\n",
      "\titers: 1900, epoch: 4 | loss: 0.1381495\r\n",
      "\tspeed: 0.3202s/iter; left time: 830.9980s\r\n",
      "\titers: 2000, epoch: 4 | loss: 0.1681359\r\n",
      "\tspeed: 0.3199s/iter; left time: 798.1701s\r\n",
      "\titers: 2100, epoch: 4 | loss: 0.1235186\r\n",
      "\tspeed: 0.3205s/iter; left time: 767.6215s\r\n",
      "\titers: 2200, epoch: 4 | loss: 0.1442446\r\n",
      "\tspeed: 0.3199s/iter; left time: 734.0898s\r\n",
      "Epoch: 4 cost time: 720.2298529148102\r\n",
      "Epoch: 4, Steps: 2247 | Train Loss: 0.1467590 Vali Loss: 0.1193646 Test Loss: 0.1377885\r\n",
      "Validation loss decreased (0.124175 --> 0.119365).  Saving model ...\r\n",
      "Updating learning rate to 9e-05\r\n",
      "\titers: 100, epoch: 5 | loss: 0.1183372\r\n",
      "\tspeed: 1.6338s/iter; left time: 3509.3777s\r\n",
      "\titers: 200, epoch: 5 | loss: 0.1147463\r\n",
      "\tspeed: 0.3224s/iter; left time: 660.2442s\r\n",
      "\titers: 300, epoch: 5 | loss: 0.1243312\r\n",
      "\tspeed: 0.3217s/iter; left time: 626.7218s\r\n",
      "\titers: 400, epoch: 5 | loss: 0.1183321\r\n",
      "\tspeed: 0.3203s/iter; left time: 591.8886s\r\n",
      "\titers: 500, epoch: 5 | loss: 0.1447271\r\n",
      "\tspeed: 0.3196s/iter; left time: 558.7305s\r\n",
      "\titers: 600, epoch: 5 | loss: 0.1157906\r\n",
      "\tspeed: 0.3202s/iter; left time: 527.6610s\r\n",
      "\titers: 700, epoch: 5 | loss: 0.1364062\r\n",
      "\tspeed: 0.3197s/iter; left time: 494.9464s\r\n",
      "\titers: 800, epoch: 5 | loss: 0.1141705\r\n",
      "\tspeed: 0.3192s/iter; left time: 462.1938s\r\n",
      "\titers: 900, epoch: 5 | loss: 0.1897314\r\n",
      "\tspeed: 0.3191s/iter; left time: 430.1327s\r\n",
      "\titers: 1000, epoch: 5 | loss: 0.1713048\r\n",
      "\tspeed: 0.3185s/iter; left time: 397.4814s\r\n",
      "\titers: 1100, epoch: 5 | loss: 0.1268114\r\n",
      "\tspeed: 0.3213s/iter; left time: 368.8076s\r\n",
      "\titers: 1200, epoch: 5 | loss: 0.1425439\r\n",
      "\tspeed: 0.3228s/iter; left time: 338.3267s\r\n",
      "\titers: 1300, epoch: 5 | loss: 0.1486848\r\n",
      "\tspeed: 0.3194s/iter; left time: 302.7864s\r\n",
      "\titers: 1400, epoch: 5 | loss: 0.1500425\r\n",
      "\tspeed: 0.3186s/iter; left time: 270.1942s\r\n",
      "\titers: 1500, epoch: 5 | loss: 0.1440477\r\n",
      "\tspeed: 0.3187s/iter; left time: 238.3966s\r\n",
      "\titers: 1600, epoch: 5 | loss: 0.1068596\r\n",
      "\tspeed: 0.3203s/iter; left time: 207.5782s\r\n",
      "\titers: 1700, epoch: 5 | loss: 0.1328326\r\n",
      "\tspeed: 0.3222s/iter; left time: 176.5795s\r\n",
      "\titers: 1800, epoch: 5 | loss: 0.1322572\r\n",
      "\tspeed: 0.3214s/iter; left time: 143.9915s\r\n",
      "\titers: 1900, epoch: 5 | loss: 0.1272332\r\n",
      "\tspeed: 0.3215s/iter; left time: 111.8688s\r\n",
      "\titers: 2000, epoch: 5 | loss: 0.1183878\r\n",
      "\tspeed: 0.3231s/iter; left time: 80.1314s\r\n",
      "\titers: 2100, epoch: 5 | loss: 0.1203903\r\n",
      "\tspeed: 0.3205s/iter; left time: 47.4289s\r\n",
      "\titers: 2200, epoch: 5 | loss: 0.1260573\r\n",
      "\tspeed: 0.3220s/iter; left time: 15.4573s\r\n",
      "Epoch: 5 cost time: 721.1879856586456\r\n",
      "Epoch: 5, Steps: 2247 | Train Loss: 0.1429079 Vali Loss: 0.1191798 Test Loss: 0.1379354\r\n",
      "Validation loss decreased (0.119365 --> 0.119180).  Saving model ...\r\n",
      "Updating learning rate to 8.1e-05\r\n",
      ">>>>>>>testing : electricity_336_96_base_model_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\r\n",
      "test 5165\r\n",
      "mse:0.13793540000915527, mae:0.2342091202735901, rse:0.36917001008987427\r\n"
     ]
    }
   ],
   "source": [
    "!python run_longExp.py \\\n",
    "    --random_seed 2021 \\\n",
    "    --is_training 1 \\\n",
    "    --root_path ./dataset/ \\\n",
    "    --data_path electricity.csv \\\n",
    "    --model_id electricity_336_96_base_model \\\n",
    "    --model PatchTST \\\n",
    "    --data custom \\\n",
    "    --features M \\\n",
    "    --seq_len 336 \\\n",
    "    --pred_len 96 \\\n",
    "    --enc_in 321 \\\n",
    "    --e_layers 3 \\\n",
    "    --n_heads 16 \\\n",
    "    --d_model 128 \\\n",
    "    --d_ff 256 \\\n",
    "    --dropout 0.2 \\\n",
    "    --fc_dropout 0.2 \\\n",
    "    --head_dropout 0 \\\n",
    "    --patch_len 16 \\\n",
    "    --stride 8 \\\n",
    "    --des 'Exp' \\\n",
    "    --train_epochs 5 \\\n",
    "    --patience 10 \\\n",
    "    --itr 1 \\\n",
    "    --batch_size 8 \\\n",
    "    --learning_rate 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "391763f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T16:27:14.537376Z",
     "iopub.status.busy": "2025-10-14T16:27:14.537151Z",
     "iopub.status.idle": "2025-10-14T20:13:40.407046Z",
     "shell.execute_reply": "2025-10-14T20:13:40.405916Z"
    },
    "papermill": {
     "duration": 13585.883672,
     "end_time": "2025-10-14T20:13:40.408677",
     "exception": false,
     "start_time": "2025-10-14T16:27:14.525005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\r\n",
      "Namespace(random_seed=2021, is_training=1, model_id='electricity_336_96_multi_scale', model='PatchTST', data='custom', root_path='./dataset/', data_path='electricity.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=96, fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=321, dec_in=7, c_out=7, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=5, batch_size=8, patience=10, learning_rate=0.0001, des='Exp', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, multi_scale=['small', 'large'])\r\n",
      "Use GPU: cuda:0\r\n",
      "PatchTST Multi-Scale Configuration: ['small', 'large']\r\n",
      "Using scales: ['small', 'large']\r\n",
      "Patch sizes: [8, 32]\r\n",
      "Strides: [4, 16]\r\n",
      ">>>>>>>start training : electricity_336_96_multi_scale_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\r\n",
      "train 17981\r\n",
      "val 2537\r\n",
      "test 5165\r\n",
      "\titers: 100, epoch: 1 | loss: 0.5209990\r\n",
      "\tspeed: 1.0321s/iter; left time: 11493.8288s\r\n",
      "\titers: 200, epoch: 1 | loss: 0.5361645\r\n",
      "\tspeed: 1.0225s/iter; left time: 11283.9557s\r\n",
      "\titers: 300, epoch: 1 | loss: 0.4265775\r\n",
      "\tspeed: 1.0227s/iter; left time: 11184.2083s\r\n",
      "\titers: 400, epoch: 1 | loss: 0.3466761\r\n",
      "\tspeed: 1.0216s/iter; left time: 11070.2324s\r\n",
      "\titers: 500, epoch: 1 | loss: 0.3306397\r\n",
      "\tspeed: 1.0234s/iter; left time: 10987.3627s\r\n",
      "\titers: 600, epoch: 1 | loss: 0.3082964\r\n",
      "\tspeed: 1.0216s/iter; left time: 10865.7011s\r\n",
      "\titers: 700, epoch: 1 | loss: 0.3058453\r\n",
      "\tspeed: 1.0167s/iter; left time: 10711.8859s\r\n",
      "\titers: 800, epoch: 1 | loss: 0.2378226\r\n",
      "\tspeed: 1.0226s/iter; left time: 10671.6919s\r\n",
      "\titers: 900, epoch: 1 | loss: 0.2754136\r\n",
      "\tspeed: 1.0214s/iter; left time: 10556.6862s\r\n",
      "\titers: 1000, epoch: 1 | loss: 0.2969073\r\n",
      "\tspeed: 1.0197s/iter; left time: 10437.7906s\r\n",
      "\titers: 1100, epoch: 1 | loss: 0.2001367\r\n",
      "\tspeed: 1.0205s/iter; left time: 10343.5321s\r\n",
      "\titers: 1200, epoch: 1 | loss: 0.2712776\r\n",
      "\tspeed: 1.0219s/iter; left time: 10256.1327s\r\n",
      "\titers: 1300, epoch: 1 | loss: 0.2453880\r\n",
      "\tspeed: 1.0247s/iter; left time: 10181.7287s\r\n",
      "\titers: 1400, epoch: 1 | loss: 0.2582878\r\n",
      "\tspeed: 1.0168s/iter; left time: 10001.4767s\r\n",
      "\titers: 1500, epoch: 1 | loss: 0.2310904\r\n",
      "\tspeed: 1.0205s/iter; left time: 9935.2491s\r\n",
      "\titers: 1600, epoch: 1 | loss: 0.2377471\r\n",
      "\tspeed: 1.0226s/iter; left time: 9853.7564s\r\n",
      "\titers: 1700, epoch: 1 | loss: 0.1690586\r\n",
      "\tspeed: 1.0251s/iter; left time: 9775.3158s\r\n",
      "\titers: 1800, epoch: 1 | loss: 0.2100630\r\n",
      "\tspeed: 1.0221s/iter; left time: 9644.8651s\r\n",
      "\titers: 1900, epoch: 1 | loss: 0.2325654\r\n",
      "\tspeed: 1.0223s/iter; left time: 9544.2659s\r\n",
      "\titers: 2000, epoch: 1 | loss: 0.2006111\r\n",
      "\tspeed: 1.0227s/iter; left time: 9445.7293s\r\n",
      "\titers: 2100, epoch: 1 | loss: 0.1850803\r\n",
      "\tspeed: 1.0261s/iter; left time: 9374.3235s\r\n",
      "\titers: 2200, epoch: 1 | loss: 0.1647065\r\n",
      "\tspeed: 1.0241s/iter; left time: 9253.3351s\r\n",
      "Epoch: 1 cost time: 2296.492915391922\r\n",
      "Epoch: 1, Steps: 2247 | Train Loss: 0.2958092 Vali Loss: 0.1538542 Test Loss: 0.1755869\r\n",
      "Validation loss decreased (inf --> 0.153854).  Saving model ...\r\n",
      "Updating learning rate to 0.0001\r\n",
      "\titers: 100, epoch: 2 | loss: 0.1659652\r\n",
      "\tspeed: 5.1488s/iter; left time: 45767.4678s\r\n",
      "\titers: 200, epoch: 2 | loss: 0.2085687\r\n",
      "\tspeed: 1.0184s/iter; left time: 8951.0172s\r\n",
      "\titers: 300, epoch: 2 | loss: 0.2141621\r\n",
      "\tspeed: 1.0220s/iter; left time: 8880.5693s\r\n",
      "\titers: 400, epoch: 2 | loss: 0.2014842\r\n",
      "\tspeed: 1.0221s/iter; left time: 8778.8089s\r\n",
      "\titers: 500, epoch: 2 | loss: 0.1458619\r\n",
      "\tspeed: 1.0248s/iter; left time: 8699.3473s\r\n",
      "\titers: 600, epoch: 2 | loss: 0.1963604\r\n",
      "\tspeed: 1.0238s/iter; left time: 8588.8476s\r\n",
      "\titers: 700, epoch: 2 | loss: 0.1288856\r\n",
      "\tspeed: 1.0240s/iter; left time: 8487.6187s\r\n",
      "\titers: 800, epoch: 2 | loss: 0.1295415\r\n",
      "\tspeed: 1.0233s/iter; left time: 8380.0283s\r\n",
      "\titers: 900, epoch: 2 | loss: 0.1391398\r\n",
      "\tspeed: 1.0216s/iter; left time: 8263.3794s\r\n",
      "\titers: 1000, epoch: 2 | loss: 0.1525184\r\n",
      "\tspeed: 1.0232s/iter; left time: 8174.1416s\r\n",
      "\titers: 1100, epoch: 2 | loss: 0.1807039\r\n",
      "\tspeed: 1.0221s/iter; left time: 8063.2431s\r\n",
      "\titers: 1200, epoch: 2 | loss: 0.1557963\r\n",
      "\tspeed: 1.0203s/iter; left time: 7947.4952s\r\n",
      "\titers: 1300, epoch: 2 | loss: 0.1779004\r\n",
      "\tspeed: 1.0267s/iter; left time: 7894.4466s\r\n",
      "\titers: 1400, epoch: 2 | loss: 0.1674764\r\n",
      "\tspeed: 1.0233s/iter; left time: 7766.0603s\r\n",
      "\titers: 1500, epoch: 2 | loss: 0.1375239\r\n",
      "\tspeed: 1.0213s/iter; left time: 7648.2611s\r\n",
      "\titers: 1600, epoch: 2 | loss: 0.1378441\r\n",
      "\tspeed: 1.0198s/iter; left time: 7535.5826s\r\n",
      "\titers: 1700, epoch: 2 | loss: 0.1460732\r\n",
      "\tspeed: 1.0220s/iter; left time: 7449.0524s\r\n",
      "\titers: 1800, epoch: 2 | loss: 0.1249010\r\n",
      "\tspeed: 1.0205s/iter; left time: 7336.6224s\r\n",
      "\titers: 1900, epoch: 2 | loss: 0.1377840\r\n",
      "\tspeed: 1.0225s/iter; left time: 7248.2557s\r\n",
      "\titers: 2000, epoch: 2 | loss: 0.1144079\r\n",
      "\tspeed: 1.0236s/iter; left time: 7153.6309s\r\n",
      "\titers: 2100, epoch: 2 | loss: 0.1683934\r\n",
      "\tspeed: 1.0245s/iter; left time: 7057.8101s\r\n",
      "\titers: 2200, epoch: 2 | loss: 0.1365948\r\n",
      "\tspeed: 1.0257s/iter; left time: 6963.4186s\r\n",
      "Epoch: 2 cost time: 2297.863739013672\r\n",
      "Epoch: 2, Steps: 2247 | Train Loss: 0.1533964 Vali Loss: 0.1234004 Test Loss: 0.1437116\r\n",
      "Validation loss decreased (0.153854 --> 0.123400).  Saving model ...\r\n",
      "Updating learning rate to 0.0001\r\n",
      "\titers: 100, epoch: 3 | loss: 0.1569089\r\n",
      "\tspeed: 5.1649s/iter; left time: 34305.5577s\r\n",
      "\titers: 200, epoch: 3 | loss: 0.1394489\r\n",
      "\tspeed: 1.0172s/iter; left time: 6654.7906s\r\n",
      "\titers: 300, epoch: 3 | loss: 0.1424840\r\n",
      "\tspeed: 1.0188s/iter; left time: 6562.9423s\r\n",
      "\titers: 400, epoch: 3 | loss: 0.1369122\r\n",
      "\tspeed: 1.0201s/iter; left time: 6469.5675s\r\n",
      "\titers: 500, epoch: 3 | loss: 0.1451033\r\n",
      "\tspeed: 1.0205s/iter; left time: 6369.8541s\r\n",
      "\titers: 600, epoch: 3 | loss: 0.1413785\r\n",
      "\tspeed: 1.0238s/iter; left time: 6288.3790s\r\n",
      "\titers: 700, epoch: 3 | loss: 0.1515661\r\n",
      "\tspeed: 1.0164s/iter; left time: 6141.3719s\r\n",
      "\titers: 800, epoch: 3 | loss: 0.1381743\r\n",
      "\tspeed: 1.0184s/iter; left time: 6051.1167s\r\n",
      "\titers: 900, epoch: 3 | loss: 0.1253605\r\n",
      "\tspeed: 1.0205s/iter; left time: 5961.7006s\r\n",
      "\titers: 1000, epoch: 3 | loss: 0.1322041\r\n",
      "\tspeed: 1.0196s/iter; left time: 5854.6176s\r\n",
      "\titers: 1100, epoch: 3 | loss: 0.1373726\r\n",
      "\tspeed: 1.0201s/iter; left time: 5755.5725s\r\n",
      "\titers: 1200, epoch: 3 | loss: 0.1726974\r\n",
      "\tspeed: 1.0213s/iter; left time: 5659.7806s\r\n",
      "\titers: 1300, epoch: 3 | loss: 0.1075847\r\n",
      "\tspeed: 1.0193s/iter; left time: 5547.2821s\r\n",
      "\titers: 1400, epoch: 3 | loss: 0.1462746\r\n",
      "\tspeed: 1.0221s/iter; left time: 5460.0187s\r\n",
      "\titers: 1500, epoch: 3 | loss: 0.1201147\r\n",
      "\tspeed: 1.0233s/iter; left time: 5364.1881s\r\n",
      "\titers: 1600, epoch: 3 | loss: 0.1336476\r\n",
      "\tspeed: 1.0211s/iter; left time: 5250.4273s\r\n",
      "\titers: 1700, epoch: 3 | loss: 0.1324848\r\n",
      "\tspeed: 1.0205s/iter; left time: 5145.3263s\r\n",
      "\titers: 1800, epoch: 3 | loss: 0.1291686\r\n",
      "\tspeed: 1.0188s/iter; left time: 5034.9296s\r\n",
      "\titers: 1900, epoch: 3 | loss: 0.1150481\r\n",
      "\tspeed: 1.0192s/iter; left time: 4935.1989s\r\n",
      "\titers: 2000, epoch: 3 | loss: 0.1155438\r\n",
      "\tspeed: 1.0274s/iter; left time: 4872.1463s\r\n",
      "\titers: 2100, epoch: 3 | loss: 0.1183468\r\n",
      "\tspeed: 1.0211s/iter; left time: 4740.0787s\r\n",
      "\titers: 2200, epoch: 3 | loss: 0.1398053\r\n",
      "\tspeed: 1.0245s/iter; left time: 4653.3871s\r\n",
      "Epoch: 3 cost time: 2293.7275092601776\r\n",
      "Epoch: 3, Steps: 2247 | Train Loss: 0.1425500 Vali Loss: 0.1189597 Test Loss: 0.1378653\r\n",
      "Validation loss decreased (0.123400 --> 0.118960).  Saving model ...\r\n",
      "Updating learning rate to 0.0001\r\n",
      "\titers: 100, epoch: 4 | loss: 0.1862257\r\n",
      "\tspeed: 5.1708s/iter; left time: 22725.8787s\r\n",
      "\titers: 200, epoch: 4 | loss: 0.1330462\r\n",
      "\tspeed: 1.0189s/iter; left time: 4376.2999s\r\n",
      "\titers: 300, epoch: 4 | loss: 0.1310090\r\n",
      "\tspeed: 1.0209s/iter; left time: 4282.5926s\r\n",
      "\titers: 400, epoch: 4 | loss: 0.1831839\r\n",
      "\tspeed: 1.0221s/iter; left time: 4185.5569s\r\n",
      "\titers: 500, epoch: 4 | loss: 0.1237536\r\n",
      "\tspeed: 1.0207s/iter; left time: 4077.6532s\r\n",
      "\titers: 600, epoch: 4 | loss: 0.1099698\r\n",
      "\tspeed: 1.0209s/iter; left time: 3976.2585s\r\n",
      "\titers: 700, epoch: 4 | loss: 0.1163194\r\n",
      "\tspeed: 1.0233s/iter; left time: 3883.2642s\r\n",
      "\titers: 800, epoch: 4 | loss: 0.1199735\r\n",
      "\tspeed: 1.0190s/iter; left time: 3765.3761s\r\n",
      "\titers: 900, epoch: 4 | loss: 0.1179361\r\n",
      "\tspeed: 1.0221s/iter; left time: 3674.5022s\r\n",
      "\titers: 1000, epoch: 4 | loss: 0.1251738\r\n",
      "\tspeed: 1.0216s/iter; left time: 3570.3773s\r\n",
      "\titers: 1100, epoch: 4 | loss: 0.1139986\r\n",
      "\tspeed: 1.0246s/iter; left time: 3478.3818s\r\n",
      "\titers: 1200, epoch: 4 | loss: 0.1540188\r\n",
      "\tspeed: 1.0189s/iter; left time: 3357.4045s\r\n",
      "\titers: 1300, epoch: 4 | loss: 0.1155372\r\n",
      "\tspeed: 1.0210s/iter; left time: 3261.9360s\r\n",
      "\titers: 1400, epoch: 4 | loss: 0.1797957\r\n",
      "\tspeed: 1.0269s/iter; left time: 3178.3067s\r\n",
      "\titers: 1500, epoch: 4 | loss: 0.1336570\r\n",
      "\tspeed: 1.0233s/iter; left time: 3064.8913s\r\n",
      "\titers: 1600, epoch: 4 | loss: 0.1319159\r\n",
      "\tspeed: 1.0256s/iter; left time: 2969.0766s\r\n",
      "\titers: 1700, epoch: 4 | loss: 0.1181577\r\n",
      "\tspeed: 1.0272s/iter; left time: 2871.1316s\r\n",
      "\titers: 1800, epoch: 4 | loss: 0.1555360\r\n",
      "\tspeed: 1.0214s/iter; left time: 2752.7484s\r\n",
      "\titers: 1900, epoch: 4 | loss: 0.1473048\r\n",
      "\tspeed: 1.0230s/iter; left time: 2654.7945s\r\n",
      "\titers: 2000, epoch: 4 | loss: 0.1355204\r\n",
      "\tspeed: 1.0237s/iter; left time: 2554.2456s\r\n",
      "\titers: 2100, epoch: 4 | loss: 0.1227420\r\n",
      "\tspeed: 1.0219s/iter; left time: 2447.4688s\r\n",
      "\titers: 2200, epoch: 4 | loss: 0.1244330\r\n",
      "\tspeed: 1.0245s/iter; left time: 2351.3403s\r\n",
      "Epoch: 4 cost time: 2297.691461086273\r\n",
      "Epoch: 4, Steps: 2247 | Train Loss: 0.1387211 Vali Loss: 0.1172344 Test Loss: 0.1364111\r\n",
      "Validation loss decreased (0.118960 --> 0.117234).  Saving model ...\r\n",
      "Updating learning rate to 9e-05\r\n",
      "\titers: 100, epoch: 5 | loss: 0.1148385\r\n",
      "\tspeed: 5.1753s/iter; left time: 11116.4862s\r\n",
      "\titers: 200, epoch: 5 | loss: 0.1419082\r\n",
      "\tspeed: 1.0239s/iter; left time: 2097.0084s\r\n",
      "\titers: 300, epoch: 5 | loss: 0.1244396\r\n",
      "\tspeed: 1.0257s/iter; left time: 1998.1202s\r\n",
      "\titers: 400, epoch: 5 | loss: 0.1187613\r\n",
      "\tspeed: 1.0255s/iter; left time: 1895.1458s\r\n",
      "\titers: 500, epoch: 5 | loss: 0.1389550\r\n",
      "\tspeed: 1.0240s/iter; left time: 1790.0093s\r\n",
      "\titers: 600, epoch: 5 | loss: 0.1112053\r\n",
      "\tspeed: 1.0247s/iter; left time: 1688.7838s\r\n",
      "\titers: 700, epoch: 5 | loss: 0.1176029\r\n",
      "\tspeed: 1.0232s/iter; left time: 1583.9211s\r\n",
      "\titers: 800, epoch: 5 | loss: 0.1288663\r\n",
      "\tspeed: 1.0245s/iter; left time: 1483.4537s\r\n",
      "\titers: 900, epoch: 5 | loss: 0.1439840\r\n",
      "\tspeed: 1.0249s/iter; left time: 1381.6253s\r\n",
      "\titers: 1000, epoch: 5 | loss: 0.1063569\r\n",
      "\tspeed: 1.0253s/iter; left time: 1279.5309s\r\n",
      "\titers: 1100, epoch: 5 | loss: 0.1427363\r\n",
      "\tspeed: 1.0260s/iter; left time: 1177.8480s\r\n",
      "\titers: 1200, epoch: 5 | loss: 0.1428627\r\n",
      "\tspeed: 1.0219s/iter; left time: 1070.9623s\r\n",
      "\titers: 1300, epoch: 5 | loss: 0.1427735\r\n",
      "\tspeed: 1.0213s/iter; left time: 968.1998s\r\n",
      "\titers: 1400, epoch: 5 | loss: 0.1725123\r\n",
      "\tspeed: 1.0220s/iter; left time: 866.6365s\r\n",
      "\titers: 1500, epoch: 5 | loss: 0.1351039\r\n",
      "\tspeed: 1.0222s/iter; left time: 764.6169s\r\n",
      "\titers: 1600, epoch: 5 | loss: 0.1304280\r\n",
      "\tspeed: 1.0223s/iter; left time: 662.4726s\r\n",
      "\titers: 1700, epoch: 5 | loss: 0.1272692\r\n",
      "\tspeed: 1.0198s/iter; left time: 558.8698s\r\n",
      "\titers: 1800, epoch: 5 | loss: 0.1393540\r\n",
      "\tspeed: 1.0224s/iter; left time: 458.0145s\r\n",
      "\titers: 1900, epoch: 5 | loss: 0.1277827\r\n",
      "\tspeed: 1.0219s/iter; left time: 355.6330s\r\n",
      "\titers: 2000, epoch: 5 | loss: 0.1239078\r\n",
      "\tspeed: 1.0198s/iter; left time: 252.9070s\r\n",
      "\titers: 2100, epoch: 5 | loss: 0.1218556\r\n",
      "\tspeed: 1.0221s/iter; left time: 151.2766s\r\n",
      "\titers: 2200, epoch: 5 | loss: 0.1412728\r\n",
      "\tspeed: 1.0245s/iter; left time: 49.1744s\r\n",
      "Epoch: 5 cost time: 2299.244769334793\r\n",
      "Epoch: 5, Steps: 2247 | Train Loss: 0.1358207 Vali Loss: 0.1168883 Test Loss: 0.1340664\r\n",
      "Validation loss decreased (0.117234 --> 0.116888).  Saving model ...\r\n",
      "Updating learning rate to 8.1e-05\r\n",
      ">>>>>>>testing : electricity_336_96_multi_scale_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\r\n",
      "test 5165\r\n",
      "mse:0.1340664178133011, mae:0.23279228806495667, rse:0.36395567655563354\r\n"
     ]
    }
   ],
   "source": [
    "!python run_longExp.py \\\n",
    "    --random_seed 2021 \\\n",
    "    --is_training 1 \\\n",
    "    --root_path ./dataset/ \\\n",
    "    --data_path electricity.csv \\\n",
    "    --model_id electricity_336_96_multi_scale \\\n",
    "    --model PatchTST \\\n",
    "    --data custom \\\n",
    "    --features M \\\n",
    "    --seq_len 336 \\\n",
    "    --pred_len 96 \\\n",
    "    --enc_in 321 \\\n",
    "    --e_layers 3 \\\n",
    "    --n_heads 16 \\\n",
    "    --d_model 128 \\\n",
    "    --d_ff 256 \\\n",
    "    --dropout 0.2 \\\n",
    "    --fc_dropout 0.2 \\\n",
    "    --head_dropout 0 \\\n",
    "    --patch_len 16 \\\n",
    "    --stride 8 \\\n",
    "    --des 'Exp' \\\n",
    "    --train_epochs 5 \\\n",
    "    --patience 10 \\\n",
    "    --itr 1 \\\n",
    "    --batch_size 8 \\\n",
    "    --learning_rate 0.0001 \\\n",
    "    --multi_scale small large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9559a8f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T20:13:40.443478Z",
     "iopub.status.busy": "2025-10-14T20:13:40.443183Z",
     "iopub.status.idle": "2025-10-14T22:20:02.802892Z",
     "shell.execute_reply": "2025-10-14T22:20:02.802137Z"
    },
    "papermill": {
     "duration": 7582.378279,
     "end_time": "2025-10-14T22:20:02.804185",
     "exception": false,
     "start_time": "2025-10-14T20:13:40.425906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\r\n",
      "Namespace(random_seed=2021, is_training=1, model_id='traffic_336_96_base_model', model='PatchTST', data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=96, fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=5, batch_size=4, patience=10, learning_rate=0.0001, des='Exp', loss='mse', lradj='TST', pct_start=0.2, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, multi_scale=None)\r\n",
      "Use GPU: cuda:0\r\n",
      "PatchTST Single-Scale Mode\r\n",
      ">>>>>>>start training : traffic_336_96_base_model_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\r\n",
      "train 11849\r\n",
      "val 1661\r\n",
      "test 3413\r\n",
      "\titers: 100, epoch: 1 | loss: 0.6813059\r\n",
      "\tspeed: 0.4555s/iter; left time: 6700.5029s\r\n",
      "\titers: 200, epoch: 1 | loss: 0.6148511\r\n",
      "\tspeed: 0.4274s/iter; left time: 6245.3965s\r\n",
      "\titers: 300, epoch: 1 | loss: 0.5839069\r\n",
      "\tspeed: 0.4327s/iter; left time: 6279.2956s\r\n",
      "\titers: 400, epoch: 1 | loss: 0.5546581\r\n",
      "\tspeed: 0.4299s/iter; left time: 6195.7947s\r\n",
      "\titers: 500, epoch: 1 | loss: 0.4210814\r\n",
      "\tspeed: 0.4312s/iter; left time: 6171.2749s\r\n",
      "\titers: 600, epoch: 1 | loss: 0.4325137\r\n",
      "\tspeed: 0.4312s/iter; left time: 6127.2929s\r\n",
      "\titers: 700, epoch: 1 | loss: 0.3700450\r\n",
      "\tspeed: 0.4305s/iter; left time: 6074.2823s\r\n",
      "\titers: 800, epoch: 1 | loss: 0.3821527\r\n",
      "\tspeed: 0.4330s/iter; left time: 6066.4284s\r\n",
      "\titers: 900, epoch: 1 | loss: 0.3577602\r\n",
      "\tspeed: 0.4337s/iter; left time: 6033.5850s\r\n",
      "\titers: 1000, epoch: 1 | loss: 0.3109240\r\n",
      "\tspeed: 0.4345s/iter; left time: 6000.7577s\r\n",
      "\titers: 1100, epoch: 1 | loss: 0.3712744\r\n",
      "\tspeed: 0.4337s/iter; left time: 5946.3546s\r\n",
      "\titers: 1200, epoch: 1 | loss: 0.2952279\r\n",
      "\tspeed: 0.4325s/iter; left time: 5886.2866s\r\n",
      "\titers: 1300, epoch: 1 | loss: 0.2885067\r\n",
      "\tspeed: 0.4316s/iter; left time: 5831.5099s\r\n",
      "\titers: 1400, epoch: 1 | loss: 0.2645552\r\n",
      "\tspeed: 0.4314s/iter; left time: 5785.7937s\r\n",
      "\titers: 1500, epoch: 1 | loss: 0.3622590\r\n",
      "\tspeed: 0.4302s/iter; left time: 5726.1567s\r\n",
      "\titers: 1600, epoch: 1 | loss: 0.3321784\r\n",
      "\tspeed: 0.4306s/iter; left time: 5689.1027s\r\n",
      "\titers: 1700, epoch: 1 | loss: 0.3273246\r\n",
      "\tspeed: 0.4302s/iter; left time: 5639.9529s\r\n",
      "\titers: 1800, epoch: 1 | loss: 0.2910302\r\n",
      "\tspeed: 0.4301s/iter; left time: 5595.5888s\r\n",
      "\titers: 1900, epoch: 1 | loss: 0.2475802\r\n",
      "\tspeed: 0.4296s/iter; left time: 5546.6823s\r\n",
      "\titers: 2000, epoch: 1 | loss: 0.2544829\r\n",
      "\tspeed: 0.4302s/iter; left time: 5511.2063s\r\n",
      "\titers: 2100, epoch: 1 | loss: 0.2622221\r\n",
      "\tspeed: 0.4327s/iter; left time: 5499.4691s\r\n",
      "\titers: 2200, epoch: 1 | loss: 0.2506374\r\n",
      "\tspeed: 0.4343s/iter; left time: 5476.7680s\r\n",
      "\titers: 2300, epoch: 1 | loss: 0.3188117\r\n",
      "\tspeed: 0.4331s/iter; left time: 5419.0250s\r\n",
      "\titers: 2400, epoch: 1 | loss: 0.4068163\r\n",
      "\tspeed: 0.4313s/iter; left time: 5352.5610s\r\n",
      "\titers: 2500, epoch: 1 | loss: 0.2618185\r\n",
      "\tspeed: 0.4304s/iter; left time: 5298.8893s\r\n",
      "\titers: 2600, epoch: 1 | loss: 0.2438567\r\n",
      "\tspeed: 0.4304s/iter; left time: 5255.1048s\r\n",
      "\titers: 2700, epoch: 1 | loss: 0.2915447\r\n",
      "\tspeed: 0.4293s/iter; left time: 5199.1419s\r\n",
      "\titers: 2800, epoch: 1 | loss: 0.2599921\r\n",
      "\tspeed: 0.4301s/iter; left time: 5165.7927s\r\n",
      "\titers: 2900, epoch: 1 | loss: 0.2330395\r\n",
      "\tspeed: 0.4302s/iter; left time: 5123.7178s\r\n",
      "Epoch: 1 cost time: 1278.7845244407654\r\n",
      "Epoch: 1, Steps: 2962 | Train Loss: 0.3774006 Vali Loss: 0.3575200 Test Loss: 0.4269135\r\n",
      "Validation loss decreased (inf --> 0.357520).  Saving model ...\r\n",
      "Updating learning rate to 9.999999824228717e-05\r\n",
      "\titers: 100, epoch: 2 | loss: 0.2992319\r\n",
      "\tspeed: 2.7373s/iter; left time: 32160.9857s\r\n",
      "\titers: 200, epoch: 2 | loss: 0.2977328\r\n",
      "\tspeed: 0.4333s/iter; left time: 5047.6270s\r\n",
      "\titers: 300, epoch: 2 | loss: 0.2401937\r\n",
      "\tspeed: 0.4341s/iter; left time: 5013.4652s\r\n",
      "\titers: 400, epoch: 2 | loss: 0.2424376\r\n",
      "\tspeed: 0.4324s/iter; left time: 4950.0784s\r\n",
      "\titers: 500, epoch: 2 | loss: 0.3860863\r\n",
      "\tspeed: 0.4303s/iter; left time: 4883.7389s\r\n",
      "\titers: 600, epoch: 2 | loss: 0.3600133\r\n",
      "\tspeed: 0.4291s/iter; left time: 4826.9794s\r\n",
      "\titers: 700, epoch: 2 | loss: 0.2589579\r\n",
      "\tspeed: 0.4287s/iter; left time: 4779.8227s\r\n",
      "\titers: 800, epoch: 2 | loss: 0.3496418\r\n",
      "\tspeed: 0.4285s/iter; left time: 4734.9278s\r\n",
      "\titers: 900, epoch: 2 | loss: 0.2730749\r\n",
      "\tspeed: 0.4290s/iter; left time: 4697.0124s\r\n",
      "\titers: 1000, epoch: 2 | loss: 0.2821222\r\n",
      "\tspeed: 0.4292s/iter; left time: 4656.9072s\r\n",
      "\titers: 1100, epoch: 2 | loss: 0.2764991\r\n",
      "\tspeed: 0.4292s/iter; left time: 4613.9264s\r\n",
      "\titers: 1200, epoch: 2 | loss: 0.2700369\r\n",
      "\tspeed: 0.4299s/iter; left time: 4577.8814s\r\n",
      "\titers: 1300, epoch: 2 | loss: 0.3590299\r\n",
      "\tspeed: 0.4294s/iter; left time: 4529.2739s\r\n",
      "\titers: 1400, epoch: 2 | loss: 0.2655078\r\n",
      "\tspeed: 0.4294s/iter; left time: 4487.2286s\r\n",
      "\titers: 1500, epoch: 2 | loss: 0.2762090\r\n",
      "\tspeed: 0.4296s/iter; left time: 4446.2479s\r\n",
      "\titers: 1600, epoch: 2 | loss: 0.2403228\r\n",
      "\tspeed: 0.4301s/iter; left time: 4407.6479s\r\n",
      "\titers: 1700, epoch: 2 | loss: 0.3169266\r\n",
      "\tspeed: 0.4294s/iter; left time: 4357.8885s\r\n",
      "\titers: 1800, epoch: 2 | loss: 0.2733106\r\n",
      "\tspeed: 0.4295s/iter; left time: 4316.1832s\r\n",
      "\titers: 1900, epoch: 2 | loss: 0.2450262\r\n",
      "\tspeed: 0.4294s/iter; left time: 4271.7374s\r\n",
      "\titers: 2000, epoch: 2 | loss: 0.2101574\r\n",
      "\tspeed: 0.4299s/iter; left time: 4234.0064s\r\n",
      "\titers: 2100, epoch: 2 | loss: 0.2614353\r\n",
      "\tspeed: 0.4295s/iter; left time: 4186.9809s\r\n",
      "\titers: 2200, epoch: 2 | loss: 0.1981487\r\n",
      "\tspeed: 0.4299s/iter; left time: 4148.5089s\r\n",
      "\titers: 2300, epoch: 2 | loss: 0.2584703\r\n",
      "\tspeed: 0.4296s/iter; left time: 4102.6747s\r\n",
      "\titers: 2400, epoch: 2 | loss: 0.2473536\r\n",
      "\tspeed: 0.4296s/iter; left time: 4059.0757s\r\n",
      "\titers: 2500, epoch: 2 | loss: 0.2681819\r\n",
      "\tspeed: 0.4295s/iter; left time: 4015.0542s\r\n",
      "\titers: 2600, epoch: 2 | loss: 0.2391331\r\n",
      "\tspeed: 0.4292s/iter; left time: 3969.5583s\r\n",
      "\titers: 2700, epoch: 2 | loss: 0.1844966\r\n",
      "\tspeed: 0.4285s/iter; left time: 3920.3874s\r\n",
      "\titers: 2800, epoch: 2 | loss: 0.2408306\r\n",
      "\tspeed: 0.4290s/iter; left time: 3881.9415s\r\n",
      "\titers: 2900, epoch: 2 | loss: 0.2601624\r\n",
      "\tspeed: 0.4295s/iter; left time: 3843.5013s\r\n",
      "Epoch: 2 cost time: 1273.7174842357635\r\n",
      "Epoch: 2, Steps: 2962 | Train Loss: 0.2696533 Vali Loss: 0.3462002 Test Loss: 0.4120664\r\n",
      "Validation loss decreased (0.357520 --> 0.346200).  Saving model ...\r\n",
      "Updating learning rate to 8.534602167969696e-05\r\n",
      "\titers: 100, epoch: 3 | loss: 0.2190340\r\n",
      "\tspeed: 2.7608s/iter; left time: 24258.7505s\r\n",
      "\titers: 200, epoch: 3 | loss: 0.2584509\r\n",
      "\tspeed: 0.4316s/iter; left time: 3749.1415s\r\n",
      "\titers: 300, epoch: 3 | loss: 0.2807728\r\n",
      "\tspeed: 0.4280s/iter; left time: 3675.0661s\r\n",
      "\titers: 400, epoch: 3 | loss: 0.2591832\r\n",
      "\tspeed: 0.4301s/iter; left time: 3650.6213s\r\n",
      "\titers: 500, epoch: 3 | loss: 0.2181114\r\n",
      "\tspeed: 0.4341s/iter; left time: 3640.5743s\r\n",
      "\titers: 600, epoch: 3 | loss: 0.2443443\r\n",
      "\tspeed: 0.4339s/iter; left time: 3595.6184s\r\n",
      "\titers: 700, epoch: 3 | loss: 0.2821846\r\n",
      "\tspeed: 0.4310s/iter; left time: 3528.7299s\r\n",
      "\titers: 800, epoch: 3 | loss: 0.3115394\r\n",
      "\tspeed: 0.4300s/iter; left time: 3477.5375s\r\n",
      "\titers: 900, epoch: 3 | loss: 0.2640344\r\n",
      "\tspeed: 0.4294s/iter; left time: 3429.8174s\r\n",
      "\titers: 1000, epoch: 3 | loss: 0.2331087\r\n",
      "\tspeed: 0.4291s/iter; left time: 3384.6251s\r\n",
      "\titers: 1100, epoch: 3 | loss: 0.2620914\r\n",
      "\tspeed: 0.4286s/iter; left time: 3337.6153s\r\n",
      "\titers: 1200, epoch: 3 | loss: 0.2312185\r\n",
      "\tspeed: 0.4285s/iter; left time: 3293.8010s\r\n",
      "\titers: 1300, epoch: 3 | loss: 0.2156139\r\n",
      "\tspeed: 0.4287s/iter; left time: 3252.7734s\r\n",
      "\titers: 1400, epoch: 3 | loss: 0.2565834\r\n",
      "\tspeed: 0.4281s/iter; left time: 3205.4578s\r\n",
      "\titers: 1500, epoch: 3 | loss: 0.2484309\r\n",
      "\tspeed: 0.4284s/iter; left time: 3164.4511s\r\n",
      "\titers: 1600, epoch: 3 | loss: 0.3577105\r\n",
      "\tspeed: 0.4284s/iter; left time: 3121.8440s\r\n",
      "\titers: 1700, epoch: 3 | loss: 0.3335352\r\n",
      "\tspeed: 0.4290s/iter; left time: 3083.3371s\r\n",
      "\titers: 1800, epoch: 3 | loss: 0.2722984\r\n",
      "\tspeed: 0.4289s/iter; left time: 3039.5849s\r\n",
      "\titers: 1900, epoch: 3 | loss: 0.3517177\r\n",
      "\tspeed: 0.4293s/iter; left time: 2999.3664s\r\n",
      "\titers: 2000, epoch: 3 | loss: 0.2414652\r\n",
      "\tspeed: 0.4297s/iter; left time: 2959.1689s\r\n",
      "\titers: 2100, epoch: 3 | loss: 0.2678403\r\n",
      "\tspeed: 0.4295s/iter; left time: 2915.0014s\r\n",
      "\titers: 2200, epoch: 3 | loss: 0.2359060\r\n",
      "\tspeed: 0.4299s/iter; left time: 2875.0570s\r\n",
      "\titers: 2300, epoch: 3 | loss: 0.2468108\r\n",
      "\tspeed: 0.4296s/iter; left time: 2830.0350s\r\n",
      "\titers: 2400, epoch: 3 | loss: 0.2545513\r\n",
      "\tspeed: 0.4300s/iter; left time: 2789.4964s\r\n",
      "\titers: 2500, epoch: 3 | loss: 0.3135764\r\n",
      "\tspeed: 0.4301s/iter; left time: 2746.7709s\r\n",
      "\titers: 2600, epoch: 3 | loss: 0.2229096\r\n",
      "\tspeed: 0.4301s/iter; left time: 2704.2566s\r\n",
      "\titers: 2700, epoch: 3 | loss: 0.2553001\r\n",
      "\tspeed: 0.4298s/iter; left time: 2659.1472s\r\n",
      "\titers: 2800, epoch: 3 | loss: 0.2319528\r\n",
      "\tspeed: 0.4301s/iter; left time: 2617.9437s\r\n",
      "\titers: 2900, epoch: 3 | loss: 0.2518277\r\n",
      "\tspeed: 0.4300s/iter; left time: 2574.6459s\r\n",
      "Epoch: 3 cost time: 1273.7978031635284\r\n",
      "Epoch: 3, Steps: 2962 | Train Loss: 0.2541387 Vali Loss: 0.3398988 Test Loss: 0.4037113\r\n",
      "Validation loss decreased (0.346200 --> 0.339899).  Saving model ...\r\n",
      "Updating learning rate to 4.9986942150361225e-05\r\n",
      "\titers: 100, epoch: 4 | loss: 0.1873500\r\n",
      "\tspeed: 2.7634s/iter; left time: 16096.6334s\r\n",
      "\titers: 200, epoch: 4 | loss: 0.2070878\r\n",
      "\tspeed: 0.4343s/iter; left time: 2486.3069s\r\n",
      "\titers: 300, epoch: 4 | loss: 0.2063248\r\n",
      "\tspeed: 0.4337s/iter; left time: 2439.3138s\r\n",
      "\titers: 400, epoch: 4 | loss: 0.1999679\r\n",
      "\tspeed: 0.4328s/iter; left time: 2391.2202s\r\n",
      "\titers: 500, epoch: 4 | loss: 0.2413857\r\n",
      "\tspeed: 0.4324s/iter; left time: 2345.8059s\r\n",
      "\titers: 600, epoch: 4 | loss: 0.1611398\r\n",
      "\tspeed: 0.4324s/iter; left time: 2302.6474s\r\n",
      "\titers: 700, epoch: 4 | loss: 0.2114852\r\n",
      "\tspeed: 0.4321s/iter; left time: 2257.7904s\r\n",
      "\titers: 800, epoch: 4 | loss: 0.2240718\r\n",
      "\tspeed: 0.4325s/iter; left time: 2216.3861s\r\n",
      "\titers: 900, epoch: 4 | loss: 0.2534606\r\n",
      "\tspeed: 0.4319s/iter; left time: 2170.4178s\r\n",
      "\titers: 1000, epoch: 4 | loss: 0.2554344\r\n",
      "\tspeed: 0.4323s/iter; left time: 2128.9817s\r\n",
      "\titers: 1100, epoch: 4 | loss: 0.2423982\r\n",
      "\tspeed: 0.4326s/iter; left time: 2087.0826s\r\n",
      "\titers: 1200, epoch: 4 | loss: 0.2294011\r\n",
      "\tspeed: 0.4323s/iter; left time: 2042.7632s\r\n",
      "\titers: 1300, epoch: 4 | loss: 0.2592005\r\n",
      "\tspeed: 0.4326s/iter; left time: 2000.7246s\r\n",
      "\titers: 1400, epoch: 4 | loss: 0.2238720\r\n",
      "\tspeed: 0.4319s/iter; left time: 1954.5047s\r\n",
      "\titers: 1500, epoch: 4 | loss: 0.2307791\r\n",
      "\tspeed: 0.4322s/iter; left time: 1912.6037s\r\n",
      "\titers: 1600, epoch: 4 | loss: 0.3413756\r\n",
      "\tspeed: 0.4317s/iter; left time: 1866.9020s\r\n",
      "\titers: 1700, epoch: 4 | loss: 0.2547133\r\n",
      "\tspeed: 0.4327s/iter; left time: 1828.2883s\r\n",
      "\titers: 1800, epoch: 4 | loss: 0.2179320\r\n",
      "\tspeed: 0.4327s/iter; left time: 1785.0740s\r\n",
      "\titers: 1900, epoch: 4 | loss: 0.2039700\r\n",
      "\tspeed: 0.4327s/iter; left time: 1741.7246s\r\n",
      "\titers: 2000, epoch: 4 | loss: 0.2417160\r\n",
      "\tspeed: 0.4327s/iter; left time: 1698.2719s\r\n",
      "\titers: 2100, epoch: 4 | loss: 0.3039003\r\n",
      "\tspeed: 0.4324s/iter; left time: 1653.8124s\r\n",
      "\titers: 2200, epoch: 4 | loss: 0.2663855\r\n",
      "\tspeed: 0.4333s/iter; left time: 1613.9878s\r\n",
      "\titers: 2300, epoch: 4 | loss: 0.2349559\r\n",
      "\tspeed: 0.4333s/iter; left time: 1570.5486s\r\n",
      "\titers: 2400, epoch: 4 | loss: 0.2409748\r\n",
      "\tspeed: 0.4331s/iter; left time: 1526.8282s\r\n",
      "\titers: 2500, epoch: 4 | loss: 0.2162991\r\n",
      "\tspeed: 0.4336s/iter; left time: 1485.0214s\r\n",
      "\titers: 2600, epoch: 4 | loss: 0.2360470\r\n",
      "\tspeed: 0.4332s/iter; left time: 1440.2820s\r\n",
      "\titers: 2700, epoch: 4 | loss: 0.2923697\r\n",
      "\tspeed: 0.4334s/iter; left time: 1397.7702s\r\n",
      "\titers: 2800, epoch: 4 | loss: 0.2081357\r\n",
      "\tspeed: 0.4333s/iter; left time: 1354.1798s\r\n",
      "\titers: 2900, epoch: 4 | loss: 0.2023735\r\n",
      "\tspeed: 0.4327s/iter; left time: 1308.8793s\r\n",
      "Epoch: 4 cost time: 1282.2260956764221\r\n",
      "Epoch: 4, Steps: 2962 | Train Loss: 0.2444190 Vali Loss: 0.3328125 Test Loss: 0.3939243\r\n",
      "Validation loss decreased (0.339899 --> 0.332813).  Saving model ...\r\n",
      "Updating learning rate to 1.4635628889535989e-05\r\n",
      "\titers: 100, epoch: 5 | loss: 0.2226357\r\n",
      "\tspeed: 2.7677s/iter; left time: 7923.9170s\r\n",
      "\titers: 200, epoch: 5 | loss: 0.3135079\r\n",
      "\tspeed: 0.4336s/iter; left time: 1198.1055s\r\n",
      "\titers: 300, epoch: 5 | loss: 0.2717617\r\n",
      "\tspeed: 0.4327s/iter; left time: 1152.2849s\r\n",
      "\titers: 400, epoch: 5 | loss: 0.2466853\r\n",
      "\tspeed: 0.4316s/iter; left time: 1106.1089s\r\n",
      "\titers: 500, epoch: 5 | loss: 0.2574951\r\n",
      "\tspeed: 0.4311s/iter; left time: 1061.8223s\r\n",
      "\titers: 600, epoch: 5 | loss: 0.2164030\r\n",
      "\tspeed: 0.4305s/iter; left time: 1017.2794s\r\n",
      "\titers: 700, epoch: 5 | loss: 0.2218429\r\n",
      "\tspeed: 0.4304s/iter; left time: 974.1039s\r\n",
      "\titers: 800, epoch: 5 | loss: 0.2413135\r\n",
      "\tspeed: 0.4301s/iter; left time: 930.3206s\r\n",
      "\titers: 900, epoch: 5 | loss: 0.3253246\r\n",
      "\tspeed: 0.4302s/iter; left time: 887.4317s\r\n",
      "\titers: 1000, epoch: 5 | loss: 0.2173137\r\n",
      "\tspeed: 0.4301s/iter; left time: 844.2127s\r\n",
      "\titers: 1100, epoch: 5 | loss: 0.2081499\r\n",
      "\tspeed: 0.4305s/iter; left time: 802.0890s\r\n",
      "\titers: 1200, epoch: 5 | loss: 0.2999850\r\n",
      "\tspeed: 0.4296s/iter; left time: 757.4703s\r\n",
      "\titers: 1300, epoch: 5 | loss: 0.2614424\r\n",
      "\tspeed: 0.4305s/iter; left time: 715.8843s\r\n",
      "\titers: 1400, epoch: 5 | loss: 0.2716666\r\n",
      "\tspeed: 0.4304s/iter; left time: 672.7899s\r\n",
      "\titers: 1500, epoch: 5 | loss: 0.3118046\r\n",
      "\tspeed: 0.4306s/iter; left time: 629.9448s\r\n",
      "\titers: 1600, epoch: 5 | loss: 0.1957388\r\n",
      "\tspeed: 0.4301s/iter; left time: 586.1687s\r\n",
      "\titers: 1700, epoch: 5 | loss: 0.3016706\r\n",
      "\tspeed: 0.4304s/iter; left time: 543.5366s\r\n",
      "\titers: 1800, epoch: 5 | loss: 0.2085395\r\n",
      "\tspeed: 0.4305s/iter; left time: 500.7191s\r\n",
      "\titers: 1900, epoch: 5 | loss: 0.2614121\r\n",
      "\tspeed: 0.4302s/iter; left time: 457.2807s\r\n",
      "\titers: 2000, epoch: 5 | loss: 0.3326198\r\n",
      "\tspeed: 0.4305s/iter; left time: 414.5615s\r\n",
      "\titers: 2100, epoch: 5 | loss: 0.2283450\r\n",
      "\tspeed: 0.4304s/iter; left time: 371.4094s\r\n",
      "\titers: 2200, epoch: 5 | loss: 0.2477147\r\n",
      "\tspeed: 0.4305s/iter; left time: 328.4447s\r\n",
      "\titers: 2300, epoch: 5 | loss: 0.2005095\r\n",
      "\tspeed: 0.4305s/iter; left time: 285.4293s\r\n",
      "\titers: 2400, epoch: 5 | loss: 0.2787879\r\n",
      "\tspeed: 0.4309s/iter; left time: 242.5737s\r\n",
      "\titers: 2500, epoch: 5 | loss: 0.1976909\r\n",
      "\tspeed: 0.4307s/iter; left time: 199.4093s\r\n",
      "\titers: 2600, epoch: 5 | loss: 0.2489273\r\n",
      "\tspeed: 0.4304s/iter; left time: 156.2202s\r\n",
      "\titers: 2700, epoch: 5 | loss: 0.2610798\r\n",
      "\tspeed: 0.4304s/iter; left time: 113.2067s\r\n",
      "\titers: 2800, epoch: 5 | loss: 0.2745966\r\n",
      "\tspeed: 0.4300s/iter; left time: 70.0856s\r\n",
      "\titers: 2900, epoch: 5 | loss: 0.2826751\r\n",
      "\tspeed: 0.4303s/iter; left time: 27.1113s\r\n",
      "Epoch: 5 cost time: 1276.2430336475372\r\n",
      "Epoch: 5, Steps: 2962 | Train Loss: 0.2395931 Vali Loss: 0.3302827 Test Loss: 0.3910212\r\n",
      "Validation loss decreased (0.332813 --> 0.330283).  Saving model ...\r\n",
      "Updating learning rate to 4.017577128332244e-10\r\n",
      ">>>>>>>testing : traffic_336_96_base_model_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\r\n",
      "test 3413\r\n",
      "mse:0.3910212516784668, mae:0.26835453510284424, rse:0.5177574753761292\r\n"
     ]
    }
   ],
   "source": [
    "!python run_longExp.py \\\n",
    "    --random_seed 2021 \\\n",
    "    --is_training 1 \\\n",
    "    --root_path ./dataset/ \\\n",
    "    --data_path traffic.csv \\\n",
    "    --model_id traffic_336_96_base_model \\\n",
    "    --model PatchTST \\\n",
    "    --data custom \\\n",
    "    --features M \\\n",
    "    --seq_len 336 \\\n",
    "    --pred_len 96 \\\n",
    "    --enc_in 862 \\\n",
    "    --e_layers 3 \\\n",
    "    --n_heads 16 \\\n",
    "    --d_model 128 \\\n",
    "    --d_ff 256 \\\n",
    "    --dropout 0.2 \\\n",
    "    --fc_dropout 0.2 \\\n",
    "    --head_dropout 0 \\\n",
    "    --patch_len 16 \\\n",
    "    --stride 8 \\\n",
    "    --des 'Exp' \\\n",
    "    --train_epochs 5 \\\n",
    "    --patience 10 \\\n",
    "    --lradj 'TST'\\\n",
    "    --pct_start 0.2\\\n",
    "    --itr 1 \\\n",
    "    --batch_size 4 \\\n",
    "    --learning_rate 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dddc70c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T22:20:02.851522Z",
     "iopub.status.busy": "2025-10-14T22:20:02.851226Z",
     "iopub.status.idle": "2025-10-14T22:20:16.405413Z",
     "shell.execute_reply": "2025-10-14T22:20:16.404673Z"
    },
    "papermill": {
     "duration": 13.579449,
     "end_time": "2025-10-14T22:20:16.406932",
     "exception": false,
     "start_time": "2025-10-14T22:20:02.827483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\r\n",
      "Namespace(random_seed=2021, is_training=1, model_id='traffic_336_96_multi_scale', model='PatchTST', data='custom', root_path='./dataset/', data_path='traffic.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=96, fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=862, dec_in=7, c_out=7, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=5, batch_size=4, patience=10, learning_rate=0.0001, des='Exp', loss='mse', lradj='TST', pct_start=0.2, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, multi_scale=['small', 'large'])\r\n",
      "Use GPU: cuda:0\r\n",
      "PatchTST Multi-Scale Configuration: ['small', 'large']\r\n",
      "Using scales: ['small', 'large']\r\n",
      "Patch sizes: [8, 32]\r\n",
      "Strides: [4, 16]\r\n",
      ">>>>>>>start training : traffic_336_96_multi_scale_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\r\n",
      "train 11849\r\n",
      "val 1661\r\n",
      "test 3413\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/kaggle/working/PatchTST/PatchTST_supervised/run_longExp.py\", line 144, in <module>\r\n",
      "    exp.train(setting)\r\n",
      "  File \"/kaggle/working/PatchTST/PatchTST_supervised/exp/exp_main.py\", line 196, in train\r\n",
      "    loss.backward()\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 626, in backward\r\n",
      "    torch.autograd.backward(\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\r\n",
      "    _engine_run_backward(\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 823, in _engine_run_backward\r\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.43 GiB is free. Process 157686 has 13.31 GiB memory in use. Of the allocated memory 12.25 GiB is allocated by PyTorch, and 949.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\r\n"
     ]
    }
   ],
   "source": [
    "!python run_longExp.py \\\n",
    "    --random_seed 2021 \\\n",
    "    --is_training 1 \\\n",
    "    --root_path ./dataset/ \\\n",
    "    --data_path traffic.csv \\\n",
    "    --model_id traffic_336_96_multi_scale \\\n",
    "    --model PatchTST \\\n",
    "    --data custom \\\n",
    "    --features M \\\n",
    "    --seq_len 336 \\\n",
    "    --pred_len 96 \\\n",
    "    --enc_in 862 \\\n",
    "    --e_layers 3 \\\n",
    "    --n_heads 16 \\\n",
    "    --d_model 128 \\\n",
    "    --d_ff 256 \\\n",
    "    --dropout 0.2 \\\n",
    "    --fc_dropout 0.2 \\\n",
    "    --head_dropout 0 \\\n",
    "    --patch_len 16 \\\n",
    "    --stride 8 \\\n",
    "    --des 'Exp' \\\n",
    "    --train_epochs 5 \\\n",
    "    --patience 10 \\\n",
    "    --lradj 'TST'\\\n",
    "    --pct_start 0.2\\\n",
    "    --itr 1 \\\n",
    "    --batch_size 4 \\\n",
    "    --learning_rate 0.0001 \\\n",
    "    --multi_scale small large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68030ea1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T22:20:16.454643Z",
     "iopub.status.busy": "2025-10-14T22:20:16.454390Z",
     "iopub.status.idle": "2025-10-14T22:20:55.720014Z",
     "shell.execute_reply": "2025-10-14T22:20:55.719228Z"
    },
    "papermill": {
     "duration": 39.290672,
     "end_time": "2025-10-14T22:20:55.721386",
     "exception": false,
     "start_time": "2025-10-14T22:20:16.430714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\r\n",
      "Namespace(random_seed=2021, is_training=1, model_id='exchange_rate_336_96_base_model', model='PatchTST', data='custom', root_path='./dataset/', data_path='exchange_rate.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=96, fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=321, dec_in=7, c_out=7, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=5, batch_size=64, patience=20, learning_rate=0.0001, des='Exp', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, multi_scale=None)\r\n",
      "Use GPU: cuda:0\r\n",
      "PatchTST Single-Scale Mode\r\n",
      ">>>>>>>start training : exchange_rate_336_96_base_model_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\r\n",
      "train 4880\r\n",
      "val 665\r\n",
      "test 1422\r\n",
      "Epoch: 1 cost time: 5.321061134338379\r\n",
      "Epoch: 1, Steps: 76 | Train Loss: 0.3421338 Vali Loss: 0.2389861 Test Loss: 0.1923112\r\n",
      "Validation loss decreased (inf --> 0.238986).  Saving model ...\r\n",
      "Updating learning rate to 0.0001\r\n",
      "Epoch: 2 cost time: 5.05757474899292\r\n",
      "Epoch: 2, Steps: 76 | Train Loss: 0.1905713 Vali Loss: 0.1449946 Test Loss: 0.0973432\r\n",
      "Validation loss decreased (0.238986 --> 0.144995).  Saving model ...\r\n",
      "Updating learning rate to 0.0001\r\n",
      "Epoch: 3 cost time: 5.061761856079102\r\n",
      "Epoch: 3, Steps: 76 | Train Loss: 0.1329700 Vali Loss: 0.1433092 Test Loss: 0.0951038\r\n",
      "Validation loss decreased (0.144995 --> 0.143309).  Saving model ...\r\n",
      "Updating learning rate to 0.0001\r\n",
      "Epoch: 4 cost time: 5.140539884567261\r\n",
      "Epoch: 4, Steps: 76 | Train Loss: 0.1225351 Vali Loss: 0.1442025 Test Loss: 0.0935585\r\n",
      "EarlyStopping counter: 1 out of 20\r\n",
      "Updating learning rate to 9e-05\r\n",
      "Epoch: 5 cost time: 5.2474751472473145\r\n",
      "Epoch: 5, Steps: 76 | Train Loss: 0.1190698 Vali Loss: 0.1483426 Test Loss: 0.0967327\r\n",
      "EarlyStopping counter: 2 out of 20\r\n",
      "Updating learning rate to 8.1e-05\r\n",
      ">>>>>>>testing : exchange_rate_336_96_base_model_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\r\n",
      "test 1422\r\n",
      "mse:0.09510381519794464, mae:0.21780996024608612, rse:0.23487992584705353\r\n"
     ]
    }
   ],
   "source": [
    "!python run_longExp.py \\\n",
    "    --random_seed 2021 \\\n",
    "    --is_training 1 \\\n",
    "    --root_path ./dataset/ \\\n",
    "    --data_path exchange_rate.csv \\\n",
    "    --model_id exchange_rate_336_96_base_model \\\n",
    "    --model PatchTST \\\n",
    "    --data custom \\\n",
    "    --features M \\\n",
    "    --seq_len 336 \\\n",
    "    --pred_len 96 \\\n",
    "    --enc_in 321 \\\n",
    "    --e_layers 3 \\\n",
    "    --n_heads 16 \\\n",
    "    --d_model 128 \\\n",
    "    --d_ff 256 \\\n",
    "    --dropout 0.2 \\\n",
    "    --fc_dropout 0.2 \\\n",
    "    --head_dropout 0 \\\n",
    "    --patch_len 16 \\\n",
    "    --stride 8 \\\n",
    "    --des 'Exp' \\\n",
    "    --train_epochs 5 \\\n",
    "    --patience 20 \\\n",
    "    --itr 1 \\\n",
    "    --batch_size 64 \\\n",
    "    --learning_rate 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2388b8e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T22:20:55.769467Z",
     "iopub.status.busy": "2025-10-14T22:20:55.769237Z",
     "iopub.status.idle": "2025-10-14T22:22:36.376826Z",
     "shell.execute_reply": "2025-10-14T22:22:36.376068Z"
    },
    "papermill": {
     "duration": 100.632432,
     "end_time": "2025-10-14T22:22:36.378161",
     "exception": false,
     "start_time": "2025-10-14T22:20:55.745729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\r\n",
      "Namespace(random_seed=2021, is_training=1, model_id='exchange_rate_336_96_multi_model', model='PatchTST', data='custom', root_path='./dataset/', data_path='exchange_rate.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=336, label_len=48, pred_len=96, fc_dropout=0.2, head_dropout=0.0, patch_len=16, stride=8, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=321, dec_in=7, c_out=7, d_model=128, n_heads=16, e_layers=3, d_layers=1, d_ff=256, moving_avg=25, factor=1, distil=True, dropout=0.2, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=5, batch_size=64, patience=20, learning_rate=0.0001, des='Exp', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, multi_scale=['small', 'large'])\r\n",
      "Use GPU: cuda:0\r\n",
      "PatchTST Multi-Scale Configuration: ['small', 'large']\r\n",
      "Using scales: ['small', 'large']\r\n",
      "Patch sizes: [8, 32]\r\n",
      "Strides: [4, 16]\r\n",
      ">>>>>>>start training : exchange_rate_336_96_multi_model_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\r\n",
      "train 4880\r\n",
      "val 665\r\n",
      "test 1422\r\n",
      "Epoch: 1 cost time: 15.942816495895386\r\n",
      "Epoch: 1, Steps: 76 | Train Loss: 0.9659454 Vali Loss: 5.2608910 Test Loss: 3.3597136\r\n",
      "Validation loss decreased (inf --> 5.260891).  Saving model ...\r\n",
      "Updating learning rate to 0.0001\r\n",
      "Epoch: 2 cost time: 15.61583423614502\r\n",
      "Epoch: 2, Steps: 76 | Train Loss: 0.3960965 Vali Loss: 0.4813749 Test Loss: 0.3185830\r\n",
      "Validation loss decreased (5.260891 --> 0.481375).  Saving model ...\r\n",
      "Updating learning rate to 0.0001\r\n",
      "Epoch: 3 cost time: 15.418795108795166\r\n",
      "Epoch: 3, Steps: 76 | Train Loss: 0.1362253 Vali Loss: 0.2002428 Test Loss: 0.1120671\r\n",
      "Validation loss decreased (0.481375 --> 0.200243).  Saving model ...\r\n",
      "Updating learning rate to 0.0001\r\n",
      "Epoch: 4 cost time: 15.451454877853394\r\n",
      "Epoch: 4, Steps: 76 | Train Loss: 0.1141182 Vali Loss: 0.1932693 Test Loss: 0.0964516\r\n",
      "Validation loss decreased (0.200243 --> 0.193269).  Saving model ...\r\n",
      "Updating learning rate to 9e-05\r\n",
      "Epoch: 5 cost time: 15.53856897354126\r\n",
      "Epoch: 5, Steps: 76 | Train Loss: 0.1088098 Vali Loss: 0.1952435 Test Loss: 0.0949101\r\n",
      "EarlyStopping counter: 1 out of 20\r\n",
      "Updating learning rate to 8.1e-05\r\n",
      ">>>>>>>testing : exchange_rate_336_96_multi_model_PatchTST_custom_ftM_sl336_ll48_pl96_dm128_nh16_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\r\n",
      "test 1422\r\n",
      "mse:0.09645161032676697, mae:0.22875994443893433, rse:0.23653839528560638\r\n"
     ]
    }
   ],
   "source": [
    "!python run_longExp.py \\\n",
    "    --random_seed 2021 \\\n",
    "    --is_training 1 \\\n",
    "    --root_path ./dataset/ \\\n",
    "    --data_path exchange_rate.csv \\\n",
    "    --model_id exchange_rate_336_96_multi_model \\\n",
    "    --model PatchTST \\\n",
    "    --data custom \\\n",
    "    --features M \\\n",
    "    --seq_len 336 \\\n",
    "    --pred_len 96 \\\n",
    "    --enc_in 321 \\\n",
    "    --e_layers 3 \\\n",
    "    --n_heads 16 \\\n",
    "    --d_model 128 \\\n",
    "    --d_ff 256 \\\n",
    "    --dropout 0.2 \\\n",
    "    --fc_dropout 0.2 \\\n",
    "    --head_dropout 0 \\\n",
    "    --patch_len 16 \\\n",
    "    --stride 8 \\\n",
    "    --des 'Exp' \\\n",
    "    --train_epochs 5 \\\n",
    "    --patience 20 \\\n",
    "    --itr 1 \\\n",
    "    --batch_size 64 \\\n",
    "    --learning_rate 0.0001 \\\n",
    "    --multi_scale small large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb86bd31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T22:22:36.427855Z",
     "iopub.status.busy": "2025-10-14T22:22:36.427609Z",
     "iopub.status.idle": "2025-10-14T22:24:44.649078Z",
     "shell.execute_reply": "2025-10-14T22:24:44.648323Z"
    },
    "papermill": {
     "duration": 128.247386,
     "end_time": "2025-10-14T22:24:44.650379",
     "exception": false,
     "start_time": "2025-10-14T22:22:36.402993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\r\n",
      "Namespace(random_seed=2021, is_training=1, model_id='national_illness_104_24_base_model', model='PatchTST', data='custom', root_path='./dataset/', data_path='national_illness.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=104, label_len=48, pred_len=24, fc_dropout=0.3, head_dropout=0.0, patch_len=16, stride=2, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=16, n_heads=4, e_layers=3, d_layers=1, d_ff=128, moving_avg=25, factor=1, distil=True, dropout=0.3, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=16, patience=100, learning_rate=0.0025, des='Exp', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, multi_scale=None)\r\n",
      "Use GPU: cuda:0\r\n",
      "PatchTST Single-Scale Mode\r\n",
      ">>>>>>>start training : national_illness_104_24_base_model_PatchTST_custom_ftM_sl104_ll48_pl24_dm16_nh4_el3_dl1_df128_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\r\n",
      "train 549\r\n",
      "val 74\r\n",
      "test 170\r\n",
      "Epoch: 1 cost time: 0.9862918853759766\r\n",
      "Epoch: 1, Steps: 34 | Train Loss: 0.9355727 Vali Loss: 0.4142841 Test Loss: 2.7512002\r\n",
      "Validation loss decreased (inf --> 0.414284).  Saving model ...\r\n",
      "Updating learning rate to 0.0025\r\n",
      "Epoch: 2 cost time: 0.6789512634277344\r\n",
      "Epoch: 2, Steps: 34 | Train Loss: 0.7664477 Vali Loss: 0.3658940 Test Loss: 1.9017687\r\n",
      "Validation loss decreased (0.414284 --> 0.365894).  Saving model ...\r\n",
      "Updating learning rate to 0.0025\r\n",
      "Epoch: 3 cost time: 0.6878576278686523\r\n",
      "Epoch: 3, Steps: 34 | Train Loss: 0.5174255 Vali Loss: 0.2741014 Test Loss: 1.4990273\r\n",
      "Validation loss decreased (0.365894 --> 0.274101).  Saving model ...\r\n",
      "Updating learning rate to 0.0025\r\n",
      "Epoch: 4 cost time: 0.674849271774292\r\n",
      "Epoch: 4, Steps: 34 | Train Loss: 0.4447532 Vali Loss: 0.2820521 Test Loss: 1.5554858\r\n",
      "EarlyStopping counter: 1 out of 100\r\n",
      "Updating learning rate to 0.0022500000000000003\r\n",
      "Epoch: 5 cost time: 0.6753113269805908\r\n",
      "Epoch: 5, Steps: 34 | Train Loss: 0.4157571 Vali Loss: 0.3066933 Test Loss: 1.7319320\r\n",
      "EarlyStopping counter: 2 out of 100\r\n",
      "Updating learning rate to 0.0020250000000000003\r\n",
      "Epoch: 6 cost time: 0.6805217266082764\r\n",
      "Epoch: 6, Steps: 34 | Train Loss: 0.3925918 Vali Loss: 0.3056278 Test Loss: 1.9524523\r\n",
      "EarlyStopping counter: 3 out of 100\r\n",
      "Updating learning rate to 0.0018225000000000003\r\n",
      "Epoch: 7 cost time: 0.6707475185394287\r\n",
      "Epoch: 7, Steps: 34 | Train Loss: 0.3666792 Vali Loss: 0.2493924 Test Loss: 1.8573748\r\n",
      "Validation loss decreased (0.274101 --> 0.249392).  Saving model ...\r\n",
      "Updating learning rate to 0.00164025\r\n",
      "Epoch: 8 cost time: 0.6744370460510254\r\n",
      "Epoch: 8, Steps: 34 | Train Loss: 0.3585594 Vali Loss: 0.3038099 Test Loss: 1.7209079\r\n",
      "EarlyStopping counter: 1 out of 100\r\n",
      "Updating learning rate to 0.0014762250000000003\r\n",
      "Epoch: 9 cost time: 0.6966111660003662\r\n",
      "Epoch: 9, Steps: 34 | Train Loss: 0.3397525 Vali Loss: 0.2715213 Test Loss: 2.0001991\r\n",
      "EarlyStopping counter: 2 out of 100\r\n",
      "Updating learning rate to 0.0013286025\r\n",
      "Epoch: 10 cost time: 0.7078132629394531\r\n",
      "Epoch: 10, Steps: 34 | Train Loss: 0.3196675 Vali Loss: 0.2663032 Test Loss: 1.8992894\r\n",
      "EarlyStopping counter: 3 out of 100\r\n",
      "Updating learning rate to 0.0011957422500000002\r\n",
      "Epoch: 11 cost time: 0.6740458011627197\r\n",
      "Epoch: 11, Steps: 34 | Train Loss: 0.2919274 Vali Loss: 0.2655148 Test Loss: 1.9038594\r\n",
      "EarlyStopping counter: 4 out of 100\r\n",
      "Updating learning rate to 0.0010761680250000003\r\n",
      "Epoch: 12 cost time: 0.6863183975219727\r\n",
      "Epoch: 12, Steps: 34 | Train Loss: 0.2805382 Vali Loss: 0.2532253 Test Loss: 1.7292175\r\n",
      "EarlyStopping counter: 5 out of 100\r\n",
      "Updating learning rate to 0.0009685512225000002\r\n",
      "Epoch: 13 cost time: 0.6947929859161377\r\n",
      "Epoch: 13, Steps: 34 | Train Loss: 0.2657003 Vali Loss: 0.2363527 Test Loss: 1.5977179\r\n",
      "Validation loss decreased (0.249392 --> 0.236353).  Saving model ...\r\n",
      "Updating learning rate to 0.0008716961002500003\r\n",
      "Epoch: 14 cost time: 0.6648821830749512\r\n",
      "Epoch: 14, Steps: 34 | Train Loss: 0.2525062 Vali Loss: 0.2401177 Test Loss: 1.5974422\r\n",
      "EarlyStopping counter: 1 out of 100\r\n",
      "Updating learning rate to 0.0007845264902250002\r\n",
      "Epoch: 15 cost time: 0.6770567893981934\r\n",
      "Epoch: 15, Steps: 34 | Train Loss: 0.2512855 Vali Loss: 0.2354360 Test Loss: 1.4677371\r\n",
      "Validation loss decreased (0.236353 --> 0.235436).  Saving model ...\r\n",
      "Updating learning rate to 0.0007060738412025003\r\n",
      "Epoch: 16 cost time: 0.6670856475830078\r\n",
      "Epoch: 16, Steps: 34 | Train Loss: 0.2410240 Vali Loss: 0.2412520 Test Loss: 1.4884483\r\n",
      "EarlyStopping counter: 1 out of 100\r\n",
      "Updating learning rate to 0.0006354664570822502\r\n",
      "Epoch: 17 cost time: 0.6790540218353271\r\n",
      "Epoch: 17, Steps: 34 | Train Loss: 0.2330864 Vali Loss: 0.2557257 Test Loss: 1.5263480\r\n",
      "EarlyStopping counter: 2 out of 100\r\n",
      "Updating learning rate to 0.0005719198113740252\r\n",
      "Epoch: 18 cost time: 0.721815824508667\r\n",
      "Epoch: 18, Steps: 34 | Train Loss: 0.2263705 Vali Loss: 0.2317762 Test Loss: 1.4757650\r\n",
      "Validation loss decreased (0.235436 --> 0.231776).  Saving model ...\r\n",
      "Updating learning rate to 0.0005147278302366227\r\n",
      "Epoch: 19 cost time: 0.6711692810058594\r\n",
      "Epoch: 19, Steps: 34 | Train Loss: 0.2191800 Vali Loss: 0.2453061 Test Loss: 1.4303484\r\n",
      "EarlyStopping counter: 1 out of 100\r\n",
      "Updating learning rate to 0.0004632550472129604\r\n",
      "Epoch: 20 cost time: 0.6722977161407471\r\n",
      "Epoch: 20, Steps: 34 | Train Loss: 0.2188873 Vali Loss: 0.2239039 Test Loss: 1.4122970\r\n",
      "Validation loss decreased (0.231776 --> 0.223904).  Saving model ...\r\n",
      "Updating learning rate to 0.0004169295424916644\r\n",
      "Epoch: 21 cost time: 0.6841449737548828\r\n",
      "Epoch: 21, Steps: 34 | Train Loss: 0.2132223 Vali Loss: 0.2506065 Test Loss: 1.4571861\r\n",
      "EarlyStopping counter: 1 out of 100\r\n",
      "Updating learning rate to 0.00037523658824249794\r\n",
      "Epoch: 22 cost time: 0.6981892585754395\r\n",
      "Epoch: 22, Steps: 34 | Train Loss: 0.2130027 Vali Loss: 0.2548918 Test Loss: 1.4307922\r\n",
      "EarlyStopping counter: 2 out of 100\r\n",
      "Updating learning rate to 0.0003377129294182482\r\n",
      "Epoch: 23 cost time: 0.7073981761932373\r\n",
      "Epoch: 23, Steps: 34 | Train Loss: 0.2113182 Vali Loss: 0.2387422 Test Loss: 1.3776336\r\n",
      "EarlyStopping counter: 3 out of 100\r\n",
      "Updating learning rate to 0.0003039416364764234\r\n",
      "Epoch: 24 cost time: 0.6793193817138672\r\n",
      "Epoch: 24, Steps: 34 | Train Loss: 0.2060814 Vali Loss: 0.2222595 Test Loss: 1.4337158\r\n",
      "Validation loss decreased (0.223904 --> 0.222259).  Saving model ...\r\n",
      "Updating learning rate to 0.0002735474728287811\r\n",
      "Epoch: 25 cost time: 0.6750533580780029\r\n",
      "Epoch: 25, Steps: 34 | Train Loss: 0.2070481 Vali Loss: 0.2472817 Test Loss: 1.4451019\r\n",
      "EarlyStopping counter: 1 out of 100\r\n",
      "Updating learning rate to 0.00024619272554590293\r\n",
      "Epoch: 26 cost time: 0.6711475849151611\r\n",
      "Epoch: 26, Steps: 34 | Train Loss: 0.2036888 Vali Loss: 0.2498669 Test Loss: 1.3880084\r\n",
      "EarlyStopping counter: 2 out of 100\r\n",
      "Updating learning rate to 0.00022157345299131267\r\n",
      "Epoch: 27 cost time: 0.6650760173797607\r\n",
      "Epoch: 27, Steps: 34 | Train Loss: 0.2056956 Vali Loss: 0.2363582 Test Loss: 1.4144897\r\n",
      "EarlyStopping counter: 3 out of 100\r\n",
      "Updating learning rate to 0.0001994161076921814\r\n",
      "Epoch: 28 cost time: 0.6863887310028076\r\n",
      "Epoch: 28, Steps: 34 | Train Loss: 0.2068692 Vali Loss: 0.2592909 Test Loss: 1.4183736\r\n",
      "EarlyStopping counter: 4 out of 100\r\n",
      "Updating learning rate to 0.00017947449692296325\r\n",
      "Epoch: 29 cost time: 0.6652028560638428\r\n",
      "Epoch: 29, Steps: 34 | Train Loss: 0.2062746 Vali Loss: 0.2570161 Test Loss: 1.3982286\r\n",
      "EarlyStopping counter: 5 out of 100\r\n",
      "Updating learning rate to 0.00016152704723066693\r\n",
      "Epoch: 30 cost time: 0.7024631500244141\r\n",
      "Epoch: 30, Steps: 34 | Train Loss: 0.1997622 Vali Loss: 0.2455801 Test Loss: 1.3940513\r\n",
      "EarlyStopping counter: 6 out of 100\r\n",
      "Updating learning rate to 0.00014537434250760025\r\n",
      "Epoch: 31 cost time: 0.6750268936157227\r\n",
      "Epoch: 31, Steps: 34 | Train Loss: 0.1967109 Vali Loss: 0.2521612 Test Loss: 1.3988358\r\n",
      "EarlyStopping counter: 7 out of 100\r\n",
      "Updating learning rate to 0.00013083690825684024\r\n",
      "Epoch: 32 cost time: 0.6758723258972168\r\n",
      "Epoch: 32, Steps: 34 | Train Loss: 0.1965340 Vali Loss: 0.2399816 Test Loss: 1.3974009\r\n",
      "EarlyStopping counter: 8 out of 100\r\n",
      "Updating learning rate to 0.00011775321743115622\r\n",
      "Epoch: 33 cost time: 0.6823809146881104\r\n",
      "Epoch: 33, Steps: 34 | Train Loss: 0.1983330 Vali Loss: 0.2569565 Test Loss: 1.3757621\r\n",
      "EarlyStopping counter: 9 out of 100\r\n",
      "Updating learning rate to 0.0001059778956880406\r\n",
      "Epoch: 34 cost time: 0.6758203506469727\r\n",
      "Epoch: 34, Steps: 34 | Train Loss: 0.1956706 Vali Loss: 0.2375707 Test Loss: 1.3652458\r\n",
      "EarlyStopping counter: 10 out of 100\r\n",
      "Updating learning rate to 9.538010611923654e-05\r\n",
      "Epoch: 35 cost time: 0.673163652420044\r\n",
      "Epoch: 35, Steps: 34 | Train Loss: 0.1992736 Vali Loss: 0.2356668 Test Loss: 1.3889915\r\n",
      "EarlyStopping counter: 11 out of 100\r\n",
      "Updating learning rate to 8.584209550731288e-05\r\n",
      "Epoch: 36 cost time: 0.6604166030883789\r\n",
      "Epoch: 36, Steps: 34 | Train Loss: 0.1945885 Vali Loss: 0.2514604 Test Loss: 1.3785865\r\n",
      "EarlyStopping counter: 12 out of 100\r\n",
      "Updating learning rate to 7.725788595658159e-05\r\n",
      "Epoch: 37 cost time: 0.6776270866394043\r\n",
      "Epoch: 37, Steps: 34 | Train Loss: 0.1978581 Vali Loss: 0.2477165 Test Loss: 1.3821340\r\n",
      "EarlyStopping counter: 13 out of 100\r\n",
      "Updating learning rate to 6.953209736092344e-05\r\n",
      "Epoch: 38 cost time: 0.6789376735687256\r\n",
      "Epoch: 38, Steps: 34 | Train Loss: 0.1940142 Vali Loss: 0.2464505 Test Loss: 1.3766869\r\n",
      "EarlyStopping counter: 14 out of 100\r\n",
      "Updating learning rate to 6.25788876248311e-05\r\n",
      "Epoch: 39 cost time: 0.6822810173034668\r\n",
      "Epoch: 39, Steps: 34 | Train Loss: 0.1969440 Vali Loss: 0.2475248 Test Loss: 1.3574028\r\n",
      "EarlyStopping counter: 15 out of 100\r\n",
      "Updating learning rate to 5.632099886234799e-05\r\n",
      "Epoch: 40 cost time: 0.6752283573150635\r\n",
      "Epoch: 40, Steps: 34 | Train Loss: 0.1939492 Vali Loss: 0.2534092 Test Loss: 1.3762054\r\n",
      "EarlyStopping counter: 16 out of 100\r\n",
      "Updating learning rate to 5.068889897611319e-05\r\n",
      "Epoch: 41 cost time: 0.6704535484313965\r\n",
      "Epoch: 41, Steps: 34 | Train Loss: 0.1924674 Vali Loss: 0.2366369 Test Loss: 1.3718374\r\n",
      "EarlyStopping counter: 17 out of 100\r\n",
      "Updating learning rate to 4.562000907850187e-05\r\n",
      "Epoch: 42 cost time: 0.6804904937744141\r\n",
      "Epoch: 42, Steps: 34 | Train Loss: 0.1922473 Vali Loss: 0.2632233 Test Loss: 1.3710802\r\n",
      "EarlyStopping counter: 18 out of 100\r\n",
      "Updating learning rate to 4.105800817065169e-05\r\n",
      "Epoch: 43 cost time: 0.7297143936157227\r\n",
      "Epoch: 43, Steps: 34 | Train Loss: 0.1943093 Vali Loss: 0.2490863 Test Loss: 1.3751892\r\n",
      "EarlyStopping counter: 19 out of 100\r\n",
      "Updating learning rate to 3.695220735358652e-05\r\n",
      "Epoch: 44 cost time: 0.6705093383789062\r\n",
      "Epoch: 44, Steps: 34 | Train Loss: 0.1934301 Vali Loss: 0.2581749 Test Loss: 1.3742106\r\n",
      "EarlyStopping counter: 20 out of 100\r\n",
      "Updating learning rate to 3.3256986618227866e-05\r\n",
      "Epoch: 45 cost time: 0.6716246604919434\r\n",
      "Epoch: 45, Steps: 34 | Train Loss: 0.1943905 Vali Loss: 0.2662653 Test Loss: 1.3771890\r\n",
      "EarlyStopping counter: 21 out of 100\r\n",
      "Updating learning rate to 2.9931287956405084e-05\r\n",
      "Epoch: 46 cost time: 0.7001025676727295\r\n",
      "Epoch: 46, Steps: 34 | Train Loss: 0.1912960 Vali Loss: 0.2518250 Test Loss: 1.3741143\r\n",
      "EarlyStopping counter: 22 out of 100\r\n",
      "Updating learning rate to 2.6938159160764573e-05\r\n",
      "Epoch: 47 cost time: 0.6819987297058105\r\n",
      "Epoch: 47, Steps: 34 | Train Loss: 0.1875608 Vali Loss: 0.2340867 Test Loss: 1.3616550\r\n",
      "EarlyStopping counter: 23 out of 100\r\n",
      "Updating learning rate to 2.424434324468812e-05\r\n",
      "Epoch: 48 cost time: 0.6791679859161377\r\n",
      "Epoch: 48, Steps: 34 | Train Loss: 0.1876557 Vali Loss: 0.2618122 Test Loss: 1.3458061\r\n",
      "EarlyStopping counter: 24 out of 100\r\n",
      "Updating learning rate to 2.1819908920219307e-05\r\n",
      "Epoch: 49 cost time: 0.6898059844970703\r\n",
      "Epoch: 49, Steps: 34 | Train Loss: 0.1928257 Vali Loss: 0.2525005 Test Loss: 1.3693708\r\n",
      "EarlyStopping counter: 25 out of 100\r\n",
      "Updating learning rate to 1.9637918028197378e-05\r\n",
      "Epoch: 50 cost time: 0.7017407417297363\r\n",
      "Epoch: 50, Steps: 34 | Train Loss: 0.1879601 Vali Loss: 0.2424592 Test Loss: 1.3584116\r\n",
      "EarlyStopping counter: 26 out of 100\r\n",
      "Updating learning rate to 1.7674126225377637e-05\r\n",
      "Epoch: 51 cost time: 0.7180416584014893\r\n",
      "Epoch: 51, Steps: 34 | Train Loss: 0.1892271 Vali Loss: 0.2473374 Test Loss: 1.3697551\r\n",
      "EarlyStopping counter: 27 out of 100\r\n",
      "Updating learning rate to 1.5906713602839874e-05\r\n",
      "Epoch: 52 cost time: 0.6816904544830322\r\n",
      "Epoch: 52, Steps: 34 | Train Loss: 0.1919977 Vali Loss: 0.2511603 Test Loss: 1.3512580\r\n",
      "EarlyStopping counter: 28 out of 100\r\n",
      "Updating learning rate to 1.4316042242555887e-05\r\n",
      "Epoch: 53 cost time: 0.6887414455413818\r\n",
      "Epoch: 53, Steps: 34 | Train Loss: 0.1968984 Vali Loss: 0.2448822 Test Loss: 1.3700936\r\n",
      "EarlyStopping counter: 29 out of 100\r\n",
      "Updating learning rate to 1.28844380183003e-05\r\n",
      "Epoch: 54 cost time: 0.7036294937133789\r\n",
      "Epoch: 54, Steps: 34 | Train Loss: 0.1917527 Vali Loss: 0.2469908 Test Loss: 1.3601840\r\n",
      "EarlyStopping counter: 30 out of 100\r\n",
      "Updating learning rate to 1.1595994216470272e-05\r\n",
      "Epoch: 55 cost time: 0.67071533203125\r\n",
      "Epoch: 55, Steps: 34 | Train Loss: 0.1934655 Vali Loss: 0.2413725 Test Loss: 1.3562273\r\n",
      "EarlyStopping counter: 31 out of 100\r\n",
      "Updating learning rate to 1.0436394794823243e-05\r\n",
      "Epoch: 56 cost time: 0.6815178394317627\r\n",
      "Epoch: 56, Steps: 34 | Train Loss: 0.1928069 Vali Loss: 0.2583472 Test Loss: 1.3710319\r\n",
      "EarlyStopping counter: 32 out of 100\r\n",
      "Updating learning rate to 9.392755315340918e-06\r\n",
      "Epoch: 57 cost time: 0.6663880348205566\r\n",
      "Epoch: 57, Steps: 34 | Train Loss: 0.1970041 Vali Loss: 0.2596202 Test Loss: 1.3718308\r\n",
      "EarlyStopping counter: 33 out of 100\r\n",
      "Updating learning rate to 8.453479783806827e-06\r\n",
      "Epoch: 58 cost time: 0.6866848468780518\r\n",
      "Epoch: 58, Steps: 34 | Train Loss: 0.1961164 Vali Loss: 0.2549118 Test Loss: 1.3624626\r\n",
      "EarlyStopping counter: 34 out of 100\r\n",
      "Updating learning rate to 7.608131805426145e-06\r\n",
      "Epoch: 59 cost time: 0.7017643451690674\r\n",
      "Epoch: 59, Steps: 34 | Train Loss: 0.1900429 Vali Loss: 0.2541530 Test Loss: 1.3676096\r\n",
      "EarlyStopping counter: 35 out of 100\r\n",
      "Updating learning rate to 6.84731862488353e-06\r\n",
      "Epoch: 60 cost time: 0.6792633533477783\r\n",
      "Epoch: 60, Steps: 34 | Train Loss: 0.1881622 Vali Loss: 0.2510213 Test Loss: 1.3638624\r\n",
      "EarlyStopping counter: 36 out of 100\r\n",
      "Updating learning rate to 6.162586762395178e-06\r\n",
      "Epoch: 61 cost time: 0.7128853797912598\r\n",
      "Epoch: 61, Steps: 34 | Train Loss: 0.1906009 Vali Loss: 0.2575127 Test Loss: 1.3661186\r\n",
      "EarlyStopping counter: 37 out of 100\r\n",
      "Updating learning rate to 5.54632808615566e-06\r\n",
      "Epoch: 62 cost time: 0.6750471591949463\r\n",
      "Epoch: 62, Steps: 34 | Train Loss: 0.1941431 Vali Loss: 0.2487643 Test Loss: 1.3529812\r\n",
      "EarlyStopping counter: 38 out of 100\r\n",
      "Updating learning rate to 4.991695277540094e-06\r\n",
      "Epoch: 63 cost time: 0.7057998180389404\r\n",
      "Epoch: 63, Steps: 34 | Train Loss: 0.1857332 Vali Loss: 0.2580937 Test Loss: 1.3591290\r\n",
      "EarlyStopping counter: 39 out of 100\r\n",
      "Updating learning rate to 4.492525749786085e-06\r\n",
      "Epoch: 64 cost time: 0.678861141204834\r\n",
      "Epoch: 64, Steps: 34 | Train Loss: 0.1892476 Vali Loss: 0.2553592 Test Loss: 1.3583660\r\n",
      "EarlyStopping counter: 40 out of 100\r\n",
      "Updating learning rate to 4.043273174807477e-06\r\n",
      "Epoch: 65 cost time: 0.6882503032684326\r\n",
      "Epoch: 65, Steps: 34 | Train Loss: 0.1902239 Vali Loss: 0.2377793 Test Loss: 1.3644054\r\n",
      "EarlyStopping counter: 41 out of 100\r\n",
      "Updating learning rate to 3.6389458573267288e-06\r\n",
      "Epoch: 66 cost time: 0.68477463722229\r\n",
      "Epoch: 66, Steps: 34 | Train Loss: 0.1950851 Vali Loss: 0.2611175 Test Loss: 1.3538833\r\n",
      "EarlyStopping counter: 42 out of 100\r\n",
      "Updating learning rate to 3.2750512715940557e-06\r\n",
      "Epoch: 67 cost time: 0.6983420848846436\r\n",
      "Epoch: 67, Steps: 34 | Train Loss: 0.1921424 Vali Loss: 0.2561126 Test Loss: 1.3621536\r\n",
      "EarlyStopping counter: 43 out of 100\r\n",
      "Updating learning rate to 2.9475461444346506e-06\r\n",
      "Epoch: 68 cost time: 0.7011275291442871\r\n",
      "Epoch: 68, Steps: 34 | Train Loss: 0.1904084 Vali Loss: 0.2628884 Test Loss: 1.3616837\r\n",
      "EarlyStopping counter: 44 out of 100\r\n",
      "Updating learning rate to 2.652791529991185e-06\r\n",
      "Epoch: 69 cost time: 0.6717729568481445\r\n",
      "Epoch: 69, Steps: 34 | Train Loss: 0.1935479 Vali Loss: 0.2458910 Test Loss: 1.3498669\r\n",
      "EarlyStopping counter: 45 out of 100\r\n",
      "Updating learning rate to 2.387512376992067e-06\r\n",
      "Epoch: 70 cost time: 0.6777431964874268\r\n",
      "Epoch: 70, Steps: 34 | Train Loss: 0.1956097 Vali Loss: 0.2452980 Test Loss: 1.3641821\r\n",
      "EarlyStopping counter: 46 out of 100\r\n",
      "Updating learning rate to 2.14876113929286e-06\r\n",
      "Epoch: 71 cost time: 0.6802585124969482\r\n",
      "Epoch: 71, Steps: 34 | Train Loss: 0.1919351 Vali Loss: 0.2445899 Test Loss: 1.3533249\r\n",
      "EarlyStopping counter: 47 out of 100\r\n",
      "Updating learning rate to 1.9338850253635745e-06\r\n",
      "Epoch: 72 cost time: 0.6608126163482666\r\n",
      "Epoch: 72, Steps: 34 | Train Loss: 0.1937907 Vali Loss: 0.2609368 Test Loss: 1.3446989\r\n",
      "EarlyStopping counter: 48 out of 100\r\n",
      "Updating learning rate to 1.740496522827217e-06\r\n",
      "Epoch: 73 cost time: 0.6908035278320312\r\n",
      "Epoch: 73, Steps: 34 | Train Loss: 0.1891404 Vali Loss: 0.2419941 Test Loss: 1.3568134\r\n",
      "EarlyStopping counter: 49 out of 100\r\n",
      "Updating learning rate to 1.5664468705444951e-06\r\n",
      "Epoch: 74 cost time: 0.6804742813110352\r\n",
      "Epoch: 74, Steps: 34 | Train Loss: 0.1902753 Vali Loss: 0.2456968 Test Loss: 1.3663808\r\n",
      "EarlyStopping counter: 50 out of 100\r\n",
      "Updating learning rate to 1.4098021834900458e-06\r\n",
      "Epoch: 75 cost time: 0.7009022235870361\r\n",
      "Epoch: 75, Steps: 34 | Train Loss: 0.1914658 Vali Loss: 0.2424780 Test Loss: 1.3448077\r\n",
      "EarlyStopping counter: 51 out of 100\r\n",
      "Updating learning rate to 1.2688219651410413e-06\r\n",
      "Epoch: 76 cost time: 0.6811513900756836\r\n",
      "Epoch: 76, Steps: 34 | Train Loss: 0.1923614 Vali Loss: 0.2479651 Test Loss: 1.3465135\r\n",
      "EarlyStopping counter: 52 out of 100\r\n",
      "Updating learning rate to 1.1419397686269372e-06\r\n",
      "Epoch: 77 cost time: 0.6820948123931885\r\n",
      "Epoch: 77, Steps: 34 | Train Loss: 0.1905682 Vali Loss: 0.2421599 Test Loss: 1.3605468\r\n",
      "EarlyStopping counter: 53 out of 100\r\n",
      "Updating learning rate to 1.0277457917642436e-06\r\n",
      "Epoch: 78 cost time: 0.6957197189331055\r\n",
      "Epoch: 78, Steps: 34 | Train Loss: 0.1898421 Vali Loss: 0.2490158 Test Loss: 1.3583777\r\n",
      "EarlyStopping counter: 54 out of 100\r\n",
      "Updating learning rate to 9.249712125878192e-07\r\n",
      "Epoch: 79 cost time: 0.6811163425445557\r\n",
      "Epoch: 79, Steps: 34 | Train Loss: 0.1949673 Vali Loss: 0.2607857 Test Loss: 1.3575819\r\n",
      "EarlyStopping counter: 55 out of 100\r\n",
      "Updating learning rate to 8.324740913290373e-07\r\n",
      "Epoch: 80 cost time: 0.6898951530456543\r\n",
      "Epoch: 80, Steps: 34 | Train Loss: 0.1924073 Vali Loss: 0.2576427 Test Loss: 1.3538325\r\n",
      "EarlyStopping counter: 56 out of 100\r\n",
      "Updating learning rate to 7.492266821961335e-07\r\n",
      "Epoch: 81 cost time: 0.7201735973358154\r\n",
      "Epoch: 81, Steps: 34 | Train Loss: 0.1897743 Vali Loss: 0.2539830 Test Loss: 1.3504894\r\n",
      "EarlyStopping counter: 57 out of 100\r\n",
      "Updating learning rate to 6.743040139765202e-07\r\n",
      "Epoch: 82 cost time: 0.6653988361358643\r\n",
      "Epoch: 82, Steps: 34 | Train Loss: 0.1965765 Vali Loss: 0.2481533 Test Loss: 1.3562958\r\n",
      "EarlyStopping counter: 58 out of 100\r\n",
      "Updating learning rate to 6.068736125788682e-07\r\n",
      "Epoch: 83 cost time: 0.6880922317504883\r\n",
      "Epoch: 83, Steps: 34 | Train Loss: 0.1888277 Vali Loss: 0.2564876 Test Loss: 1.3609693\r\n",
      "EarlyStopping counter: 59 out of 100\r\n",
      "Updating learning rate to 5.461862513209814e-07\r\n",
      "Epoch: 84 cost time: 0.6739795207977295\r\n",
      "Epoch: 84, Steps: 34 | Train Loss: 0.1902532 Vali Loss: 0.2506338 Test Loss: 1.3561090\r\n",
      "EarlyStopping counter: 60 out of 100\r\n",
      "Updating learning rate to 4.915676261888833e-07\r\n",
      "Epoch: 85 cost time: 0.6875176429748535\r\n",
      "Epoch: 85, Steps: 34 | Train Loss: 0.1950319 Vali Loss: 0.2519476 Test Loss: 1.3649313\r\n",
      "EarlyStopping counter: 61 out of 100\r\n",
      "Updating learning rate to 4.4241086356999495e-07\r\n",
      "Epoch: 86 cost time: 0.6761684417724609\r\n",
      "Epoch: 86, Steps: 34 | Train Loss: 0.1884888 Vali Loss: 0.2571112 Test Loss: 1.3642788\r\n",
      "EarlyStopping counter: 62 out of 100\r\n",
      "Updating learning rate to 3.9816977721299544e-07\r\n",
      "Epoch: 87 cost time: 0.6774756908416748\r\n",
      "Epoch: 87, Steps: 34 | Train Loss: 0.1902206 Vali Loss: 0.2451359 Test Loss: 1.3666017\r\n",
      "EarlyStopping counter: 63 out of 100\r\n",
      "Updating learning rate to 3.583527994916959e-07\r\n",
      "Epoch: 88 cost time: 0.6860220432281494\r\n",
      "Epoch: 88, Steps: 34 | Train Loss: 0.1888930 Vali Loss: 0.2629745 Test Loss: 1.3730272\r\n",
      "EarlyStopping counter: 64 out of 100\r\n",
      "Updating learning rate to 3.2251751954252636e-07\r\n",
      "Epoch: 89 cost time: 0.678886890411377\r\n",
      "Epoch: 89, Steps: 34 | Train Loss: 0.1854432 Vali Loss: 0.2528087 Test Loss: 1.3740100\r\n",
      "EarlyStopping counter: 65 out of 100\r\n",
      "Updating learning rate to 2.902657675882737e-07\r\n",
      "Epoch: 90 cost time: 0.6747171878814697\r\n",
      "Epoch: 90, Steps: 34 | Train Loss: 0.1886396 Vali Loss: 0.2464664 Test Loss: 1.3697500\r\n",
      "EarlyStopping counter: 66 out of 100\r\n",
      "Updating learning rate to 2.612391908294464e-07\r\n",
      "Epoch: 91 cost time: 0.7051866054534912\r\n",
      "Epoch: 91, Steps: 34 | Train Loss: 0.1964589 Vali Loss: 0.2362561 Test Loss: 1.3706959\r\n",
      "EarlyStopping counter: 67 out of 100\r\n",
      "Updating learning rate to 2.351152717465017e-07\r\n",
      "Epoch: 92 cost time: 0.6586408615112305\r\n",
      "Epoch: 92, Steps: 34 | Train Loss: 0.1915128 Vali Loss: 0.2428853 Test Loss: 1.3756902\r\n",
      "EarlyStopping counter: 68 out of 100\r\n",
      "Updating learning rate to 2.1160374457185157e-07\r\n",
      "Epoch: 93 cost time: 0.6849956512451172\r\n",
      "Epoch: 93, Steps: 34 | Train Loss: 0.1923648 Vali Loss: 0.2521068 Test Loss: 1.3583590\r\n",
      "EarlyStopping counter: 69 out of 100\r\n",
      "Updating learning rate to 1.9044337011466642e-07\r\n",
      "Epoch: 94 cost time: 0.676424503326416\r\n",
      "Epoch: 94, Steps: 34 | Train Loss: 0.1939492 Vali Loss: 0.2512978 Test Loss: 1.3630512\r\n",
      "EarlyStopping counter: 70 out of 100\r\n",
      "Updating learning rate to 1.7139903310319977e-07\r\n",
      "Epoch: 95 cost time: 0.6831498146057129\r\n",
      "Epoch: 95, Steps: 34 | Train Loss: 0.1932855 Vali Loss: 0.2738972 Test Loss: 1.3659708\r\n",
      "EarlyStopping counter: 71 out of 100\r\n",
      "Updating learning rate to 1.542591297928798e-07\r\n",
      "Epoch: 96 cost time: 0.6708776950836182\r\n",
      "Epoch: 96, Steps: 34 | Train Loss: 0.1867921 Vali Loss: 0.2507877 Test Loss: 1.3611188\r\n",
      "EarlyStopping counter: 72 out of 100\r\n",
      "Updating learning rate to 1.3883321681359182e-07\r\n",
      "Epoch: 97 cost time: 0.6703977584838867\r\n",
      "Epoch: 97, Steps: 34 | Train Loss: 0.1912584 Vali Loss: 0.2450213 Test Loss: 1.3455980\r\n",
      "EarlyStopping counter: 73 out of 100\r\n",
      "Updating learning rate to 1.2494989513223265e-07\r\n",
      "Epoch: 98 cost time: 0.6673293113708496\r\n",
      "Epoch: 98, Steps: 34 | Train Loss: 0.1928521 Vali Loss: 0.2549994 Test Loss: 1.3667272\r\n",
      "EarlyStopping counter: 74 out of 100\r\n",
      "Updating learning rate to 1.1245490561900937e-07\r\n",
      "Epoch: 99 cost time: 0.6972358226776123\r\n",
      "Epoch: 99, Steps: 34 | Train Loss: 0.1956238 Vali Loss: 0.2586922 Test Loss: 1.3515079\r\n",
      "EarlyStopping counter: 75 out of 100\r\n",
      "Updating learning rate to 1.0120941505710844e-07\r\n",
      "Epoch: 100 cost time: 0.7307403087615967\r\n",
      "Epoch: 100, Steps: 34 | Train Loss: 0.1902433 Vali Loss: 0.2499997 Test Loss: 1.3657284\r\n",
      "EarlyStopping counter: 76 out of 100\r\n",
      "Updating learning rate to 9.10884735513976e-08\r\n",
      ">>>>>>>testing : national_illness_104_24_base_model_PatchTST_custom_ftM_sl104_ll48_pl24_dm16_nh4_el3_dl1_df128_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\r\n",
      "test 170\r\n",
      "mse:1.4337158203125, mae:0.7968036532402039, rse:0.5778344869613647\r\n"
     ]
    }
   ],
   "source": [
    "!python run_longExp.py \\\n",
    "    --random_seed 2021 \\\n",
    "    --is_training 1 \\\n",
    "    --root_path ./dataset/ \\\n",
    "    --data_path national_illness.csv \\\n",
    "    --model_id national_illness_104_24_base_model \\\n",
    "    --model PatchTST \\\n",
    "    --data custom \\\n",
    "    --features M \\\n",
    "    --seq_len 104 \\\n",
    "    --pred_len 24 \\\n",
    "    --enc_in 7 \\\n",
    "    --e_layers 3 \\\n",
    "    --n_heads 4 \\\n",
    "    --d_model 16 \\\n",
    "    --d_ff 128 \\\n",
    "    --dropout 0.3 \\\n",
    "    --fc_dropout 0.3 \\\n",
    "    --head_dropout 0 \\\n",
    "    --patch_len 16 \\\n",
    "    --stride 2 \\\n",
    "    --des 'Exp' \\\n",
    "    --train_epochs 100 \\\n",
    "    --itr 1 \\\n",
    "    --batch_size 16 \\\n",
    "    --learning_rate 0.0025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d4f9a2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T22:24:44.715272Z",
     "iopub.status.busy": "2025-10-14T22:24:44.715004Z",
     "iopub.status.idle": "2025-10-14T22:27:30.768094Z",
     "shell.execute_reply": "2025-10-14T22:27:30.767262Z"
    },
    "papermill": {
     "duration": 166.086415,
     "end_time": "2025-10-14T22:27:30.769463",
     "exception": false,
     "start_time": "2025-10-14T22:24:44.683048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\r\n",
      "Namespace(random_seed=2021, is_training=1, model_id='national_illness_104_24_multi_model', model='PatchTST', data='custom', root_path='./dataset/', data_path='national_illness.csv', features='M', target='OT', freq='h', checkpoints='./checkpoints/', seq_len=104, label_len=48, pred_len=24, fc_dropout=0.3, head_dropout=0.0, patch_len=16, stride=2, padding_patch='end', revin=1, affine=0, subtract_last=0, decomposition=0, kernel_size=25, individual=0, embed_type=0, enc_in=7, dec_in=7, c_out=7, d_model=16, n_heads=4, e_layers=3, d_layers=1, d_ff=128, moving_avg=25, factor=1, distil=True, dropout=0.3, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=100, batch_size=16, patience=100, learning_rate=0.0025, des='Exp', loss='mse', lradj='type3', pct_start=0.3, use_amp=False, use_gpu=True, gpu=0, use_multi_gpu=False, devices='0,1,2,3', test_flop=False, multi_scale=['small', 'large'])\r\n",
      "Use GPU: cuda:0\r\n",
      "PatchTST Multi-Scale Configuration: ['small', 'large']\r\n",
      "Using scales: ['small', 'large']\r\n",
      "Patch sizes: [8, 26]\r\n",
      "Strides: [2, 4]\r\n",
      ">>>>>>>start training : national_illness_104_24_multi_model_PatchTST_custom_ftM_sl104_ll48_pl24_dm16_nh4_el3_dl1_df128_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\r\n",
      "train 549\r\n",
      "val 74\r\n",
      "test 170\r\n",
      "Epoch: 1 cost time: 1.2895805835723877\r\n",
      "Epoch: 1, Steps: 34 | Train Loss: 1.0109324 Vali Loss: 0.9312797 Test Loss: 6.1959329\r\n",
      "Validation loss decreased (inf --> 0.931280).  Saving model ...\r\n",
      "Updating learning rate to 0.0025\r\n",
      "Epoch: 2 cost time: 1.0184803009033203\r\n",
      "Epoch: 2, Steps: 34 | Train Loss: 0.6724022 Vali Loss: 0.4402644 Test Loss: 2.2337146\r\n",
      "Validation loss decreased (0.931280 --> 0.440264).  Saving model ...\r\n",
      "Updating learning rate to 0.0025\r\n",
      "Epoch: 3 cost time: 1.0139031410217285\r\n",
      "Epoch: 3, Steps: 34 | Train Loss: 0.4857321 Vali Loss: 0.2743338 Test Loss: 2.2524416\r\n",
      "Validation loss decreased (0.440264 --> 0.274334).  Saving model ...\r\n",
      "Updating learning rate to 0.0025\r\n",
      "Epoch: 4 cost time: 1.011289119720459\r\n",
      "Epoch: 4, Steps: 34 | Train Loss: 0.4050681 Vali Loss: 0.2653673 Test Loss: 1.7185510\r\n",
      "Validation loss decreased (0.274334 --> 0.265367).  Saving model ...\r\n",
      "Updating learning rate to 0.0022500000000000003\r\n",
      "Epoch: 5 cost time: 1.0147664546966553\r\n",
      "Epoch: 5, Steps: 34 | Train Loss: 0.3713899 Vali Loss: 0.2580899 Test Loss: 1.8774176\r\n",
      "Validation loss decreased (0.265367 --> 0.258090).  Saving model ...\r\n",
      "Updating learning rate to 0.0020250000000000003\r\n",
      "Epoch: 6 cost time: 0.9951472282409668\r\n",
      "Epoch: 6, Steps: 34 | Train Loss: 0.3514929 Vali Loss: 0.2492674 Test Loss: 1.8645134\r\n",
      "Validation loss decreased (0.258090 --> 0.249267).  Saving model ...\r\n",
      "Updating learning rate to 0.0018225000000000003\r\n",
      "Epoch: 7 cost time: 1.0119695663452148\r\n",
      "Epoch: 7, Steps: 34 | Train Loss: 0.3253034 Vali Loss: 0.2853910 Test Loss: 1.8347571\r\n",
      "EarlyStopping counter: 1 out of 100\r\n",
      "Updating learning rate to 0.00164025\r\n",
      "Epoch: 8 cost time: 1.0205953121185303\r\n",
      "Epoch: 8, Steps: 34 | Train Loss: 0.3094872 Vali Loss: 0.2663669 Test Loss: 1.7457727\r\n",
      "EarlyStopping counter: 2 out of 100\r\n",
      "Updating learning rate to 0.0014762250000000003\r\n",
      "Epoch: 9 cost time: 1.0284740924835205\r\n",
      "Epoch: 9, Steps: 34 | Train Loss: 0.2899628 Vali Loss: 0.2534344 Test Loss: 1.7617525\r\n",
      "EarlyStopping counter: 3 out of 100\r\n",
      "Updating learning rate to 0.0013286025\r\n",
      "Epoch: 10 cost time: 1.0009129047393799\r\n",
      "Epoch: 10, Steps: 34 | Train Loss: 0.2743686 Vali Loss: 0.2760863 Test Loss: 1.7478781\r\n",
      "EarlyStopping counter: 4 out of 100\r\n",
      "Updating learning rate to 0.0011957422500000002\r\n",
      "Epoch: 11 cost time: 1.004267930984497\r\n",
      "Epoch: 11, Steps: 34 | Train Loss: 0.2693550 Vali Loss: 0.2556424 Test Loss: 1.8640592\r\n",
      "EarlyStopping counter: 5 out of 100\r\n",
      "Updating learning rate to 0.0010761680250000003\r\n",
      "Epoch: 12 cost time: 1.0001466274261475\r\n",
      "Epoch: 12, Steps: 34 | Train Loss: 0.2527728 Vali Loss: 0.2454653 Test Loss: 1.7391179\r\n",
      "Validation loss decreased (0.249267 --> 0.245465).  Saving model ...\r\n",
      "Updating learning rate to 0.0009685512225000002\r\n",
      "Epoch: 13 cost time: 1.041107177734375\r\n",
      "Epoch: 13, Steps: 34 | Train Loss: 0.2439665 Vali Loss: 0.2753538 Test Loss: 1.9005018\r\n",
      "EarlyStopping counter: 1 out of 100\r\n",
      "Updating learning rate to 0.0008716961002500003\r\n",
      "Epoch: 14 cost time: 1.0134282112121582\r\n",
      "Epoch: 14, Steps: 34 | Train Loss: 0.2305331 Vali Loss: 0.2759079 Test Loss: 1.7368482\r\n",
      "EarlyStopping counter: 2 out of 100\r\n",
      "Updating learning rate to 0.0007845264902250002\r\n",
      "Epoch: 15 cost time: 1.0495085716247559\r\n",
      "Epoch: 15, Steps: 34 | Train Loss: 0.2204971 Vali Loss: 0.2712551 Test Loss: 1.6320410\r\n",
      "EarlyStopping counter: 3 out of 100\r\n",
      "Updating learning rate to 0.0007060738412025003\r\n",
      "Epoch: 16 cost time: 1.0049257278442383\r\n",
      "Epoch: 16, Steps: 34 | Train Loss: 0.2135898 Vali Loss: 0.2638006 Test Loss: 1.6657665\r\n",
      "EarlyStopping counter: 4 out of 100\r\n",
      "Updating learning rate to 0.0006354664570822502\r\n",
      "Epoch: 17 cost time: 1.0464558601379395\r\n",
      "Epoch: 17, Steps: 34 | Train Loss: 0.2058915 Vali Loss: 0.2641149 Test Loss: 1.7048877\r\n",
      "EarlyStopping counter: 5 out of 100\r\n",
      "Updating learning rate to 0.0005719198113740252\r\n",
      "Epoch: 18 cost time: 1.0220279693603516\r\n",
      "Epoch: 18, Steps: 34 | Train Loss: 0.1997685 Vali Loss: 0.2948258 Test Loss: 1.5689481\r\n",
      "EarlyStopping counter: 6 out of 100\r\n",
      "Updating learning rate to 0.0005147278302366227\r\n",
      "Epoch: 19 cost time: 0.9972257614135742\r\n",
      "Epoch: 19, Steps: 34 | Train Loss: 0.1967560 Vali Loss: 0.2772554 Test Loss: 1.6517181\r\n",
      "EarlyStopping counter: 7 out of 100\r\n",
      "Updating learning rate to 0.0004632550472129604\r\n",
      "Epoch: 20 cost time: 1.0055627822875977\r\n",
      "Epoch: 20, Steps: 34 | Train Loss: 0.1988246 Vali Loss: 0.2731912 Test Loss: 1.5797732\r\n",
      "EarlyStopping counter: 8 out of 100\r\n",
      "Updating learning rate to 0.0004169295424916644\r\n",
      "Epoch: 21 cost time: 1.053060531616211\r\n",
      "Epoch: 21, Steps: 34 | Train Loss: 0.1956038 Vali Loss: 0.2832600 Test Loss: 1.6197201\r\n",
      "EarlyStopping counter: 9 out of 100\r\n",
      "Updating learning rate to 0.00037523658824249794\r\n",
      "Epoch: 22 cost time: 0.9884161949157715\r\n",
      "Epoch: 22, Steps: 34 | Train Loss: 0.1908531 Vali Loss: 0.2972187 Test Loss: 1.6176029\r\n",
      "EarlyStopping counter: 10 out of 100\r\n",
      "Updating learning rate to 0.0003377129294182482\r\n",
      "Epoch: 23 cost time: 1.012911081314087\r\n",
      "Epoch: 23, Steps: 34 | Train Loss: 0.1902131 Vali Loss: 0.2799985 Test Loss: 1.6365678\r\n",
      "EarlyStopping counter: 11 out of 100\r\n",
      "Updating learning rate to 0.0003039416364764234\r\n",
      "Epoch: 24 cost time: 1.015146017074585\r\n",
      "Epoch: 24, Steps: 34 | Train Loss: 0.1887966 Vali Loss: 0.2889749 Test Loss: 1.5793657\r\n",
      "EarlyStopping counter: 12 out of 100\r\n",
      "Updating learning rate to 0.0002735474728287811\r\n",
      "Epoch: 25 cost time: 1.0093867778778076\r\n",
      "Epoch: 25, Steps: 34 | Train Loss: 0.1827848 Vali Loss: 0.2633505 Test Loss: 1.6034324\r\n",
      "EarlyStopping counter: 13 out of 100\r\n",
      "Updating learning rate to 0.00024619272554590293\r\n",
      "Epoch: 26 cost time: 0.9913737773895264\r\n",
      "Epoch: 26, Steps: 34 | Train Loss: 0.1833576 Vali Loss: 0.2825638 Test Loss: 1.6061213\r\n",
      "EarlyStopping counter: 14 out of 100\r\n",
      "Updating learning rate to 0.00022157345299131267\r\n",
      "Epoch: 27 cost time: 1.0115489959716797\r\n",
      "Epoch: 27, Steps: 34 | Train Loss: 0.1841244 Vali Loss: 0.2948104 Test Loss: 1.6046340\r\n",
      "EarlyStopping counter: 15 out of 100\r\n",
      "Updating learning rate to 0.0001994161076921814\r\n",
      "Epoch: 28 cost time: 1.0283143520355225\r\n",
      "Epoch: 28, Steps: 34 | Train Loss: 0.1842238 Vali Loss: 0.2711160 Test Loss: 1.6091526\r\n",
      "EarlyStopping counter: 16 out of 100\r\n",
      "Updating learning rate to 0.00017947449692296325\r\n",
      "Epoch: 29 cost time: 0.997258186340332\r\n",
      "Epoch: 29, Steps: 34 | Train Loss: 0.1866895 Vali Loss: 0.2969016 Test Loss: 1.5967208\r\n",
      "EarlyStopping counter: 17 out of 100\r\n",
      "Updating learning rate to 0.00016152704723066693\r\n",
      "Epoch: 30 cost time: 0.9962356090545654\r\n",
      "Epoch: 30, Steps: 34 | Train Loss: 0.1782954 Vali Loss: 0.2942250 Test Loss: 1.6265030\r\n",
      "EarlyStopping counter: 18 out of 100\r\n",
      "Updating learning rate to 0.00014537434250760025\r\n",
      "Epoch: 31 cost time: 0.9875330924987793\r\n",
      "Epoch: 31, Steps: 34 | Train Loss: 0.1796148 Vali Loss: 0.2733635 Test Loss: 1.6060848\r\n",
      "EarlyStopping counter: 19 out of 100\r\n",
      "Updating learning rate to 0.00013083690825684024\r\n",
      "Epoch: 32 cost time: 0.9850480556488037\r\n",
      "Epoch: 32, Steps: 34 | Train Loss: 0.1794722 Vali Loss: 0.2901593 Test Loss: 1.6380322\r\n",
      "EarlyStopping counter: 20 out of 100\r\n",
      "Updating learning rate to 0.00011775321743115622\r\n",
      "Epoch: 33 cost time: 1.0378968715667725\r\n",
      "Epoch: 33, Steps: 34 | Train Loss: 0.1734482 Vali Loss: 0.2924955 Test Loss: 1.6073637\r\n",
      "EarlyStopping counter: 21 out of 100\r\n",
      "Updating learning rate to 0.0001059778956880406\r\n",
      "Epoch: 34 cost time: 1.064967155456543\r\n",
      "Epoch: 34, Steps: 34 | Train Loss: 0.1829345 Vali Loss: 0.2924070 Test Loss: 1.5955026\r\n",
      "EarlyStopping counter: 22 out of 100\r\n",
      "Updating learning rate to 9.538010611923654e-05\r\n",
      "Epoch: 35 cost time: 0.993474006652832\r\n",
      "Epoch: 35, Steps: 34 | Train Loss: 0.1809301 Vali Loss: 0.3015812 Test Loss: 1.5985626\r\n",
      "EarlyStopping counter: 23 out of 100\r\n",
      "Updating learning rate to 8.584209550731288e-05\r\n",
      "Epoch: 36 cost time: 1.043456792831421\r\n",
      "Epoch: 36, Steps: 34 | Train Loss: 0.1822423 Vali Loss: 0.2715369 Test Loss: 1.5992625\r\n",
      "EarlyStopping counter: 24 out of 100\r\n",
      "Updating learning rate to 7.725788595658159e-05\r\n",
      "Epoch: 37 cost time: 1.0092499256134033\r\n",
      "Epoch: 37, Steps: 34 | Train Loss: 0.1775520 Vali Loss: 0.2837483 Test Loss: 1.6102251\r\n",
      "EarlyStopping counter: 25 out of 100\r\n",
      "Updating learning rate to 6.953209736092344e-05\r\n",
      "Epoch: 38 cost time: 1.0186431407928467\r\n",
      "Epoch: 38, Steps: 34 | Train Loss: 0.1765316 Vali Loss: 0.3013092 Test Loss: 1.6173557\r\n",
      "EarlyStopping counter: 26 out of 100\r\n",
      "Updating learning rate to 6.25788876248311e-05\r\n",
      "Epoch: 39 cost time: 0.9885108470916748\r\n",
      "Epoch: 39, Steps: 34 | Train Loss: 0.1766246 Vali Loss: 0.2835023 Test Loss: 1.5947969\r\n",
      "EarlyStopping counter: 27 out of 100\r\n",
      "Updating learning rate to 5.632099886234799e-05\r\n",
      "Epoch: 40 cost time: 1.0258257389068604\r\n",
      "Epoch: 40, Steps: 34 | Train Loss: 0.1788508 Vali Loss: 0.3012384 Test Loss: 1.6178077\r\n",
      "EarlyStopping counter: 28 out of 100\r\n",
      "Updating learning rate to 5.068889897611319e-05\r\n",
      "Epoch: 41 cost time: 1.0050525665283203\r\n",
      "Epoch: 41, Steps: 34 | Train Loss: 0.1753361 Vali Loss: 0.2937643 Test Loss: 1.5894340\r\n",
      "EarlyStopping counter: 29 out of 100\r\n",
      "Updating learning rate to 4.562000907850187e-05\r\n",
      "Epoch: 42 cost time: 0.9904608726501465\r\n",
      "Epoch: 42, Steps: 34 | Train Loss: 0.1717801 Vali Loss: 0.2910452 Test Loss: 1.6116927\r\n",
      "EarlyStopping counter: 30 out of 100\r\n",
      "Updating learning rate to 4.105800817065169e-05\r\n",
      "Epoch: 43 cost time: 0.9855966567993164\r\n",
      "Epoch: 43, Steps: 34 | Train Loss: 0.1724348 Vali Loss: 0.2927972 Test Loss: 1.6081997\r\n",
      "EarlyStopping counter: 31 out of 100\r\n",
      "Updating learning rate to 3.695220735358652e-05\r\n",
      "Epoch: 44 cost time: 0.9960541725158691\r\n",
      "Epoch: 44, Steps: 34 | Train Loss: 0.1759551 Vali Loss: 0.2715009 Test Loss: 1.6060482\r\n",
      "EarlyStopping counter: 32 out of 100\r\n",
      "Updating learning rate to 3.3256986618227866e-05\r\n",
      "Epoch: 45 cost time: 1.0035185813903809\r\n",
      "Epoch: 45, Steps: 34 | Train Loss: 0.1753152 Vali Loss: 0.3058792 Test Loss: 1.5963407\r\n",
      "EarlyStopping counter: 33 out of 100\r\n",
      "Updating learning rate to 2.9931287956405084e-05\r\n",
      "Epoch: 46 cost time: 1.056487798690796\r\n",
      "Epoch: 46, Steps: 34 | Train Loss: 0.1710713 Vali Loss: 0.2975633 Test Loss: 1.5935709\r\n",
      "EarlyStopping counter: 34 out of 100\r\n",
      "Updating learning rate to 2.6938159160764573e-05\r\n",
      "Epoch: 47 cost time: 1.0011110305786133\r\n",
      "Epoch: 47, Steps: 34 | Train Loss: 0.1689016 Vali Loss: 0.2976630 Test Loss: 1.5828419\r\n",
      "EarlyStopping counter: 35 out of 100\r\n",
      "Updating learning rate to 2.424434324468812e-05\r\n",
      "Epoch: 48 cost time: 1.011056661605835\r\n",
      "Epoch: 48, Steps: 34 | Train Loss: 0.1726624 Vali Loss: 0.2894279 Test Loss: 1.6004804\r\n",
      "EarlyStopping counter: 36 out of 100\r\n",
      "Updating learning rate to 2.1819908920219307e-05\r\n",
      "Epoch: 49 cost time: 1.0021226406097412\r\n",
      "Epoch: 49, Steps: 34 | Train Loss: 0.1697160 Vali Loss: 0.2966386 Test Loss: 1.6042929\r\n",
      "EarlyStopping counter: 37 out of 100\r\n",
      "Updating learning rate to 1.9637918028197378e-05\r\n",
      "Epoch: 50 cost time: 1.0079681873321533\r\n",
      "Epoch: 50, Steps: 34 | Train Loss: 0.1769345 Vali Loss: 0.2760420 Test Loss: 1.5985638\r\n",
      "EarlyStopping counter: 38 out of 100\r\n",
      "Updating learning rate to 1.7674126225377637e-05\r\n",
      "Epoch: 51 cost time: 1.0055198669433594\r\n",
      "Epoch: 51, Steps: 34 | Train Loss: 0.1758527 Vali Loss: 0.2881494 Test Loss: 1.6027399\r\n",
      "EarlyStopping counter: 39 out of 100\r\n",
      "Updating learning rate to 1.5906713602839874e-05\r\n",
      "Epoch: 52 cost time: 0.9957559108734131\r\n",
      "Epoch: 52, Steps: 34 | Train Loss: 0.1689275 Vali Loss: 0.3115970 Test Loss: 1.6012388\r\n",
      "EarlyStopping counter: 40 out of 100\r\n",
      "Updating learning rate to 1.4316042242555887e-05\r\n",
      "Epoch: 53 cost time: 1.0130531787872314\r\n",
      "Epoch: 53, Steps: 34 | Train Loss: 0.1725104 Vali Loss: 0.2934642 Test Loss: 1.5715231\r\n",
      "EarlyStopping counter: 41 out of 100\r\n",
      "Updating learning rate to 1.28844380183003e-05\r\n",
      "Epoch: 54 cost time: 1.003580093383789\r\n",
      "Epoch: 54, Steps: 34 | Train Loss: 0.1702216 Vali Loss: 0.2807386 Test Loss: 1.5855246\r\n",
      "EarlyStopping counter: 42 out of 100\r\n",
      "Updating learning rate to 1.1595994216470272e-05\r\n",
      "Epoch: 55 cost time: 1.0506739616394043\r\n",
      "Epoch: 55, Steps: 34 | Train Loss: 0.1727085 Vali Loss: 0.2847733 Test Loss: 1.5979203\r\n",
      "EarlyStopping counter: 43 out of 100\r\n",
      "Updating learning rate to 1.0436394794823243e-05\r\n",
      "Epoch: 56 cost time: 1.0188589096069336\r\n",
      "Epoch: 56, Steps: 34 | Train Loss: 0.1715976 Vali Loss: 0.2927402 Test Loss: 1.5988249\r\n",
      "EarlyStopping counter: 44 out of 100\r\n",
      "Updating learning rate to 9.392755315340918e-06\r\n",
      "Epoch: 57 cost time: 1.0098192691802979\r\n",
      "Epoch: 57, Steps: 34 | Train Loss: 0.1736767 Vali Loss: 0.3007234 Test Loss: 1.5994837\r\n",
      "EarlyStopping counter: 45 out of 100\r\n",
      "Updating learning rate to 8.453479783806827e-06\r\n",
      "Epoch: 58 cost time: 1.0033257007598877\r\n",
      "Epoch: 58, Steps: 34 | Train Loss: 0.1702594 Vali Loss: 0.2820063 Test Loss: 1.6072403\r\n",
      "EarlyStopping counter: 46 out of 100\r\n",
      "Updating learning rate to 7.608131805426145e-06\r\n",
      "Epoch: 59 cost time: 1.0715744495391846\r\n",
      "Epoch: 59, Steps: 34 | Train Loss: 0.1720211 Vali Loss: 0.2960211 Test Loss: 1.5899270\r\n",
      "EarlyStopping counter: 47 out of 100\r\n",
      "Updating learning rate to 6.84731862488353e-06\r\n",
      "Epoch: 60 cost time: 0.9801862239837646\r\n",
      "Epoch: 60, Steps: 34 | Train Loss: 0.1687817 Vali Loss: 0.3018619 Test Loss: 1.5960610\r\n",
      "EarlyStopping counter: 48 out of 100\r\n",
      "Updating learning rate to 6.162586762395178e-06\r\n",
      "Epoch: 61 cost time: 1.0095453262329102\r\n",
      "Epoch: 61, Steps: 34 | Train Loss: 0.1753673 Vali Loss: 0.3039501 Test Loss: 1.5876105\r\n",
      "EarlyStopping counter: 49 out of 100\r\n",
      "Updating learning rate to 5.54632808615566e-06\r\n",
      "Epoch: 62 cost time: 1.0074429512023926\r\n",
      "Epoch: 62, Steps: 34 | Train Loss: 0.1680271 Vali Loss: 0.3006974 Test Loss: 1.6023092\r\n",
      "EarlyStopping counter: 50 out of 100\r\n",
      "Updating learning rate to 4.991695277540094e-06\r\n",
      "Epoch: 63 cost time: 0.9914267063140869\r\n",
      "Epoch: 63, Steps: 34 | Train Loss: 0.1706692 Vali Loss: 0.2744855 Test Loss: 1.5951895\r\n",
      "EarlyStopping counter: 51 out of 100\r\n",
      "Updating learning rate to 4.492525749786085e-06\r\n",
      "Epoch: 64 cost time: 1.002143144607544\r\n",
      "Epoch: 64, Steps: 34 | Train Loss: 0.1721803 Vali Loss: 0.2814811 Test Loss: 1.6037819\r\n",
      "EarlyStopping counter: 52 out of 100\r\n",
      "Updating learning rate to 4.043273174807477e-06\r\n",
      "Epoch: 65 cost time: 1.0169317722320557\r\n",
      "Epoch: 65, Steps: 34 | Train Loss: 0.1735669 Vali Loss: 0.2948297 Test Loss: 1.5956033\r\n",
      "EarlyStopping counter: 53 out of 100\r\n",
      "Updating learning rate to 3.6389458573267288e-06\r\n",
      "Epoch: 66 cost time: 0.9926464557647705\r\n",
      "Epoch: 66, Steps: 34 | Train Loss: 0.1754859 Vali Loss: 0.2703765 Test Loss: 1.6055340\r\n",
      "EarlyStopping counter: 54 out of 100\r\n",
      "Updating learning rate to 3.2750512715940557e-06\r\n",
      "Epoch: 67 cost time: 1.0480303764343262\r\n",
      "Epoch: 67, Steps: 34 | Train Loss: 0.1738846 Vali Loss: 0.2987824 Test Loss: 1.5834763\r\n",
      "EarlyStopping counter: 55 out of 100\r\n",
      "Updating learning rate to 2.9475461444346506e-06\r\n",
      "Epoch: 68 cost time: 0.9908504486083984\r\n",
      "Epoch: 68, Steps: 34 | Train Loss: 0.1722947 Vali Loss: 0.2892609 Test Loss: 1.5875248\r\n",
      "EarlyStopping counter: 56 out of 100\r\n",
      "Updating learning rate to 2.652791529991185e-06\r\n",
      "Epoch: 69 cost time: 0.9922611713409424\r\n",
      "Epoch: 69, Steps: 34 | Train Loss: 0.1728061 Vali Loss: 0.2797222 Test Loss: 1.5864416\r\n",
      "EarlyStopping counter: 57 out of 100\r\n",
      "Updating learning rate to 2.387512376992067e-06\r\n",
      "Epoch: 70 cost time: 1.0103378295898438\r\n",
      "Epoch: 70, Steps: 34 | Train Loss: 0.1772674 Vali Loss: 0.2848727 Test Loss: 1.5931348\r\n",
      "EarlyStopping counter: 58 out of 100\r\n",
      "Updating learning rate to 2.14876113929286e-06\r\n",
      "Epoch: 71 cost time: 1.0390231609344482\r\n",
      "Epoch: 71, Steps: 34 | Train Loss: 0.1746840 Vali Loss: 0.2947063 Test Loss: 1.6023022\r\n",
      "EarlyStopping counter: 59 out of 100\r\n",
      "Updating learning rate to 1.9338850253635745e-06\r\n",
      "Epoch: 72 cost time: 1.004213571548462\r\n",
      "Epoch: 72, Steps: 34 | Train Loss: 0.1720324 Vali Loss: 0.2861449 Test Loss: 1.5959473\r\n",
      "EarlyStopping counter: 60 out of 100\r\n",
      "Updating learning rate to 1.740496522827217e-06\r\n",
      "Epoch: 73 cost time: 1.0221388339996338\r\n",
      "Epoch: 73, Steps: 34 | Train Loss: 0.1744066 Vali Loss: 0.2936455 Test Loss: 1.6011374\r\n",
      "EarlyStopping counter: 61 out of 100\r\n",
      "Updating learning rate to 1.5664468705444951e-06\r\n",
      "Epoch: 74 cost time: 1.0256133079528809\r\n",
      "Epoch: 74, Steps: 34 | Train Loss: 0.1743893 Vali Loss: 0.3095227 Test Loss: 1.5934694\r\n",
      "EarlyStopping counter: 62 out of 100\r\n",
      "Updating learning rate to 1.4098021834900458e-06\r\n",
      "Epoch: 75 cost time: 1.0085480213165283\r\n",
      "Epoch: 75, Steps: 34 | Train Loss: 0.1728423 Vali Loss: 0.2840612 Test Loss: 1.5916319\r\n",
      "EarlyStopping counter: 63 out of 100\r\n",
      "Updating learning rate to 1.2688219651410413e-06\r\n",
      "Epoch: 76 cost time: 1.0074946880340576\r\n",
      "Epoch: 76, Steps: 34 | Train Loss: 0.1785678 Vali Loss: 0.2896562 Test Loss: 1.6024259\r\n",
      "EarlyStopping counter: 64 out of 100\r\n",
      "Updating learning rate to 1.1419397686269372e-06\r\n",
      "Epoch: 77 cost time: 0.990328311920166\r\n",
      "Epoch: 77, Steps: 34 | Train Loss: 0.1722635 Vali Loss: 0.3002866 Test Loss: 1.5964476\r\n",
      "EarlyStopping counter: 65 out of 100\r\n",
      "Updating learning rate to 1.0277457917642436e-06\r\n",
      "Epoch: 78 cost time: 0.9950096607208252\r\n",
      "Epoch: 78, Steps: 34 | Train Loss: 0.1722586 Vali Loss: 0.2633269 Test Loss: 1.5995770\r\n",
      "EarlyStopping counter: 66 out of 100\r\n",
      "Updating learning rate to 9.249712125878192e-07\r\n",
      "Epoch: 79 cost time: 1.0378198623657227\r\n",
      "Epoch: 79, Steps: 34 | Train Loss: 0.1678379 Vali Loss: 0.2849341 Test Loss: 1.5883191\r\n",
      "EarlyStopping counter: 67 out of 100\r\n",
      "Updating learning rate to 8.324740913290373e-07\r\n",
      "Epoch: 80 cost time: 1.0232295989990234\r\n",
      "Epoch: 80, Steps: 34 | Train Loss: 0.1738829 Vali Loss: 0.2944975 Test Loss: 1.6090975\r\n",
      "EarlyStopping counter: 68 out of 100\r\n",
      "Updating learning rate to 7.492266821961335e-07\r\n",
      "Epoch: 81 cost time: 1.003145694732666\r\n",
      "Epoch: 81, Steps: 34 | Train Loss: 0.1691656 Vali Loss: 0.2732981 Test Loss: 1.5880358\r\n",
      "EarlyStopping counter: 69 out of 100\r\n",
      "Updating learning rate to 6.743040139765202e-07\r\n",
      "Epoch: 82 cost time: 1.0197279453277588\r\n",
      "Epoch: 82, Steps: 34 | Train Loss: 0.1688742 Vali Loss: 0.2990830 Test Loss: 1.5917188\r\n",
      "EarlyStopping counter: 70 out of 100\r\n",
      "Updating learning rate to 6.068736125788682e-07\r\n",
      "Epoch: 83 cost time: 1.0022358894348145\r\n",
      "Epoch: 83, Steps: 34 | Train Loss: 0.1722330 Vali Loss: 0.2758120 Test Loss: 1.5941197\r\n",
      "EarlyStopping counter: 71 out of 100\r\n",
      "Updating learning rate to 5.461862513209814e-07\r\n",
      "Epoch: 84 cost time: 1.0218241214752197\r\n",
      "Epoch: 84, Steps: 34 | Train Loss: 0.1737094 Vali Loss: 0.2926444 Test Loss: 1.5924184\r\n",
      "EarlyStopping counter: 72 out of 100\r\n",
      "Updating learning rate to 4.915676261888833e-07\r\n",
      "Epoch: 85 cost time: 1.007399320602417\r\n",
      "Epoch: 85, Steps: 34 | Train Loss: 0.1723002 Vali Loss: 0.2904211 Test Loss: 1.5971143\r\n",
      "EarlyStopping counter: 73 out of 100\r\n",
      "Updating learning rate to 4.4241086356999495e-07\r\n",
      "Epoch: 86 cost time: 0.9997897148132324\r\n",
      "Epoch: 86, Steps: 34 | Train Loss: 0.1753533 Vali Loss: 0.2974603 Test Loss: 1.6011063\r\n",
      "EarlyStopping counter: 74 out of 100\r\n",
      "Updating learning rate to 3.9816977721299544e-07\r\n",
      "Epoch: 87 cost time: 0.9960241317749023\r\n",
      "Epoch: 87, Steps: 34 | Train Loss: 0.1749218 Vali Loss: 0.2867559 Test Loss: 1.6023060\r\n",
      "EarlyStopping counter: 75 out of 100\r\n",
      "Updating learning rate to 3.583527994916959e-07\r\n",
      "Epoch: 88 cost time: 0.9777927398681641\r\n",
      "Epoch: 88, Steps: 34 | Train Loss: 0.1750997 Vali Loss: 0.2993854 Test Loss: 1.5903759\r\n",
      "EarlyStopping counter: 76 out of 100\r\n",
      "Updating learning rate to 3.2251751954252636e-07\r\n",
      "Epoch: 89 cost time: 1.0137584209442139\r\n",
      "Epoch: 89, Steps: 34 | Train Loss: 0.1701641 Vali Loss: 0.2940309 Test Loss: 1.5881743\r\n",
      "EarlyStopping counter: 77 out of 100\r\n",
      "Updating learning rate to 2.902657675882737e-07\r\n",
      "Epoch: 90 cost time: 1.0602004528045654\r\n",
      "Epoch: 90, Steps: 34 | Train Loss: 0.1732605 Vali Loss: 0.2897059 Test Loss: 1.5863304\r\n",
      "EarlyStopping counter: 78 out of 100\r\n",
      "Updating learning rate to 2.612391908294464e-07\r\n",
      "Epoch: 91 cost time: 1.0122661590576172\r\n",
      "Epoch: 91, Steps: 34 | Train Loss: 0.1741142 Vali Loss: 0.2919148 Test Loss: 1.6141722\r\n",
      "EarlyStopping counter: 79 out of 100\r\n",
      "Updating learning rate to 2.351152717465017e-07\r\n",
      "Epoch: 92 cost time: 0.993741512298584\r\n",
      "Epoch: 92, Steps: 34 | Train Loss: 0.1763273 Vali Loss: 0.2885400 Test Loss: 1.5876509\r\n",
      "EarlyStopping counter: 80 out of 100\r\n",
      "Updating learning rate to 2.1160374457185157e-07\r\n",
      "Epoch: 93 cost time: 1.0310134887695312\r\n",
      "Epoch: 93, Steps: 34 | Train Loss: 0.1780534 Vali Loss: 0.2895791 Test Loss: 1.6058162\r\n",
      "EarlyStopping counter: 81 out of 100\r\n",
      "Updating learning rate to 1.9044337011466642e-07\r\n",
      "Epoch: 94 cost time: 1.013272762298584\r\n",
      "Epoch: 94, Steps: 34 | Train Loss: 0.1672959 Vali Loss: 0.2896079 Test Loss: 1.5782989\r\n",
      "EarlyStopping counter: 82 out of 100\r\n",
      "Updating learning rate to 1.7139903310319977e-07\r\n",
      "Epoch: 95 cost time: 1.0125701427459717\r\n",
      "Epoch: 95, Steps: 34 | Train Loss: 0.1712926 Vali Loss: 0.2995508 Test Loss: 1.6000077\r\n",
      "EarlyStopping counter: 83 out of 100\r\n",
      "Updating learning rate to 1.542591297928798e-07\r\n",
      "Epoch: 96 cost time: 1.0326507091522217\r\n",
      "Epoch: 96, Steps: 34 | Train Loss: 0.1721495 Vali Loss: 0.2993699 Test Loss: 1.5906481\r\n",
      "EarlyStopping counter: 84 out of 100\r\n",
      "Updating learning rate to 1.3883321681359182e-07\r\n",
      "Epoch: 97 cost time: 0.9963161945343018\r\n",
      "Epoch: 97, Steps: 34 | Train Loss: 0.1679856 Vali Loss: 0.2868298 Test Loss: 1.6033684\r\n",
      "EarlyStopping counter: 85 out of 100\r\n",
      "Updating learning rate to 1.2494989513223265e-07\r\n",
      "Epoch: 98 cost time: 1.0077641010284424\r\n",
      "Epoch: 98, Steps: 34 | Train Loss: 0.1729750 Vali Loss: 0.2805603 Test Loss: 1.5852534\r\n",
      "EarlyStopping counter: 86 out of 100\r\n",
      "Updating learning rate to 1.1245490561900937e-07\r\n",
      "Epoch: 99 cost time: 1.0058329105377197\r\n",
      "Epoch: 99, Steps: 34 | Train Loss: 0.1801146 Vali Loss: 0.2891579 Test Loss: 1.5928266\r\n",
      "EarlyStopping counter: 87 out of 100\r\n",
      "Updating learning rate to 1.0120941505710844e-07\r\n",
      "Epoch: 100 cost time: 0.9916810989379883\r\n",
      "Epoch: 100, Steps: 34 | Train Loss: 0.1738597 Vali Loss: 0.3049771 Test Loss: 1.5956957\r\n",
      "EarlyStopping counter: 88 out of 100\r\n",
      "Updating learning rate to 9.10884735513976e-08\r\n",
      ">>>>>>>testing : national_illness_104_24_multi_model_PatchTST_custom_ftM_sl104_ll48_pl24_dm16_nh4_el3_dl1_df128_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\r\n",
      "test 170\r\n",
      "mse:1.7391177415847778, mae:0.9023001790046692, rse:0.6364091634750366\r\n"
     ]
    }
   ],
   "source": [
    "!python run_longExp.py \\\n",
    "    --random_seed 2021 \\\n",
    "    --is_training 1 \\\n",
    "    --root_path ./dataset/ \\\n",
    "    --data_path national_illness.csv \\\n",
    "    --model_id national_illness_104_24_multi_model \\\n",
    "    --model PatchTST \\\n",
    "    --data custom \\\n",
    "    --features M \\\n",
    "    --seq_len 104 \\\n",
    "    --pred_len 24 \\\n",
    "    --enc_in 7 \\\n",
    "    --e_layers 3 \\\n",
    "    --n_heads 4 \\\n",
    "    --d_model 16 \\\n",
    "    --d_ff 128 \\\n",
    "    --dropout 0.3 \\\n",
    "    --fc_dropout 0.3 \\\n",
    "    --head_dropout 0 \\\n",
    "    --patch_len 16 \\\n",
    "    --stride 2 \\\n",
    "    --des 'Exp' \\\n",
    "    --train_epochs 100 \\\n",
    "    --itr 1 \\\n",
    "    --batch_size 16 \\\n",
    "    --learning_rate 0.0025 \\\n",
    "    --multi_scale small large"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8202323,
     "sourceId": 12960319,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8486992,
     "sourceId": 13377055,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8487934,
     "sourceId": 13378343,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8488128,
     "sourceId": 13378583,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8488321,
     "sourceId": 13378855,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 28284.385084,
   "end_time": "2025-10-14T22:27:31.151854",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-14T14:36:06.766770",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
