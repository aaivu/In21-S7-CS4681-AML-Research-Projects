# AI Evaluation:Task Completion - 210706H

## Student Information

- **Index Number:** 210706H
- **Research Area:** AI Evaluation:Task Completion
- **GitHub Username:** @JaneeshaJ2001
- **Email:** janeesha.21@cse.mrt.ac.lk

# CAFE: Context-Aware and Fairness-Weighted Framework for Toxicity Evaluation

A comprehensive framework for evaluating toxicity in language models that addresses context-insensitivity and fairness issues in existing approaches like the Perspective API.

## Overview

CAFE (Context-Aware Fairness-Weighted Toxicity Evaluator) is a novel framework that integrates:

- **Contextual Embeddings**: RoBERTa-based embeddings to capture nuanced linguistic context
- **Fairness-Aware Loss**: Multi-objective optimization that reduces bias against marginalized groups
- **Data Augmentation**: Paraphrasing and adversarial crafting to improve robustness

## Key Features

- ğŸ¯ **Context-Aware**: Distinguishes between literal and non-literal expressions (sarcasm, slang)
- âš–ï¸ **Fairness-Weighted**: Reduces systematic bias against minority dialects
- ğŸš€ **Robust Evaluation**: Extended dataset with adversarial and paraphrased prompts
- ğŸ“Š **Comprehensive Metrics**: F1 score, fairness gap, Expected Maximum Toxicity, and more

## Installation

1. Clone the repository:
```bash
git clone https://github.com/aaivu/In21-S7-CS4681-AML-Research-Projects.git
cd 210706H-AI-Evaluation_Task-Completion
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Install the package:
```bash
pip install -e .
```

## Quick Start

### 1. Run Full Experiment Pipeline

```bash
python experiments/run_experiments.py
```

### 2. Run Quick Experiment (for testing)

```bash
python experiments/run_experiments.py --quick
```

### 3. Train Individual Components

```bash
# Train CAFE model
python src/train.py

# Evaluate models
python src/evaluate.py

# Augment data
python src/data_augmentation.py
```

## Project Structure
```
210706H-AI-Evaluation:Task-Completion/
â”œâ”€â”€ data/                     # Datasets
â”‚   â”œâ”€â”€ raw/                  # Original datasets
â”‚   â”œâ”€â”€ processed/            # Processed datasets
â”‚   â””â”€â”€ augmented/            # Augmented datasets
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ research_proposal.md  # Your research proposal (Required)
â”‚   â”œâ”€â”€ literature_review.md  # Literature review and references
â”‚   â”œâ”€â”€ methodology.md        # Detailed methodology
â”‚   â””â”€â”€ progress_reports/     # Weekly progress reports
â”œâ”€â”€ experiments/              # Experiment configurations
â”‚   â”œâ”€â”€ configs/              # YAML configuration files
â”‚   â””â”€â”€ run_experiments.py    # Main experiment runner
â”œâ”€â”€ results/                  # Experimental results
â”‚   â”œâ”€â”€ models/               # Trained model checkpoints
â”‚   â”œâ”€â”€ metrics/              # Evaluation metrics
â”‚   â””â”€â”€ plots/                # Visualization plots
â”œâ”€â”€ src/                      # Source code
â”‚   â”œâ”€â”€ data_augmentation.py  # Data augmentation utilities
â”‚   â”œâ”€â”€ model.py              # CAFE and baseline models
â”‚   â”œâ”€â”€ loss_functions.py     # Multi-objective loss functions
â”‚   â”œâ”€â”€ train.py              # Training pipeline
â”‚   â”œâ”€â”€ evaluate.py           # Evaluation pipeline
â”‚   â””â”€â”€ utils.py              # Utility functions
â”œâ”€â”€ README.md                 # This file
â””â”€â”€ requirements.txt          # Project dependencies
```
## Configuration

Modify `experiments/configs/base_config.yaml` to customize:

- Model parameters (RoBERTa variant, dimensions)
- Training settings (batch size, learning rate, epochs)
- Loss function weights (Î±, Î², Î³)
- Data augmentation parameters

## Research Paper

This implementation accompanies the research paper:

*"CAFE: A Context-Aware and Fairness-Weighted Framework for Toxicity Evaluation in Language Models"*

By Wickramasinghe J.J., Department of Computer Science and Engineering, University of Moratuwa

## Citation

```bibtex
@article{wickramasinghe2025cafe,
  title={CAFE: A Context-Aware and Fairness-Weighted Framework for Toxicity Evaluation in Language Models},
  author={Wickramasinghe, J.J.},
  journal={arXiv preprint},
  year={2025}
}
```

## Acknowledgments

- RealToxicityPrompts dataset by Gehman et al.
- Perspective API by Google Jigsaw
- Transformers library by Hugging Face
- University of Moratuwa, Department of Computer Science and Engineering